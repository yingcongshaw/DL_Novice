{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程，尝试不同的正则化系数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from criterion import EuclideanLossLayer,SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "exec_result = pd.DataFrame(columns=['mode','batch_size','learning_rate_SGD', 'momentum','weight_decay','time','loss_validate','acc_validate'])\n",
    "\n",
    "max_epoch = 20\n",
    "disp_freq = 50\n",
    "init_std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shaw/work/DL/hw2/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][5500]\t Training Loss 6.6369\t Accuracy 0.0000\n",
      "Epoch [0][20]\t Batch [50][5500]\t Training Loss 1.4241\t Accuracy 0.0882\n",
      "Epoch [0][20]\t Batch [100][5500]\t Training Loss 1.0243\t Accuracy 0.1010\n",
      "Epoch [0][20]\t Batch [150][5500]\t Training Loss 0.8809\t Accuracy 0.1146\n",
      "Epoch [0][20]\t Batch [200][5500]\t Training Loss 0.8026\t Accuracy 0.1274\n",
      "Epoch [0][20]\t Batch [250][5500]\t Training Loss 0.7516\t Accuracy 0.1426\n",
      "Epoch [0][20]\t Batch [300][5500]\t Training Loss 0.7136\t Accuracy 0.1664\n",
      "Epoch [0][20]\t Batch [350][5500]\t Training Loss 0.6821\t Accuracy 0.1818\n",
      "Epoch [0][20]\t Batch [400][5500]\t Training Loss 0.6578\t Accuracy 0.1995\n",
      "Epoch [0][20]\t Batch [450][5500]\t Training Loss 0.6362\t Accuracy 0.2164\n",
      "Epoch [0][20]\t Batch [500][5500]\t Training Loss 0.6177\t Accuracy 0.2345\n",
      "Epoch [0][20]\t Batch [550][5500]\t Training Loss 0.6007\t Accuracy 0.2543\n",
      "Epoch [0][20]\t Batch [600][5500]\t Training Loss 0.5853\t Accuracy 0.2729\n",
      "Epoch [0][20]\t Batch [650][5500]\t Training Loss 0.5716\t Accuracy 0.2903\n",
      "Epoch [0][20]\t Batch [700][5500]\t Training Loss 0.5589\t Accuracy 0.3064\n",
      "Epoch [0][20]\t Batch [750][5500]\t Training Loss 0.5497\t Accuracy 0.3141\n",
      "Epoch [0][20]\t Batch [800][5500]\t Training Loss 0.5404\t Accuracy 0.3266\n",
      "Epoch [0][20]\t Batch [850][5500]\t Training Loss 0.5319\t Accuracy 0.3377\n",
      "Epoch [0][20]\t Batch [900][5500]\t Training Loss 0.5234\t Accuracy 0.3512\n",
      "Epoch [0][20]\t Batch [950][5500]\t Training Loss 0.5149\t Accuracy 0.3626\n",
      "Epoch [0][20]\t Batch [1000][5500]\t Training Loss 0.5073\t Accuracy 0.3736\n",
      "Epoch [0][20]\t Batch [1050][5500]\t Training Loss 0.4998\t Accuracy 0.3846\n",
      "Epoch [0][20]\t Batch [1100][5500]\t Training Loss 0.4928\t Accuracy 0.3952\n",
      "Epoch [0][20]\t Batch [1150][5500]\t Training Loss 0.4858\t Accuracy 0.4057\n",
      "Epoch [0][20]\t Batch [1200][5500]\t Training Loss 0.4807\t Accuracy 0.4130\n",
      "Epoch [0][20]\t Batch [1250][5500]\t Training Loss 0.4754\t Accuracy 0.4213\n",
      "Epoch [0][20]\t Batch [1300][5500]\t Training Loss 0.4710\t Accuracy 0.4272\n",
      "Epoch [0][20]\t Batch [1350][5500]\t Training Loss 0.4664\t Accuracy 0.4346\n",
      "Epoch [0][20]\t Batch [1400][5500]\t Training Loss 0.4625\t Accuracy 0.4394\n",
      "Epoch [0][20]\t Batch [1450][5500]\t Training Loss 0.4589\t Accuracy 0.4452\n",
      "Epoch [0][20]\t Batch [1500][5500]\t Training Loss 0.4554\t Accuracy 0.4498\n",
      "Epoch [0][20]\t Batch [1550][5500]\t Training Loss 0.4511\t Accuracy 0.4569\n",
      "Epoch [0][20]\t Batch [1600][5500]\t Training Loss 0.4477\t Accuracy 0.4617\n",
      "Epoch [0][20]\t Batch [1650][5500]\t Training Loss 0.4438\t Accuracy 0.4672\n",
      "Epoch [0][20]\t Batch [1700][5500]\t Training Loss 0.4404\t Accuracy 0.4715\n",
      "Epoch [0][20]\t Batch [1750][5500]\t Training Loss 0.4374\t Accuracy 0.4761\n",
      "Epoch [0][20]\t Batch [1800][5500]\t Training Loss 0.4348\t Accuracy 0.4795\n",
      "Epoch [0][20]\t Batch [1850][5500]\t Training Loss 0.4314\t Accuracy 0.4854\n",
      "Epoch [0][20]\t Batch [1900][5500]\t Training Loss 0.4280\t Accuracy 0.4909\n",
      "Epoch [0][20]\t Batch [1950][5500]\t Training Loss 0.4252\t Accuracy 0.4954\n",
      "Epoch [0][20]\t Batch [2000][5500]\t Training Loss 0.4223\t Accuracy 0.5006\n",
      "Epoch [0][20]\t Batch [2050][5500]\t Training Loss 0.4196\t Accuracy 0.5058\n",
      "Epoch [0][20]\t Batch [2100][5500]\t Training Loss 0.4170\t Accuracy 0.5095\n",
      "Epoch [0][20]\t Batch [2150][5500]\t Training Loss 0.4143\t Accuracy 0.5140\n",
      "Epoch [0][20]\t Batch [2200][5500]\t Training Loss 0.4114\t Accuracy 0.5189\n",
      "Epoch [0][20]\t Batch [2250][5500]\t Training Loss 0.4093\t Accuracy 0.5225\n",
      "Epoch [0][20]\t Batch [2300][5500]\t Training Loss 0.4071\t Accuracy 0.5268\n",
      "Epoch [0][20]\t Batch [2350][5500]\t Training Loss 0.4047\t Accuracy 0.5308\n",
      "Epoch [0][20]\t Batch [2400][5500]\t Training Loss 0.4029\t Accuracy 0.5340\n",
      "Epoch [0][20]\t Batch [2450][5500]\t Training Loss 0.4008\t Accuracy 0.5375\n",
      "Epoch [0][20]\t Batch [2500][5500]\t Training Loss 0.3991\t Accuracy 0.5399\n",
      "Epoch [0][20]\t Batch [2550][5500]\t Training Loss 0.3968\t Accuracy 0.5437\n",
      "Epoch [0][20]\t Batch [2600][5500]\t Training Loss 0.3948\t Accuracy 0.5472\n",
      "Epoch [0][20]\t Batch [2650][5500]\t Training Loss 0.3928\t Accuracy 0.5507\n",
      "Epoch [0][20]\t Batch [2700][5500]\t Training Loss 0.3912\t Accuracy 0.5537\n",
      "Epoch [0][20]\t Batch [2750][5500]\t Training Loss 0.3896\t Accuracy 0.5569\n",
      "Epoch [0][20]\t Batch [2800][5500]\t Training Loss 0.3876\t Accuracy 0.5603\n",
      "Epoch [0][20]\t Batch [2850][5500]\t Training Loss 0.3855\t Accuracy 0.5637\n",
      "Epoch [0][20]\t Batch [2900][5500]\t Training Loss 0.3838\t Accuracy 0.5664\n",
      "Epoch [0][20]\t Batch [2950][5500]\t Training Loss 0.3821\t Accuracy 0.5691\n",
      "Epoch [0][20]\t Batch [3000][5500]\t Training Loss 0.3807\t Accuracy 0.5716\n",
      "Epoch [0][20]\t Batch [3050][5500]\t Training Loss 0.3794\t Accuracy 0.5737\n",
      "Epoch [0][20]\t Batch [3100][5500]\t Training Loss 0.3781\t Accuracy 0.5760\n",
      "Epoch [0][20]\t Batch [3150][5500]\t Training Loss 0.3768\t Accuracy 0.5780\n",
      "Epoch [0][20]\t Batch [3200][5500]\t Training Loss 0.3754\t Accuracy 0.5802\n",
      "Epoch [0][20]\t Batch [3250][5500]\t Training Loss 0.3745\t Accuracy 0.5818\n",
      "Epoch [0][20]\t Batch [3300][5500]\t Training Loss 0.3731\t Accuracy 0.5847\n",
      "Epoch [0][20]\t Batch [3350][5500]\t Training Loss 0.3719\t Accuracy 0.5866\n",
      "Epoch [0][20]\t Batch [3400][5500]\t Training Loss 0.3700\t Accuracy 0.5899\n",
      "Epoch [0][20]\t Batch [3450][5500]\t Training Loss 0.3686\t Accuracy 0.5926\n",
      "Epoch [0][20]\t Batch [3500][5500]\t Training Loss 0.3675\t Accuracy 0.5944\n",
      "Epoch [0][20]\t Batch [3550][5500]\t Training Loss 0.3662\t Accuracy 0.5968\n",
      "Epoch [0][20]\t Batch [3600][5500]\t Training Loss 0.3650\t Accuracy 0.5986\n",
      "Epoch [0][20]\t Batch [3650][5500]\t Training Loss 0.3639\t Accuracy 0.6007\n",
      "Epoch [0][20]\t Batch [3700][5500]\t Training Loss 0.3624\t Accuracy 0.6037\n",
      "Epoch [0][20]\t Batch [3750][5500]\t Training Loss 0.3615\t Accuracy 0.6053\n",
      "Epoch [0][20]\t Batch [3800][5500]\t Training Loss 0.3604\t Accuracy 0.6072\n",
      "Epoch [0][20]\t Batch [3850][5500]\t Training Loss 0.3592\t Accuracy 0.6093\n",
      "Epoch [0][20]\t Batch [3900][5500]\t Training Loss 0.3579\t Accuracy 0.6115\n",
      "Epoch [0][20]\t Batch [3950][5500]\t Training Loss 0.3566\t Accuracy 0.6134\n",
      "Epoch [0][20]\t Batch [4000][5500]\t Training Loss 0.3558\t Accuracy 0.6145\n",
      "Epoch [0][20]\t Batch [4050][5500]\t Training Loss 0.3547\t Accuracy 0.6166\n",
      "Epoch [0][20]\t Batch [4100][5500]\t Training Loss 0.3535\t Accuracy 0.6185\n",
      "Epoch [0][20]\t Batch [4150][5500]\t Training Loss 0.3527\t Accuracy 0.6197\n",
      "Epoch [0][20]\t Batch [4200][5500]\t Training Loss 0.3517\t Accuracy 0.6216\n",
      "Epoch [0][20]\t Batch [4250][5500]\t Training Loss 0.3510\t Accuracy 0.6228\n",
      "Epoch [0][20]\t Batch [4300][5500]\t Training Loss 0.3500\t Accuracy 0.6245\n",
      "Epoch [0][20]\t Batch [4350][5500]\t Training Loss 0.3488\t Accuracy 0.6265\n",
      "Epoch [0][20]\t Batch [4400][5500]\t Training Loss 0.3479\t Accuracy 0.6282\n",
      "Epoch [0][20]\t Batch [4450][5500]\t Training Loss 0.3472\t Accuracy 0.6295\n",
      "Epoch [0][20]\t Batch [4500][5500]\t Training Loss 0.3461\t Accuracy 0.6315\n",
      "Epoch [0][20]\t Batch [4550][5500]\t Training Loss 0.3453\t Accuracy 0.6330\n",
      "Epoch [0][20]\t Batch [4600][5500]\t Training Loss 0.3445\t Accuracy 0.6344\n",
      "Epoch [0][20]\t Batch [4650][5500]\t Training Loss 0.3438\t Accuracy 0.6356\n",
      "Epoch [0][20]\t Batch [4700][5500]\t Training Loss 0.3427\t Accuracy 0.6375\n",
      "Epoch [0][20]\t Batch [4750][5500]\t Training Loss 0.3420\t Accuracy 0.6387\n",
      "Epoch [0][20]\t Batch [4800][5500]\t Training Loss 0.3411\t Accuracy 0.6405\n",
      "Epoch [0][20]\t Batch [4850][5500]\t Training Loss 0.3400\t Accuracy 0.6425\n",
      "Epoch [0][20]\t Batch [4900][5500]\t Training Loss 0.3392\t Accuracy 0.6438\n",
      "Epoch [0][20]\t Batch [4950][5500]\t Training Loss 0.3385\t Accuracy 0.6449\n",
      "Epoch [0][20]\t Batch [5000][5500]\t Training Loss 0.3381\t Accuracy 0.6458\n",
      "Epoch [0][20]\t Batch [5050][5500]\t Training Loss 0.3373\t Accuracy 0.6470\n",
      "Epoch [0][20]\t Batch [5100][5500]\t Training Loss 0.3365\t Accuracy 0.6485\n",
      "Epoch [0][20]\t Batch [5150][5500]\t Training Loss 0.3356\t Accuracy 0.6501\n",
      "Epoch [0][20]\t Batch [5200][5500]\t Training Loss 0.3347\t Accuracy 0.6520\n",
      "Epoch [0][20]\t Batch [5250][5500]\t Training Loss 0.3340\t Accuracy 0.6531\n",
      "Epoch [0][20]\t Batch [5300][5500]\t Training Loss 0.3333\t Accuracy 0.6542\n",
      "Epoch [0][20]\t Batch [5350][5500]\t Training Loss 0.3324\t Accuracy 0.6557\n",
      "Epoch [0][20]\t Batch [5400][5500]\t Training Loss 0.3317\t Accuracy 0.6570\n",
      "Epoch [0][20]\t Batch [5450][5500]\t Training Loss 0.3309\t Accuracy 0.6582\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3303\t Average training accuracy 0.6595\n",
      "Epoch [0]\t Average validation loss 0.2370\t Average validation accuracy 0.8358\n",
      "\n",
      "Epoch [1][20]\t Batch [0][5500]\t Training Loss 0.2018\t Accuracy 1.0000\n",
      "Epoch [1][20]\t Batch [50][5500]\t Training Loss 0.2364\t Accuracy 0.8275\n",
      "Epoch [1][20]\t Batch [100][5500]\t Training Loss 0.2512\t Accuracy 0.7871\n",
      "Epoch [1][20]\t Batch [150][5500]\t Training Loss 0.2566\t Accuracy 0.7775\n",
      "Epoch [1][20]\t Batch [200][5500]\t Training Loss 0.2523\t Accuracy 0.7856\n",
      "Epoch [1][20]\t Batch [250][5500]\t Training Loss 0.2484\t Accuracy 0.7916\n",
      "Epoch [1][20]\t Batch [300][5500]\t Training Loss 0.2467\t Accuracy 0.7970\n",
      "Epoch [1][20]\t Batch [350][5500]\t Training Loss 0.2465\t Accuracy 0.8020\n",
      "Epoch [1][20]\t Batch [400][5500]\t Training Loss 0.2474\t Accuracy 0.8005\n",
      "Epoch [1][20]\t Batch [450][5500]\t Training Loss 0.2475\t Accuracy 0.8018\n",
      "Epoch [1][20]\t Batch [500][5500]\t Training Loss 0.2471\t Accuracy 0.8032\n",
      "Epoch [1][20]\t Batch [550][5500]\t Training Loss 0.2461\t Accuracy 0.8040\n",
      "Epoch [1][20]\t Batch [600][5500]\t Training Loss 0.2453\t Accuracy 0.8062\n",
      "Epoch [1][20]\t Batch [650][5500]\t Training Loss 0.2442\t Accuracy 0.8095\n",
      "Epoch [1][20]\t Batch [700][5500]\t Training Loss 0.2438\t Accuracy 0.8103\n",
      "Epoch [1][20]\t Batch [750][5500]\t Training Loss 0.2451\t Accuracy 0.8068\n",
      "Epoch [1][20]\t Batch [800][5500]\t Training Loss 0.2460\t Accuracy 0.8052\n",
      "Epoch [1][20]\t Batch [850][5500]\t Training Loss 0.2468\t Accuracy 0.8051\n",
      "Epoch [1][20]\t Batch [900][5500]\t Training Loss 0.2476\t Accuracy 0.8034\n",
      "Epoch [1][20]\t Batch [950][5500]\t Training Loss 0.2469\t Accuracy 0.8053\n",
      "Epoch [1][20]\t Batch [1000][5500]\t Training Loss 0.2459\t Accuracy 0.8064\n",
      "Epoch [1][20]\t Batch [1050][5500]\t Training Loss 0.2452\t Accuracy 0.8087\n",
      "Epoch [1][20]\t Batch [1100][5500]\t Training Loss 0.2446\t Accuracy 0.8104\n",
      "Epoch [1][20]\t Batch [1150][5500]\t Training Loss 0.2439\t Accuracy 0.8117\n",
      "Epoch [1][20]\t Batch [1200][5500]\t Training Loss 0.2447\t Accuracy 0.8104\n",
      "Epoch [1][20]\t Batch [1250][5500]\t Training Loss 0.2449\t Accuracy 0.8096\n",
      "Epoch [1][20]\t Batch [1300][5500]\t Training Loss 0.2454\t Accuracy 0.8080\n",
      "Epoch [1][20]\t Batch [1350][5500]\t Training Loss 0.2456\t Accuracy 0.8077\n",
      "Epoch [1][20]\t Batch [1400][5500]\t Training Loss 0.2462\t Accuracy 0.8056\n",
      "Epoch [1][20]\t Batch [1450][5500]\t Training Loss 0.2471\t Accuracy 0.8042\n",
      "Epoch [1][20]\t Batch [1500][5500]\t Training Loss 0.2480\t Accuracy 0.8018\n",
      "Epoch [1][20]\t Batch [1550][5500]\t Training Loss 0.2475\t Accuracy 0.8028\n",
      "Epoch [1][20]\t Batch [1600][5500]\t Training Loss 0.2479\t Accuracy 0.8021\n",
      "Epoch [1][20]\t Batch [1650][5500]\t Training Loss 0.2475\t Accuracy 0.8021\n",
      "Epoch [1][20]\t Batch [1700][5500]\t Training Loss 0.2476\t Accuracy 0.8017\n",
      "Epoch [1][20]\t Batch [1750][5500]\t Training Loss 0.2477\t Accuracy 0.8019\n",
      "Epoch [1][20]\t Batch [1800][5500]\t Training Loss 0.2483\t Accuracy 0.8008\n",
      "Epoch [1][20]\t Batch [1850][5500]\t Training Loss 0.2477\t Accuracy 0.8019\n",
      "Epoch [1][20]\t Batch [1900][5500]\t Training Loss 0.2470\t Accuracy 0.8039\n",
      "Epoch [1][20]\t Batch [1950][5500]\t Training Loss 0.2471\t Accuracy 0.8038\n",
      "Epoch [1][20]\t Batch [2000][5500]\t Training Loss 0.2469\t Accuracy 0.8042\n",
      "Epoch [1][20]\t Batch [2050][5500]\t Training Loss 0.2468\t Accuracy 0.8045\n",
      "Epoch [1][20]\t Batch [2100][5500]\t Training Loss 0.2468\t Accuracy 0.8040\n",
      "Epoch [1][20]\t Batch [2150][5500]\t Training Loss 0.2464\t Accuracy 0.8042\n",
      "Epoch [1][20]\t Batch [2200][5500]\t Training Loss 0.2459\t Accuracy 0.8053\n",
      "Epoch [1][20]\t Batch [2250][5500]\t Training Loss 0.2460\t Accuracy 0.8047\n",
      "Epoch [1][20]\t Batch [2300][5500]\t Training Loss 0.2462\t Accuracy 0.8048\n",
      "Epoch [1][20]\t Batch [2350][5500]\t Training Loss 0.2461\t Accuracy 0.8053\n",
      "Epoch [1][20]\t Batch [2400][5500]\t Training Loss 0.2463\t Accuracy 0.8052\n",
      "Epoch [1][20]\t Batch [2450][5500]\t Training Loss 0.2461\t Accuracy 0.8054\n",
      "Epoch [1][20]\t Batch [2500][5500]\t Training Loss 0.2463\t Accuracy 0.8048\n",
      "Epoch [1][20]\t Batch [2550][5500]\t Training Loss 0.2459\t Accuracy 0.8050\n",
      "Epoch [1][20]\t Batch [2600][5500]\t Training Loss 0.2457\t Accuracy 0.8050\n",
      "Epoch [1][20]\t Batch [2650][5500]\t Training Loss 0.2455\t Accuracy 0.8053\n",
      "Epoch [1][20]\t Batch [2700][5500]\t Training Loss 0.2457\t Accuracy 0.8048\n",
      "Epoch [1][20]\t Batch [2750][5500]\t Training Loss 0.2458\t Accuracy 0.8049\n",
      "Epoch [1][20]\t Batch [2800][5500]\t Training Loss 0.2454\t Accuracy 0.8057\n",
      "Epoch [1][20]\t Batch [2850][5500]\t Training Loss 0.2451\t Accuracy 0.8062\n",
      "Epoch [1][20]\t Batch [2900][5500]\t Training Loss 0.2449\t Accuracy 0.8064\n",
      "Epoch [1][20]\t Batch [2950][5500]\t Training Loss 0.2449\t Accuracy 0.8062\n",
      "Epoch [1][20]\t Batch [3000][5500]\t Training Loss 0.2450\t Accuracy 0.8059\n",
      "Epoch [1][20]\t Batch [3050][5500]\t Training Loss 0.2452\t Accuracy 0.8054\n",
      "Epoch [1][20]\t Batch [3100][5500]\t Training Loss 0.2453\t Accuracy 0.8051\n",
      "Epoch [1][20]\t Batch [3150][5500]\t Training Loss 0.2454\t Accuracy 0.8051\n",
      "Epoch [1][20]\t Batch [3200][5500]\t Training Loss 0.2454\t Accuracy 0.8051\n",
      "Epoch [1][20]\t Batch [3250][5500]\t Training Loss 0.2457\t Accuracy 0.8045\n",
      "Epoch [1][20]\t Batch [3300][5500]\t Training Loss 0.2455\t Accuracy 0.8050\n",
      "Epoch [1][20]\t Batch [3350][5500]\t Training Loss 0.2455\t Accuracy 0.8053\n",
      "Epoch [1][20]\t Batch [3400][5500]\t Training Loss 0.2450\t Accuracy 0.8063\n",
      "Epoch [1][20]\t Batch [3450][5500]\t Training Loss 0.2447\t Accuracy 0.8068\n",
      "Epoch [1][20]\t Batch [3500][5500]\t Training Loss 0.2449\t Accuracy 0.8063\n",
      "Epoch [1][20]\t Batch [3550][5500]\t Training Loss 0.2447\t Accuracy 0.8066\n",
      "Epoch [1][20]\t Batch [3600][5500]\t Training Loss 0.2446\t Accuracy 0.8068\n",
      "Epoch [1][20]\t Batch [3650][5500]\t Training Loss 0.2446\t Accuracy 0.8071\n",
      "Epoch [1][20]\t Batch [3700][5500]\t Training Loss 0.2443\t Accuracy 0.8079\n",
      "Epoch [1][20]\t Batch [3750][5500]\t Training Loss 0.2445\t Accuracy 0.8075\n",
      "Epoch [1][20]\t Batch [3800][5500]\t Training Loss 0.2445\t Accuracy 0.8076\n",
      "Epoch [1][20]\t Batch [3850][5500]\t Training Loss 0.2443\t Accuracy 0.8079\n",
      "Epoch [1][20]\t Batch [3900][5500]\t Training Loss 0.2440\t Accuracy 0.8080\n",
      "Epoch [1][20]\t Batch [3950][5500]\t Training Loss 0.2438\t Accuracy 0.8082\n",
      "Epoch [1][20]\t Batch [4000][5500]\t Training Loss 0.2439\t Accuracy 0.8080\n",
      "Epoch [1][20]\t Batch [4050][5500]\t Training Loss 0.2438\t Accuracy 0.8083\n",
      "Epoch [1][20]\t Batch [4100][5500]\t Training Loss 0.2435\t Accuracy 0.8087\n",
      "Epoch [1][20]\t Batch [4150][5500]\t Training Loss 0.2437\t Accuracy 0.8084\n",
      "Epoch [1][20]\t Batch [4200][5500]\t Training Loss 0.2436\t Accuracy 0.8086\n",
      "Epoch [1][20]\t Batch [4250][5500]\t Training Loss 0.2438\t Accuracy 0.8082\n",
      "Epoch [1][20]\t Batch [4300][5500]\t Training Loss 0.2436\t Accuracy 0.8083\n",
      "Epoch [1][20]\t Batch [4350][5500]\t Training Loss 0.2433\t Accuracy 0.8089\n",
      "Epoch [1][20]\t Batch [4400][5500]\t Training Loss 0.2432\t Accuracy 0.8090\n",
      "Epoch [1][20]\t Batch [4450][5500]\t Training Loss 0.2433\t Accuracy 0.8087\n",
      "Epoch [1][20]\t Batch [4500][5500]\t Training Loss 0.2431\t Accuracy 0.8091\n",
      "Epoch [1][20]\t Batch [4550][5500]\t Training Loss 0.2431\t Accuracy 0.8092\n",
      "Epoch [1][20]\t Batch [4600][5500]\t Training Loss 0.2431\t Accuracy 0.8091\n",
      "Epoch [1][20]\t Batch [4650][5500]\t Training Loss 0.2431\t Accuracy 0.8089\n",
      "Epoch [1][20]\t Batch [4700][5500]\t Training Loss 0.2429\t Accuracy 0.8095\n",
      "Epoch [1][20]\t Batch [4750][5500]\t Training Loss 0.2429\t Accuracy 0.8095\n",
      "Epoch [1][20]\t Batch [4800][5500]\t Training Loss 0.2427\t Accuracy 0.8097\n",
      "Epoch [1][20]\t Batch [4850][5500]\t Training Loss 0.2424\t Accuracy 0.8103\n",
      "Epoch [1][20]\t Batch [4900][5500]\t Training Loss 0.2423\t Accuracy 0.8103\n",
      "Epoch [1][20]\t Batch [4950][5500]\t Training Loss 0.2424\t Accuracy 0.8104\n",
      "Epoch [1][20]\t Batch [5000][5500]\t Training Loss 0.2426\t Accuracy 0.8102\n",
      "Epoch [1][20]\t Batch [5050][5500]\t Training Loss 0.2425\t Accuracy 0.8103\n",
      "Epoch [1][20]\t Batch [5100][5500]\t Training Loss 0.2423\t Accuracy 0.8105\n",
      "Epoch [1][20]\t Batch [5150][5500]\t Training Loss 0.2421\t Accuracy 0.8108\n",
      "Epoch [1][20]\t Batch [5200][5500]\t Training Loss 0.2418\t Accuracy 0.8115\n",
      "Epoch [1][20]\t Batch [5250][5500]\t Training Loss 0.2418\t Accuracy 0.8113\n",
      "Epoch [1][20]\t Batch [5300][5500]\t Training Loss 0.2417\t Accuracy 0.8114\n",
      "Epoch [1][20]\t Batch [5350][5500]\t Training Loss 0.2415\t Accuracy 0.8117\n",
      "Epoch [1][20]\t Batch [5400][5500]\t Training Loss 0.2413\t Accuracy 0.8120\n",
      "Epoch [1][20]\t Batch [5450][5500]\t Training Loss 0.2412\t Accuracy 0.8121\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2411\t Average training accuracy 0.8122\n",
      "Epoch [1]\t Average validation loss 0.2130\t Average validation accuracy 0.8702\n",
      "\n",
      "Epoch [2][20]\t Batch [0][5500]\t Training Loss 0.1681\t Accuracy 1.0000\n",
      "Epoch [2][20]\t Batch [50][5500]\t Training Loss 0.2123\t Accuracy 0.8549\n",
      "Epoch [2][20]\t Batch [100][5500]\t Training Loss 0.2268\t Accuracy 0.8257\n",
      "Epoch [2][20]\t Batch [150][5500]\t Training Loss 0.2333\t Accuracy 0.8126\n",
      "Epoch [2][20]\t Batch [200][5500]\t Training Loss 0.2290\t Accuracy 0.8199\n",
      "Epoch [2][20]\t Batch [250][5500]\t Training Loss 0.2249\t Accuracy 0.8283\n",
      "Epoch [2][20]\t Batch [300][5500]\t Training Loss 0.2231\t Accuracy 0.8349\n",
      "Epoch [2][20]\t Batch [350][5500]\t Training Loss 0.2229\t Accuracy 0.8385\n",
      "Epoch [2][20]\t Batch [400][5500]\t Training Loss 0.2237\t Accuracy 0.8369\n",
      "Epoch [2][20]\t Batch [450][5500]\t Training Loss 0.2242\t Accuracy 0.8370\n",
      "Epoch [2][20]\t Batch [500][5500]\t Training Loss 0.2239\t Accuracy 0.8397\n",
      "Epoch [2][20]\t Batch [550][5500]\t Training Loss 0.2232\t Accuracy 0.8390\n",
      "Epoch [2][20]\t Batch [600][5500]\t Training Loss 0.2226\t Accuracy 0.8401\n",
      "Epoch [2][20]\t Batch [650][5500]\t Training Loss 0.2217\t Accuracy 0.8416\n",
      "Epoch [2][20]\t Batch [700][5500]\t Training Loss 0.2216\t Accuracy 0.8417\n",
      "Epoch [2][20]\t Batch [750][5500]\t Training Loss 0.2229\t Accuracy 0.8389\n",
      "Epoch [2][20]\t Batch [800][5500]\t Training Loss 0.2239\t Accuracy 0.8385\n",
      "Epoch [2][20]\t Batch [850][5500]\t Training Loss 0.2249\t Accuracy 0.8370\n",
      "Epoch [2][20]\t Batch [900][5500]\t Training Loss 0.2260\t Accuracy 0.8346\n",
      "Epoch [2][20]\t Batch [950][5500]\t Training Loss 0.2254\t Accuracy 0.8355\n",
      "Epoch [2][20]\t Batch [1000][5500]\t Training Loss 0.2244\t Accuracy 0.8369\n",
      "Epoch [2][20]\t Batch [1050][5500]\t Training Loss 0.2239\t Accuracy 0.8379\n",
      "Epoch [2][20]\t Batch [1100][5500]\t Training Loss 0.2234\t Accuracy 0.8390\n",
      "Epoch [2][20]\t Batch [1150][5500]\t Training Loss 0.2229\t Accuracy 0.8400\n",
      "Epoch [2][20]\t Batch [1200][5500]\t Training Loss 0.2239\t Accuracy 0.8384\n",
      "Epoch [2][20]\t Batch [1250][5500]\t Training Loss 0.2241\t Accuracy 0.8379\n",
      "Epoch [2][20]\t Batch [1300][5500]\t Training Loss 0.2248\t Accuracy 0.8361\n",
      "Epoch [2][20]\t Batch [1350][5500]\t Training Loss 0.2251\t Accuracy 0.8353\n",
      "Epoch [2][20]\t Batch [1400][5500]\t Training Loss 0.2257\t Accuracy 0.8333\n",
      "Epoch [2][20]\t Batch [1450][5500]\t Training Loss 0.2267\t Accuracy 0.8318\n",
      "Epoch [2][20]\t Batch [1500][5500]\t Training Loss 0.2278\t Accuracy 0.8291\n",
      "Epoch [2][20]\t Batch [1550][5500]\t Training Loss 0.2274\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [1600][5500]\t Training Loss 0.2278\t Accuracy 0.8280\n",
      "Epoch [2][20]\t Batch [1650][5500]\t Training Loss 0.2276\t Accuracy 0.8279\n",
      "Epoch [2][20]\t Batch [1700][5500]\t Training Loss 0.2277\t Accuracy 0.8275\n",
      "Epoch [2][20]\t Batch [1750][5500]\t Training Loss 0.2279\t Accuracy 0.8278\n",
      "Epoch [2][20]\t Batch [1800][5500]\t Training Loss 0.2285\t Accuracy 0.8271\n",
      "Epoch [2][20]\t Batch [1850][5500]\t Training Loss 0.2280\t Accuracy 0.8279\n",
      "Epoch [2][20]\t Batch [1900][5500]\t Training Loss 0.2273\t Accuracy 0.8297\n",
      "Epoch [2][20]\t Batch [1950][5500]\t Training Loss 0.2275\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [2000][5500]\t Training Loss 0.2273\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [2050][5500]\t Training Loss 0.2272\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [2100][5500]\t Training Loss 0.2273\t Accuracy 0.8296\n",
      "Epoch [2][20]\t Batch [2150][5500]\t Training Loss 0.2270\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [2200][5500]\t Training Loss 0.2265\t Accuracy 0.8307\n",
      "Epoch [2][20]\t Batch [2250][5500]\t Training Loss 0.2268\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [2300][5500]\t Training Loss 0.2271\t Accuracy 0.8297\n",
      "Epoch [2][20]\t Batch [2350][5500]\t Training Loss 0.2271\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [2400][5500]\t Training Loss 0.2272\t Accuracy 0.8297\n",
      "Epoch [2][20]\t Batch [2450][5500]\t Training Loss 0.2272\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [2500][5500]\t Training Loss 0.2274\t Accuracy 0.8292\n",
      "Epoch [2][20]\t Batch [2550][5500]\t Training Loss 0.2270\t Accuracy 0.8294\n",
      "Epoch [2][20]\t Batch [2600][5500]\t Training Loss 0.2269\t Accuracy 0.8293\n",
      "Epoch [2][20]\t Batch [2650][5500]\t Training Loss 0.2268\t Accuracy 0.8296\n",
      "Epoch [2][20]\t Batch [2700][5500]\t Training Loss 0.2270\t Accuracy 0.8291\n",
      "Epoch [2][20]\t Batch [2750][5500]\t Training Loss 0.2272\t Accuracy 0.8290\n",
      "Epoch [2][20]\t Batch [2800][5500]\t Training Loss 0.2269\t Accuracy 0.8296\n",
      "Epoch [2][20]\t Batch [2850][5500]\t Training Loss 0.2267\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [2900][5500]\t Training Loss 0.2265\t Accuracy 0.8301\n",
      "Epoch [2][20]\t Batch [2950][5500]\t Training Loss 0.2266\t Accuracy 0.8297\n",
      "Epoch [2][20]\t Batch [3000][5500]\t Training Loss 0.2268\t Accuracy 0.8293\n",
      "Epoch [2][20]\t Batch [3050][5500]\t Training Loss 0.2270\t Accuracy 0.8289\n",
      "Epoch [2][20]\t Batch [3100][5500]\t Training Loss 0.2272\t Accuracy 0.8285\n",
      "Epoch [2][20]\t Batch [3150][5500]\t Training Loss 0.2274\t Accuracy 0.8283\n",
      "Epoch [2][20]\t Batch [3200][5500]\t Training Loss 0.2275\t Accuracy 0.8282\n",
      "Epoch [2][20]\t Batch [3250][5500]\t Training Loss 0.2278\t Accuracy 0.8275\n",
      "Epoch [2][20]\t Batch [3300][5500]\t Training Loss 0.2277\t Accuracy 0.8279\n",
      "Epoch [2][20]\t Batch [3350][5500]\t Training Loss 0.2277\t Accuracy 0.8280\n",
      "Epoch [2][20]\t Batch [3400][5500]\t Training Loss 0.2272\t Accuracy 0.8288\n",
      "Epoch [2][20]\t Batch [3450][5500]\t Training Loss 0.2270\t Accuracy 0.8292\n",
      "Epoch [2][20]\t Batch [3500][5500]\t Training Loss 0.2272\t Accuracy 0.8286\n",
      "Epoch [2][20]\t Batch [3550][5500]\t Training Loss 0.2272\t Accuracy 0.8289\n",
      "Epoch [2][20]\t Batch [3600][5500]\t Training Loss 0.2271\t Accuracy 0.8290\n",
      "Epoch [2][20]\t Batch [3650][5500]\t Training Loss 0.2272\t Accuracy 0.8292\n",
      "Epoch [2][20]\t Batch [3700][5500]\t Training Loss 0.2269\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [3750][5500]\t Training Loss 0.2272\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [3800][5500]\t Training Loss 0.2272\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [3850][5500]\t Training Loss 0.2271\t Accuracy 0.8297\n",
      "Epoch [2][20]\t Batch [3900][5500]\t Training Loss 0.2269\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [3950][5500]\t Training Loss 0.2268\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [4000][5500]\t Training Loss 0.2269\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [4050][5500]\t Training Loss 0.2269\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [4100][5500]\t Training Loss 0.2267\t Accuracy 0.8301\n",
      "Epoch [2][20]\t Batch [4150][5500]\t Training Loss 0.2269\t Accuracy 0.8296\n",
      "Epoch [2][20]\t Batch [4200][5500]\t Training Loss 0.2268\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [4250][5500]\t Training Loss 0.2271\t Accuracy 0.8293\n",
      "Epoch [2][20]\t Batch [4300][5500]\t Training Loss 0.2270\t Accuracy 0.8293\n",
      "Epoch [2][20]\t Batch [4350][5500]\t Training Loss 0.2268\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [4400][5500]\t Training Loss 0.2267\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [4450][5500]\t Training Loss 0.2269\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [4500][5500]\t Training Loss 0.2267\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [4550][5500]\t Training Loss 0.2267\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [4600][5500]\t Training Loss 0.2268\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [4650][5500]\t Training Loss 0.2269\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [4700][5500]\t Training Loss 0.2267\t Accuracy 0.8301\n",
      "Epoch [2][20]\t Batch [4750][5500]\t Training Loss 0.2267\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [4800][5500]\t Training Loss 0.2266\t Accuracy 0.8301\n",
      "Epoch [2][20]\t Batch [4850][5500]\t Training Loss 0.2264\t Accuracy 0.8306\n",
      "Epoch [2][20]\t Batch [4900][5500]\t Training Loss 0.2263\t Accuracy 0.8306\n",
      "Epoch [2][20]\t Batch [4950][5500]\t Training Loss 0.2264\t Accuracy 0.8306\n",
      "Epoch [2][20]\t Batch [5000][5500]\t Training Loss 0.2267\t Accuracy 0.8304\n",
      "Epoch [2][20]\t Batch [5050][5500]\t Training Loss 0.2267\t Accuracy 0.8304\n",
      "Epoch [2][20]\t Batch [5100][5500]\t Training Loss 0.2266\t Accuracy 0.8306\n",
      "Epoch [2][20]\t Batch [5150][5500]\t Training Loss 0.2264\t Accuracy 0.8308\n",
      "Epoch [2][20]\t Batch [5200][5500]\t Training Loss 0.2262\t Accuracy 0.8313\n",
      "Epoch [2][20]\t Batch [5250][5500]\t Training Loss 0.2262\t Accuracy 0.8311\n",
      "Epoch [2][20]\t Batch [5300][5500]\t Training Loss 0.2261\t Accuracy 0.8311\n",
      "Epoch [2][20]\t Batch [5350][5500]\t Training Loss 0.2260\t Accuracy 0.8312\n",
      "Epoch [2][20]\t Batch [5400][5500]\t Training Loss 0.2258\t Accuracy 0.8314\n",
      "Epoch [2][20]\t Batch [5450][5500]\t Training Loss 0.2258\t Accuracy 0.8314\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2257\t Average training accuracy 0.8314\n",
      "Epoch [2]\t Average validation loss 0.2028\t Average validation accuracy 0.8794\n",
      "\n",
      "Epoch [3][20]\t Batch [0][5500]\t Training Loss 0.1538\t Accuracy 1.0000\n",
      "Epoch [3][20]\t Batch [50][5500]\t Training Loss 0.2021\t Accuracy 0.8647\n",
      "Epoch [3][20]\t Batch [100][5500]\t Training Loss 0.2161\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [150][5500]\t Training Loss 0.2231\t Accuracy 0.8245\n",
      "Epoch [3][20]\t Batch [200][5500]\t Training Loss 0.2189\t Accuracy 0.8338\n",
      "Epoch [3][20]\t Batch [250][5500]\t Training Loss 0.2147\t Accuracy 0.8430\n",
      "Epoch [3][20]\t Batch [300][5500]\t Training Loss 0.2128\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [350][5500]\t Training Loss 0.2126\t Accuracy 0.8504\n",
      "Epoch [3][20]\t Batch [400][5500]\t Training Loss 0.2133\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [450][5500]\t Training Loss 0.2139\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [500][5500]\t Training Loss 0.2135\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [550][5500]\t Training Loss 0.2130\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [600][5500]\t Training Loss 0.2124\t Accuracy 0.8512\n",
      "Epoch [3][20]\t Batch [650][5500]\t Training Loss 0.2117\t Accuracy 0.8527\n",
      "Epoch [3][20]\t Batch [700][5500]\t Training Loss 0.2117\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [750][5500]\t Training Loss 0.2130\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [800][5500]\t Training Loss 0.2139\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [850][5500]\t Training Loss 0.2149\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [900][5500]\t Training Loss 0.2161\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [950][5500]\t Training Loss 0.2156\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [1000][5500]\t Training Loss 0.2147\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [1050][5500]\t Training Loss 0.2142\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [1100][5500]\t Training Loss 0.2137\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [1150][5500]\t Training Loss 0.2133\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [1200][5500]\t Training Loss 0.2143\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [1250][5500]\t Training Loss 0.2146\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [1300][5500]\t Training Loss 0.2153\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [1350][5500]\t Training Loss 0.2156\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [1400][5500]\t Training Loss 0.2162\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [1450][5500]\t Training Loss 0.2172\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [1500][5500]\t Training Loss 0.2183\t Accuracy 0.8385\n",
      "Epoch [3][20]\t Batch [1550][5500]\t Training Loss 0.2180\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [1600][5500]\t Training Loss 0.2185\t Accuracy 0.8377\n",
      "Epoch [3][20]\t Batch [1650][5500]\t Training Loss 0.2183\t Accuracy 0.8378\n",
      "Epoch [3][20]\t Batch [1700][5500]\t Training Loss 0.2184\t Accuracy 0.8376\n",
      "Epoch [3][20]\t Batch [1750][5500]\t Training Loss 0.2185\t Accuracy 0.8375\n",
      "Epoch [3][20]\t Batch [1800][5500]\t Training Loss 0.2192\t Accuracy 0.8370\n",
      "Epoch [3][20]\t Batch [1850][5500]\t Training Loss 0.2187\t Accuracy 0.8380\n",
      "Epoch [3][20]\t Batch [1900][5500]\t Training Loss 0.2180\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [1950][5500]\t Training Loss 0.2182\t Accuracy 0.8393\n",
      "Epoch [3][20]\t Batch [2000][5500]\t Training Loss 0.2180\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [2050][5500]\t Training Loss 0.2180\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [2100][5500]\t Training Loss 0.2181\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [2150][5500]\t Training Loss 0.2178\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [2200][5500]\t Training Loss 0.2173\t Accuracy 0.8403\n",
      "Epoch [3][20]\t Batch [2250][5500]\t Training Loss 0.2175\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [2300][5500]\t Training Loss 0.2180\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [2350][5500]\t Training Loss 0.2180\t Accuracy 0.8399\n",
      "Epoch [3][20]\t Batch [2400][5500]\t Training Loss 0.2181\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [2450][5500]\t Training Loss 0.2180\t Accuracy 0.8399\n",
      "Epoch [3][20]\t Batch [2500][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [2550][5500]\t Training Loss 0.2179\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [2600][5500]\t Training Loss 0.2178\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [2650][5500]\t Training Loss 0.2177\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [2700][5500]\t Training Loss 0.2180\t Accuracy 0.8394\n",
      "Epoch [3][20]\t Batch [2750][5500]\t Training Loss 0.2182\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [2800][5500]\t Training Loss 0.2179\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [2850][5500]\t Training Loss 0.2177\t Accuracy 0.8401\n",
      "Epoch [3][20]\t Batch [2900][5500]\t Training Loss 0.2176\t Accuracy 0.8401\n",
      "Epoch [3][20]\t Batch [2950][5500]\t Training Loss 0.2176\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [3000][5500]\t Training Loss 0.2179\t Accuracy 0.8394\n",
      "Epoch [3][20]\t Batch [3050][5500]\t Training Loss 0.2181\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [3100][5500]\t Training Loss 0.2183\t Accuracy 0.8386\n",
      "Epoch [3][20]\t Batch [3150][5500]\t Training Loss 0.2185\t Accuracy 0.8382\n",
      "Epoch [3][20]\t Batch [3200][5500]\t Training Loss 0.2186\t Accuracy 0.8381\n",
      "Epoch [3][20]\t Batch [3250][5500]\t Training Loss 0.2190\t Accuracy 0.8375\n",
      "Epoch [3][20]\t Batch [3300][5500]\t Training Loss 0.2188\t Accuracy 0.8378\n",
      "Epoch [3][20]\t Batch [3350][5500]\t Training Loss 0.2189\t Accuracy 0.8379\n",
      "Epoch [3][20]\t Batch [3400][5500]\t Training Loss 0.2184\t Accuracy 0.8387\n",
      "Epoch [3][20]\t Batch [3450][5500]\t Training Loss 0.2183\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [3500][5500]\t Training Loss 0.2185\t Accuracy 0.8384\n",
      "Epoch [3][20]\t Batch [3550][5500]\t Training Loss 0.2184\t Accuracy 0.8387\n",
      "Epoch [3][20]\t Batch [3600][5500]\t Training Loss 0.2184\t Accuracy 0.8388\n",
      "Epoch [3][20]\t Batch [3650][5500]\t Training Loss 0.2184\t Accuracy 0.8390\n",
      "Epoch [3][20]\t Batch [3700][5500]\t Training Loss 0.2182\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [3750][5500]\t Training Loss 0.2185\t Accuracy 0.8392\n",
      "Epoch [3][20]\t Batch [3800][5500]\t Training Loss 0.2186\t Accuracy 0.8392\n",
      "Epoch [3][20]\t Batch [3850][5500]\t Training Loss 0.2185\t Accuracy 0.8394\n",
      "Epoch [3][20]\t Batch [3900][5500]\t Training Loss 0.2183\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [3950][5500]\t Training Loss 0.2182\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [4000][5500]\t Training Loss 0.2184\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4050][5500]\t Training Loss 0.2183\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [4100][5500]\t Training Loss 0.2182\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [4150][5500]\t Training Loss 0.2183\t Accuracy 0.8394\n",
      "Epoch [3][20]\t Batch [4200][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4250][5500]\t Training Loss 0.2186\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [4300][5500]\t Training Loss 0.2185\t Accuracy 0.8393\n",
      "Epoch [3][20]\t Batch [4350][5500]\t Training Loss 0.2183\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [4400][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4450][5500]\t Training Loss 0.2185\t Accuracy 0.8392\n",
      "Epoch [3][20]\t Batch [4500][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4550][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4600][5500]\t Training Loss 0.2184\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4650][5500]\t Training Loss 0.2186\t Accuracy 0.8392\n",
      "Epoch [3][20]\t Batch [4700][5500]\t Training Loss 0.2183\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [4750][5500]\t Training Loss 0.2184\t Accuracy 0.8395\n",
      "Epoch [3][20]\t Batch [4800][5500]\t Training Loss 0.2184\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [4850][5500]\t Training Loss 0.2181\t Accuracy 0.8402\n",
      "Epoch [3][20]\t Batch [4900][5500]\t Training Loss 0.2181\t Accuracy 0.8401\n",
      "Epoch [3][20]\t Batch [4950][5500]\t Training Loss 0.2182\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [5000][5500]\t Training Loss 0.2185\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [5050][5500]\t Training Loss 0.2185\t Accuracy 0.8396\n",
      "Epoch [3][20]\t Batch [5100][5500]\t Training Loss 0.2184\t Accuracy 0.8398\n",
      "Epoch [3][20]\t Batch [5150][5500]\t Training Loss 0.2182\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [5200][5500]\t Training Loss 0.2180\t Accuracy 0.8405\n",
      "Epoch [3][20]\t Batch [5250][5500]\t Training Loss 0.2180\t Accuracy 0.8402\n",
      "Epoch [3][20]\t Batch [5300][5500]\t Training Loss 0.2180\t Accuracy 0.8401\n",
      "Epoch [3][20]\t Batch [5350][5500]\t Training Loss 0.2179\t Accuracy 0.8402\n",
      "Epoch [3][20]\t Batch [5400][5500]\t Training Loss 0.2178\t Accuracy 0.8404\n",
      "Epoch [3][20]\t Batch [5450][5500]\t Training Loss 0.2177\t Accuracy 0.8405\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2177\t Average training accuracy 0.8404\n",
      "Epoch [3]\t Average validation loss 0.1966\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [4][20]\t Batch [0][5500]\t Training Loss 0.1456\t Accuracy 1.0000\n",
      "Epoch [4][20]\t Batch [50][5500]\t Training Loss 0.1958\t Accuracy 0.8627\n",
      "Epoch [4][20]\t Batch [100][5500]\t Training Loss 0.2096\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [150][5500]\t Training Loss 0.2169\t Accuracy 0.8305\n",
      "Epoch [4][20]\t Batch [200][5500]\t Training Loss 0.2126\t Accuracy 0.8408\n",
      "Epoch [4][20]\t Batch [250][5500]\t Training Loss 0.2084\t Accuracy 0.8502\n",
      "Epoch [4][20]\t Batch [300][5500]\t Training Loss 0.2065\t Accuracy 0.8535\n",
      "Epoch [4][20]\t Batch [350][5500]\t Training Loss 0.2063\t Accuracy 0.8556\n",
      "Epoch [4][20]\t Batch [400][5500]\t Training Loss 0.2068\t Accuracy 0.8554\n",
      "Epoch [4][20]\t Batch [450][5500]\t Training Loss 0.2075\t Accuracy 0.8548\n",
      "Epoch [4][20]\t Batch [500][5500]\t Training Loss 0.2071\t Accuracy 0.8557\n",
      "Epoch [4][20]\t Batch [550][5500]\t Training Loss 0.2066\t Accuracy 0.8546\n",
      "Epoch [4][20]\t Batch [600][5500]\t Training Loss 0.2061\t Accuracy 0.8562\n",
      "Epoch [4][20]\t Batch [650][5500]\t Training Loss 0.2054\t Accuracy 0.8579\n",
      "Epoch [4][20]\t Batch [700][5500]\t Training Loss 0.2054\t Accuracy 0.8575\n",
      "Epoch [4][20]\t Batch [750][5500]\t Training Loss 0.2067\t Accuracy 0.8547\n",
      "Epoch [4][20]\t Batch [800][5500]\t Training Loss 0.2077\t Accuracy 0.8534\n",
      "Epoch [4][20]\t Batch [850][5500]\t Training Loss 0.2086\t Accuracy 0.8524\n",
      "Epoch [4][20]\t Batch [900][5500]\t Training Loss 0.2099\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [950][5500]\t Training Loss 0.2094\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [1000][5500]\t Training Loss 0.2084\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [1050][5500]\t Training Loss 0.2080\t Accuracy 0.8532\n",
      "Epoch [4][20]\t Batch [1100][5500]\t Training Loss 0.2076\t Accuracy 0.8545\n",
      "Epoch [4][20]\t Batch [1150][5500]\t Training Loss 0.2072\t Accuracy 0.8553\n",
      "Epoch [4][20]\t Batch [1200][5500]\t Training Loss 0.2082\t Accuracy 0.8533\n",
      "Epoch [4][20]\t Batch [1250][5500]\t Training Loss 0.2085\t Accuracy 0.8525\n",
      "Epoch [4][20]\t Batch [1300][5500]\t Training Loss 0.2092\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [1350][5500]\t Training Loss 0.2095\t Accuracy 0.8503\n",
      "Epoch [4][20]\t Batch [1400][5500]\t Training Loss 0.2101\t Accuracy 0.8484\n",
      "Epoch [4][20]\t Batch [1450][5500]\t Training Loss 0.2111\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [1500][5500]\t Training Loss 0.2122\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [1550][5500]\t Training Loss 0.2119\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [1600][5500]\t Training Loss 0.2124\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [1650][5500]\t Training Loss 0.2122\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [1700][5500]\t Training Loss 0.2124\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [1750][5500]\t Training Loss 0.2125\t Accuracy 0.8433\n",
      "Epoch [4][20]\t Batch [1800][5500]\t Training Loss 0.2131\t Accuracy 0.8428\n",
      "Epoch [4][20]\t Batch [1850][5500]\t Training Loss 0.2127\t Accuracy 0.8439\n",
      "Epoch [4][20]\t Batch [1900][5500]\t Training Loss 0.2120\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [1950][5500]\t Training Loss 0.2122\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [2000][5500]\t Training Loss 0.2120\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [2050][5500]\t Training Loss 0.2119\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [2100][5500]\t Training Loss 0.2121\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [2150][5500]\t Training Loss 0.2118\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [2200][5500]\t Training Loss 0.2113\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [2250][5500]\t Training Loss 0.2116\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [2300][5500]\t Training Loss 0.2120\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [2350][5500]\t Training Loss 0.2120\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [2400][5500]\t Training Loss 0.2121\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [2450][5500]\t Training Loss 0.2121\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [2500][5500]\t Training Loss 0.2123\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [2550][5500]\t Training Loss 0.2119\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [2600][5500]\t Training Loss 0.2118\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [2650][5500]\t Training Loss 0.2118\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [2700][5500]\t Training Loss 0.2120\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [2750][5500]\t Training Loss 0.2123\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [2800][5500]\t Training Loss 0.2120\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [2850][5500]\t Training Loss 0.2118\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [2900][5500]\t Training Loss 0.2117\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [2950][5500]\t Training Loss 0.2118\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [3000][5500]\t Training Loss 0.2120\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [3050][5500]\t Training Loss 0.2122\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [3100][5500]\t Training Loss 0.2124\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [3150][5500]\t Training Loss 0.2127\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [3200][5500]\t Training Loss 0.2128\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [3250][5500]\t Training Loss 0.2131\t Accuracy 0.8431\n",
      "Epoch [4][20]\t Batch [3300][5500]\t Training Loss 0.2130\t Accuracy 0.8433\n",
      "Epoch [4][20]\t Batch [3350][5500]\t Training Loss 0.2130\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [3400][5500]\t Training Loss 0.2126\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [3450][5500]\t Training Loss 0.2124\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [3500][5500]\t Training Loss 0.2127\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [3550][5500]\t Training Loss 0.2126\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [3600][5500]\t Training Loss 0.2126\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [3650][5500]\t Training Loss 0.2126\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [3700][5500]\t Training Loss 0.2124\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [3750][5500]\t Training Loss 0.2128\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [3800][5500]\t Training Loss 0.2128\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [3850][5500]\t Training Loss 0.2127\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [3900][5500]\t Training Loss 0.2125\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [3950][5500]\t Training Loss 0.2125\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [4000][5500]\t Training Loss 0.2126\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [4050][5500]\t Training Loss 0.2126\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [4100][5500]\t Training Loss 0.2124\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [4150][5500]\t Training Loss 0.2126\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [4200][5500]\t Training Loss 0.2126\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4250][5500]\t Training Loss 0.2129\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [4300][5500]\t Training Loss 0.2128\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [4350][5500]\t Training Loss 0.2127\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [4400][5500]\t Training Loss 0.2126\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4450][5500]\t Training Loss 0.2128\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [4500][5500]\t Training Loss 0.2127\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4550][5500]\t Training Loss 0.2127\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4600][5500]\t Training Loss 0.2128\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [4650][5500]\t Training Loss 0.2129\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [4700][5500]\t Training Loss 0.2127\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [4750][5500]\t Training Loss 0.2128\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4800][5500]\t Training Loss 0.2128\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4850][5500]\t Training Loss 0.2125\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [4900][5500]\t Training Loss 0.2125\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [4950][5500]\t Training Loss 0.2126\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [5000][5500]\t Training Loss 0.2129\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [5050][5500]\t Training Loss 0.2129\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [5100][5500]\t Training Loss 0.2128\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [5150][5500]\t Training Loss 0.2127\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [5200][5500]\t Training Loss 0.2125\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [5250][5500]\t Training Loss 0.2125\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [5300][5500]\t Training Loss 0.2125\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [5350][5500]\t Training Loss 0.2124\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [5400][5500]\t Training Loss 0.2123\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [5450][5500]\t Training Loss 0.2122\t Accuracy 0.8455\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2122\t Average training accuracy 0.8453\n",
      "Epoch [4]\t Average validation loss 0.1921\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [5][20]\t Batch [0][5500]\t Training Loss 0.1400\t Accuracy 1.0000\n",
      "Epoch [5][20]\t Batch [50][5500]\t Training Loss 0.1912\t Accuracy 0.8686\n",
      "Epoch [5][20]\t Batch [100][5500]\t Training Loss 0.2049\t Accuracy 0.8505\n",
      "Epoch [5][20]\t Batch [150][5500]\t Training Loss 0.2124\t Accuracy 0.8371\n",
      "Epoch [5][20]\t Batch [200][5500]\t Training Loss 0.2079\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [250][5500]\t Training Loss 0.2038\t Accuracy 0.8562\n",
      "Epoch [5][20]\t Batch [300][5500]\t Training Loss 0.2018\t Accuracy 0.8591\n",
      "Epoch [5][20]\t Batch [350][5500]\t Training Loss 0.2016\t Accuracy 0.8601\n",
      "Epoch [5][20]\t Batch [400][5500]\t Training Loss 0.2020\t Accuracy 0.8603\n",
      "Epoch [5][20]\t Batch [450][5500]\t Training Loss 0.2027\t Accuracy 0.8603\n",
      "Epoch [5][20]\t Batch [500][5500]\t Training Loss 0.2023\t Accuracy 0.8613\n",
      "Epoch [5][20]\t Batch [550][5500]\t Training Loss 0.2019\t Accuracy 0.8603\n",
      "Epoch [5][20]\t Batch [600][5500]\t Training Loss 0.2013\t Accuracy 0.8616\n",
      "Epoch [5][20]\t Batch [650][5500]\t Training Loss 0.2007\t Accuracy 0.8627\n",
      "Epoch [5][20]\t Batch [700][5500]\t Training Loss 0.2007\t Accuracy 0.8618\n",
      "Epoch [5][20]\t Batch [750][5500]\t Training Loss 0.2021\t Accuracy 0.8590\n",
      "Epoch [5][20]\t Batch [800][5500]\t Training Loss 0.2030\t Accuracy 0.8579\n",
      "Epoch [5][20]\t Batch [850][5500]\t Training Loss 0.2039\t Accuracy 0.8568\n",
      "Epoch [5][20]\t Batch [900][5500]\t Training Loss 0.2052\t Accuracy 0.8538\n",
      "Epoch [5][20]\t Batch [950][5500]\t Training Loss 0.2047\t Accuracy 0.8546\n",
      "Epoch [5][20]\t Batch [1000][5500]\t Training Loss 0.2038\t Accuracy 0.8558\n",
      "Epoch [5][20]\t Batch [1050][5500]\t Training Loss 0.2033\t Accuracy 0.8570\n",
      "Epoch [5][20]\t Batch [1100][5500]\t Training Loss 0.2029\t Accuracy 0.8583\n",
      "Epoch [5][20]\t Batch [1150][5500]\t Training Loss 0.2025\t Accuracy 0.8590\n",
      "Epoch [5][20]\t Batch [1200][5500]\t Training Loss 0.2036\t Accuracy 0.8570\n",
      "Epoch [5][20]\t Batch [1250][5500]\t Training Loss 0.2039\t Accuracy 0.8560\n",
      "Epoch [5][20]\t Batch [1300][5500]\t Training Loss 0.2046\t Accuracy 0.8547\n",
      "Epoch [5][20]\t Batch [1350][5500]\t Training Loss 0.2049\t Accuracy 0.8541\n",
      "Epoch [5][20]\t Batch [1400][5500]\t Training Loss 0.2055\t Accuracy 0.8525\n",
      "Epoch [5][20]\t Batch [1450][5500]\t Training Loss 0.2065\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [1500][5500]\t Training Loss 0.2076\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [1550][5500]\t Training Loss 0.2073\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [1600][5500]\t Training Loss 0.2078\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [1650][5500]\t Training Loss 0.2076\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [1700][5500]\t Training Loss 0.2078\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [1750][5500]\t Training Loss 0.2079\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [1800][5500]\t Training Loss 0.2086\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [1850][5500]\t Training Loss 0.2081\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [1900][5500]\t Training Loss 0.2075\t Accuracy 0.8497\n",
      "Epoch [5][20]\t Batch [1950][5500]\t Training Loss 0.2076\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [2000][5500]\t Training Loss 0.2074\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [2050][5500]\t Training Loss 0.2074\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [2100][5500]\t Training Loss 0.2075\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [2150][5500]\t Training Loss 0.2072\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [2200][5500]\t Training Loss 0.2068\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [2250][5500]\t Training Loss 0.2070\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [2300][5500]\t Training Loss 0.2074\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [2350][5500]\t Training Loss 0.2075\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [2400][5500]\t Training Loss 0.2076\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [2450][5500]\t Training Loss 0.2075\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [2500][5500]\t Training Loss 0.2077\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [2550][5500]\t Training Loss 0.2074\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [2600][5500]\t Training Loss 0.2073\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [2650][5500]\t Training Loss 0.2072\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [2700][5500]\t Training Loss 0.2075\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [2750][5500]\t Training Loss 0.2077\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [2800][5500]\t Training Loss 0.2075\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [2850][5500]\t Training Loss 0.2072\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [2900][5500]\t Training Loss 0.2071\t Accuracy 0.8503\n",
      "Epoch [5][20]\t Batch [2950][5500]\t Training Loss 0.2072\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [3000][5500]\t Training Loss 0.2075\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [3050][5500]\t Training Loss 0.2077\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [3100][5500]\t Training Loss 0.2079\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [3150][5500]\t Training Loss 0.2082\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [3200][5500]\t Training Loss 0.2083\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [3250][5500]\t Training Loss 0.2086\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [3300][5500]\t Training Loss 0.2085\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [3350][5500]\t Training Loss 0.2085\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [3400][5500]\t Training Loss 0.2081\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [3450][5500]\t Training Loss 0.2080\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [3500][5500]\t Training Loss 0.2082\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [3550][5500]\t Training Loss 0.2081\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [3600][5500]\t Training Loss 0.2081\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [3650][5500]\t Training Loss 0.2081\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [3700][5500]\t Training Loss 0.2080\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [3750][5500]\t Training Loss 0.2083\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [3800][5500]\t Training Loss 0.2084\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [3850][5500]\t Training Loss 0.2083\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [3900][5500]\t Training Loss 0.2081\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [3950][5500]\t Training Loss 0.2080\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [4000][5500]\t Training Loss 0.2082\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [4050][5500]\t Training Loss 0.2082\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [4100][5500]\t Training Loss 0.2080\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [4150][5500]\t Training Loss 0.2082\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [4200][5500]\t Training Loss 0.2082\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [4250][5500]\t Training Loss 0.2085\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [4300][5500]\t Training Loss 0.2084\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [4350][5500]\t Training Loss 0.2083\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [4400][5500]\t Training Loss 0.2083\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [4450][5500]\t Training Loss 0.2084\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [4500][5500]\t Training Loss 0.2083\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [4550][5500]\t Training Loss 0.2083\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [4600][5500]\t Training Loss 0.2084\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [4650][5500]\t Training Loss 0.2086\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [4700][5500]\t Training Loss 0.2084\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [4750][5500]\t Training Loss 0.2084\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [4800][5500]\t Training Loss 0.2084\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [4850][5500]\t Training Loss 0.2082\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [4900][5500]\t Training Loss 0.2081\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [4950][5500]\t Training Loss 0.2083\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [5000][5500]\t Training Loss 0.2085\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [5050][5500]\t Training Loss 0.2086\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [5100][5500]\t Training Loss 0.2085\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [5150][5500]\t Training Loss 0.2083\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [5200][5500]\t Training Loss 0.2082\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [5250][5500]\t Training Loss 0.2082\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [5300][5500]\t Training Loss 0.2082\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [5350][5500]\t Training Loss 0.2081\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [5400][5500]\t Training Loss 0.2080\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [5450][5500]\t Training Loss 0.2079\t Accuracy 0.8492\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2079\t Average training accuracy 0.8491\n",
      "Epoch [5]\t Average validation loss 0.1884\t Average validation accuracy 0.8900\n",
      "\n",
      "Epoch [6][20]\t Batch [0][5500]\t Training Loss 0.1357\t Accuracy 1.0000\n",
      "Epoch [6][20]\t Batch [50][5500]\t Training Loss 0.1875\t Accuracy 0.8627\n",
      "Epoch [6][20]\t Batch [100][5500]\t Training Loss 0.2010\t Accuracy 0.8505\n",
      "Epoch [6][20]\t Batch [150][5500]\t Training Loss 0.2086\t Accuracy 0.8364\n",
      "Epoch [6][20]\t Batch [200][5500]\t Training Loss 0.2041\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [250][5500]\t Training Loss 0.2000\t Accuracy 0.8562\n",
      "Epoch [6][20]\t Batch [300][5500]\t Training Loss 0.1980\t Accuracy 0.8591\n",
      "Epoch [6][20]\t Batch [350][5500]\t Training Loss 0.1977\t Accuracy 0.8601\n",
      "Epoch [6][20]\t Batch [400][5500]\t Training Loss 0.1981\t Accuracy 0.8603\n",
      "Epoch [6][20]\t Batch [450][5500]\t Training Loss 0.1988\t Accuracy 0.8605\n",
      "Epoch [6][20]\t Batch [500][5500]\t Training Loss 0.1984\t Accuracy 0.8621\n",
      "Epoch [6][20]\t Batch [550][5500]\t Training Loss 0.1979\t Accuracy 0.8610\n",
      "Epoch [6][20]\t Batch [600][5500]\t Training Loss 0.1974\t Accuracy 0.8624\n",
      "Epoch [6][20]\t Batch [650][5500]\t Training Loss 0.1968\t Accuracy 0.8637\n",
      "Epoch [6][20]\t Batch [700][5500]\t Training Loss 0.1969\t Accuracy 0.8631\n",
      "Epoch [6][20]\t Batch [750][5500]\t Training Loss 0.1982\t Accuracy 0.8603\n",
      "Epoch [6][20]\t Batch [800][5500]\t Training Loss 0.1991\t Accuracy 0.8596\n",
      "Epoch [6][20]\t Batch [850][5500]\t Training Loss 0.2000\t Accuracy 0.8584\n",
      "Epoch [6][20]\t Batch [900][5500]\t Training Loss 0.2013\t Accuracy 0.8557\n",
      "Epoch [6][20]\t Batch [950][5500]\t Training Loss 0.2008\t Accuracy 0.8565\n",
      "Epoch [6][20]\t Batch [1000][5500]\t Training Loss 0.1999\t Accuracy 0.8578\n",
      "Epoch [6][20]\t Batch [1050][5500]\t Training Loss 0.1994\t Accuracy 0.8589\n",
      "Epoch [6][20]\t Batch [1100][5500]\t Training Loss 0.1990\t Accuracy 0.8602\n",
      "Epoch [6][20]\t Batch [1150][5500]\t Training Loss 0.1987\t Accuracy 0.8608\n",
      "Epoch [6][20]\t Batch [1200][5500]\t Training Loss 0.1997\t Accuracy 0.8588\n",
      "Epoch [6][20]\t Batch [1250][5500]\t Training Loss 0.2000\t Accuracy 0.8580\n",
      "Epoch [6][20]\t Batch [1300][5500]\t Training Loss 0.2007\t Accuracy 0.8566\n",
      "Epoch [6][20]\t Batch [1350][5500]\t Training Loss 0.2010\t Accuracy 0.8561\n",
      "Epoch [6][20]\t Batch [1400][5500]\t Training Loss 0.2017\t Accuracy 0.8546\n",
      "Epoch [6][20]\t Batch [1450][5500]\t Training Loss 0.2027\t Accuracy 0.8529\n",
      "Epoch [6][20]\t Batch [1500][5500]\t Training Loss 0.2038\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [1550][5500]\t Training Loss 0.2035\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [1600][5500]\t Training Loss 0.2040\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [1650][5500]\t Training Loss 0.2038\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [1700][5500]\t Training Loss 0.2040\t Accuracy 0.8504\n",
      "Epoch [6][20]\t Batch [1750][5500]\t Training Loss 0.2041\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [1800][5500]\t Training Loss 0.2047\t Accuracy 0.8498\n",
      "Epoch [6][20]\t Batch [1850][5500]\t Training Loss 0.2043\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [1900][5500]\t Training Loss 0.2037\t Accuracy 0.8524\n",
      "Epoch [6][20]\t Batch [1950][5500]\t Training Loss 0.2038\t Accuracy 0.8521\n",
      "Epoch [6][20]\t Batch [2000][5500]\t Training Loss 0.2036\t Accuracy 0.8526\n",
      "Epoch [6][20]\t Batch [2050][5500]\t Training Loss 0.2035\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2100][5500]\t Training Loss 0.2037\t Accuracy 0.8526\n",
      "Epoch [6][20]\t Batch [2150][5500]\t Training Loss 0.2034\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2200][5500]\t Training Loss 0.2029\t Accuracy 0.8537\n",
      "Epoch [6][20]\t Batch [2250][5500]\t Training Loss 0.2032\t Accuracy 0.8530\n",
      "Epoch [6][20]\t Batch [2300][5500]\t Training Loss 0.2036\t Accuracy 0.8529\n",
      "Epoch [6][20]\t Batch [2350][5500]\t Training Loss 0.2036\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2400][5500]\t Training Loss 0.2037\t Accuracy 0.8531\n",
      "Epoch [6][20]\t Batch [2450][5500]\t Training Loss 0.2037\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2500][5500]\t Training Loss 0.2039\t Accuracy 0.8529\n",
      "Epoch [6][20]\t Batch [2550][5500]\t Training Loss 0.2035\t Accuracy 0.8531\n",
      "Epoch [6][20]\t Batch [2600][5500]\t Training Loss 0.2034\t Accuracy 0.8530\n",
      "Epoch [6][20]\t Batch [2650][5500]\t Training Loss 0.2034\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2700][5500]\t Training Loss 0.2036\t Accuracy 0.8529\n",
      "Epoch [6][20]\t Batch [2750][5500]\t Training Loss 0.2039\t Accuracy 0.8523\n",
      "Epoch [6][20]\t Batch [2800][5500]\t Training Loss 0.2036\t Accuracy 0.8528\n",
      "Epoch [6][20]\t Batch [2850][5500]\t Training Loss 0.2034\t Accuracy 0.8531\n",
      "Epoch [6][20]\t Batch [2900][5500]\t Training Loss 0.2033\t Accuracy 0.8532\n",
      "Epoch [6][20]\t Batch [2950][5500]\t Training Loss 0.2034\t Accuracy 0.8528\n",
      "Epoch [6][20]\t Batch [3000][5500]\t Training Loss 0.2037\t Accuracy 0.8522\n",
      "Epoch [6][20]\t Batch [3050][5500]\t Training Loss 0.2039\t Accuracy 0.8520\n",
      "Epoch [6][20]\t Batch [3100][5500]\t Training Loss 0.2041\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [3150][5500]\t Training Loss 0.2043\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [3200][5500]\t Training Loss 0.2045\t Accuracy 0.8506\n",
      "Epoch [6][20]\t Batch [3250][5500]\t Training Loss 0.2048\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [3300][5500]\t Training Loss 0.2047\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [3350][5500]\t Training Loss 0.2047\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [3400][5500]\t Training Loss 0.2043\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [3450][5500]\t Training Loss 0.2042\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [3500][5500]\t Training Loss 0.2044\t Accuracy 0.8506\n",
      "Epoch [6][20]\t Batch [3550][5500]\t Training Loss 0.2043\t Accuracy 0.8508\n",
      "Epoch [6][20]\t Batch [3600][5500]\t Training Loss 0.2043\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [3650][5500]\t Training Loss 0.2043\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [3700][5500]\t Training Loss 0.2042\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [3750][5500]\t Training Loss 0.2045\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [3800][5500]\t Training Loss 0.2046\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [3850][5500]\t Training Loss 0.2045\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [3900][5500]\t Training Loss 0.2043\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [3950][5500]\t Training Loss 0.2043\t Accuracy 0.8516\n",
      "Epoch [6][20]\t Batch [4000][5500]\t Training Loss 0.2044\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [4050][5500]\t Training Loss 0.2044\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [4100][5500]\t Training Loss 0.2042\t Accuracy 0.8516\n",
      "Epoch [6][20]\t Batch [4150][5500]\t Training Loss 0.2044\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [4200][5500]\t Training Loss 0.2044\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [4250][5500]\t Training Loss 0.2047\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [4300][5500]\t Training Loss 0.2047\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [4350][5500]\t Training Loss 0.2045\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [4400][5500]\t Training Loss 0.2045\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [4450][5500]\t Training Loss 0.2047\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [4500][5500]\t Training Loss 0.2046\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [4550][5500]\t Training Loss 0.2046\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [4600][5500]\t Training Loss 0.2047\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [4650][5500]\t Training Loss 0.2048\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [4700][5500]\t Training Loss 0.2047\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [4750][5500]\t Training Loss 0.2047\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [4800][5500]\t Training Loss 0.2047\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [4850][5500]\t Training Loss 0.2045\t Accuracy 0.8517\n",
      "Epoch [6][20]\t Batch [4900][5500]\t Training Loss 0.2044\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [4950][5500]\t Training Loss 0.2046\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [5000][5500]\t Training Loss 0.2048\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [5050][5500]\t Training Loss 0.2049\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [5100][5500]\t Training Loss 0.2048\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [5150][5500]\t Training Loss 0.2046\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [5200][5500]\t Training Loss 0.2045\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [5250][5500]\t Training Loss 0.2045\t Accuracy 0.8516\n",
      "Epoch [6][20]\t Batch [5300][5500]\t Training Loss 0.2045\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [5350][5500]\t Training Loss 0.2044\t Accuracy 0.8517\n",
      "Epoch [6][20]\t Batch [5400][5500]\t Training Loss 0.2043\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [5450][5500]\t Training Loss 0.2042\t Accuracy 0.8520\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2042\t Average training accuracy 0.8519\n",
      "Epoch [6]\t Average validation loss 0.1851\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [7][20]\t Batch [0][5500]\t Training Loss 0.1321\t Accuracy 1.0000\n",
      "Epoch [7][20]\t Batch [50][5500]\t Training Loss 0.1842\t Accuracy 0.8647\n",
      "Epoch [7][20]\t Batch [100][5500]\t Training Loss 0.1976\t Accuracy 0.8535\n",
      "Epoch [7][20]\t Batch [150][5500]\t Training Loss 0.2053\t Accuracy 0.8391\n",
      "Epoch [7][20]\t Batch [200][5500]\t Training Loss 0.2007\t Accuracy 0.8507\n",
      "Epoch [7][20]\t Batch [250][5500]\t Training Loss 0.1966\t Accuracy 0.8590\n",
      "Epoch [7][20]\t Batch [300][5500]\t Training Loss 0.1946\t Accuracy 0.8611\n",
      "Epoch [7][20]\t Batch [350][5500]\t Training Loss 0.1943\t Accuracy 0.8621\n",
      "Epoch [7][20]\t Batch [400][5500]\t Training Loss 0.1947\t Accuracy 0.8623\n",
      "Epoch [7][20]\t Batch [450][5500]\t Training Loss 0.1953\t Accuracy 0.8625\n",
      "Epoch [7][20]\t Batch [500][5500]\t Training Loss 0.1949\t Accuracy 0.8641\n",
      "Epoch [7][20]\t Batch [550][5500]\t Training Loss 0.1945\t Accuracy 0.8626\n",
      "Epoch [7][20]\t Batch [600][5500]\t Training Loss 0.1939\t Accuracy 0.8642\n",
      "Epoch [7][20]\t Batch [650][5500]\t Training Loss 0.1933\t Accuracy 0.8657\n",
      "Epoch [7][20]\t Batch [700][5500]\t Training Loss 0.1934\t Accuracy 0.8649\n",
      "Epoch [7][20]\t Batch [750][5500]\t Training Loss 0.1947\t Accuracy 0.8622\n",
      "Epoch [7][20]\t Batch [800][5500]\t Training Loss 0.1956\t Accuracy 0.8614\n",
      "Epoch [7][20]\t Batch [850][5500]\t Training Loss 0.1965\t Accuracy 0.8606\n",
      "Epoch [7][20]\t Batch [900][5500]\t Training Loss 0.1978\t Accuracy 0.8580\n",
      "Epoch [7][20]\t Batch [950][5500]\t Training Loss 0.1974\t Accuracy 0.8588\n",
      "Epoch [7][20]\t Batch [1000][5500]\t Training Loss 0.1964\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [1050][5500]\t Training Loss 0.1960\t Accuracy 0.8612\n",
      "Epoch [7][20]\t Batch [1100][5500]\t Training Loss 0.1955\t Accuracy 0.8624\n",
      "Epoch [7][20]\t Batch [1150][5500]\t Training Loss 0.1952\t Accuracy 0.8629\n",
      "Epoch [7][20]\t Batch [1200][5500]\t Training Loss 0.1963\t Accuracy 0.8609\n",
      "Epoch [7][20]\t Batch [1250][5500]\t Training Loss 0.1966\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [1300][5500]\t Training Loss 0.1973\t Accuracy 0.8586\n",
      "Epoch [7][20]\t Batch [1350][5500]\t Training Loss 0.1976\t Accuracy 0.8581\n",
      "Epoch [7][20]\t Batch [1400][5500]\t Training Loss 0.1982\t Accuracy 0.8565\n",
      "Epoch [7][20]\t Batch [1450][5500]\t Training Loss 0.1992\t Accuracy 0.8549\n",
      "Epoch [7][20]\t Batch [1500][5500]\t Training Loss 0.2003\t Accuracy 0.8530\n",
      "Epoch [7][20]\t Batch [1550][5500]\t Training Loss 0.2001\t Accuracy 0.8534\n",
      "Epoch [7][20]\t Batch [1600][5500]\t Training Loss 0.2005\t Accuracy 0.8523\n",
      "Epoch [7][20]\t Batch [1650][5500]\t Training Loss 0.2003\t Accuracy 0.8526\n",
      "Epoch [7][20]\t Batch [1700][5500]\t Training Loss 0.2005\t Accuracy 0.8526\n",
      "Epoch [7][20]\t Batch [1750][5500]\t Training Loss 0.2006\t Accuracy 0.8524\n",
      "Epoch [7][20]\t Batch [1800][5500]\t Training Loss 0.2013\t Accuracy 0.8520\n",
      "Epoch [7][20]\t Batch [1850][5500]\t Training Loss 0.2008\t Accuracy 0.8532\n",
      "Epoch [7][20]\t Batch [1900][5500]\t Training Loss 0.2002\t Accuracy 0.8545\n",
      "Epoch [7][20]\t Batch [1950][5500]\t Training Loss 0.2003\t Accuracy 0.8545\n",
      "Epoch [7][20]\t Batch [2000][5500]\t Training Loss 0.2001\t Accuracy 0.8550\n",
      "Epoch [7][20]\t Batch [2050][5500]\t Training Loss 0.2001\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [2100][5500]\t Training Loss 0.2002\t Accuracy 0.8552\n",
      "Epoch [7][20]\t Batch [2150][5500]\t Training Loss 0.2000\t Accuracy 0.8556\n",
      "Epoch [7][20]\t Batch [2200][5500]\t Training Loss 0.1995\t Accuracy 0.8561\n",
      "Epoch [7][20]\t Batch [2250][5500]\t Training Loss 0.1997\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [2300][5500]\t Training Loss 0.2002\t Accuracy 0.8555\n",
      "Epoch [7][20]\t Batch [2350][5500]\t Training Loss 0.2002\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [2400][5500]\t Training Loss 0.2003\t Accuracy 0.8558\n",
      "Epoch [7][20]\t Batch [2450][5500]\t Training Loss 0.2002\t Accuracy 0.8558\n",
      "Epoch [7][20]\t Batch [2500][5500]\t Training Loss 0.2004\t Accuracy 0.8556\n",
      "Epoch [7][20]\t Batch [2550][5500]\t Training Loss 0.2001\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [2600][5500]\t Training Loss 0.2000\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [2650][5500]\t Training Loss 0.1999\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [2700][5500]\t Training Loss 0.2002\t Accuracy 0.8555\n",
      "Epoch [7][20]\t Batch [2750][5500]\t Training Loss 0.2004\t Accuracy 0.8550\n",
      "Epoch [7][20]\t Batch [2800][5500]\t Training Loss 0.2002\t Accuracy 0.8554\n",
      "Epoch [7][20]\t Batch [2850][5500]\t Training Loss 0.2000\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [2900][5500]\t Training Loss 0.1998\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [2950][5500]\t Training Loss 0.2000\t Accuracy 0.8554\n",
      "Epoch [7][20]\t Batch [3000][5500]\t Training Loss 0.2002\t Accuracy 0.8548\n",
      "Epoch [7][20]\t Batch [3050][5500]\t Training Loss 0.2004\t Accuracy 0.8546\n",
      "Epoch [7][20]\t Batch [3100][5500]\t Training Loss 0.2007\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [3150][5500]\t Training Loss 0.2009\t Accuracy 0.8536\n",
      "Epoch [7][20]\t Batch [3200][5500]\t Training Loss 0.2010\t Accuracy 0.8533\n",
      "Epoch [7][20]\t Batch [3250][5500]\t Training Loss 0.2014\t Accuracy 0.8526\n",
      "Epoch [7][20]\t Batch [3300][5500]\t Training Loss 0.2013\t Accuracy 0.8529\n",
      "Epoch [7][20]\t Batch [3350][5500]\t Training Loss 0.2013\t Accuracy 0.8529\n",
      "Epoch [7][20]\t Batch [3400][5500]\t Training Loss 0.2009\t Accuracy 0.8538\n",
      "Epoch [7][20]\t Batch [3450][5500]\t Training Loss 0.2007\t Accuracy 0.8539\n",
      "Epoch [7][20]\t Batch [3500][5500]\t Training Loss 0.2009\t Accuracy 0.8532\n",
      "Epoch [7][20]\t Batch [3550][5500]\t Training Loss 0.2009\t Accuracy 0.8534\n",
      "Epoch [7][20]\t Batch [3600][5500]\t Training Loss 0.2009\t Accuracy 0.8535\n",
      "Epoch [7][20]\t Batch [3650][5500]\t Training Loss 0.2009\t Accuracy 0.8536\n",
      "Epoch [7][20]\t Batch [3700][5500]\t Training Loss 0.2007\t Accuracy 0.8543\n",
      "Epoch [7][20]\t Batch [3750][5500]\t Training Loss 0.2011\t Accuracy 0.8537\n",
      "Epoch [7][20]\t Batch [3800][5500]\t Training Loss 0.2012\t Accuracy 0.8536\n",
      "Epoch [7][20]\t Batch [3850][5500]\t Training Loss 0.2011\t Accuracy 0.8539\n",
      "Epoch [7][20]\t Batch [3900][5500]\t Training Loss 0.2009\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [3950][5500]\t Training Loss 0.2008\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [4000][5500]\t Training Loss 0.2010\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [4050][5500]\t Training Loss 0.2010\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [4100][5500]\t Training Loss 0.2008\t Accuracy 0.8542\n",
      "Epoch [7][20]\t Batch [4150][5500]\t Training Loss 0.2010\t Accuracy 0.8538\n",
      "Epoch [7][20]\t Batch [4200][5500]\t Training Loss 0.2010\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [4250][5500]\t Training Loss 0.2013\t Accuracy 0.8537\n",
      "Epoch [7][20]\t Batch [4300][5500]\t Training Loss 0.2013\t Accuracy 0.8538\n",
      "Epoch [7][20]\t Batch [4350][5500]\t Training Loss 0.2011\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [4400][5500]\t Training Loss 0.2011\t Accuracy 0.8539\n",
      "Epoch [7][20]\t Batch [4450][5500]\t Training Loss 0.2013\t Accuracy 0.8538\n",
      "Epoch [7][20]\t Batch [4500][5500]\t Training Loss 0.2012\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [4550][5500]\t Training Loss 0.2012\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [4600][5500]\t Training Loss 0.2013\t Accuracy 0.8542\n",
      "Epoch [7][20]\t Batch [4650][5500]\t Training Loss 0.2015\t Accuracy 0.8539\n",
      "Epoch [7][20]\t Batch [4700][5500]\t Training Loss 0.2013\t Accuracy 0.8543\n",
      "Epoch [7][20]\t Batch [4750][5500]\t Training Loss 0.2013\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [4800][5500]\t Training Loss 0.2013\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [4850][5500]\t Training Loss 0.2011\t Accuracy 0.8545\n",
      "Epoch [7][20]\t Batch [4900][5500]\t Training Loss 0.2010\t Accuracy 0.8546\n",
      "Epoch [7][20]\t Batch [4950][5500]\t Training Loss 0.2012\t Accuracy 0.8544\n",
      "Epoch [7][20]\t Batch [5000][5500]\t Training Loss 0.2014\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [5050][5500]\t Training Loss 0.2015\t Accuracy 0.8540\n",
      "Epoch [7][20]\t Batch [5100][5500]\t Training Loss 0.2014\t Accuracy 0.8541\n",
      "Epoch [7][20]\t Batch [5150][5500]\t Training Loss 0.2013\t Accuracy 0.8542\n",
      "Epoch [7][20]\t Batch [5200][5500]\t Training Loss 0.2011\t Accuracy 0.8546\n",
      "Epoch [7][20]\t Batch [5250][5500]\t Training Loss 0.2011\t Accuracy 0.8544\n",
      "Epoch [7][20]\t Batch [5300][5500]\t Training Loss 0.2011\t Accuracy 0.8543\n",
      "Epoch [7][20]\t Batch [5350][5500]\t Training Loss 0.2010\t Accuracy 0.8544\n",
      "Epoch [7][20]\t Batch [5400][5500]\t Training Loss 0.2009\t Accuracy 0.8546\n",
      "Epoch [7][20]\t Batch [5450][5500]\t Training Loss 0.2009\t Accuracy 0.8547\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2009\t Average training accuracy 0.8546\n",
      "Epoch [7]\t Average validation loss 0.1819\t Average validation accuracy 0.8914\n",
      "\n",
      "Epoch [8][20]\t Batch [0][5500]\t Training Loss 0.1287\t Accuracy 1.0000\n",
      "Epoch [8][20]\t Batch [50][5500]\t Training Loss 0.1812\t Accuracy 0.8647\n",
      "Epoch [8][20]\t Batch [100][5500]\t Training Loss 0.1945\t Accuracy 0.8564\n",
      "Epoch [8][20]\t Batch [150][5500]\t Training Loss 0.2022\t Accuracy 0.8424\n",
      "Epoch [8][20]\t Batch [200][5500]\t Training Loss 0.1976\t Accuracy 0.8532\n",
      "Epoch [8][20]\t Batch [250][5500]\t Training Loss 0.1935\t Accuracy 0.8618\n",
      "Epoch [8][20]\t Batch [300][5500]\t Training Loss 0.1915\t Accuracy 0.8635\n",
      "Epoch [8][20]\t Batch [350][5500]\t Training Loss 0.1912\t Accuracy 0.8641\n",
      "Epoch [8][20]\t Batch [400][5500]\t Training Loss 0.1914\t Accuracy 0.8643\n",
      "Epoch [8][20]\t Batch [450][5500]\t Training Loss 0.1921\t Accuracy 0.8652\n",
      "Epoch [8][20]\t Batch [500][5500]\t Training Loss 0.1916\t Accuracy 0.8665\n",
      "Epoch [8][20]\t Batch [550][5500]\t Training Loss 0.1912\t Accuracy 0.8650\n",
      "Epoch [8][20]\t Batch [600][5500]\t Training Loss 0.1907\t Accuracy 0.8666\n",
      "Epoch [8][20]\t Batch [650][5500]\t Training Loss 0.1900\t Accuracy 0.8680\n",
      "Epoch [8][20]\t Batch [700][5500]\t Training Loss 0.1902\t Accuracy 0.8670\n",
      "Epoch [8][20]\t Batch [750][5500]\t Training Loss 0.1915\t Accuracy 0.8643\n",
      "Epoch [8][20]\t Batch [800][5500]\t Training Loss 0.1923\t Accuracy 0.8639\n",
      "Epoch [8][20]\t Batch [850][5500]\t Training Loss 0.1933\t Accuracy 0.8632\n",
      "Epoch [8][20]\t Batch [900][5500]\t Training Loss 0.1946\t Accuracy 0.8608\n",
      "Epoch [8][20]\t Batch [950][5500]\t Training Loss 0.1941\t Accuracy 0.8612\n",
      "Epoch [8][20]\t Batch [1000][5500]\t Training Loss 0.1931\t Accuracy 0.8623\n",
      "Epoch [8][20]\t Batch [1050][5500]\t Training Loss 0.1927\t Accuracy 0.8636\n",
      "Epoch [8][20]\t Batch [1100][5500]\t Training Loss 0.1923\t Accuracy 0.8651\n",
      "Epoch [8][20]\t Batch [1150][5500]\t Training Loss 0.1919\t Accuracy 0.8655\n",
      "Epoch [8][20]\t Batch [1200][5500]\t Training Loss 0.1931\t Accuracy 0.8634\n",
      "Epoch [8][20]\t Batch [1250][5500]\t Training Loss 0.1934\t Accuracy 0.8624\n",
      "Epoch [8][20]\t Batch [1300][5500]\t Training Loss 0.1941\t Accuracy 0.8611\n",
      "Epoch [8][20]\t Batch [1350][5500]\t Training Loss 0.1944\t Accuracy 0.8605\n",
      "Epoch [8][20]\t Batch [1400][5500]\t Training Loss 0.1950\t Accuracy 0.8588\n",
      "Epoch [8][20]\t Batch [1450][5500]\t Training Loss 0.1960\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [1500][5500]\t Training Loss 0.1971\t Accuracy 0.8554\n",
      "Epoch [8][20]\t Batch [1550][5500]\t Training Loss 0.1968\t Accuracy 0.8557\n",
      "Epoch [8][20]\t Batch [1600][5500]\t Training Loss 0.1973\t Accuracy 0.8547\n",
      "Epoch [8][20]\t Batch [1650][5500]\t Training Loss 0.1971\t Accuracy 0.8551\n",
      "Epoch [8][20]\t Batch [1700][5500]\t Training Loss 0.1973\t Accuracy 0.8551\n",
      "Epoch [8][20]\t Batch [1750][5500]\t Training Loss 0.1974\t Accuracy 0.8550\n",
      "Epoch [8][20]\t Batch [1800][5500]\t Training Loss 0.1981\t Accuracy 0.8547\n",
      "Epoch [8][20]\t Batch [1850][5500]\t Training Loss 0.1976\t Accuracy 0.8560\n",
      "Epoch [8][20]\t Batch [1900][5500]\t Training Loss 0.1970\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [1950][5500]\t Training Loss 0.1971\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [2000][5500]\t Training Loss 0.1968\t Accuracy 0.8578\n",
      "Epoch [8][20]\t Batch [2050][5500]\t Training Loss 0.1968\t Accuracy 0.8586\n",
      "Epoch [8][20]\t Batch [2100][5500]\t Training Loss 0.1970\t Accuracy 0.8580\n",
      "Epoch [8][20]\t Batch [2150][5500]\t Training Loss 0.1967\t Accuracy 0.8583\n",
      "Epoch [8][20]\t Batch [2200][5500]\t Training Loss 0.1962\t Accuracy 0.8587\n",
      "Epoch [8][20]\t Batch [2250][5500]\t Training Loss 0.1965\t Accuracy 0.8585\n",
      "Epoch [8][20]\t Batch [2300][5500]\t Training Loss 0.1969\t Accuracy 0.8582\n",
      "Epoch [8][20]\t Batch [2350][5500]\t Training Loss 0.1969\t Accuracy 0.8587\n",
      "Epoch [8][20]\t Batch [2400][5500]\t Training Loss 0.1970\t Accuracy 0.8586\n",
      "Epoch [8][20]\t Batch [2450][5500]\t Training Loss 0.1970\t Accuracy 0.8586\n",
      "Epoch [8][20]\t Batch [2500][5500]\t Training Loss 0.1972\t Accuracy 0.8584\n",
      "Epoch [8][20]\t Batch [2550][5500]\t Training Loss 0.1968\t Accuracy 0.8587\n",
      "Epoch [8][20]\t Batch [2600][5500]\t Training Loss 0.1967\t Accuracy 0.8584\n",
      "Epoch [8][20]\t Batch [2650][5500]\t Training Loss 0.1967\t Accuracy 0.8586\n",
      "Epoch [8][20]\t Batch [2700][5500]\t Training Loss 0.1969\t Accuracy 0.8583\n",
      "Epoch [8][20]\t Batch [2750][5500]\t Training Loss 0.1971\t Accuracy 0.8579\n",
      "Epoch [8][20]\t Batch [2800][5500]\t Training Loss 0.1969\t Accuracy 0.8583\n",
      "Epoch [8][20]\t Batch [2850][5500]\t Training Loss 0.1967\t Accuracy 0.8587\n",
      "Epoch [8][20]\t Batch [2900][5500]\t Training Loss 0.1966\t Accuracy 0.8587\n",
      "Epoch [8][20]\t Batch [2950][5500]\t Training Loss 0.1967\t Accuracy 0.8584\n",
      "Epoch [8][20]\t Batch [3000][5500]\t Training Loss 0.1970\t Accuracy 0.8577\n",
      "Epoch [8][20]\t Batch [3050][5500]\t Training Loss 0.1971\t Accuracy 0.8575\n",
      "Epoch [8][20]\t Batch [3100][5500]\t Training Loss 0.1974\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [3150][5500]\t Training Loss 0.1977\t Accuracy 0.8564\n",
      "Epoch [8][20]\t Batch [3200][5500]\t Training Loss 0.1978\t Accuracy 0.8560\n",
      "Epoch [8][20]\t Batch [3250][5500]\t Training Loss 0.1981\t Accuracy 0.8553\n",
      "Epoch [8][20]\t Batch [3300][5500]\t Training Loss 0.1980\t Accuracy 0.8556\n",
      "Epoch [8][20]\t Batch [3350][5500]\t Training Loss 0.1981\t Accuracy 0.8556\n",
      "Epoch [8][20]\t Batch [3400][5500]\t Training Loss 0.1976\t Accuracy 0.8565\n",
      "Epoch [8][20]\t Batch [3450][5500]\t Training Loss 0.1975\t Accuracy 0.8567\n",
      "Epoch [8][20]\t Batch [3500][5500]\t Training Loss 0.1977\t Accuracy 0.8559\n",
      "Epoch [8][20]\t Batch [3550][5500]\t Training Loss 0.1976\t Accuracy 0.8560\n",
      "Epoch [8][20]\t Batch [3600][5500]\t Training Loss 0.1976\t Accuracy 0.8562\n",
      "Epoch [8][20]\t Batch [3650][5500]\t Training Loss 0.1976\t Accuracy 0.8563\n",
      "Epoch [8][20]\t Batch [3700][5500]\t Training Loss 0.1975\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [3750][5500]\t Training Loss 0.1979\t Accuracy 0.8564\n",
      "Epoch [8][20]\t Batch [3800][5500]\t Training Loss 0.1979\t Accuracy 0.8564\n",
      "Epoch [8][20]\t Batch [3850][5500]\t Training Loss 0.1978\t Accuracy 0.8566\n",
      "Epoch [8][20]\t Batch [3900][5500]\t Training Loss 0.1977\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [3950][5500]\t Training Loss 0.1976\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4000][5500]\t Training Loss 0.1978\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4050][5500]\t Training Loss 0.1977\t Accuracy 0.8569\n",
      "Epoch [8][20]\t Batch [4100][5500]\t Training Loss 0.1976\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [4150][5500]\t Training Loss 0.1978\t Accuracy 0.8567\n",
      "Epoch [8][20]\t Batch [4200][5500]\t Training Loss 0.1978\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [4250][5500]\t Training Loss 0.1981\t Accuracy 0.8566\n",
      "Epoch [8][20]\t Batch [4300][5500]\t Training Loss 0.1980\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4350][5500]\t Training Loss 0.1979\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [4400][5500]\t Training Loss 0.1979\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4450][5500]\t Training Loss 0.1981\t Accuracy 0.8567\n",
      "Epoch [8][20]\t Batch [4500][5500]\t Training Loss 0.1979\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [4550][5500]\t Training Loss 0.1980\t Accuracy 0.8569\n",
      "Epoch [8][20]\t Batch [4600][5500]\t Training Loss 0.1981\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [4650][5500]\t Training Loss 0.1982\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4700][5500]\t Training Loss 0.1980\t Accuracy 0.8571\n",
      "Epoch [8][20]\t Batch [4750][5500]\t Training Loss 0.1981\t Accuracy 0.8569\n",
      "Epoch [8][20]\t Batch [4800][5500]\t Training Loss 0.1981\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [4850][5500]\t Training Loss 0.1979\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [4900][5500]\t Training Loss 0.1978\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [4950][5500]\t Training Loss 0.1979\t Accuracy 0.8571\n",
      "Epoch [8][20]\t Batch [5000][5500]\t Training Loss 0.1982\t Accuracy 0.8569\n",
      "Epoch [8][20]\t Batch [5050][5500]\t Training Loss 0.1983\t Accuracy 0.8568\n",
      "Epoch [8][20]\t Batch [5100][5500]\t Training Loss 0.1982\t Accuracy 0.8569\n",
      "Epoch [8][20]\t Batch [5150][5500]\t Training Loss 0.1980\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [5200][5500]\t Training Loss 0.1979\t Accuracy 0.8574\n",
      "Epoch [8][20]\t Batch [5250][5500]\t Training Loss 0.1979\t Accuracy 0.8572\n",
      "Epoch [8][20]\t Batch [5300][5500]\t Training Loss 0.1979\t Accuracy 0.8571\n",
      "Epoch [8][20]\t Batch [5350][5500]\t Training Loss 0.1978\t Accuracy 0.8572\n",
      "Epoch [8][20]\t Batch [5400][5500]\t Training Loss 0.1977\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [5450][5500]\t Training Loss 0.1977\t Accuracy 0.8574\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1977\t Average training accuracy 0.8573\n",
      "Epoch [8]\t Average validation loss 0.1789\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [9][20]\t Batch [0][5500]\t Training Loss 0.1255\t Accuracy 1.0000\n",
      "Epoch [9][20]\t Batch [50][5500]\t Training Loss 0.1782\t Accuracy 0.8667\n",
      "Epoch [9][20]\t Batch [100][5500]\t Training Loss 0.1914\t Accuracy 0.8634\n",
      "Epoch [9][20]\t Batch [150][5500]\t Training Loss 0.1992\t Accuracy 0.8483\n",
      "Epoch [9][20]\t Batch [200][5500]\t Training Loss 0.1945\t Accuracy 0.8572\n",
      "Epoch [9][20]\t Batch [250][5500]\t Training Loss 0.1905\t Accuracy 0.8653\n",
      "Epoch [9][20]\t Batch [300][5500]\t Training Loss 0.1884\t Accuracy 0.8664\n",
      "Epoch [9][20]\t Batch [350][5500]\t Training Loss 0.1881\t Accuracy 0.8667\n",
      "Epoch [9][20]\t Batch [400][5500]\t Training Loss 0.1883\t Accuracy 0.8666\n",
      "Epoch [9][20]\t Batch [450][5500]\t Training Loss 0.1890\t Accuracy 0.8676\n",
      "Epoch [9][20]\t Batch [500][5500]\t Training Loss 0.1884\t Accuracy 0.8687\n",
      "Epoch [9][20]\t Batch [550][5500]\t Training Loss 0.1880\t Accuracy 0.8673\n",
      "Epoch [9][20]\t Batch [600][5500]\t Training Loss 0.1875\t Accuracy 0.8689\n",
      "Epoch [9][20]\t Batch [650][5500]\t Training Loss 0.1869\t Accuracy 0.8707\n",
      "Epoch [9][20]\t Batch [700][5500]\t Training Loss 0.1870\t Accuracy 0.8698\n",
      "Epoch [9][20]\t Batch [750][5500]\t Training Loss 0.1883\t Accuracy 0.8674\n",
      "Epoch [9][20]\t Batch [800][5500]\t Training Loss 0.1891\t Accuracy 0.8669\n",
      "Epoch [9][20]\t Batch [850][5500]\t Training Loss 0.1901\t Accuracy 0.8662\n",
      "Epoch [9][20]\t Batch [900][5500]\t Training Loss 0.1914\t Accuracy 0.8636\n",
      "Epoch [9][20]\t Batch [950][5500]\t Training Loss 0.1909\t Accuracy 0.8640\n",
      "Epoch [9][20]\t Batch [1000][5500]\t Training Loss 0.1899\t Accuracy 0.8650\n",
      "Epoch [9][20]\t Batch [1050][5500]\t Training Loss 0.1895\t Accuracy 0.8663\n",
      "Epoch [9][20]\t Batch [1100][5500]\t Training Loss 0.1891\t Accuracy 0.8678\n",
      "Epoch [9][20]\t Batch [1150][5500]\t Training Loss 0.1888\t Accuracy 0.8682\n",
      "Epoch [9][20]\t Batch [1200][5500]\t Training Loss 0.1899\t Accuracy 0.8662\n",
      "Epoch [9][20]\t Batch [1250][5500]\t Training Loss 0.1902\t Accuracy 0.8653\n",
      "Epoch [9][20]\t Batch [1300][5500]\t Training Loss 0.1909\t Accuracy 0.8638\n",
      "Epoch [9][20]\t Batch [1350][5500]\t Training Loss 0.1912\t Accuracy 0.8631\n",
      "Epoch [9][20]\t Batch [1400][5500]\t Training Loss 0.1918\t Accuracy 0.8616\n",
      "Epoch [9][20]\t Batch [1450][5500]\t Training Loss 0.1928\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [1500][5500]\t Training Loss 0.1939\t Accuracy 0.8583\n",
      "Epoch [9][20]\t Batch [1550][5500]\t Training Loss 0.1936\t Accuracy 0.8586\n",
      "Epoch [9][20]\t Batch [1600][5500]\t Training Loss 0.1941\t Accuracy 0.8576\n",
      "Epoch [9][20]\t Batch [1650][5500]\t Training Loss 0.1939\t Accuracy 0.8583\n",
      "Epoch [9][20]\t Batch [1700][5500]\t Training Loss 0.1941\t Accuracy 0.8582\n",
      "Epoch [9][20]\t Batch [1750][5500]\t Training Loss 0.1942\t Accuracy 0.8580\n",
      "Epoch [9][20]\t Batch [1800][5500]\t Training Loss 0.1949\t Accuracy 0.8576\n",
      "Epoch [9][20]\t Batch [1850][5500]\t Training Loss 0.1944\t Accuracy 0.8588\n",
      "Epoch [9][20]\t Batch [1900][5500]\t Training Loss 0.1938\t Accuracy 0.8601\n",
      "Epoch [9][20]\t Batch [1950][5500]\t Training Loss 0.1939\t Accuracy 0.8601\n",
      "Epoch [9][20]\t Batch [2000][5500]\t Training Loss 0.1936\t Accuracy 0.8607\n",
      "Epoch [9][20]\t Batch [2050][5500]\t Training Loss 0.1936\t Accuracy 0.8613\n",
      "Epoch [9][20]\t Batch [2100][5500]\t Training Loss 0.1938\t Accuracy 0.8607\n",
      "Epoch [9][20]\t Batch [2150][5500]\t Training Loss 0.1935\t Accuracy 0.8610\n",
      "Epoch [9][20]\t Batch [2200][5500]\t Training Loss 0.1930\t Accuracy 0.8614\n",
      "Epoch [9][20]\t Batch [2250][5500]\t Training Loss 0.1933\t Accuracy 0.8613\n",
      "Epoch [9][20]\t Batch [2300][5500]\t Training Loss 0.1937\t Accuracy 0.8610\n",
      "Epoch [9][20]\t Batch [2350][5500]\t Training Loss 0.1937\t Accuracy 0.8615\n",
      "Epoch [9][20]\t Batch [2400][5500]\t Training Loss 0.1938\t Accuracy 0.8614\n",
      "Epoch [9][20]\t Batch [2450][5500]\t Training Loss 0.1938\t Accuracy 0.8614\n",
      "Epoch [9][20]\t Batch [2500][5500]\t Training Loss 0.1939\t Accuracy 0.8613\n",
      "Epoch [9][20]\t Batch [2550][5500]\t Training Loss 0.1936\t Accuracy 0.8615\n",
      "Epoch [9][20]\t Batch [2600][5500]\t Training Loss 0.1935\t Accuracy 0.8613\n",
      "Epoch [9][20]\t Batch [2650][5500]\t Training Loss 0.1935\t Accuracy 0.8615\n",
      "Epoch [9][20]\t Batch [2700][5500]\t Training Loss 0.1937\t Accuracy 0.8612\n",
      "Epoch [9][20]\t Batch [2750][5500]\t Training Loss 0.1939\t Accuracy 0.8609\n",
      "Epoch [9][20]\t Batch [2800][5500]\t Training Loss 0.1937\t Accuracy 0.8612\n",
      "Epoch [9][20]\t Batch [2850][5500]\t Training Loss 0.1935\t Accuracy 0.8617\n",
      "Epoch [9][20]\t Batch [2900][5500]\t Training Loss 0.1934\t Accuracy 0.8617\n",
      "Epoch [9][20]\t Batch [2950][5500]\t Training Loss 0.1935\t Accuracy 0.8613\n",
      "Epoch [9][20]\t Batch [3000][5500]\t Training Loss 0.1938\t Accuracy 0.8607\n",
      "Epoch [9][20]\t Batch [3050][5500]\t Training Loss 0.1939\t Accuracy 0.8607\n",
      "Epoch [9][20]\t Batch [3100][5500]\t Training Loss 0.1942\t Accuracy 0.8601\n",
      "Epoch [9][20]\t Batch [3150][5500]\t Training Loss 0.1945\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [3200][5500]\t Training Loss 0.1946\t Accuracy 0.8591\n",
      "Epoch [9][20]\t Batch [3250][5500]\t Training Loss 0.1950\t Accuracy 0.8584\n",
      "Epoch [9][20]\t Batch [3300][5500]\t Training Loss 0.1948\t Accuracy 0.8587\n",
      "Epoch [9][20]\t Batch [3350][5500]\t Training Loss 0.1949\t Accuracy 0.8587\n",
      "Epoch [9][20]\t Batch [3400][5500]\t Training Loss 0.1944\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [3450][5500]\t Training Loss 0.1943\t Accuracy 0.8598\n",
      "Epoch [9][20]\t Batch [3500][5500]\t Training Loss 0.1945\t Accuracy 0.8591\n",
      "Epoch [9][20]\t Batch [3550][5500]\t Training Loss 0.1945\t Accuracy 0.8592\n",
      "Epoch [9][20]\t Batch [3600][5500]\t Training Loss 0.1944\t Accuracy 0.8593\n",
      "Epoch [9][20]\t Batch [3650][5500]\t Training Loss 0.1944\t Accuracy 0.8593\n",
      "Epoch [9][20]\t Batch [3700][5500]\t Training Loss 0.1943\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [3750][5500]\t Training Loss 0.1947\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [3800][5500]\t Training Loss 0.1947\t Accuracy 0.8594\n",
      "Epoch [9][20]\t Batch [3850][5500]\t Training Loss 0.1947\t Accuracy 0.8596\n",
      "Epoch [9][20]\t Batch [3900][5500]\t Training Loss 0.1945\t Accuracy 0.8598\n",
      "Epoch [9][20]\t Batch [3950][5500]\t Training Loss 0.1944\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [4000][5500]\t Training Loss 0.1946\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [4050][5500]\t Training Loss 0.1945\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [4100][5500]\t Training Loss 0.1944\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [4150][5500]\t Training Loss 0.1946\t Accuracy 0.8596\n",
      "Epoch [9][20]\t Batch [4200][5500]\t Training Loss 0.1946\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [4250][5500]\t Training Loss 0.1949\t Accuracy 0.8594\n",
      "Epoch [9][20]\t Batch [4300][5500]\t Training Loss 0.1949\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [4350][5500]\t Training Loss 0.1947\t Accuracy 0.8598\n",
      "Epoch [9][20]\t Batch [4400][5500]\t Training Loss 0.1947\t Accuracy 0.8596\n",
      "Epoch [9][20]\t Batch [4450][5500]\t Training Loss 0.1949\t Accuracy 0.8594\n",
      "Epoch [9][20]\t Batch [4500][5500]\t Training Loss 0.1948\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [4550][5500]\t Training Loss 0.1948\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [4600][5500]\t Training Loss 0.1949\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [4650][5500]\t Training Loss 0.1951\t Accuracy 0.8594\n",
      "Epoch [9][20]\t Batch [4700][5500]\t Training Loss 0.1949\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [4750][5500]\t Training Loss 0.1949\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [4800][5500]\t Training Loss 0.1949\t Accuracy 0.8593\n",
      "Epoch [9][20]\t Batch [4850][5500]\t Training Loss 0.1947\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [4900][5500]\t Training Loss 0.1946\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [4950][5500]\t Training Loss 0.1948\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [5000][5500]\t Training Loss 0.1950\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [5050][5500]\t Training Loss 0.1951\t Accuracy 0.8594\n",
      "Epoch [9][20]\t Batch [5100][5500]\t Training Loss 0.1950\t Accuracy 0.8595\n",
      "Epoch [9][20]\t Batch [5150][5500]\t Training Loss 0.1949\t Accuracy 0.8596\n",
      "Epoch [9][20]\t Batch [5200][5500]\t Training Loss 0.1947\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [5250][5500]\t Training Loss 0.1947\t Accuracy 0.8598\n",
      "Epoch [9][20]\t Batch [5300][5500]\t Training Loss 0.1948\t Accuracy 0.8597\n",
      "Epoch [9][20]\t Batch [5350][5500]\t Training Loss 0.1946\t Accuracy 0.8598\n",
      "Epoch [9][20]\t Batch [5400][5500]\t Training Loss 0.1945\t Accuracy 0.8599\n",
      "Epoch [9][20]\t Batch [5450][5500]\t Training Loss 0.1945\t Accuracy 0.8600\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1945\t Average training accuracy 0.8599\n",
      "Epoch [9]\t Average validation loss 0.1758\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [10][20]\t Batch [0][5500]\t Training Loss 0.1224\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [50][5500]\t Training Loss 0.1753\t Accuracy 0.8647\n",
      "Epoch [10][20]\t Batch [100][5500]\t Training Loss 0.1884\t Accuracy 0.8644\n",
      "Epoch [10][20]\t Batch [150][5500]\t Training Loss 0.1962\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [200][5500]\t Training Loss 0.1915\t Accuracy 0.8577\n",
      "Epoch [10][20]\t Batch [250][5500]\t Training Loss 0.1875\t Accuracy 0.8653\n",
      "Epoch [10][20]\t Batch [300][5500]\t Training Loss 0.1854\t Accuracy 0.8668\n",
      "Epoch [10][20]\t Batch [350][5500]\t Training Loss 0.1850\t Accuracy 0.8681\n",
      "Epoch [10][20]\t Batch [400][5500]\t Training Loss 0.1852\t Accuracy 0.8678\n",
      "Epoch [10][20]\t Batch [450][5500]\t Training Loss 0.1858\t Accuracy 0.8690\n",
      "Epoch [10][20]\t Batch [500][5500]\t Training Loss 0.1853\t Accuracy 0.8701\n",
      "Epoch [10][20]\t Batch [550][5500]\t Training Loss 0.1849\t Accuracy 0.8686\n",
      "Epoch [10][20]\t Batch [600][5500]\t Training Loss 0.1844\t Accuracy 0.8707\n",
      "Epoch [10][20]\t Batch [650][5500]\t Training Loss 0.1837\t Accuracy 0.8727\n",
      "Epoch [10][20]\t Batch [700][5500]\t Training Loss 0.1838\t Accuracy 0.8716\n",
      "Epoch [10][20]\t Batch [750][5500]\t Training Loss 0.1851\t Accuracy 0.8692\n",
      "Epoch [10][20]\t Batch [800][5500]\t Training Loss 0.1860\t Accuracy 0.8688\n",
      "Epoch [10][20]\t Batch [850][5500]\t Training Loss 0.1869\t Accuracy 0.8680\n",
      "Epoch [10][20]\t Batch [900][5500]\t Training Loss 0.1882\t Accuracy 0.8654\n",
      "Epoch [10][20]\t Batch [950][5500]\t Training Loss 0.1877\t Accuracy 0.8658\n",
      "Epoch [10][20]\t Batch [1000][5500]\t Training Loss 0.1868\t Accuracy 0.8668\n",
      "Epoch [10][20]\t Batch [1050][5500]\t Training Loss 0.1864\t Accuracy 0.8681\n",
      "Epoch [10][20]\t Batch [1100][5500]\t Training Loss 0.1859\t Accuracy 0.8697\n",
      "Epoch [10][20]\t Batch [1150][5500]\t Training Loss 0.1856\t Accuracy 0.8699\n",
      "Epoch [10][20]\t Batch [1200][5500]\t Training Loss 0.1867\t Accuracy 0.8681\n",
      "Epoch [10][20]\t Batch [1250][5500]\t Training Loss 0.1870\t Accuracy 0.8673\n",
      "Epoch [10][20]\t Batch [1300][5500]\t Training Loss 0.1877\t Accuracy 0.8656\n",
      "Epoch [10][20]\t Batch [1350][5500]\t Training Loss 0.1880\t Accuracy 0.8648\n",
      "Epoch [10][20]\t Batch [1400][5500]\t Training Loss 0.1886\t Accuracy 0.8634\n",
      "Epoch [10][20]\t Batch [1450][5500]\t Training Loss 0.1896\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [1500][5500]\t Training Loss 0.1907\t Accuracy 0.8602\n",
      "Epoch [10][20]\t Batch [1550][5500]\t Training Loss 0.1905\t Accuracy 0.8605\n",
      "Epoch [10][20]\t Batch [1600][5500]\t Training Loss 0.1909\t Accuracy 0.8598\n",
      "Epoch [10][20]\t Batch [1650][5500]\t Training Loss 0.1907\t Accuracy 0.8604\n",
      "Epoch [10][20]\t Batch [1700][5500]\t Training Loss 0.1909\t Accuracy 0.8603\n",
      "Epoch [10][20]\t Batch [1750][5500]\t Training Loss 0.1910\t Accuracy 0.8601\n",
      "Epoch [10][20]\t Batch [1800][5500]\t Training Loss 0.1917\t Accuracy 0.8597\n",
      "Epoch [10][20]\t Batch [1850][5500]\t Training Loss 0.1912\t Accuracy 0.8608\n",
      "Epoch [10][20]\t Batch [1900][5500]\t Training Loss 0.1906\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [1950][5500]\t Training Loss 0.1907\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [2000][5500]\t Training Loss 0.1904\t Accuracy 0.8626\n",
      "Epoch [10][20]\t Batch [2050][5500]\t Training Loss 0.1904\t Accuracy 0.8634\n",
      "Epoch [10][20]\t Batch [2100][5500]\t Training Loss 0.1906\t Accuracy 0.8628\n",
      "Epoch [10][20]\t Batch [2150][5500]\t Training Loss 0.1903\t Accuracy 0.8631\n",
      "Epoch [10][20]\t Batch [2200][5500]\t Training Loss 0.1899\t Accuracy 0.8636\n",
      "Epoch [10][20]\t Batch [2250][5500]\t Training Loss 0.1901\t Accuracy 0.8634\n",
      "Epoch [10][20]\t Batch [2300][5500]\t Training Loss 0.1905\t Accuracy 0.8632\n",
      "Epoch [10][20]\t Batch [2350][5500]\t Training Loss 0.1905\t Accuracy 0.8636\n",
      "Epoch [10][20]\t Batch [2400][5500]\t Training Loss 0.1906\t Accuracy 0.8636\n",
      "Epoch [10][20]\t Batch [2450][5500]\t Training Loss 0.1906\t Accuracy 0.8636\n",
      "Epoch [10][20]\t Batch [2500][5500]\t Training Loss 0.1907\t Accuracy 0.8636\n",
      "Epoch [10][20]\t Batch [2550][5500]\t Training Loss 0.1904\t Accuracy 0.8639\n",
      "Epoch [10][20]\t Batch [2600][5500]\t Training Loss 0.1903\t Accuracy 0.8637\n",
      "Epoch [10][20]\t Batch [2650][5500]\t Training Loss 0.1903\t Accuracy 0.8639\n",
      "Epoch [10][20]\t Batch [2700][5500]\t Training Loss 0.1905\t Accuracy 0.8637\n",
      "Epoch [10][20]\t Batch [2750][5500]\t Training Loss 0.1907\t Accuracy 0.8634\n",
      "Epoch [10][20]\t Batch [2800][5500]\t Training Loss 0.1905\t Accuracy 0.8638\n",
      "Epoch [10][20]\t Batch [2850][5500]\t Training Loss 0.1903\t Accuracy 0.8642\n",
      "Epoch [10][20]\t Batch [2900][5500]\t Training Loss 0.1902\t Accuracy 0.8643\n",
      "Epoch [10][20]\t Batch [2950][5500]\t Training Loss 0.1903\t Accuracy 0.8640\n",
      "Epoch [10][20]\t Batch [3000][5500]\t Training Loss 0.1906\t Accuracy 0.8634\n",
      "Epoch [10][20]\t Batch [3050][5500]\t Training Loss 0.1907\t Accuracy 0.8633\n",
      "Epoch [10][20]\t Batch [3100][5500]\t Training Loss 0.1910\t Accuracy 0.8628\n",
      "Epoch [10][20]\t Batch [3150][5500]\t Training Loss 0.1913\t Accuracy 0.8622\n",
      "Epoch [10][20]\t Batch [3200][5500]\t Training Loss 0.1914\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [3250][5500]\t Training Loss 0.1918\t Accuracy 0.8610\n",
      "Epoch [10][20]\t Batch [3300][5500]\t Training Loss 0.1916\t Accuracy 0.8613\n",
      "Epoch [10][20]\t Batch [3350][5500]\t Training Loss 0.1917\t Accuracy 0.8613\n",
      "Epoch [10][20]\t Batch [3400][5500]\t Training Loss 0.1912\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [3450][5500]\t Training Loss 0.1911\t Accuracy 0.8624\n",
      "Epoch [10][20]\t Batch [3500][5500]\t Training Loss 0.1913\t Accuracy 0.8617\n",
      "Epoch [10][20]\t Batch [3550][5500]\t Training Loss 0.1913\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [3600][5500]\t Training Loss 0.1912\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [3650][5500]\t Training Loss 0.1912\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [3700][5500]\t Training Loss 0.1911\t Accuracy 0.8625\n",
      "Epoch [10][20]\t Batch [3750][5500]\t Training Loss 0.1915\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [3800][5500]\t Training Loss 0.1915\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [3850][5500]\t Training Loss 0.1914\t Accuracy 0.8622\n",
      "Epoch [10][20]\t Batch [3900][5500]\t Training Loss 0.1913\t Accuracy 0.8624\n",
      "Epoch [10][20]\t Batch [3950][5500]\t Training Loss 0.1912\t Accuracy 0.8624\n",
      "Epoch [10][20]\t Batch [4000][5500]\t Training Loss 0.1914\t Accuracy 0.8623\n",
      "Epoch [10][20]\t Batch [4050][5500]\t Training Loss 0.1913\t Accuracy 0.8623\n",
      "Epoch [10][20]\t Batch [4100][5500]\t Training Loss 0.1912\t Accuracy 0.8624\n",
      "Epoch [10][20]\t Batch [4150][5500]\t Training Loss 0.1914\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [4200][5500]\t Training Loss 0.1914\t Accuracy 0.8624\n",
      "Epoch [10][20]\t Batch [4250][5500]\t Training Loss 0.1917\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [4300][5500]\t Training Loss 0.1917\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [4350][5500]\t Training Loss 0.1915\t Accuracy 0.8623\n",
      "Epoch [10][20]\t Batch [4400][5500]\t Training Loss 0.1915\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [4450][5500]\t Training Loss 0.1917\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [4500][5500]\t Training Loss 0.1916\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [4550][5500]\t Training Loss 0.1916\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [4600][5500]\t Training Loss 0.1917\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [4650][5500]\t Training Loss 0.1919\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [4700][5500]\t Training Loss 0.1917\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [4750][5500]\t Training Loss 0.1917\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [4800][5500]\t Training Loss 0.1917\t Accuracy 0.8617\n",
      "Epoch [10][20]\t Batch [4850][5500]\t Training Loss 0.1915\t Accuracy 0.8622\n",
      "Epoch [10][20]\t Batch [4900][5500]\t Training Loss 0.1914\t Accuracy 0.8622\n",
      "Epoch [10][20]\t Batch [4950][5500]\t Training Loss 0.1916\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [5000][5500]\t Training Loss 0.1918\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [5050][5500]\t Training Loss 0.1919\t Accuracy 0.8617\n",
      "Epoch [10][20]\t Batch [5100][5500]\t Training Loss 0.1918\t Accuracy 0.8618\n",
      "Epoch [10][20]\t Batch [5150][5500]\t Training Loss 0.1917\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [5200][5500]\t Training Loss 0.1915\t Accuracy 0.8623\n",
      "Epoch [10][20]\t Batch [5250][5500]\t Training Loss 0.1916\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [5300][5500]\t Training Loss 0.1916\t Accuracy 0.8620\n",
      "Epoch [10][20]\t Batch [5350][5500]\t Training Loss 0.1914\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [5400][5500]\t Training Loss 0.1913\t Accuracy 0.8622\n",
      "Epoch [10][20]\t Batch [5450][5500]\t Training Loss 0.1913\t Accuracy 0.8624\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1913\t Average training accuracy 0.8622\n",
      "Epoch [10]\t Average validation loss 0.1727\t Average validation accuracy 0.8976\n",
      "\n",
      "Epoch [11][20]\t Batch [0][5500]\t Training Loss 0.1191\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [50][5500]\t Training Loss 0.1723\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [100][5500]\t Training Loss 0.1853\t Accuracy 0.8673\n",
      "Epoch [11][20]\t Batch [150][5500]\t Training Loss 0.1931\t Accuracy 0.8530\n",
      "Epoch [11][20]\t Batch [200][5500]\t Training Loss 0.1883\t Accuracy 0.8607\n",
      "Epoch [11][20]\t Batch [250][5500]\t Training Loss 0.1844\t Accuracy 0.8681\n",
      "Epoch [11][20]\t Batch [300][5500]\t Training Loss 0.1823\t Accuracy 0.8691\n",
      "Epoch [11][20]\t Batch [350][5500]\t Training Loss 0.1819\t Accuracy 0.8709\n",
      "Epoch [11][20]\t Batch [400][5500]\t Training Loss 0.1820\t Accuracy 0.8711\n",
      "Epoch [11][20]\t Batch [450][5500]\t Training Loss 0.1827\t Accuracy 0.8723\n",
      "Epoch [11][20]\t Batch [500][5500]\t Training Loss 0.1821\t Accuracy 0.8733\n",
      "Epoch [11][20]\t Batch [550][5500]\t Training Loss 0.1817\t Accuracy 0.8721\n",
      "Epoch [11][20]\t Batch [600][5500]\t Training Loss 0.1812\t Accuracy 0.8737\n",
      "Epoch [11][20]\t Batch [650][5500]\t Training Loss 0.1805\t Accuracy 0.8756\n",
      "Epoch [11][20]\t Batch [700][5500]\t Training Loss 0.1806\t Accuracy 0.8746\n",
      "Epoch [11][20]\t Batch [750][5500]\t Training Loss 0.1819\t Accuracy 0.8722\n",
      "Epoch [11][20]\t Batch [800][5500]\t Training Loss 0.1828\t Accuracy 0.8715\n",
      "Epoch [11][20]\t Batch [850][5500]\t Training Loss 0.1837\t Accuracy 0.8706\n",
      "Epoch [11][20]\t Batch [900][5500]\t Training Loss 0.1850\t Accuracy 0.8680\n",
      "Epoch [11][20]\t Batch [950][5500]\t Training Loss 0.1845\t Accuracy 0.8685\n",
      "Epoch [11][20]\t Batch [1000][5500]\t Training Loss 0.1835\t Accuracy 0.8694\n",
      "Epoch [11][20]\t Batch [1050][5500]\t Training Loss 0.1831\t Accuracy 0.8707\n",
      "Epoch [11][20]\t Batch [1100][5500]\t Training Loss 0.1827\t Accuracy 0.8722\n",
      "Epoch [11][20]\t Batch [1150][5500]\t Training Loss 0.1824\t Accuracy 0.8725\n",
      "Epoch [11][20]\t Batch [1200][5500]\t Training Loss 0.1835\t Accuracy 0.8708\n",
      "Epoch [11][20]\t Batch [1250][5500]\t Training Loss 0.1838\t Accuracy 0.8701\n",
      "Epoch [11][20]\t Batch [1300][5500]\t Training Loss 0.1845\t Accuracy 0.8686\n",
      "Epoch [11][20]\t Batch [1350][5500]\t Training Loss 0.1848\t Accuracy 0.8677\n",
      "Epoch [11][20]\t Batch [1400][5500]\t Training Loss 0.1854\t Accuracy 0.8662\n",
      "Epoch [11][20]\t Batch [1450][5500]\t Training Loss 0.1864\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [1500][5500]\t Training Loss 0.1875\t Accuracy 0.8630\n",
      "Epoch [11][20]\t Batch [1550][5500]\t Training Loss 0.1872\t Accuracy 0.8634\n",
      "Epoch [11][20]\t Batch [1600][5500]\t Training Loss 0.1877\t Accuracy 0.8626\n",
      "Epoch [11][20]\t Batch [1650][5500]\t Training Loss 0.1874\t Accuracy 0.8632\n",
      "Epoch [11][20]\t Batch [1700][5500]\t Training Loss 0.1877\t Accuracy 0.8632\n",
      "Epoch [11][20]\t Batch [1750][5500]\t Training Loss 0.1878\t Accuracy 0.8630\n",
      "Epoch [11][20]\t Batch [1800][5500]\t Training Loss 0.1885\t Accuracy 0.8627\n",
      "Epoch [11][20]\t Batch [1850][5500]\t Training Loss 0.1880\t Accuracy 0.8637\n",
      "Epoch [11][20]\t Batch [1900][5500]\t Training Loss 0.1874\t Accuracy 0.8650\n",
      "Epoch [11][20]\t Batch [1950][5500]\t Training Loss 0.1875\t Accuracy 0.8650\n",
      "Epoch [11][20]\t Batch [2000][5500]\t Training Loss 0.1872\t Accuracy 0.8655\n",
      "Epoch [11][20]\t Batch [2050][5500]\t Training Loss 0.1872\t Accuracy 0.8663\n",
      "Epoch [11][20]\t Batch [2100][5500]\t Training Loss 0.1874\t Accuracy 0.8657\n",
      "Epoch [11][20]\t Batch [2150][5500]\t Training Loss 0.1871\t Accuracy 0.8660\n",
      "Epoch [11][20]\t Batch [2200][5500]\t Training Loss 0.1866\t Accuracy 0.8664\n",
      "Epoch [11][20]\t Batch [2250][5500]\t Training Loss 0.1868\t Accuracy 0.8661\n",
      "Epoch [11][20]\t Batch [2300][5500]\t Training Loss 0.1873\t Accuracy 0.8659\n",
      "Epoch [11][20]\t Batch [2350][5500]\t Training Loss 0.1873\t Accuracy 0.8663\n",
      "Epoch [11][20]\t Batch [2400][5500]\t Training Loss 0.1873\t Accuracy 0.8662\n",
      "Epoch [11][20]\t Batch [2450][5500]\t Training Loss 0.1873\t Accuracy 0.8663\n",
      "Epoch [11][20]\t Batch [2500][5500]\t Training Loss 0.1875\t Accuracy 0.8665\n",
      "Epoch [11][20]\t Batch [2550][5500]\t Training Loss 0.1871\t Accuracy 0.8667\n",
      "Epoch [11][20]\t Batch [2600][5500]\t Training Loss 0.1870\t Accuracy 0.8666\n",
      "Epoch [11][20]\t Batch [2650][5500]\t Training Loss 0.1870\t Accuracy 0.8668\n",
      "Epoch [11][20]\t Batch [2700][5500]\t Training Loss 0.1872\t Accuracy 0.8667\n",
      "Epoch [11][20]\t Batch [2750][5500]\t Training Loss 0.1875\t Accuracy 0.8664\n",
      "Epoch [11][20]\t Batch [2800][5500]\t Training Loss 0.1872\t Accuracy 0.8667\n",
      "Epoch [11][20]\t Batch [2850][5500]\t Training Loss 0.1870\t Accuracy 0.8672\n",
      "Epoch [11][20]\t Batch [2900][5500]\t Training Loss 0.1869\t Accuracy 0.8672\n",
      "Epoch [11][20]\t Batch [2950][5500]\t Training Loss 0.1870\t Accuracy 0.8670\n",
      "Epoch [11][20]\t Batch [3000][5500]\t Training Loss 0.1873\t Accuracy 0.8663\n",
      "Epoch [11][20]\t Batch [3050][5500]\t Training Loss 0.1875\t Accuracy 0.8663\n",
      "Epoch [11][20]\t Batch [3100][5500]\t Training Loss 0.1877\t Accuracy 0.8657\n",
      "Epoch [11][20]\t Batch [3150][5500]\t Training Loss 0.1880\t Accuracy 0.8651\n",
      "Epoch [11][20]\t Batch [3200][5500]\t Training Loss 0.1881\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [3250][5500]\t Training Loss 0.1885\t Accuracy 0.8639\n",
      "Epoch [11][20]\t Batch [3300][5500]\t Training Loss 0.1884\t Accuracy 0.8642\n",
      "Epoch [11][20]\t Batch [3350][5500]\t Training Loss 0.1884\t Accuracy 0.8641\n",
      "Epoch [11][20]\t Batch [3400][5500]\t Training Loss 0.1880\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [3450][5500]\t Training Loss 0.1878\t Accuracy 0.8652\n",
      "Epoch [11][20]\t Batch [3500][5500]\t Training Loss 0.1880\t Accuracy 0.8645\n",
      "Epoch [11][20]\t Batch [3550][5500]\t Training Loss 0.1880\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [3600][5500]\t Training Loss 0.1880\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [3650][5500]\t Training Loss 0.1880\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [3700][5500]\t Training Loss 0.1878\t Accuracy 0.8653\n",
      "Epoch [11][20]\t Batch [3750][5500]\t Training Loss 0.1882\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [3800][5500]\t Training Loss 0.1882\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [3850][5500]\t Training Loss 0.1882\t Accuracy 0.8650\n",
      "Epoch [11][20]\t Batch [3900][5500]\t Training Loss 0.1880\t Accuracy 0.8653\n",
      "Epoch [11][20]\t Batch [3950][5500]\t Training Loss 0.1880\t Accuracy 0.8653\n",
      "Epoch [11][20]\t Batch [4000][5500]\t Training Loss 0.1881\t Accuracy 0.8652\n",
      "Epoch [11][20]\t Batch [4050][5500]\t Training Loss 0.1881\t Accuracy 0.8652\n",
      "Epoch [11][20]\t Batch [4100][5500]\t Training Loss 0.1879\t Accuracy 0.8653\n",
      "Epoch [11][20]\t Batch [4150][5500]\t Training Loss 0.1881\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [4200][5500]\t Training Loss 0.1881\t Accuracy 0.8651\n",
      "Epoch [11][20]\t Batch [4250][5500]\t Training Loss 0.1884\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [4300][5500]\t Training Loss 0.1884\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [4350][5500]\t Training Loss 0.1883\t Accuracy 0.8650\n",
      "Epoch [11][20]\t Batch [4400][5500]\t Training Loss 0.1882\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [4450][5500]\t Training Loss 0.1884\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [4500][5500]\t Training Loss 0.1883\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [4550][5500]\t Training Loss 0.1883\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [4600][5500]\t Training Loss 0.1884\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [4650][5500]\t Training Loss 0.1886\t Accuracy 0.8645\n",
      "Epoch [11][20]\t Batch [4700][5500]\t Training Loss 0.1884\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [4750][5500]\t Training Loss 0.1885\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [4800][5500]\t Training Loss 0.1885\t Accuracy 0.8644\n",
      "Epoch [11][20]\t Batch [4850][5500]\t Training Loss 0.1882\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [4900][5500]\t Training Loss 0.1882\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [4950][5500]\t Training Loss 0.1883\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [5000][5500]\t Training Loss 0.1886\t Accuracy 0.8644\n",
      "Epoch [11][20]\t Batch [5050][5500]\t Training Loss 0.1886\t Accuracy 0.8643\n",
      "Epoch [11][20]\t Batch [5100][5500]\t Training Loss 0.1886\t Accuracy 0.8644\n",
      "Epoch [11][20]\t Batch [5150][5500]\t Training Loss 0.1884\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [5200][5500]\t Training Loss 0.1883\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [5250][5500]\t Training Loss 0.1883\t Accuracy 0.8647\n",
      "Epoch [11][20]\t Batch [5300][5500]\t Training Loss 0.1883\t Accuracy 0.8646\n",
      "Epoch [11][20]\t Batch [5350][5500]\t Training Loss 0.1882\t Accuracy 0.8648\n",
      "Epoch [11][20]\t Batch [5400][5500]\t Training Loss 0.1881\t Accuracy 0.8649\n",
      "Epoch [11][20]\t Batch [5450][5500]\t Training Loss 0.1881\t Accuracy 0.8651\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1880\t Average training accuracy 0.8649\n",
      "Epoch [11]\t Average validation loss 0.1695\t Average validation accuracy 0.8986\n",
      "\n",
      "Epoch [12][20]\t Batch [0][5500]\t Training Loss 0.1159\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][5500]\t Training Loss 0.1693\t Accuracy 0.8667\n",
      "Epoch [12][20]\t Batch [100][5500]\t Training Loss 0.1821\t Accuracy 0.8683\n",
      "Epoch [12][20]\t Batch [150][5500]\t Training Loss 0.1899\t Accuracy 0.8543\n",
      "Epoch [12][20]\t Batch [200][5500]\t Training Loss 0.1852\t Accuracy 0.8617\n",
      "Epoch [12][20]\t Batch [250][5500]\t Training Loss 0.1813\t Accuracy 0.8689\n",
      "Epoch [12][20]\t Batch [300][5500]\t Training Loss 0.1791\t Accuracy 0.8704\n",
      "Epoch [12][20]\t Batch [350][5500]\t Training Loss 0.1787\t Accuracy 0.8729\n",
      "Epoch [12][20]\t Batch [400][5500]\t Training Loss 0.1788\t Accuracy 0.8731\n",
      "Epoch [12][20]\t Batch [450][5500]\t Training Loss 0.1794\t Accuracy 0.8741\n",
      "Epoch [12][20]\t Batch [500][5500]\t Training Loss 0.1788\t Accuracy 0.8754\n",
      "Epoch [12][20]\t Batch [550][5500]\t Training Loss 0.1784\t Accuracy 0.8744\n",
      "Epoch [12][20]\t Batch [600][5500]\t Training Loss 0.1779\t Accuracy 0.8760\n",
      "Epoch [12][20]\t Batch [650][5500]\t Training Loss 0.1772\t Accuracy 0.8780\n",
      "Epoch [12][20]\t Batch [700][5500]\t Training Loss 0.1773\t Accuracy 0.8772\n",
      "Epoch [12][20]\t Batch [750][5500]\t Training Loss 0.1787\t Accuracy 0.8747\n",
      "Epoch [12][20]\t Batch [800][5500]\t Training Loss 0.1795\t Accuracy 0.8739\n",
      "Epoch [12][20]\t Batch [850][5500]\t Training Loss 0.1804\t Accuracy 0.8731\n",
      "Epoch [12][20]\t Batch [900][5500]\t Training Loss 0.1817\t Accuracy 0.8703\n",
      "Epoch [12][20]\t Batch [950][5500]\t Training Loss 0.1812\t Accuracy 0.8707\n",
      "Epoch [12][20]\t Batch [1000][5500]\t Training Loss 0.1802\t Accuracy 0.8715\n",
      "Epoch [12][20]\t Batch [1050][5500]\t Training Loss 0.1798\t Accuracy 0.8726\n",
      "Epoch [12][20]\t Batch [1100][5500]\t Training Loss 0.1794\t Accuracy 0.8739\n",
      "Epoch [12][20]\t Batch [1150][5500]\t Training Loss 0.1791\t Accuracy 0.8742\n",
      "Epoch [12][20]\t Batch [1200][5500]\t Training Loss 0.1802\t Accuracy 0.8727\n",
      "Epoch [12][20]\t Batch [1250][5500]\t Training Loss 0.1805\t Accuracy 0.8721\n",
      "Epoch [12][20]\t Batch [1300][5500]\t Training Loss 0.1812\t Accuracy 0.8707\n",
      "Epoch [12][20]\t Batch [1350][5500]\t Training Loss 0.1815\t Accuracy 0.8699\n",
      "Epoch [12][20]\t Batch [1400][5500]\t Training Loss 0.1821\t Accuracy 0.8686\n",
      "Epoch [12][20]\t Batch [1450][5500]\t Training Loss 0.1830\t Accuracy 0.8671\n",
      "Epoch [12][20]\t Batch [1500][5500]\t Training Loss 0.1841\t Accuracy 0.8655\n",
      "Epoch [12][20]\t Batch [1550][5500]\t Training Loss 0.1839\t Accuracy 0.8660\n",
      "Epoch [12][20]\t Batch [1600][5500]\t Training Loss 0.1844\t Accuracy 0.8650\n",
      "Epoch [12][20]\t Batch [1650][5500]\t Training Loss 0.1841\t Accuracy 0.8657\n",
      "Epoch [12][20]\t Batch [1700][5500]\t Training Loss 0.1843\t Accuracy 0.8656\n",
      "Epoch [12][20]\t Batch [1750][5500]\t Training Loss 0.1845\t Accuracy 0.8654\n",
      "Epoch [12][20]\t Batch [1800][5500]\t Training Loss 0.1851\t Accuracy 0.8651\n",
      "Epoch [12][20]\t Batch [1850][5500]\t Training Loss 0.1847\t Accuracy 0.8661\n",
      "Epoch [12][20]\t Batch [1900][5500]\t Training Loss 0.1841\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [1950][5500]\t Training Loss 0.1841\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [2000][5500]\t Training Loss 0.1839\t Accuracy 0.8680\n",
      "Epoch [12][20]\t Batch [2050][5500]\t Training Loss 0.1838\t Accuracy 0.8688\n",
      "Epoch [12][20]\t Batch [2100][5500]\t Training Loss 0.1840\t Accuracy 0.8683\n",
      "Epoch [12][20]\t Batch [2150][5500]\t Training Loss 0.1838\t Accuracy 0.8686\n",
      "Epoch [12][20]\t Batch [2200][5500]\t Training Loss 0.1833\t Accuracy 0.8691\n",
      "Epoch [12][20]\t Batch [2250][5500]\t Training Loss 0.1835\t Accuracy 0.8688\n",
      "Epoch [12][20]\t Batch [2300][5500]\t Training Loss 0.1839\t Accuracy 0.8685\n",
      "Epoch [12][20]\t Batch [2350][5500]\t Training Loss 0.1839\t Accuracy 0.8689\n",
      "Epoch [12][20]\t Batch [2400][5500]\t Training Loss 0.1840\t Accuracy 0.8688\n",
      "Epoch [12][20]\t Batch [2450][5500]\t Training Loss 0.1840\t Accuracy 0.8688\n",
      "Epoch [12][20]\t Batch [2500][5500]\t Training Loss 0.1841\t Accuracy 0.8690\n",
      "Epoch [12][20]\t Batch [2550][5500]\t Training Loss 0.1838\t Accuracy 0.8693\n",
      "Epoch [12][20]\t Batch [2600][5500]\t Training Loss 0.1837\t Accuracy 0.8692\n",
      "Epoch [12][20]\t Batch [2650][5500]\t Training Loss 0.1836\t Accuracy 0.8694\n",
      "Epoch [12][20]\t Batch [2700][5500]\t Training Loss 0.1838\t Accuracy 0.8693\n",
      "Epoch [12][20]\t Batch [2750][5500]\t Training Loss 0.1841\t Accuracy 0.8689\n",
      "Epoch [12][20]\t Batch [2800][5500]\t Training Loss 0.1839\t Accuracy 0.8692\n",
      "Epoch [12][20]\t Batch [2850][5500]\t Training Loss 0.1836\t Accuracy 0.8697\n",
      "Epoch [12][20]\t Batch [2900][5500]\t Training Loss 0.1836\t Accuracy 0.8697\n",
      "Epoch [12][20]\t Batch [2950][5500]\t Training Loss 0.1837\t Accuracy 0.8695\n",
      "Epoch [12][20]\t Batch [3000][5500]\t Training Loss 0.1840\t Accuracy 0.8688\n",
      "Epoch [12][20]\t Batch [3050][5500]\t Training Loss 0.1841\t Accuracy 0.8689\n",
      "Epoch [12][20]\t Batch [3100][5500]\t Training Loss 0.1844\t Accuracy 0.8683\n",
      "Epoch [12][20]\t Batch [3150][5500]\t Training Loss 0.1847\t Accuracy 0.8676\n",
      "Epoch [12][20]\t Batch [3200][5500]\t Training Loss 0.1848\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [3250][5500]\t Training Loss 0.1852\t Accuracy 0.8665\n",
      "Epoch [12][20]\t Batch [3300][5500]\t Training Loss 0.1850\t Accuracy 0.8669\n",
      "Epoch [12][20]\t Batch [3350][5500]\t Training Loss 0.1851\t Accuracy 0.8668\n",
      "Epoch [12][20]\t Batch [3400][5500]\t Training Loss 0.1847\t Accuracy 0.8676\n",
      "Epoch [12][20]\t Batch [3450][5500]\t Training Loss 0.1845\t Accuracy 0.8679\n",
      "Epoch [12][20]\t Batch [3500][5500]\t Training Loss 0.1847\t Accuracy 0.8672\n",
      "Epoch [12][20]\t Batch [3550][5500]\t Training Loss 0.1847\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [3600][5500]\t Training Loss 0.1846\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [3650][5500]\t Training Loss 0.1846\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [3700][5500]\t Training Loss 0.1845\t Accuracy 0.8679\n",
      "Epoch [12][20]\t Batch [3750][5500]\t Training Loss 0.1848\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [3800][5500]\t Training Loss 0.1849\t Accuracy 0.8672\n",
      "Epoch [12][20]\t Batch [3850][5500]\t Training Loss 0.1849\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [3900][5500]\t Training Loss 0.1847\t Accuracy 0.8678\n",
      "Epoch [12][20]\t Batch [3950][5500]\t Training Loss 0.1846\t Accuracy 0.8678\n",
      "Epoch [12][20]\t Batch [4000][5500]\t Training Loss 0.1848\t Accuracy 0.8677\n",
      "Epoch [12][20]\t Batch [4050][5500]\t Training Loss 0.1847\t Accuracy 0.8677\n",
      "Epoch [12][20]\t Batch [4100][5500]\t Training Loss 0.1846\t Accuracy 0.8678\n",
      "Epoch [12][20]\t Batch [4150][5500]\t Training Loss 0.1848\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [4200][5500]\t Training Loss 0.1848\t Accuracy 0.8677\n",
      "Epoch [12][20]\t Batch [4250][5500]\t Training Loss 0.1851\t Accuracy 0.8672\n",
      "Epoch [12][20]\t Batch [4300][5500]\t Training Loss 0.1851\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [4350][5500]\t Training Loss 0.1849\t Accuracy 0.8676\n",
      "Epoch [12][20]\t Batch [4400][5500]\t Training Loss 0.1849\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [4450][5500]\t Training Loss 0.1851\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [4500][5500]\t Training Loss 0.1850\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [4550][5500]\t Training Loss 0.1850\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [4600][5500]\t Training Loss 0.1851\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [4650][5500]\t Training Loss 0.1853\t Accuracy 0.8671\n",
      "Epoch [12][20]\t Batch [4700][5500]\t Training Loss 0.1851\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [4750][5500]\t Training Loss 0.1851\t Accuracy 0.8672\n",
      "Epoch [12][20]\t Batch [4800][5500]\t Training Loss 0.1851\t Accuracy 0.8670\n",
      "Epoch [12][20]\t Batch [4850][5500]\t Training Loss 0.1849\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [4900][5500]\t Training Loss 0.1848\t Accuracy 0.8675\n",
      "Epoch [12][20]\t Batch [4950][5500]\t Training Loss 0.1850\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [5000][5500]\t Training Loss 0.1852\t Accuracy 0.8670\n",
      "Epoch [12][20]\t Batch [5050][5500]\t Training Loss 0.1853\t Accuracy 0.8668\n",
      "Epoch [12][20]\t Batch [5100][5500]\t Training Loss 0.1852\t Accuracy 0.8670\n",
      "Epoch [12][20]\t Batch [5150][5500]\t Training Loss 0.1851\t Accuracy 0.8671\n",
      "Epoch [12][20]\t Batch [5200][5500]\t Training Loss 0.1849\t Accuracy 0.8674\n",
      "Epoch [12][20]\t Batch [5250][5500]\t Training Loss 0.1850\t Accuracy 0.8672\n",
      "Epoch [12][20]\t Batch [5300][5500]\t Training Loss 0.1850\t Accuracy 0.8671\n",
      "Epoch [12][20]\t Batch [5350][5500]\t Training Loss 0.1848\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [5400][5500]\t Training Loss 0.1848\t Accuracy 0.8673\n",
      "Epoch [12][20]\t Batch [5450][5500]\t Training Loss 0.1847\t Accuracy 0.8675\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1847\t Average training accuracy 0.8673\n",
      "Epoch [12]\t Average validation loss 0.1662\t Average validation accuracy 0.9002\n",
      "\n",
      "Epoch [13][20]\t Batch [0][5500]\t Training Loss 0.1125\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][5500]\t Training Loss 0.1661\t Accuracy 0.8706\n",
      "Epoch [13][20]\t Batch [100][5500]\t Training Loss 0.1789\t Accuracy 0.8723\n",
      "Epoch [13][20]\t Batch [150][5500]\t Training Loss 0.1866\t Accuracy 0.8596\n",
      "Epoch [13][20]\t Batch [200][5500]\t Training Loss 0.1819\t Accuracy 0.8667\n",
      "Epoch [13][20]\t Batch [250][5500]\t Training Loss 0.1780\t Accuracy 0.8729\n",
      "Epoch [13][20]\t Batch [300][5500]\t Training Loss 0.1759\t Accuracy 0.8744\n",
      "Epoch [13][20]\t Batch [350][5500]\t Training Loss 0.1755\t Accuracy 0.8769\n",
      "Epoch [13][20]\t Batch [400][5500]\t Training Loss 0.1755\t Accuracy 0.8773\n",
      "Epoch [13][20]\t Batch [450][5500]\t Training Loss 0.1761\t Accuracy 0.8783\n",
      "Epoch [13][20]\t Batch [500][5500]\t Training Loss 0.1754\t Accuracy 0.8794\n",
      "Epoch [13][20]\t Batch [550][5500]\t Training Loss 0.1750\t Accuracy 0.8782\n",
      "Epoch [13][20]\t Batch [600][5500]\t Training Loss 0.1746\t Accuracy 0.8795\n",
      "Epoch [13][20]\t Batch [650][5500]\t Training Loss 0.1738\t Accuracy 0.8814\n",
      "Epoch [13][20]\t Batch [700][5500]\t Training Loss 0.1740\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [750][5500]\t Training Loss 0.1753\t Accuracy 0.8774\n",
      "Epoch [13][20]\t Batch [800][5500]\t Training Loss 0.1761\t Accuracy 0.8765\n",
      "Epoch [13][20]\t Batch [850][5500]\t Training Loss 0.1770\t Accuracy 0.8756\n",
      "Epoch [13][20]\t Batch [900][5500]\t Training Loss 0.1783\t Accuracy 0.8727\n",
      "Epoch [13][20]\t Batch [950][5500]\t Training Loss 0.1778\t Accuracy 0.8730\n",
      "Epoch [13][20]\t Batch [1000][5500]\t Training Loss 0.1769\t Accuracy 0.8738\n",
      "Epoch [13][20]\t Batch [1050][5500]\t Training Loss 0.1765\t Accuracy 0.8748\n",
      "Epoch [13][20]\t Batch [1100][5500]\t Training Loss 0.1760\t Accuracy 0.8761\n",
      "Epoch [13][20]\t Batch [1150][5500]\t Training Loss 0.1757\t Accuracy 0.8764\n",
      "Epoch [13][20]\t Batch [1200][5500]\t Training Loss 0.1769\t Accuracy 0.8748\n",
      "Epoch [13][20]\t Batch [1250][5500]\t Training Loss 0.1771\t Accuracy 0.8742\n",
      "Epoch [13][20]\t Batch [1300][5500]\t Training Loss 0.1778\t Accuracy 0.8726\n",
      "Epoch [13][20]\t Batch [1350][5500]\t Training Loss 0.1781\t Accuracy 0.8720\n",
      "Epoch [13][20]\t Batch [1400][5500]\t Training Loss 0.1787\t Accuracy 0.8706\n",
      "Epoch [13][20]\t Batch [1450][5500]\t Training Loss 0.1796\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [1500][5500]\t Training Loss 0.1807\t Accuracy 0.8674\n",
      "Epoch [13][20]\t Batch [1550][5500]\t Training Loss 0.1805\t Accuracy 0.8678\n",
      "Epoch [13][20]\t Batch [1600][5500]\t Training Loss 0.1810\t Accuracy 0.8669\n",
      "Epoch [13][20]\t Batch [1650][5500]\t Training Loss 0.1807\t Accuracy 0.8675\n",
      "Epoch [13][20]\t Batch [1700][5500]\t Training Loss 0.1809\t Accuracy 0.8674\n",
      "Epoch [13][20]\t Batch [1750][5500]\t Training Loss 0.1810\t Accuracy 0.8672\n",
      "Epoch [13][20]\t Batch [1800][5500]\t Training Loss 0.1817\t Accuracy 0.8666\n",
      "Epoch [13][20]\t Batch [1850][5500]\t Training Loss 0.1813\t Accuracy 0.8676\n",
      "Epoch [13][20]\t Batch [1900][5500]\t Training Loss 0.1807\t Accuracy 0.8690\n",
      "Epoch [13][20]\t Batch [1950][5500]\t Training Loss 0.1807\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [2000][5500]\t Training Loss 0.1804\t Accuracy 0.8696\n",
      "Epoch [13][20]\t Batch [2050][5500]\t Training Loss 0.1804\t Accuracy 0.8703\n",
      "Epoch [13][20]\t Batch [2100][5500]\t Training Loss 0.1806\t Accuracy 0.8698\n",
      "Epoch [13][20]\t Batch [2150][5500]\t Training Loss 0.1804\t Accuracy 0.8702\n",
      "Epoch [13][20]\t Batch [2200][5500]\t Training Loss 0.1799\t Accuracy 0.8706\n",
      "Epoch [13][20]\t Batch [2250][5500]\t Training Loss 0.1801\t Accuracy 0.8703\n",
      "Epoch [13][20]\t Batch [2300][5500]\t Training Loss 0.1805\t Accuracy 0.8701\n",
      "Epoch [13][20]\t Batch [2350][5500]\t Training Loss 0.1805\t Accuracy 0.8704\n",
      "Epoch [13][20]\t Batch [2400][5500]\t Training Loss 0.1806\t Accuracy 0.8704\n",
      "Epoch [13][20]\t Batch [2450][5500]\t Training Loss 0.1805\t Accuracy 0.8705\n",
      "Epoch [13][20]\t Batch [2500][5500]\t Training Loss 0.1807\t Accuracy 0.8707\n",
      "Epoch [13][20]\t Batch [2550][5500]\t Training Loss 0.1803\t Accuracy 0.8710\n",
      "Epoch [13][20]\t Batch [2600][5500]\t Training Loss 0.1803\t Accuracy 0.8709\n",
      "Epoch [13][20]\t Batch [2650][5500]\t Training Loss 0.1802\t Accuracy 0.8711\n",
      "Epoch [13][20]\t Batch [2700][5500]\t Training Loss 0.1804\t Accuracy 0.8711\n",
      "Epoch [13][20]\t Batch [2750][5500]\t Training Loss 0.1807\t Accuracy 0.8708\n",
      "Epoch [13][20]\t Batch [2800][5500]\t Training Loss 0.1804\t Accuracy 0.8711\n",
      "Epoch [13][20]\t Batch [2850][5500]\t Training Loss 0.1802\t Accuracy 0.8716\n",
      "Epoch [13][20]\t Batch [2900][5500]\t Training Loss 0.1801\t Accuracy 0.8716\n",
      "Epoch [13][20]\t Batch [2950][5500]\t Training Loss 0.1802\t Accuracy 0.8713\n",
      "Epoch [13][20]\t Batch [3000][5500]\t Training Loss 0.1805\t Accuracy 0.8707\n",
      "Epoch [13][20]\t Batch [3050][5500]\t Training Loss 0.1807\t Accuracy 0.8708\n",
      "Epoch [13][20]\t Batch [3100][5500]\t Training Loss 0.1810\t Accuracy 0.8702\n",
      "Epoch [13][20]\t Batch [3150][5500]\t Training Loss 0.1812\t Accuracy 0.8696\n",
      "Epoch [13][20]\t Batch [3200][5500]\t Training Loss 0.1814\t Accuracy 0.8693\n",
      "Epoch [13][20]\t Batch [3250][5500]\t Training Loss 0.1818\t Accuracy 0.8684\n",
      "Epoch [13][20]\t Batch [3300][5500]\t Training Loss 0.1816\t Accuracy 0.8688\n",
      "Epoch [13][20]\t Batch [3350][5500]\t Training Loss 0.1817\t Accuracy 0.8688\n",
      "Epoch [13][20]\t Batch [3400][5500]\t Training Loss 0.1812\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [3450][5500]\t Training Loss 0.1811\t Accuracy 0.8697\n",
      "Epoch [13][20]\t Batch [3500][5500]\t Training Loss 0.1813\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [3550][5500]\t Training Loss 0.1812\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [3600][5500]\t Training Loss 0.1812\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [3650][5500]\t Training Loss 0.1812\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [3700][5500]\t Training Loss 0.1811\t Accuracy 0.8698\n",
      "Epoch [13][20]\t Batch [3750][5500]\t Training Loss 0.1814\t Accuracy 0.8693\n",
      "Epoch [13][20]\t Batch [3800][5500]\t Training Loss 0.1815\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [3850][5500]\t Training Loss 0.1814\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [3900][5500]\t Training Loss 0.1813\t Accuracy 0.8698\n",
      "Epoch [13][20]\t Batch [3950][5500]\t Training Loss 0.1812\t Accuracy 0.8699\n",
      "Epoch [13][20]\t Batch [4000][5500]\t Training Loss 0.1814\t Accuracy 0.8698\n",
      "Epoch [13][20]\t Batch [4050][5500]\t Training Loss 0.1813\t Accuracy 0.8697\n",
      "Epoch [13][20]\t Batch [4100][5500]\t Training Loss 0.1812\t Accuracy 0.8699\n",
      "Epoch [13][20]\t Batch [4150][5500]\t Training Loss 0.1814\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [4200][5500]\t Training Loss 0.1813\t Accuracy 0.8698\n",
      "Epoch [13][20]\t Batch [4250][5500]\t Training Loss 0.1816\t Accuracy 0.8693\n",
      "Epoch [13][20]\t Batch [4300][5500]\t Training Loss 0.1816\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [4350][5500]\t Training Loss 0.1815\t Accuracy 0.8697\n",
      "Epoch [13][20]\t Batch [4400][5500]\t Training Loss 0.1815\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [4450][5500]\t Training Loss 0.1816\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [4500][5500]\t Training Loss 0.1815\t Accuracy 0.8696\n",
      "Epoch [13][20]\t Batch [4550][5500]\t Training Loss 0.1816\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [4600][5500]\t Training Loss 0.1817\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [4650][5500]\t Training Loss 0.1818\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [4700][5500]\t Training Loss 0.1816\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [4750][5500]\t Training Loss 0.1817\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [4800][5500]\t Training Loss 0.1817\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [4850][5500]\t Training Loss 0.1815\t Accuracy 0.8696\n",
      "Epoch [13][20]\t Batch [4900][5500]\t Training Loss 0.1814\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [4950][5500]\t Training Loss 0.1815\t Accuracy 0.8693\n",
      "Epoch [13][20]\t Batch [5000][5500]\t Training Loss 0.1818\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [5050][5500]\t Training Loss 0.1818\t Accuracy 0.8689\n",
      "Epoch [13][20]\t Batch [5100][5500]\t Training Loss 0.1818\t Accuracy 0.8691\n",
      "Epoch [13][20]\t Batch [5150][5500]\t Training Loss 0.1816\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [5200][5500]\t Training Loss 0.1815\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [5250][5500]\t Training Loss 0.1815\t Accuracy 0.8693\n",
      "Epoch [13][20]\t Batch [5300][5500]\t Training Loss 0.1815\t Accuracy 0.8692\n",
      "Epoch [13][20]\t Batch [5350][5500]\t Training Loss 0.1814\t Accuracy 0.8694\n",
      "Epoch [13][20]\t Batch [5400][5500]\t Training Loss 0.1813\t Accuracy 0.8695\n",
      "Epoch [13][20]\t Batch [5450][5500]\t Training Loss 0.1813\t Accuracy 0.8696\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1813\t Average training accuracy 0.8695\n",
      "Epoch [13]\t Average validation loss 0.1629\t Average validation accuracy 0.9016\n",
      "\n",
      "Epoch [14][20]\t Batch [0][5500]\t Training Loss 0.1091\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][5500]\t Training Loss 0.1629\t Accuracy 0.8706\n",
      "Epoch [14][20]\t Batch [100][5500]\t Training Loss 0.1755\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [150][5500]\t Training Loss 0.1833\t Accuracy 0.8609\n",
      "Epoch [14][20]\t Batch [200][5500]\t Training Loss 0.1785\t Accuracy 0.8687\n",
      "Epoch [14][20]\t Batch [250][5500]\t Training Loss 0.1747\t Accuracy 0.8745\n",
      "Epoch [14][20]\t Batch [300][5500]\t Training Loss 0.1726\t Accuracy 0.8767\n",
      "Epoch [14][20]\t Batch [350][5500]\t Training Loss 0.1721\t Accuracy 0.8786\n",
      "Epoch [14][20]\t Batch [400][5500]\t Training Loss 0.1721\t Accuracy 0.8796\n",
      "Epoch [14][20]\t Batch [450][5500]\t Training Loss 0.1726\t Accuracy 0.8805\n",
      "Epoch [14][20]\t Batch [500][5500]\t Training Loss 0.1719\t Accuracy 0.8818\n",
      "Epoch [14][20]\t Batch [550][5500]\t Training Loss 0.1716\t Accuracy 0.8802\n",
      "Epoch [14][20]\t Batch [600][5500]\t Training Loss 0.1711\t Accuracy 0.8815\n",
      "Epoch [14][20]\t Batch [650][5500]\t Training Loss 0.1704\t Accuracy 0.8836\n",
      "Epoch [14][20]\t Batch [700][5500]\t Training Loss 0.1705\t Accuracy 0.8825\n",
      "Epoch [14][20]\t Batch [750][5500]\t Training Loss 0.1718\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [800][5500]\t Training Loss 0.1726\t Accuracy 0.8795\n",
      "Epoch [14][20]\t Batch [850][5500]\t Training Loss 0.1735\t Accuracy 0.8786\n",
      "Epoch [14][20]\t Batch [900][5500]\t Training Loss 0.1748\t Accuracy 0.8758\n",
      "Epoch [14][20]\t Batch [950][5500]\t Training Loss 0.1743\t Accuracy 0.8759\n",
      "Epoch [14][20]\t Batch [1000][5500]\t Training Loss 0.1734\t Accuracy 0.8768\n",
      "Epoch [14][20]\t Batch [1050][5500]\t Training Loss 0.1730\t Accuracy 0.8778\n",
      "Epoch [14][20]\t Batch [1100][5500]\t Training Loss 0.1725\t Accuracy 0.8791\n",
      "Epoch [14][20]\t Batch [1150][5500]\t Training Loss 0.1723\t Accuracy 0.8791\n",
      "Epoch [14][20]\t Batch [1200][5500]\t Training Loss 0.1734\t Accuracy 0.8776\n",
      "Epoch [14][20]\t Batch [1250][5500]\t Training Loss 0.1737\t Accuracy 0.8771\n",
      "Epoch [14][20]\t Batch [1300][5500]\t Training Loss 0.1744\t Accuracy 0.8756\n",
      "Epoch [14][20]\t Batch [1350][5500]\t Training Loss 0.1747\t Accuracy 0.8749\n",
      "Epoch [14][20]\t Batch [1400][5500]\t Training Loss 0.1752\t Accuracy 0.8737\n",
      "Epoch [14][20]\t Batch [1450][5500]\t Training Loss 0.1762\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [1500][5500]\t Training Loss 0.1773\t Accuracy 0.8704\n",
      "Epoch [14][20]\t Batch [1550][5500]\t Training Loss 0.1770\t Accuracy 0.8709\n",
      "Epoch [14][20]\t Batch [1600][5500]\t Training Loss 0.1775\t Accuracy 0.8698\n",
      "Epoch [14][20]\t Batch [1650][5500]\t Training Loss 0.1772\t Accuracy 0.8704\n",
      "Epoch [14][20]\t Batch [1700][5500]\t Training Loss 0.1774\t Accuracy 0.8703\n",
      "Epoch [14][20]\t Batch [1750][5500]\t Training Loss 0.1776\t Accuracy 0.8701\n",
      "Epoch [14][20]\t Batch [1800][5500]\t Training Loss 0.1782\t Accuracy 0.8695\n",
      "Epoch [14][20]\t Batch [1850][5500]\t Training Loss 0.1778\t Accuracy 0.8706\n",
      "Epoch [14][20]\t Batch [1900][5500]\t Training Loss 0.1772\t Accuracy 0.8719\n",
      "Epoch [14][20]\t Batch [1950][5500]\t Training Loss 0.1772\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [2000][5500]\t Training Loss 0.1769\t Accuracy 0.8725\n",
      "Epoch [14][20]\t Batch [2050][5500]\t Training Loss 0.1769\t Accuracy 0.8732\n",
      "Epoch [14][20]\t Batch [2100][5500]\t Training Loss 0.1771\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [2150][5500]\t Training Loss 0.1769\t Accuracy 0.8730\n",
      "Epoch [14][20]\t Batch [2200][5500]\t Training Loss 0.1764\t Accuracy 0.8734\n",
      "Epoch [14][20]\t Batch [2250][5500]\t Training Loss 0.1766\t Accuracy 0.8732\n",
      "Epoch [14][20]\t Batch [2300][5500]\t Training Loss 0.1770\t Accuracy 0.8729\n",
      "Epoch [14][20]\t Batch [2350][5500]\t Training Loss 0.1770\t Accuracy 0.8732\n",
      "Epoch [14][20]\t Batch [2400][5500]\t Training Loss 0.1771\t Accuracy 0.8731\n",
      "Epoch [14][20]\t Batch [2450][5500]\t Training Loss 0.1770\t Accuracy 0.8732\n",
      "Epoch [14][20]\t Batch [2500][5500]\t Training Loss 0.1772\t Accuracy 0.8735\n",
      "Epoch [14][20]\t Batch [2550][5500]\t Training Loss 0.1768\t Accuracy 0.8738\n",
      "Epoch [14][20]\t Batch [2600][5500]\t Training Loss 0.1768\t Accuracy 0.8736\n",
      "Epoch [14][20]\t Batch [2650][5500]\t Training Loss 0.1767\t Accuracy 0.8740\n",
      "Epoch [14][20]\t Batch [2700][5500]\t Training Loss 0.1769\t Accuracy 0.8740\n",
      "Epoch [14][20]\t Batch [2750][5500]\t Training Loss 0.1772\t Accuracy 0.8737\n",
      "Epoch [14][20]\t Batch [2800][5500]\t Training Loss 0.1769\t Accuracy 0.8740\n",
      "Epoch [14][20]\t Batch [2850][5500]\t Training Loss 0.1767\t Accuracy 0.8745\n",
      "Epoch [14][20]\t Batch [2900][5500]\t Training Loss 0.1766\t Accuracy 0.8744\n",
      "Epoch [14][20]\t Batch [2950][5500]\t Training Loss 0.1767\t Accuracy 0.8742\n",
      "Epoch [14][20]\t Batch [3000][5500]\t Training Loss 0.1770\t Accuracy 0.8736\n",
      "Epoch [14][20]\t Batch [3050][5500]\t Training Loss 0.1772\t Accuracy 0.8737\n",
      "Epoch [14][20]\t Batch [3100][5500]\t Training Loss 0.1775\t Accuracy 0.8733\n",
      "Epoch [14][20]\t Batch [3150][5500]\t Training Loss 0.1778\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [3200][5500]\t Training Loss 0.1779\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [3250][5500]\t Training Loss 0.1783\t Accuracy 0.8715\n",
      "Epoch [14][20]\t Batch [3300][5500]\t Training Loss 0.1781\t Accuracy 0.8718\n",
      "Epoch [14][20]\t Batch [3350][5500]\t Training Loss 0.1782\t Accuracy 0.8718\n",
      "Epoch [14][20]\t Batch [3400][5500]\t Training Loss 0.1777\t Accuracy 0.8724\n",
      "Epoch [14][20]\t Batch [3450][5500]\t Training Loss 0.1776\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [3500][5500]\t Training Loss 0.1778\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [3550][5500]\t Training Loss 0.1777\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [3600][5500]\t Training Loss 0.1777\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [3650][5500]\t Training Loss 0.1777\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [3700][5500]\t Training Loss 0.1775\t Accuracy 0.8729\n",
      "Epoch [14][20]\t Batch [3750][5500]\t Training Loss 0.1779\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [3800][5500]\t Training Loss 0.1780\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [3850][5500]\t Training Loss 0.1779\t Accuracy 0.8725\n",
      "Epoch [14][20]\t Batch [3900][5500]\t Training Loss 0.1778\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [3950][5500]\t Training Loss 0.1777\t Accuracy 0.8728\n",
      "Epoch [14][20]\t Batch [4000][5500]\t Training Loss 0.1779\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [4050][5500]\t Training Loss 0.1778\t Accuracy 0.8726\n",
      "Epoch [14][20]\t Batch [4100][5500]\t Training Loss 0.1777\t Accuracy 0.8727\n",
      "Epoch [14][20]\t Batch [4150][5500]\t Training Loss 0.1779\t Accuracy 0.8724\n",
      "Epoch [14][20]\t Batch [4200][5500]\t Training Loss 0.1778\t Accuracy 0.8726\n",
      "Epoch [14][20]\t Batch [4250][5500]\t Training Loss 0.1781\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [4300][5500]\t Training Loss 0.1781\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [4350][5500]\t Training Loss 0.1780\t Accuracy 0.8724\n",
      "Epoch [14][20]\t Batch [4400][5500]\t Training Loss 0.1780\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [4450][5500]\t Training Loss 0.1781\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [4500][5500]\t Training Loss 0.1780\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [4550][5500]\t Training Loss 0.1781\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [4600][5500]\t Training Loss 0.1782\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [4650][5500]\t Training Loss 0.1783\t Accuracy 0.8719\n",
      "Epoch [14][20]\t Batch [4700][5500]\t Training Loss 0.1781\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [4750][5500]\t Training Loss 0.1782\t Accuracy 0.8720\n",
      "Epoch [14][20]\t Batch [4800][5500]\t Training Loss 0.1782\t Accuracy 0.8719\n",
      "Epoch [14][20]\t Batch [4850][5500]\t Training Loss 0.1780\t Accuracy 0.8723\n",
      "Epoch [14][20]\t Batch [4900][5500]\t Training Loss 0.1779\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [4950][5500]\t Training Loss 0.1780\t Accuracy 0.8720\n",
      "Epoch [14][20]\t Batch [5000][5500]\t Training Loss 0.1783\t Accuracy 0.8717\n",
      "Epoch [14][20]\t Batch [5050][5500]\t Training Loss 0.1783\t Accuracy 0.8715\n",
      "Epoch [14][20]\t Batch [5100][5500]\t Training Loss 0.1783\t Accuracy 0.8717\n",
      "Epoch [14][20]\t Batch [5150][5500]\t Training Loss 0.1781\t Accuracy 0.8718\n",
      "Epoch [14][20]\t Batch [5200][5500]\t Training Loss 0.1780\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [5250][5500]\t Training Loss 0.1780\t Accuracy 0.8719\n",
      "Epoch [14][20]\t Batch [5300][5500]\t Training Loss 0.1780\t Accuracy 0.8718\n",
      "Epoch [14][20]\t Batch [5350][5500]\t Training Loss 0.1779\t Accuracy 0.8720\n",
      "Epoch [14][20]\t Batch [5400][5500]\t Training Loss 0.1778\t Accuracy 0.8721\n",
      "Epoch [14][20]\t Batch [5450][5500]\t Training Loss 0.1778\t Accuracy 0.8722\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1778\t Average training accuracy 0.8721\n",
      "Epoch [14]\t Average validation loss 0.1594\t Average validation accuracy 0.9030\n",
      "\n",
      "Epoch [15][20]\t Batch [0][5500]\t Training Loss 0.1056\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][5500]\t Training Loss 0.1597\t Accuracy 0.8725\n",
      "Epoch [15][20]\t Batch [100][5500]\t Training Loss 0.1721\t Accuracy 0.8733\n",
      "Epoch [15][20]\t Batch [150][5500]\t Training Loss 0.1798\t Accuracy 0.8623\n",
      "Epoch [15][20]\t Batch [200][5500]\t Training Loss 0.1750\t Accuracy 0.8701\n",
      "Epoch [15][20]\t Batch [250][5500]\t Training Loss 0.1713\t Accuracy 0.8765\n",
      "Epoch [15][20]\t Batch [300][5500]\t Training Loss 0.1692\t Accuracy 0.8791\n",
      "Epoch [15][20]\t Batch [350][5500]\t Training Loss 0.1687\t Accuracy 0.8809\n",
      "Epoch [15][20]\t Batch [400][5500]\t Training Loss 0.1686\t Accuracy 0.8825\n",
      "Epoch [15][20]\t Batch [450][5500]\t Training Loss 0.1691\t Accuracy 0.8829\n",
      "Epoch [15][20]\t Batch [500][5500]\t Training Loss 0.1684\t Accuracy 0.8844\n",
      "Epoch [15][20]\t Batch [550][5500]\t Training Loss 0.1681\t Accuracy 0.8826\n",
      "Epoch [15][20]\t Batch [600][5500]\t Training Loss 0.1676\t Accuracy 0.8840\n",
      "Epoch [15][20]\t Batch [650][5500]\t Training Loss 0.1669\t Accuracy 0.8859\n",
      "Epoch [15][20]\t Batch [700][5500]\t Training Loss 0.1670\t Accuracy 0.8846\n",
      "Epoch [15][20]\t Batch [750][5500]\t Training Loss 0.1683\t Accuracy 0.8824\n",
      "Epoch [15][20]\t Batch [800][5500]\t Training Loss 0.1691\t Accuracy 0.8818\n",
      "Epoch [15][20]\t Batch [850][5500]\t Training Loss 0.1700\t Accuracy 0.8808\n",
      "Epoch [15][20]\t Batch [900][5500]\t Training Loss 0.1713\t Accuracy 0.8781\n",
      "Epoch [15][20]\t Batch [950][5500]\t Training Loss 0.1708\t Accuracy 0.8782\n",
      "Epoch [15][20]\t Batch [1000][5500]\t Training Loss 0.1699\t Accuracy 0.8790\n",
      "Epoch [15][20]\t Batch [1050][5500]\t Training Loss 0.1695\t Accuracy 0.8799\n",
      "Epoch [15][20]\t Batch [1100][5500]\t Training Loss 0.1690\t Accuracy 0.8813\n",
      "Epoch [15][20]\t Batch [1150][5500]\t Training Loss 0.1688\t Accuracy 0.8813\n",
      "Epoch [15][20]\t Batch [1200][5500]\t Training Loss 0.1699\t Accuracy 0.8798\n",
      "Epoch [15][20]\t Batch [1250][5500]\t Training Loss 0.1702\t Accuracy 0.8792\n",
      "Epoch [15][20]\t Batch [1300][5500]\t Training Loss 0.1708\t Accuracy 0.8778\n",
      "Epoch [15][20]\t Batch [1350][5500]\t Training Loss 0.1711\t Accuracy 0.8773\n",
      "Epoch [15][20]\t Batch [1400][5500]\t Training Loss 0.1717\t Accuracy 0.8762\n",
      "Epoch [15][20]\t Batch [1450][5500]\t Training Loss 0.1726\t Accuracy 0.8746\n",
      "Epoch [15][20]\t Batch [1500][5500]\t Training Loss 0.1737\t Accuracy 0.8728\n",
      "Epoch [15][20]\t Batch [1550][5500]\t Training Loss 0.1735\t Accuracy 0.8733\n",
      "Epoch [15][20]\t Batch [1600][5500]\t Training Loss 0.1740\t Accuracy 0.8723\n",
      "Epoch [15][20]\t Batch [1650][5500]\t Training Loss 0.1736\t Accuracy 0.8728\n",
      "Epoch [15][20]\t Batch [1700][5500]\t Training Loss 0.1739\t Accuracy 0.8725\n",
      "Epoch [15][20]\t Batch [1750][5500]\t Training Loss 0.1740\t Accuracy 0.8725\n",
      "Epoch [15][20]\t Batch [1800][5500]\t Training Loss 0.1747\t Accuracy 0.8719\n",
      "Epoch [15][20]\t Batch [1850][5500]\t Training Loss 0.1742\t Accuracy 0.8730\n",
      "Epoch [15][20]\t Batch [1900][5500]\t Training Loss 0.1737\t Accuracy 0.8743\n",
      "Epoch [15][20]\t Batch [1950][5500]\t Training Loss 0.1737\t Accuracy 0.8744\n",
      "Epoch [15][20]\t Batch [2000][5500]\t Training Loss 0.1734\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [2050][5500]\t Training Loss 0.1733\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [2100][5500]\t Training Loss 0.1735\t Accuracy 0.8751\n",
      "Epoch [15][20]\t Batch [2150][5500]\t Training Loss 0.1733\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [2200][5500]\t Training Loss 0.1728\t Accuracy 0.8759\n",
      "Epoch [15][20]\t Batch [2250][5500]\t Training Loss 0.1731\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [2300][5500]\t Training Loss 0.1735\t Accuracy 0.8752\n",
      "Epoch [15][20]\t Batch [2350][5500]\t Training Loss 0.1734\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [2400][5500]\t Training Loss 0.1735\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [2450][5500]\t Training Loss 0.1735\t Accuracy 0.8757\n",
      "Epoch [15][20]\t Batch [2500][5500]\t Training Loss 0.1736\t Accuracy 0.8759\n",
      "Epoch [15][20]\t Batch [2550][5500]\t Training Loss 0.1733\t Accuracy 0.8763\n",
      "Epoch [15][20]\t Batch [2600][5500]\t Training Loss 0.1732\t Accuracy 0.8762\n",
      "Epoch [15][20]\t Batch [2650][5500]\t Training Loss 0.1731\t Accuracy 0.8767\n",
      "Epoch [15][20]\t Batch [2700][5500]\t Training Loss 0.1733\t Accuracy 0.8768\n",
      "Epoch [15][20]\t Batch [2750][5500]\t Training Loss 0.1736\t Accuracy 0.8764\n",
      "Epoch [15][20]\t Batch [2800][5500]\t Training Loss 0.1733\t Accuracy 0.8767\n",
      "Epoch [15][20]\t Batch [2850][5500]\t Training Loss 0.1731\t Accuracy 0.8772\n",
      "Epoch [15][20]\t Batch [2900][5500]\t Training Loss 0.1730\t Accuracy 0.8772\n",
      "Epoch [15][20]\t Batch [2950][5500]\t Training Loss 0.1732\t Accuracy 0.8770\n",
      "Epoch [15][20]\t Batch [3000][5500]\t Training Loss 0.1735\t Accuracy 0.8764\n",
      "Epoch [15][20]\t Batch [3050][5500]\t Training Loss 0.1737\t Accuracy 0.8764\n",
      "Epoch [15][20]\t Batch [3100][5500]\t Training Loss 0.1739\t Accuracy 0.8760\n",
      "Epoch [15][20]\t Batch [3150][5500]\t Training Loss 0.1742\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [3200][5500]\t Training Loss 0.1743\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [3250][5500]\t Training Loss 0.1747\t Accuracy 0.8742\n",
      "Epoch [15][20]\t Batch [3300][5500]\t Training Loss 0.1746\t Accuracy 0.8745\n",
      "Epoch [15][20]\t Batch [3350][5500]\t Training Loss 0.1746\t Accuracy 0.8745\n",
      "Epoch [15][20]\t Batch [3400][5500]\t Training Loss 0.1742\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [3450][5500]\t Training Loss 0.1740\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [3500][5500]\t Training Loss 0.1742\t Accuracy 0.8748\n",
      "Epoch [15][20]\t Batch [3550][5500]\t Training Loss 0.1742\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [3600][5500]\t Training Loss 0.1741\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [3650][5500]\t Training Loss 0.1741\t Accuracy 0.8751\n",
      "Epoch [15][20]\t Batch [3700][5500]\t Training Loss 0.1740\t Accuracy 0.8757\n",
      "Epoch [15][20]\t Batch [3750][5500]\t Training Loss 0.1743\t Accuracy 0.8753\n",
      "Epoch [15][20]\t Batch [3800][5500]\t Training Loss 0.1744\t Accuracy 0.8752\n",
      "Epoch [15][20]\t Batch [3850][5500]\t Training Loss 0.1744\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [3900][5500]\t Training Loss 0.1742\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [3950][5500]\t Training Loss 0.1741\t Accuracy 0.8756\n",
      "Epoch [15][20]\t Batch [4000][5500]\t Training Loss 0.1743\t Accuracy 0.8755\n",
      "Epoch [15][20]\t Batch [4050][5500]\t Training Loss 0.1742\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [4100][5500]\t Training Loss 0.1741\t Accuracy 0.8755\n",
      "Epoch [15][20]\t Batch [4150][5500]\t Training Loss 0.1743\t Accuracy 0.8752\n",
      "Epoch [15][20]\t Batch [4200][5500]\t Training Loss 0.1743\t Accuracy 0.8754\n",
      "Epoch [15][20]\t Batch [4250][5500]\t Training Loss 0.1746\t Accuracy 0.8749\n",
      "Epoch [15][20]\t Batch [4300][5500]\t Training Loss 0.1746\t Accuracy 0.8749\n",
      "Epoch [15][20]\t Batch [4350][5500]\t Training Loss 0.1744\t Accuracy 0.8752\n",
      "Epoch [15][20]\t Batch [4400][5500]\t Training Loss 0.1744\t Accuracy 0.8752\n",
      "Epoch [15][20]\t Batch [4450][5500]\t Training Loss 0.1746\t Accuracy 0.8751\n",
      "Epoch [15][20]\t Batch [4500][5500]\t Training Loss 0.1744\t Accuracy 0.8753\n",
      "Epoch [15][20]\t Batch [4550][5500]\t Training Loss 0.1745\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [4600][5500]\t Training Loss 0.1746\t Accuracy 0.8751\n",
      "Epoch [15][20]\t Batch [4650][5500]\t Training Loss 0.1748\t Accuracy 0.8748\n",
      "Epoch [15][20]\t Batch [4700][5500]\t Training Loss 0.1746\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [4750][5500]\t Training Loss 0.1746\t Accuracy 0.8748\n",
      "Epoch [15][20]\t Batch [4800][5500]\t Training Loss 0.1746\t Accuracy 0.8747\n",
      "Epoch [15][20]\t Batch [4850][5500]\t Training Loss 0.1744\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [4900][5500]\t Training Loss 0.1743\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [4950][5500]\t Training Loss 0.1744\t Accuracy 0.8748\n",
      "Epoch [15][20]\t Batch [5000][5500]\t Training Loss 0.1747\t Accuracy 0.8745\n",
      "Epoch [15][20]\t Batch [5050][5500]\t Training Loss 0.1748\t Accuracy 0.8743\n",
      "Epoch [15][20]\t Batch [5100][5500]\t Training Loss 0.1747\t Accuracy 0.8745\n",
      "Epoch [15][20]\t Batch [5150][5500]\t Training Loss 0.1746\t Accuracy 0.8746\n",
      "Epoch [15][20]\t Batch [5200][5500]\t Training Loss 0.1744\t Accuracy 0.8749\n",
      "Epoch [15][20]\t Batch [5250][5500]\t Training Loss 0.1745\t Accuracy 0.8748\n",
      "Epoch [15][20]\t Batch [5300][5500]\t Training Loss 0.1745\t Accuracy 0.8747\n",
      "Epoch [15][20]\t Batch [5350][5500]\t Training Loss 0.1743\t Accuracy 0.8749\n",
      "Epoch [15][20]\t Batch [5400][5500]\t Training Loss 0.1743\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [5450][5500]\t Training Loss 0.1742\t Accuracy 0.8751\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1742\t Average training accuracy 0.8750\n",
      "Epoch [15]\t Average validation loss 0.1560\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [16][20]\t Batch [0][5500]\t Training Loss 0.1022\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][5500]\t Training Loss 0.1564\t Accuracy 0.8745\n",
      "Epoch [16][20]\t Batch [100][5500]\t Training Loss 0.1686\t Accuracy 0.8762\n",
      "Epoch [16][20]\t Batch [150][5500]\t Training Loss 0.1763\t Accuracy 0.8662\n",
      "Epoch [16][20]\t Batch [200][5500]\t Training Loss 0.1715\t Accuracy 0.8741\n",
      "Epoch [16][20]\t Batch [250][5500]\t Training Loss 0.1679\t Accuracy 0.8801\n",
      "Epoch [16][20]\t Batch [300][5500]\t Training Loss 0.1657\t Accuracy 0.8834\n",
      "Epoch [16][20]\t Batch [350][5500]\t Training Loss 0.1653\t Accuracy 0.8849\n",
      "Epoch [16][20]\t Batch [400][5500]\t Training Loss 0.1651\t Accuracy 0.8863\n",
      "Epoch [16][20]\t Batch [450][5500]\t Training Loss 0.1656\t Accuracy 0.8867\n",
      "Epoch [16][20]\t Batch [500][5500]\t Training Loss 0.1649\t Accuracy 0.8880\n",
      "Epoch [16][20]\t Batch [550][5500]\t Training Loss 0.1645\t Accuracy 0.8862\n",
      "Epoch [16][20]\t Batch [600][5500]\t Training Loss 0.1641\t Accuracy 0.8872\n",
      "Epoch [16][20]\t Batch [650][5500]\t Training Loss 0.1633\t Accuracy 0.8889\n",
      "Epoch [16][20]\t Batch [700][5500]\t Training Loss 0.1635\t Accuracy 0.8876\n",
      "Epoch [16][20]\t Batch [750][5500]\t Training Loss 0.1648\t Accuracy 0.8854\n",
      "Epoch [16][20]\t Batch [800][5500]\t Training Loss 0.1655\t Accuracy 0.8845\n",
      "Epoch [16][20]\t Batch [850][5500]\t Training Loss 0.1664\t Accuracy 0.8834\n",
      "Epoch [16][20]\t Batch [900][5500]\t Training Loss 0.1677\t Accuracy 0.8808\n",
      "Epoch [16][20]\t Batch [950][5500]\t Training Loss 0.1672\t Accuracy 0.8811\n",
      "Epoch [16][20]\t Batch [1000][5500]\t Training Loss 0.1663\t Accuracy 0.8819\n",
      "Epoch [16][20]\t Batch [1050][5500]\t Training Loss 0.1660\t Accuracy 0.8829\n",
      "Epoch [16][20]\t Batch [1100][5500]\t Training Loss 0.1655\t Accuracy 0.8844\n",
      "Epoch [16][20]\t Batch [1150][5500]\t Training Loss 0.1652\t Accuracy 0.8843\n",
      "Epoch [16][20]\t Batch [1200][5500]\t Training Loss 0.1664\t Accuracy 0.8824\n",
      "Epoch [16][20]\t Batch [1250][5500]\t Training Loss 0.1666\t Accuracy 0.8819\n",
      "Epoch [16][20]\t Batch [1300][5500]\t Training Loss 0.1673\t Accuracy 0.8804\n",
      "Epoch [16][20]\t Batch [1350][5500]\t Training Loss 0.1676\t Accuracy 0.8798\n",
      "Epoch [16][20]\t Batch [1400][5500]\t Training Loss 0.1681\t Accuracy 0.8789\n",
      "Epoch [16][20]\t Batch [1450][5500]\t Training Loss 0.1690\t Accuracy 0.8774\n",
      "Epoch [16][20]\t Batch [1500][5500]\t Training Loss 0.1701\t Accuracy 0.8756\n",
      "Epoch [16][20]\t Batch [1550][5500]\t Training Loss 0.1699\t Accuracy 0.8761\n",
      "Epoch [16][20]\t Batch [1600][5500]\t Training Loss 0.1704\t Accuracy 0.8750\n",
      "Epoch [16][20]\t Batch [1650][5500]\t Training Loss 0.1700\t Accuracy 0.8756\n",
      "Epoch [16][20]\t Batch [1700][5500]\t Training Loss 0.1703\t Accuracy 0.8752\n",
      "Epoch [16][20]\t Batch [1750][5500]\t Training Loss 0.1704\t Accuracy 0.8750\n",
      "Epoch [16][20]\t Batch [1800][5500]\t Training Loss 0.1711\t Accuracy 0.8745\n",
      "Epoch [16][20]\t Batch [1850][5500]\t Training Loss 0.1706\t Accuracy 0.8756\n",
      "Epoch [16][20]\t Batch [1900][5500]\t Training Loss 0.1701\t Accuracy 0.8768\n",
      "Epoch [16][20]\t Batch [1950][5500]\t Training Loss 0.1701\t Accuracy 0.8770\n",
      "Epoch [16][20]\t Batch [2000][5500]\t Training Loss 0.1697\t Accuracy 0.8776\n",
      "Epoch [16][20]\t Batch [2050][5500]\t Training Loss 0.1697\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [2100][5500]\t Training Loss 0.1699\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [2150][5500]\t Training Loss 0.1697\t Accuracy 0.8780\n",
      "Epoch [16][20]\t Batch [2200][5500]\t Training Loss 0.1692\t Accuracy 0.8785\n",
      "Epoch [16][20]\t Batch [2250][5500]\t Training Loss 0.1695\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [2300][5500]\t Training Loss 0.1699\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [2350][5500]\t Training Loss 0.1698\t Accuracy 0.8783\n",
      "Epoch [16][20]\t Batch [2400][5500]\t Training Loss 0.1699\t Accuracy 0.8783\n",
      "Epoch [16][20]\t Batch [2450][5500]\t Training Loss 0.1698\t Accuracy 0.8784\n",
      "Epoch [16][20]\t Batch [2500][5500]\t Training Loss 0.1700\t Accuracy 0.8786\n",
      "Epoch [16][20]\t Batch [2550][5500]\t Training Loss 0.1696\t Accuracy 0.8789\n",
      "Epoch [16][20]\t Batch [2600][5500]\t Training Loss 0.1696\t Accuracy 0.8789\n",
      "Epoch [16][20]\t Batch [2650][5500]\t Training Loss 0.1695\t Accuracy 0.8793\n",
      "Epoch [16][20]\t Batch [2700][5500]\t Training Loss 0.1697\t Accuracy 0.8795\n",
      "Epoch [16][20]\t Batch [2750][5500]\t Training Loss 0.1700\t Accuracy 0.8791\n",
      "Epoch [16][20]\t Batch [2800][5500]\t Training Loss 0.1697\t Accuracy 0.8793\n",
      "Epoch [16][20]\t Batch [2850][5500]\t Training Loss 0.1695\t Accuracy 0.8798\n",
      "Epoch [16][20]\t Batch [2900][5500]\t Training Loss 0.1694\t Accuracy 0.8797\n",
      "Epoch [16][20]\t Batch [2950][5500]\t Training Loss 0.1696\t Accuracy 0.8795\n",
      "Epoch [16][20]\t Batch [3000][5500]\t Training Loss 0.1699\t Accuracy 0.8789\n",
      "Epoch [16][20]\t Batch [3050][5500]\t Training Loss 0.1700\t Accuracy 0.8789\n",
      "Epoch [16][20]\t Batch [3100][5500]\t Training Loss 0.1703\t Accuracy 0.8785\n",
      "Epoch [16][20]\t Batch [3150][5500]\t Training Loss 0.1706\t Accuracy 0.8778\n",
      "Epoch [16][20]\t Batch [3200][5500]\t Training Loss 0.1707\t Accuracy 0.8775\n",
      "Epoch [16][20]\t Batch [3250][5500]\t Training Loss 0.1711\t Accuracy 0.8767\n",
      "Epoch [16][20]\t Batch [3300][5500]\t Training Loss 0.1710\t Accuracy 0.8771\n",
      "Epoch [16][20]\t Batch [3350][5500]\t Training Loss 0.1710\t Accuracy 0.8771\n",
      "Epoch [16][20]\t Batch [3400][5500]\t Training Loss 0.1706\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [3450][5500]\t Training Loss 0.1704\t Accuracy 0.8781\n",
      "Epoch [16][20]\t Batch [3500][5500]\t Training Loss 0.1706\t Accuracy 0.8775\n",
      "Epoch [16][20]\t Batch [3550][5500]\t Training Loss 0.1706\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [3600][5500]\t Training Loss 0.1705\t Accuracy 0.8776\n",
      "Epoch [16][20]\t Batch [3650][5500]\t Training Loss 0.1705\t Accuracy 0.8778\n",
      "Epoch [16][20]\t Batch [3700][5500]\t Training Loss 0.1704\t Accuracy 0.8784\n",
      "Epoch [16][20]\t Batch [3750][5500]\t Training Loss 0.1707\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [3800][5500]\t Training Loss 0.1708\t Accuracy 0.8778\n",
      "Epoch [16][20]\t Batch [3850][5500]\t Training Loss 0.1707\t Accuracy 0.8781\n",
      "Epoch [16][20]\t Batch [3900][5500]\t Training Loss 0.1706\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [3950][5500]\t Training Loss 0.1705\t Accuracy 0.8783\n",
      "Epoch [16][20]\t Batch [4000][5500]\t Training Loss 0.1707\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [4050][5500]\t Training Loss 0.1706\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [4100][5500]\t Training Loss 0.1705\t Accuracy 0.8783\n",
      "Epoch [16][20]\t Batch [4150][5500]\t Training Loss 0.1707\t Accuracy 0.8781\n",
      "Epoch [16][20]\t Batch [4200][5500]\t Training Loss 0.1707\t Accuracy 0.8783\n",
      "Epoch [16][20]\t Batch [4250][5500]\t Training Loss 0.1710\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [4300][5500]\t Training Loss 0.1709\t Accuracy 0.8778\n",
      "Epoch [16][20]\t Batch [4350][5500]\t Training Loss 0.1708\t Accuracy 0.8781\n",
      "Epoch [16][20]\t Batch [4400][5500]\t Training Loss 0.1708\t Accuracy 0.8780\n",
      "Epoch [16][20]\t Batch [4450][5500]\t Training Loss 0.1709\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [4500][5500]\t Training Loss 0.1708\t Accuracy 0.8782\n",
      "Epoch [16][20]\t Batch [4550][5500]\t Training Loss 0.1709\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [4600][5500]\t Training Loss 0.1710\t Accuracy 0.8780\n",
      "Epoch [16][20]\t Batch [4650][5500]\t Training Loss 0.1712\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [4700][5500]\t Training Loss 0.1710\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [4750][5500]\t Training Loss 0.1710\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [4800][5500]\t Training Loss 0.1710\t Accuracy 0.8775\n",
      "Epoch [16][20]\t Batch [4850][5500]\t Training Loss 0.1708\t Accuracy 0.8779\n",
      "Epoch [16][20]\t Batch [4900][5500]\t Training Loss 0.1707\t Accuracy 0.8778\n",
      "Epoch [16][20]\t Batch [4950][5500]\t Training Loss 0.1708\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [5000][5500]\t Training Loss 0.1711\t Accuracy 0.8774\n",
      "Epoch [16][20]\t Batch [5050][5500]\t Training Loss 0.1711\t Accuracy 0.8772\n",
      "Epoch [16][20]\t Batch [5100][5500]\t Training Loss 0.1711\t Accuracy 0.8773\n",
      "Epoch [16][20]\t Batch [5150][5500]\t Training Loss 0.1710\t Accuracy 0.8774\n",
      "Epoch [16][20]\t Batch [5200][5500]\t Training Loss 0.1708\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [5250][5500]\t Training Loss 0.1708\t Accuracy 0.8775\n",
      "Epoch [16][20]\t Batch [5300][5500]\t Training Loss 0.1709\t Accuracy 0.8775\n",
      "Epoch [16][20]\t Batch [5350][5500]\t Training Loss 0.1707\t Accuracy 0.8776\n",
      "Epoch [16][20]\t Batch [5400][5500]\t Training Loss 0.1707\t Accuracy 0.8777\n",
      "Epoch [16][20]\t Batch [5450][5500]\t Training Loss 0.1706\t Accuracy 0.8778\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1706\t Average training accuracy 0.8777\n",
      "Epoch [16]\t Average validation loss 0.1525\t Average validation accuracy 0.9054\n",
      "\n",
      "Epoch [17][20]\t Batch [0][5500]\t Training Loss 0.0988\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][5500]\t Training Loss 0.1531\t Accuracy 0.8765\n",
      "Epoch [17][20]\t Batch [100][5500]\t Training Loss 0.1652\t Accuracy 0.8772\n",
      "Epoch [17][20]\t Batch [150][5500]\t Training Loss 0.1728\t Accuracy 0.8675\n",
      "Epoch [17][20]\t Batch [200][5500]\t Training Loss 0.1680\t Accuracy 0.8756\n",
      "Epoch [17][20]\t Batch [250][5500]\t Training Loss 0.1644\t Accuracy 0.8821\n",
      "Epoch [17][20]\t Batch [300][5500]\t Training Loss 0.1622\t Accuracy 0.8847\n",
      "Epoch [17][20]\t Batch [350][5500]\t Training Loss 0.1618\t Accuracy 0.8866\n",
      "Epoch [17][20]\t Batch [400][5500]\t Training Loss 0.1616\t Accuracy 0.8883\n",
      "Epoch [17][20]\t Batch [450][5500]\t Training Loss 0.1620\t Accuracy 0.8885\n",
      "Epoch [17][20]\t Batch [500][5500]\t Training Loss 0.1613\t Accuracy 0.8902\n",
      "Epoch [17][20]\t Batch [550][5500]\t Training Loss 0.1610\t Accuracy 0.8886\n",
      "Epoch [17][20]\t Batch [600][5500]\t Training Loss 0.1605\t Accuracy 0.8894\n",
      "Epoch [17][20]\t Batch [650][5500]\t Training Loss 0.1597\t Accuracy 0.8909\n",
      "Epoch [17][20]\t Batch [700][5500]\t Training Loss 0.1599\t Accuracy 0.8893\n",
      "Epoch [17][20]\t Batch [750][5500]\t Training Loss 0.1612\t Accuracy 0.8877\n",
      "Epoch [17][20]\t Batch [800][5500]\t Training Loss 0.1620\t Accuracy 0.8869\n",
      "Epoch [17][20]\t Batch [850][5500]\t Training Loss 0.1629\t Accuracy 0.8858\n",
      "Epoch [17][20]\t Batch [900][5500]\t Training Loss 0.1641\t Accuracy 0.8834\n",
      "Epoch [17][20]\t Batch [950][5500]\t Training Loss 0.1637\t Accuracy 0.8838\n",
      "Epoch [17][20]\t Batch [1000][5500]\t Training Loss 0.1627\t Accuracy 0.8846\n",
      "Epoch [17][20]\t Batch [1050][5500]\t Training Loss 0.1624\t Accuracy 0.8857\n",
      "Epoch [17][20]\t Batch [1100][5500]\t Training Loss 0.1619\t Accuracy 0.8872\n",
      "Epoch [17][20]\t Batch [1150][5500]\t Training Loss 0.1616\t Accuracy 0.8871\n",
      "Epoch [17][20]\t Batch [1200][5500]\t Training Loss 0.1628\t Accuracy 0.8853\n",
      "Epoch [17][20]\t Batch [1250][5500]\t Training Loss 0.1630\t Accuracy 0.8849\n",
      "Epoch [17][20]\t Batch [1300][5500]\t Training Loss 0.1637\t Accuracy 0.8833\n",
      "Epoch [17][20]\t Batch [1350][5500]\t Training Loss 0.1640\t Accuracy 0.8826\n",
      "Epoch [17][20]\t Batch [1400][5500]\t Training Loss 0.1645\t Accuracy 0.8817\n",
      "Epoch [17][20]\t Batch [1450][5500]\t Training Loss 0.1654\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [1500][5500]\t Training Loss 0.1665\t Accuracy 0.8786\n",
      "Epoch [17][20]\t Batch [1550][5500]\t Training Loss 0.1663\t Accuracy 0.8791\n",
      "Epoch [17][20]\t Batch [1600][5500]\t Training Loss 0.1668\t Accuracy 0.8781\n",
      "Epoch [17][20]\t Batch [1650][5500]\t Training Loss 0.1664\t Accuracy 0.8787\n",
      "Epoch [17][20]\t Batch [1700][5500]\t Training Loss 0.1667\t Accuracy 0.8782\n",
      "Epoch [17][20]\t Batch [1750][5500]\t Training Loss 0.1668\t Accuracy 0.8781\n",
      "Epoch [17][20]\t Batch [1800][5500]\t Training Loss 0.1675\t Accuracy 0.8774\n",
      "Epoch [17][20]\t Batch [1850][5500]\t Training Loss 0.1670\t Accuracy 0.8785\n",
      "Epoch [17][20]\t Batch [1900][5500]\t Training Loss 0.1665\t Accuracy 0.8796\n",
      "Epoch [17][20]\t Batch [1950][5500]\t Training Loss 0.1664\t Accuracy 0.8799\n",
      "Epoch [17][20]\t Batch [2000][5500]\t Training Loss 0.1661\t Accuracy 0.8804\n",
      "Epoch [17][20]\t Batch [2050][5500]\t Training Loss 0.1661\t Accuracy 0.8809\n",
      "Epoch [17][20]\t Batch [2100][5500]\t Training Loss 0.1663\t Accuracy 0.8804\n",
      "Epoch [17][20]\t Batch [2150][5500]\t Training Loss 0.1661\t Accuracy 0.8807\n",
      "Epoch [17][20]\t Batch [2200][5500]\t Training Loss 0.1656\t Accuracy 0.8811\n",
      "Epoch [17][20]\t Batch [2250][5500]\t Training Loss 0.1658\t Accuracy 0.8809\n",
      "Epoch [17][20]\t Batch [2300][5500]\t Training Loss 0.1662\t Accuracy 0.8806\n",
      "Epoch [17][20]\t Batch [2350][5500]\t Training Loss 0.1662\t Accuracy 0.8811\n",
      "Epoch [17][20]\t Batch [2400][5500]\t Training Loss 0.1662\t Accuracy 0.8810\n",
      "Epoch [17][20]\t Batch [2450][5500]\t Training Loss 0.1662\t Accuracy 0.8812\n",
      "Epoch [17][20]\t Batch [2500][5500]\t Training Loss 0.1664\t Accuracy 0.8813\n",
      "Epoch [17][20]\t Batch [2550][5500]\t Training Loss 0.1660\t Accuracy 0.8817\n",
      "Epoch [17][20]\t Batch [2600][5500]\t Training Loss 0.1659\t Accuracy 0.8816\n",
      "Epoch [17][20]\t Batch [2650][5500]\t Training Loss 0.1659\t Accuracy 0.8820\n",
      "Epoch [17][20]\t Batch [2700][5500]\t Training Loss 0.1661\t Accuracy 0.8821\n",
      "Epoch [17][20]\t Batch [2750][5500]\t Training Loss 0.1663\t Accuracy 0.8816\n",
      "Epoch [17][20]\t Batch [2800][5500]\t Training Loss 0.1661\t Accuracy 0.8818\n",
      "Epoch [17][20]\t Batch [2850][5500]\t Training Loss 0.1659\t Accuracy 0.8823\n",
      "Epoch [17][20]\t Batch [2900][5500]\t Training Loss 0.1658\t Accuracy 0.8821\n",
      "Epoch [17][20]\t Batch [2950][5500]\t Training Loss 0.1659\t Accuracy 0.8819\n",
      "Epoch [17][20]\t Batch [3000][5500]\t Training Loss 0.1662\t Accuracy 0.8813\n",
      "Epoch [17][20]\t Batch [3050][5500]\t Training Loss 0.1664\t Accuracy 0.8814\n",
      "Epoch [17][20]\t Batch [3100][5500]\t Training Loss 0.1667\t Accuracy 0.8809\n",
      "Epoch [17][20]\t Batch [3150][5500]\t Training Loss 0.1670\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [3200][5500]\t Training Loss 0.1671\t Accuracy 0.8799\n",
      "Epoch [17][20]\t Batch [3250][5500]\t Training Loss 0.1675\t Accuracy 0.8792\n",
      "Epoch [17][20]\t Batch [3300][5500]\t Training Loss 0.1673\t Accuracy 0.8795\n",
      "Epoch [17][20]\t Batch [3350][5500]\t Training Loss 0.1674\t Accuracy 0.8796\n",
      "Epoch [17][20]\t Batch [3400][5500]\t Training Loss 0.1670\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [3450][5500]\t Training Loss 0.1668\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [3500][5500]\t Training Loss 0.1670\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [3550][5500]\t Training Loss 0.1669\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [3600][5500]\t Training Loss 0.1669\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [3650][5500]\t Training Loss 0.1669\t Accuracy 0.8803\n",
      "Epoch [17][20]\t Batch [3700][5500]\t Training Loss 0.1667\t Accuracy 0.8809\n",
      "Epoch [17][20]\t Batch [3750][5500]\t Training Loss 0.1671\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [3800][5500]\t Training Loss 0.1672\t Accuracy 0.8803\n",
      "Epoch [17][20]\t Batch [3850][5500]\t Training Loss 0.1671\t Accuracy 0.8806\n",
      "Epoch [17][20]\t Batch [3900][5500]\t Training Loss 0.1669\t Accuracy 0.8807\n",
      "Epoch [17][20]\t Batch [3950][5500]\t Training Loss 0.1669\t Accuracy 0.8808\n",
      "Epoch [17][20]\t Batch [4000][5500]\t Training Loss 0.1670\t Accuracy 0.8808\n",
      "Epoch [17][20]\t Batch [4050][5500]\t Training Loss 0.1670\t Accuracy 0.8808\n",
      "Epoch [17][20]\t Batch [4100][5500]\t Training Loss 0.1668\t Accuracy 0.8810\n",
      "Epoch [17][20]\t Batch [4150][5500]\t Training Loss 0.1670\t Accuracy 0.8808\n",
      "Epoch [17][20]\t Batch [4200][5500]\t Training Loss 0.1670\t Accuracy 0.8809\n",
      "Epoch [17][20]\t Batch [4250][5500]\t Training Loss 0.1673\t Accuracy 0.8804\n",
      "Epoch [17][20]\t Batch [4300][5500]\t Training Loss 0.1673\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4350][5500]\t Training Loss 0.1672\t Accuracy 0.8807\n",
      "Epoch [17][20]\t Batch [4400][5500]\t Training Loss 0.1672\t Accuracy 0.8807\n",
      "Epoch [17][20]\t Batch [4450][5500]\t Training Loss 0.1673\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4500][5500]\t Training Loss 0.1672\t Accuracy 0.8808\n",
      "Epoch [17][20]\t Batch [4550][5500]\t Training Loss 0.1673\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4600][5500]\t Training Loss 0.1673\t Accuracy 0.8806\n",
      "Epoch [17][20]\t Batch [4650][5500]\t Training Loss 0.1675\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [4700][5500]\t Training Loss 0.1673\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4750][5500]\t Training Loss 0.1674\t Accuracy 0.8803\n",
      "Epoch [17][20]\t Batch [4800][5500]\t Training Loss 0.1674\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [4850][5500]\t Training Loss 0.1671\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4900][5500]\t Training Loss 0.1671\t Accuracy 0.8805\n",
      "Epoch [17][20]\t Batch [4950][5500]\t Training Loss 0.1672\t Accuracy 0.8803\n",
      "Epoch [17][20]\t Batch [5000][5500]\t Training Loss 0.1674\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [5050][5500]\t Training Loss 0.1675\t Accuracy 0.8799\n",
      "Epoch [17][20]\t Batch [5100][5500]\t Training Loss 0.1675\t Accuracy 0.8801\n",
      "Epoch [17][20]\t Batch [5150][5500]\t Training Loss 0.1673\t Accuracy 0.8801\n",
      "Epoch [17][20]\t Batch [5200][5500]\t Training Loss 0.1672\t Accuracy 0.8804\n",
      "Epoch [17][20]\t Batch [5250][5500]\t Training Loss 0.1672\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [5300][5500]\t Training Loss 0.1673\t Accuracy 0.8801\n",
      "Epoch [17][20]\t Batch [5350][5500]\t Training Loss 0.1671\t Accuracy 0.8803\n",
      "Epoch [17][20]\t Batch [5400][5500]\t Training Loss 0.1671\t Accuracy 0.8804\n",
      "Epoch [17][20]\t Batch [5450][5500]\t Training Loss 0.1670\t Accuracy 0.8805\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1670\t Average training accuracy 0.8804\n",
      "Epoch [17]\t Average validation loss 0.1490\t Average validation accuracy 0.9078\n",
      "\n",
      "Epoch [18][20]\t Batch [0][5500]\t Training Loss 0.0955\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][5500]\t Training Loss 0.1497\t Accuracy 0.8824\n",
      "Epoch [18][20]\t Batch [100][5500]\t Training Loss 0.1616\t Accuracy 0.8832\n",
      "Epoch [18][20]\t Batch [150][5500]\t Training Loss 0.1693\t Accuracy 0.8715\n",
      "Epoch [18][20]\t Batch [200][5500]\t Training Loss 0.1644\t Accuracy 0.8796\n",
      "Epoch [18][20]\t Batch [250][5500]\t Training Loss 0.1609\t Accuracy 0.8857\n",
      "Epoch [18][20]\t Batch [300][5500]\t Training Loss 0.1588\t Accuracy 0.8877\n",
      "Epoch [18][20]\t Batch [350][5500]\t Training Loss 0.1583\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [400][5500]\t Training Loss 0.1580\t Accuracy 0.8913\n",
      "Epoch [18][20]\t Batch [450][5500]\t Training Loss 0.1585\t Accuracy 0.8911\n",
      "Epoch [18][20]\t Batch [500][5500]\t Training Loss 0.1577\t Accuracy 0.8926\n",
      "Epoch [18][20]\t Batch [550][5500]\t Training Loss 0.1574\t Accuracy 0.8909\n",
      "Epoch [18][20]\t Batch [600][5500]\t Training Loss 0.1570\t Accuracy 0.8915\n",
      "Epoch [18][20]\t Batch [650][5500]\t Training Loss 0.1562\t Accuracy 0.8929\n",
      "Epoch [18][20]\t Batch [700][5500]\t Training Loss 0.1564\t Accuracy 0.8913\n",
      "Epoch [18][20]\t Batch [750][5500]\t Training Loss 0.1576\t Accuracy 0.8896\n",
      "Epoch [18][20]\t Batch [800][5500]\t Training Loss 0.1584\t Accuracy 0.8888\n",
      "Epoch [18][20]\t Batch [850][5500]\t Training Loss 0.1593\t Accuracy 0.8878\n",
      "Epoch [18][20]\t Batch [900][5500]\t Training Loss 0.1605\t Accuracy 0.8857\n",
      "Epoch [18][20]\t Batch [950][5500]\t Training Loss 0.1601\t Accuracy 0.8860\n",
      "Epoch [18][20]\t Batch [1000][5500]\t Training Loss 0.1592\t Accuracy 0.8869\n",
      "Epoch [18][20]\t Batch [1050][5500]\t Training Loss 0.1588\t Accuracy 0.8877\n",
      "Epoch [18][20]\t Batch [1100][5500]\t Training Loss 0.1583\t Accuracy 0.8891\n",
      "Epoch [18][20]\t Batch [1150][5500]\t Training Loss 0.1581\t Accuracy 0.8891\n",
      "Epoch [18][20]\t Batch [1200][5500]\t Training Loss 0.1592\t Accuracy 0.8870\n",
      "Epoch [18][20]\t Batch [1250][5500]\t Training Loss 0.1594\t Accuracy 0.8867\n",
      "Epoch [18][20]\t Batch [1300][5500]\t Training Loss 0.1601\t Accuracy 0.8851\n",
      "Epoch [18][20]\t Batch [1350][5500]\t Training Loss 0.1604\t Accuracy 0.8845\n",
      "Epoch [18][20]\t Batch [1400][5500]\t Training Loss 0.1609\t Accuracy 0.8835\n",
      "Epoch [18][20]\t Batch [1450][5500]\t Training Loss 0.1618\t Accuracy 0.8824\n",
      "Epoch [18][20]\t Batch [1500][5500]\t Training Loss 0.1629\t Accuracy 0.8805\n",
      "Epoch [18][20]\t Batch [1550][5500]\t Training Loss 0.1627\t Accuracy 0.8810\n",
      "Epoch [18][20]\t Batch [1600][5500]\t Training Loss 0.1632\t Accuracy 0.8799\n",
      "Epoch [18][20]\t Batch [1650][5500]\t Training Loss 0.1628\t Accuracy 0.8806\n",
      "Epoch [18][20]\t Batch [1700][5500]\t Training Loss 0.1630\t Accuracy 0.8802\n",
      "Epoch [18][20]\t Batch [1750][5500]\t Training Loss 0.1632\t Accuracy 0.8802\n",
      "Epoch [18][20]\t Batch [1800][5500]\t Training Loss 0.1639\t Accuracy 0.8795\n",
      "Epoch [18][20]\t Batch [1850][5500]\t Training Loss 0.1634\t Accuracy 0.8807\n",
      "Epoch [18][20]\t Batch [1900][5500]\t Training Loss 0.1628\t Accuracy 0.8817\n",
      "Epoch [18][20]\t Batch [1950][5500]\t Training Loss 0.1628\t Accuracy 0.8820\n",
      "Epoch [18][20]\t Batch [2000][5500]\t Training Loss 0.1625\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [2050][5500]\t Training Loss 0.1624\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [2100][5500]\t Training Loss 0.1627\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [2150][5500]\t Training Loss 0.1624\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [2200][5500]\t Training Loss 0.1620\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [2250][5500]\t Training Loss 0.1622\t Accuracy 0.8832\n",
      "Epoch [18][20]\t Batch [2300][5500]\t Training Loss 0.1626\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [2350][5500]\t Training Loss 0.1625\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [2400][5500]\t Training Loss 0.1626\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [2450][5500]\t Training Loss 0.1626\t Accuracy 0.8835\n",
      "Epoch [18][20]\t Batch [2500][5500]\t Training Loss 0.1627\t Accuracy 0.8837\n",
      "Epoch [18][20]\t Batch [2550][5500]\t Training Loss 0.1624\t Accuracy 0.8840\n",
      "Epoch [18][20]\t Batch [2600][5500]\t Training Loss 0.1623\t Accuracy 0.8842\n",
      "Epoch [18][20]\t Batch [2650][5500]\t Training Loss 0.1623\t Accuracy 0.8845\n",
      "Epoch [18][20]\t Batch [2700][5500]\t Training Loss 0.1624\t Accuracy 0.8846\n",
      "Epoch [18][20]\t Batch [2750][5500]\t Training Loss 0.1627\t Accuracy 0.8842\n",
      "Epoch [18][20]\t Batch [2800][5500]\t Training Loss 0.1625\t Accuracy 0.8843\n",
      "Epoch [18][20]\t Batch [2850][5500]\t Training Loss 0.1622\t Accuracy 0.8847\n",
      "Epoch [18][20]\t Batch [2900][5500]\t Training Loss 0.1622\t Accuracy 0.8846\n",
      "Epoch [18][20]\t Batch [2950][5500]\t Training Loss 0.1623\t Accuracy 0.8844\n",
      "Epoch [18][20]\t Batch [3000][5500]\t Training Loss 0.1626\t Accuracy 0.8839\n",
      "Epoch [18][20]\t Batch [3050][5500]\t Training Loss 0.1628\t Accuracy 0.8839\n",
      "Epoch [18][20]\t Batch [3100][5500]\t Training Loss 0.1631\t Accuracy 0.8836\n",
      "Epoch [18][20]\t Batch [3150][5500]\t Training Loss 0.1634\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [3200][5500]\t Training Loss 0.1635\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [3250][5500]\t Training Loss 0.1639\t Accuracy 0.8819\n",
      "Epoch [18][20]\t Batch [3300][5500]\t Training Loss 0.1637\t Accuracy 0.8822\n",
      "Epoch [18][20]\t Batch [3350][5500]\t Training Loss 0.1637\t Accuracy 0.8823\n",
      "Epoch [18][20]\t Batch [3400][5500]\t Training Loss 0.1633\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [3450][5500]\t Training Loss 0.1632\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [3500][5500]\t Training Loss 0.1634\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [3550][5500]\t Training Loss 0.1633\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [3600][5500]\t Training Loss 0.1633\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [3650][5500]\t Training Loss 0.1633\t Accuracy 0.8830\n",
      "Epoch [18][20]\t Batch [3700][5500]\t Training Loss 0.1631\t Accuracy 0.8835\n",
      "Epoch [18][20]\t Batch [3750][5500]\t Training Loss 0.1635\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [3800][5500]\t Training Loss 0.1635\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [3850][5500]\t Training Loss 0.1635\t Accuracy 0.8832\n",
      "Epoch [18][20]\t Batch [3900][5500]\t Training Loss 0.1633\t Accuracy 0.8833\n",
      "Epoch [18][20]\t Batch [3950][5500]\t Training Loss 0.1633\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [4000][5500]\t Training Loss 0.1634\t Accuracy 0.8833\n",
      "Epoch [18][20]\t Batch [4050][5500]\t Training Loss 0.1633\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [4100][5500]\t Training Loss 0.1632\t Accuracy 0.8836\n",
      "Epoch [18][20]\t Batch [4150][5500]\t Training Loss 0.1634\t Accuracy 0.8833\n",
      "Epoch [18][20]\t Batch [4200][5500]\t Training Loss 0.1634\t Accuracy 0.8834\n",
      "Epoch [18][20]\t Batch [4250][5500]\t Training Loss 0.1637\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [4300][5500]\t Training Loss 0.1637\t Accuracy 0.8830\n",
      "Epoch [18][20]\t Batch [4350][5500]\t Training Loss 0.1636\t Accuracy 0.8832\n",
      "Epoch [18][20]\t Batch [4400][5500]\t Training Loss 0.1635\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [4450][5500]\t Training Loss 0.1637\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [4500][5500]\t Training Loss 0.1636\t Accuracy 0.8833\n",
      "Epoch [18][20]\t Batch [4550][5500]\t Training Loss 0.1636\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [4600][5500]\t Training Loss 0.1637\t Accuracy 0.8831\n",
      "Epoch [18][20]\t Batch [4650][5500]\t Training Loss 0.1639\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [4700][5500]\t Training Loss 0.1637\t Accuracy 0.8830\n",
      "Epoch [18][20]\t Batch [4750][5500]\t Training Loss 0.1638\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [4800][5500]\t Training Loss 0.1638\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [4850][5500]\t Training Loss 0.1635\t Accuracy 0.8830\n",
      "Epoch [18][20]\t Batch [4900][5500]\t Training Loss 0.1635\t Accuracy 0.8830\n",
      "Epoch [18][20]\t Batch [4950][5500]\t Training Loss 0.1636\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [5000][5500]\t Training Loss 0.1638\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [5050][5500]\t Training Loss 0.1639\t Accuracy 0.8825\n",
      "Epoch [18][20]\t Batch [5100][5500]\t Training Loss 0.1638\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [5150][5500]\t Training Loss 0.1637\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [5200][5500]\t Training Loss 0.1635\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [5250][5500]\t Training Loss 0.1636\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [5300][5500]\t Training Loss 0.1637\t Accuracy 0.8826\n",
      "Epoch [18][20]\t Batch [5350][5500]\t Training Loss 0.1635\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [5400][5500]\t Training Loss 0.1635\t Accuracy 0.8828\n",
      "Epoch [18][20]\t Batch [5450][5500]\t Training Loss 0.1634\t Accuracy 0.8829\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1634\t Average training accuracy 0.8828\n",
      "Epoch [18]\t Average validation loss 0.1455\t Average validation accuracy 0.9102\n",
      "\n",
      "Epoch [19][20]\t Batch [0][5500]\t Training Loss 0.0923\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][5500]\t Training Loss 0.1465\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [100][5500]\t Training Loss 0.1582\t Accuracy 0.8851\n",
      "Epoch [19][20]\t Batch [150][5500]\t Training Loss 0.1657\t Accuracy 0.8755\n",
      "Epoch [19][20]\t Batch [200][5500]\t Training Loss 0.1608\t Accuracy 0.8836\n",
      "Epoch [19][20]\t Batch [250][5500]\t Training Loss 0.1575\t Accuracy 0.8892\n",
      "Epoch [19][20]\t Batch [300][5500]\t Training Loss 0.1553\t Accuracy 0.8910\n",
      "Epoch [19][20]\t Batch [350][5500]\t Training Loss 0.1548\t Accuracy 0.8932\n",
      "Epoch [19][20]\t Batch [400][5500]\t Training Loss 0.1545\t Accuracy 0.8943\n",
      "Epoch [19][20]\t Batch [450][5500]\t Training Loss 0.1549\t Accuracy 0.8949\n",
      "Epoch [19][20]\t Batch [500][5500]\t Training Loss 0.1542\t Accuracy 0.8964\n",
      "Epoch [19][20]\t Batch [550][5500]\t Training Loss 0.1539\t Accuracy 0.8949\n",
      "Epoch [19][20]\t Batch [600][5500]\t Training Loss 0.1535\t Accuracy 0.8957\n",
      "Epoch [19][20]\t Batch [650][5500]\t Training Loss 0.1527\t Accuracy 0.8969\n",
      "Epoch [19][20]\t Batch [700][5500]\t Training Loss 0.1529\t Accuracy 0.8950\n",
      "Epoch [19][20]\t Batch [750][5500]\t Training Loss 0.1541\t Accuracy 0.8931\n",
      "Epoch [19][20]\t Batch [800][5500]\t Training Loss 0.1549\t Accuracy 0.8921\n",
      "Epoch [19][20]\t Batch [850][5500]\t Training Loss 0.1557\t Accuracy 0.8911\n",
      "Epoch [19][20]\t Batch [900][5500]\t Training Loss 0.1570\t Accuracy 0.8889\n",
      "Epoch [19][20]\t Batch [950][5500]\t Training Loss 0.1566\t Accuracy 0.8893\n",
      "Epoch [19][20]\t Batch [1000][5500]\t Training Loss 0.1557\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [1050][5500]\t Training Loss 0.1553\t Accuracy 0.8908\n",
      "Epoch [19][20]\t Batch [1100][5500]\t Training Loss 0.1548\t Accuracy 0.8923\n",
      "Epoch [19][20]\t Batch [1150][5500]\t Training Loss 0.1546\t Accuracy 0.8923\n",
      "Epoch [19][20]\t Batch [1200][5500]\t Training Loss 0.1557\t Accuracy 0.8904\n",
      "Epoch [19][20]\t Batch [1250][5500]\t Training Loss 0.1559\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [1300][5500]\t Training Loss 0.1565\t Accuracy 0.8883\n",
      "Epoch [19][20]\t Batch [1350][5500]\t Training Loss 0.1569\t Accuracy 0.8876\n",
      "Epoch [19][20]\t Batch [1400][5500]\t Training Loss 0.1574\t Accuracy 0.8867\n",
      "Epoch [19][20]\t Batch [1450][5500]\t Training Loss 0.1583\t Accuracy 0.8855\n",
      "Epoch [19][20]\t Batch [1500][5500]\t Training Loss 0.1593\t Accuracy 0.8838\n",
      "Epoch [19][20]\t Batch [1550][5500]\t Training Loss 0.1592\t Accuracy 0.8841\n",
      "Epoch [19][20]\t Batch [1600][5500]\t Training Loss 0.1596\t Accuracy 0.8829\n",
      "Epoch [19][20]\t Batch [1650][5500]\t Training Loss 0.1592\t Accuracy 0.8837\n",
      "Epoch [19][20]\t Batch [1700][5500]\t Training Loss 0.1595\t Accuracy 0.8834\n",
      "Epoch [19][20]\t Batch [1750][5500]\t Training Loss 0.1596\t Accuracy 0.8833\n",
      "Epoch [19][20]\t Batch [1800][5500]\t Training Loss 0.1603\t Accuracy 0.8825\n",
      "Epoch [19][20]\t Batch [1850][5500]\t Training Loss 0.1598\t Accuracy 0.8836\n",
      "Epoch [19][20]\t Batch [1900][5500]\t Training Loss 0.1593\t Accuracy 0.8848\n",
      "Epoch [19][20]\t Batch [1950][5500]\t Training Loss 0.1592\t Accuracy 0.8852\n",
      "Epoch [19][20]\t Batch [2000][5500]\t Training Loss 0.1589\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [2050][5500]\t Training Loss 0.1588\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [2100][5500]\t Training Loss 0.1591\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [2150][5500]\t Training Loss 0.1589\t Accuracy 0.8862\n",
      "Epoch [19][20]\t Batch [2200][5500]\t Training Loss 0.1584\t Accuracy 0.8868\n",
      "Epoch [19][20]\t Batch [2250][5500]\t Training Loss 0.1586\t Accuracy 0.8865\n",
      "Epoch [19][20]\t Batch [2300][5500]\t Training Loss 0.1590\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [2350][5500]\t Training Loss 0.1590\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [2400][5500]\t Training Loss 0.1590\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [2450][5500]\t Training Loss 0.1590\t Accuracy 0.8865\n",
      "Epoch [19][20]\t Batch [2500][5500]\t Training Loss 0.1592\t Accuracy 0.8866\n",
      "Epoch [19][20]\t Batch [2550][5500]\t Training Loss 0.1588\t Accuracy 0.8869\n",
      "Epoch [19][20]\t Batch [2600][5500]\t Training Loss 0.1587\t Accuracy 0.8870\n",
      "Epoch [19][20]\t Batch [2650][5500]\t Training Loss 0.1587\t Accuracy 0.8874\n",
      "Epoch [19][20]\t Batch [2700][5500]\t Training Loss 0.1589\t Accuracy 0.8875\n",
      "Epoch [19][20]\t Batch [2750][5500]\t Training Loss 0.1591\t Accuracy 0.8871\n",
      "Epoch [19][20]\t Batch [2800][5500]\t Training Loss 0.1589\t Accuracy 0.8873\n",
      "Epoch [19][20]\t Batch [2850][5500]\t Training Loss 0.1587\t Accuracy 0.8877\n",
      "Epoch [19][20]\t Batch [2900][5500]\t Training Loss 0.1586\t Accuracy 0.8876\n",
      "Epoch [19][20]\t Batch [2950][5500]\t Training Loss 0.1587\t Accuracy 0.8873\n",
      "Epoch [19][20]\t Batch [3000][5500]\t Training Loss 0.1590\t Accuracy 0.8868\n",
      "Epoch [19][20]\t Batch [3050][5500]\t Training Loss 0.1592\t Accuracy 0.8869\n",
      "Epoch [19][20]\t Batch [3100][5500]\t Training Loss 0.1595\t Accuracy 0.8865\n",
      "Epoch [19][20]\t Batch [3150][5500]\t Training Loss 0.1598\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [3200][5500]\t Training Loss 0.1600\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [3250][5500]\t Training Loss 0.1603\t Accuracy 0.8850\n",
      "Epoch [19][20]\t Batch [3300][5500]\t Training Loss 0.1601\t Accuracy 0.8852\n",
      "Epoch [19][20]\t Batch [3350][5500]\t Training Loss 0.1602\t Accuracy 0.8853\n",
      "Epoch [19][20]\t Batch [3400][5500]\t Training Loss 0.1598\t Accuracy 0.8858\n",
      "Epoch [19][20]\t Batch [3450][5500]\t Training Loss 0.1596\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [3500][5500]\t Training Loss 0.1598\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [3550][5500]\t Training Loss 0.1598\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [3600][5500]\t Training Loss 0.1597\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [3650][5500]\t Training Loss 0.1597\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [3700][5500]\t Training Loss 0.1595\t Accuracy 0.8866\n",
      "Epoch [19][20]\t Batch [3750][5500]\t Training Loss 0.1599\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [3800][5500]\t Training Loss 0.1599\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [3850][5500]\t Training Loss 0.1599\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [3900][5500]\t Training Loss 0.1597\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [3950][5500]\t Training Loss 0.1597\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [4000][5500]\t Training Loss 0.1598\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [4050][5500]\t Training Loss 0.1598\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [4100][5500]\t Training Loss 0.1596\t Accuracy 0.8865\n",
      "Epoch [19][20]\t Batch [4150][5500]\t Training Loss 0.1598\t Accuracy 0.8862\n",
      "Epoch [19][20]\t Batch [4200][5500]\t Training Loss 0.1598\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [4250][5500]\t Training Loss 0.1601\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [4300][5500]\t Training Loss 0.1601\t Accuracy 0.8860\n",
      "Epoch [19][20]\t Batch [4350][5500]\t Training Loss 0.1600\t Accuracy 0.8862\n",
      "Epoch [19][20]\t Batch [4400][5500]\t Training Loss 0.1600\t Accuracy 0.8862\n",
      "Epoch [19][20]\t Batch [4450][5500]\t Training Loss 0.1601\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [4500][5500]\t Training Loss 0.1600\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [4550][5500]\t Training Loss 0.1601\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [4600][5500]\t Training Loss 0.1601\t Accuracy 0.8862\n",
      "Epoch [19][20]\t Batch [4650][5500]\t Training Loss 0.1603\t Accuracy 0.8858\n",
      "Epoch [19][20]\t Batch [4700][5500]\t Training Loss 0.1601\t Accuracy 0.8861\n",
      "Epoch [19][20]\t Batch [4750][5500]\t Training Loss 0.1602\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [4800][5500]\t Training Loss 0.1602\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [4850][5500]\t Training Loss 0.1600\t Accuracy 0.8860\n",
      "Epoch [19][20]\t Batch [4900][5500]\t Training Loss 0.1599\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [4950][5500]\t Training Loss 0.1600\t Accuracy 0.8858\n",
      "Epoch [19][20]\t Batch [5000][5500]\t Training Loss 0.1602\t Accuracy 0.8856\n",
      "Epoch [19][20]\t Batch [5050][5500]\t Training Loss 0.1603\t Accuracy 0.8856\n",
      "Epoch [19][20]\t Batch [5100][5500]\t Training Loss 0.1603\t Accuracy 0.8856\n",
      "Epoch [19][20]\t Batch [5150][5500]\t Training Loss 0.1601\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [5200][5500]\t Training Loss 0.1600\t Accuracy 0.8859\n",
      "Epoch [19][20]\t Batch [5250][5500]\t Training Loss 0.1600\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [5300][5500]\t Training Loss 0.1601\t Accuracy 0.8855\n",
      "Epoch [19][20]\t Batch [5350][5500]\t Training Loss 0.1599\t Accuracy 0.8856\n",
      "Epoch [19][20]\t Batch [5400][5500]\t Training Loss 0.1599\t Accuracy 0.8857\n",
      "Epoch [19][20]\t Batch [5450][5500]\t Training Loss 0.1598\t Accuracy 0.8858\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1598\t Average training accuracy 0.8857\n",
      "Epoch [19]\t Average validation loss 0.1421\t Average validation accuracy 0.9126\n",
      "\n",
      "Epoch [0][20]\t Batch [0][5500]\t Training Loss 2.4084\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][5500]\t Training Loss 1.1369\t Accuracy 0.2471\n",
      "Epoch [0][20]\t Batch [100][5500]\t Training Loss 0.7983\t Accuracy 0.3356\n",
      "Epoch [0][20]\t Batch [150][5500]\t Training Loss 0.6561\t Accuracy 0.3987\n",
      "Epoch [0][20]\t Batch [200][5500]\t Training Loss 0.5672\t Accuracy 0.4632\n",
      "Epoch [0][20]\t Batch [250][5500]\t Training Loss 0.5054\t Accuracy 0.5243\n",
      "Epoch [0][20]\t Batch [300][5500]\t Training Loss 0.4592\t Accuracy 0.5635\n",
      "Epoch [0][20]\t Batch [350][5500]\t Training Loss 0.4220\t Accuracy 0.5986\n",
      "Epoch [0][20]\t Batch [400][5500]\t Training Loss 0.3932\t Accuracy 0.6254\n",
      "Epoch [0][20]\t Batch [450][5500]\t Training Loss 0.3691\t Accuracy 0.6477\n",
      "Epoch [0][20]\t Batch [500][5500]\t Training Loss 0.3486\t Accuracy 0.6665\n",
      "Epoch [0][20]\t Batch [550][5500]\t Training Loss 0.3315\t Accuracy 0.6826\n",
      "Epoch [0][20]\t Batch [600][5500]\t Training Loss 0.3163\t Accuracy 0.6963\n",
      "Epoch [0][20]\t Batch [650][5500]\t Training Loss 0.3026\t Accuracy 0.7100\n",
      "Epoch [0][20]\t Batch [700][5500]\t Training Loss 0.2907\t Accuracy 0.7214\n",
      "Epoch [0][20]\t Batch [750][5500]\t Training Loss 0.2810\t Accuracy 0.7310\n",
      "Epoch [0][20]\t Batch [800][5500]\t Training Loss 0.2725\t Accuracy 0.7393\n",
      "Epoch [0][20]\t Batch [850][5500]\t Training Loss 0.2651\t Accuracy 0.7458\n",
      "Epoch [0][20]\t Batch [900][5500]\t Training Loss 0.2590\t Accuracy 0.7501\n",
      "Epoch [0][20]\t Batch [950][5500]\t Training Loss 0.2521\t Accuracy 0.7583\n",
      "Epoch [0][20]\t Batch [1000][5500]\t Training Loss 0.2450\t Accuracy 0.7650\n",
      "Epoch [0][20]\t Batch [1050][5500]\t Training Loss 0.2393\t Accuracy 0.7706\n",
      "Epoch [0][20]\t Batch [1100][5500]\t Training Loss 0.2339\t Accuracy 0.7768\n",
      "Epoch [0][20]\t Batch [1150][5500]\t Training Loss 0.2286\t Accuracy 0.7830\n",
      "Epoch [0][20]\t Batch [1200][5500]\t Training Loss 0.2245\t Accuracy 0.7868\n",
      "Epoch [0][20]\t Batch [1250][5500]\t Training Loss 0.2208\t Accuracy 0.7904\n",
      "Epoch [0][20]\t Batch [1300][5500]\t Training Loss 0.2179\t Accuracy 0.7929\n",
      "Epoch [0][20]\t Batch [1350][5500]\t Training Loss 0.2153\t Accuracy 0.7957\n",
      "Epoch [0][20]\t Batch [1400][5500]\t Training Loss 0.2121\t Accuracy 0.7987\n",
      "Epoch [0][20]\t Batch [1450][5500]\t Training Loss 0.2093\t Accuracy 0.8016\n",
      "Epoch [0][20]\t Batch [1500][5500]\t Training Loss 0.2073\t Accuracy 0.8034\n",
      "Epoch [0][20]\t Batch [1550][5500]\t Training Loss 0.2042\t Accuracy 0.8068\n",
      "Epoch [0][20]\t Batch [1600][5500]\t Training Loss 0.2020\t Accuracy 0.8087\n",
      "Epoch [0][20]\t Batch [1650][5500]\t Training Loss 0.1990\t Accuracy 0.8119\n",
      "Epoch [0][20]\t Batch [1700][5500]\t Training Loss 0.1967\t Accuracy 0.8142\n",
      "Epoch [0][20]\t Batch [1750][5500]\t Training Loss 0.1944\t Accuracy 0.8168\n",
      "Epoch [0][20]\t Batch [1800][5500]\t Training Loss 0.1925\t Accuracy 0.8186\n",
      "Epoch [0][20]\t Batch [1850][5500]\t Training Loss 0.1897\t Accuracy 0.8217\n",
      "Epoch [0][20]\t Batch [1900][5500]\t Training Loss 0.1872\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [1950][5500]\t Training Loss 0.1852\t Accuracy 0.8266\n",
      "Epoch [0][20]\t Batch [2000][5500]\t Training Loss 0.1830\t Accuracy 0.8293\n",
      "Epoch [0][20]\t Batch [2050][5500]\t Training Loss 0.1809\t Accuracy 0.8316\n",
      "Epoch [0][20]\t Batch [2100][5500]\t Training Loss 0.1795\t Accuracy 0.8329\n",
      "Epoch [0][20]\t Batch [2150][5500]\t Training Loss 0.1777\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [2200][5500]\t Training Loss 0.1758\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [2250][5500]\t Training Loss 0.1743\t Accuracy 0.8385\n",
      "Epoch [0][20]\t Batch [2300][5500]\t Training Loss 0.1730\t Accuracy 0.8394\n",
      "Epoch [0][20]\t Batch [2350][5500]\t Training Loss 0.1713\t Accuracy 0.8412\n",
      "Epoch [0][20]\t Batch [2400][5500]\t Training Loss 0.1701\t Accuracy 0.8422\n",
      "Epoch [0][20]\t Batch [2450][5500]\t Training Loss 0.1688\t Accuracy 0.8436\n",
      "Epoch [0][20]\t Batch [2500][5500]\t Training Loss 0.1676\t Accuracy 0.8445\n",
      "Epoch [0][20]\t Batch [2550][5500]\t Training Loss 0.1661\t Accuracy 0.8462\n",
      "Epoch [0][20]\t Batch [2600][5500]\t Training Loss 0.1648\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [2650][5500]\t Training Loss 0.1635\t Accuracy 0.8489\n",
      "Epoch [0][20]\t Batch [2700][5500]\t Training Loss 0.1625\t Accuracy 0.8499\n",
      "Epoch [0][20]\t Batch [2750][5500]\t Training Loss 0.1616\t Accuracy 0.8509\n",
      "Epoch [0][20]\t Batch [2800][5500]\t Training Loss 0.1603\t Accuracy 0.8522\n",
      "Epoch [0][20]\t Batch [2850][5500]\t Training Loss 0.1592\t Accuracy 0.8535\n",
      "Epoch [0][20]\t Batch [2900][5500]\t Training Loss 0.1581\t Accuracy 0.8545\n",
      "Epoch [0][20]\t Batch [2950][5500]\t Training Loss 0.1574\t Accuracy 0.8552\n",
      "Epoch [0][20]\t Batch [3000][5500]\t Training Loss 0.1566\t Accuracy 0.8560\n",
      "Epoch [0][20]\t Batch [3050][5500]\t Training Loss 0.1558\t Accuracy 0.8570\n",
      "Epoch [0][20]\t Batch [3100][5500]\t Training Loss 0.1551\t Accuracy 0.8576\n",
      "Epoch [0][20]\t Batch [3150][5500]\t Training Loss 0.1545\t Accuracy 0.8582\n",
      "Epoch [0][20]\t Batch [3200][5500]\t Training Loss 0.1538\t Accuracy 0.8589\n",
      "Epoch [0][20]\t Batch [3250][5500]\t Training Loss 0.1533\t Accuracy 0.8591\n",
      "Epoch [0][20]\t Batch [3300][5500]\t Training Loss 0.1523\t Accuracy 0.8604\n",
      "Epoch [0][20]\t Batch [3350][5500]\t Training Loss 0.1514\t Accuracy 0.8615\n",
      "Epoch [0][20]\t Batch [3400][5500]\t Training Loss 0.1503\t Accuracy 0.8630\n",
      "Epoch [0][20]\t Batch [3450][5500]\t Training Loss 0.1493\t Accuracy 0.8639\n",
      "Epoch [0][20]\t Batch [3500][5500]\t Training Loss 0.1487\t Accuracy 0.8644\n",
      "Epoch [0][20]\t Batch [3550][5500]\t Training Loss 0.1480\t Accuracy 0.8652\n",
      "Epoch [0][20]\t Batch [3600][5500]\t Training Loss 0.1472\t Accuracy 0.8660\n",
      "Epoch [0][20]\t Batch [3650][5500]\t Training Loss 0.1465\t Accuracy 0.8666\n",
      "Epoch [0][20]\t Batch [3700][5500]\t Training Loss 0.1456\t Accuracy 0.8677\n",
      "Epoch [0][20]\t Batch [3750][5500]\t Training Loss 0.1453\t Accuracy 0.8679\n",
      "Epoch [0][20]\t Batch [3800][5500]\t Training Loss 0.1448\t Accuracy 0.8683\n",
      "Epoch [0][20]\t Batch [3850][5500]\t Training Loss 0.1442\t Accuracy 0.8690\n",
      "Epoch [0][20]\t Batch [3900][5500]\t Training Loss 0.1434\t Accuracy 0.8698\n",
      "Epoch [0][20]\t Batch [3950][5500]\t Training Loss 0.1428\t Accuracy 0.8704\n",
      "Epoch [0][20]\t Batch [4000][5500]\t Training Loss 0.1423\t Accuracy 0.8710\n",
      "Epoch [0][20]\t Batch [4050][5500]\t Training Loss 0.1416\t Accuracy 0.8719\n",
      "Epoch [0][20]\t Batch [4100][5500]\t Training Loss 0.1409\t Accuracy 0.8727\n",
      "Epoch [0][20]\t Batch [4150][5500]\t Training Loss 0.1405\t Accuracy 0.8729\n",
      "Epoch [0][20]\t Batch [4200][5500]\t Training Loss 0.1399\t Accuracy 0.8735\n",
      "Epoch [0][20]\t Batch [4250][5500]\t Training Loss 0.1395\t Accuracy 0.8738\n",
      "Epoch [0][20]\t Batch [4300][5500]\t Training Loss 0.1391\t Accuracy 0.8742\n",
      "Epoch [0][20]\t Batch [4350][5500]\t Training Loss 0.1385\t Accuracy 0.8748\n",
      "Epoch [0][20]\t Batch [4400][5500]\t Training Loss 0.1379\t Accuracy 0.8755\n",
      "Epoch [0][20]\t Batch [4450][5500]\t Training Loss 0.1374\t Accuracy 0.8761\n",
      "Epoch [0][20]\t Batch [4500][5500]\t Training Loss 0.1369\t Accuracy 0.8767\n",
      "Epoch [0][20]\t Batch [4550][5500]\t Training Loss 0.1364\t Accuracy 0.8771\n",
      "Epoch [0][20]\t Batch [4600][5500]\t Training Loss 0.1359\t Accuracy 0.8774\n",
      "Epoch [0][20]\t Batch [4650][5500]\t Training Loss 0.1357\t Accuracy 0.8775\n",
      "Epoch [0][20]\t Batch [4700][5500]\t Training Loss 0.1351\t Accuracy 0.8783\n",
      "Epoch [0][20]\t Batch [4750][5500]\t Training Loss 0.1347\t Accuracy 0.8787\n",
      "Epoch [0][20]\t Batch [4800][5500]\t Training Loss 0.1343\t Accuracy 0.8791\n",
      "Epoch [0][20]\t Batch [4850][5500]\t Training Loss 0.1338\t Accuracy 0.8799\n",
      "Epoch [0][20]\t Batch [4900][5500]\t Training Loss 0.1334\t Accuracy 0.8801\n",
      "Epoch [0][20]\t Batch [4950][5500]\t Training Loss 0.1331\t Accuracy 0.8806\n",
      "Epoch [0][20]\t Batch [5000][5500]\t Training Loss 0.1328\t Accuracy 0.8807\n",
      "Epoch [0][20]\t Batch [5050][5500]\t Training Loss 0.1326\t Accuracy 0.8809\n",
      "Epoch [0][20]\t Batch [5100][5500]\t Training Loss 0.1321\t Accuracy 0.8815\n",
      "Epoch [0][20]\t Batch [5150][5500]\t Training Loss 0.1316\t Accuracy 0.8820\n",
      "Epoch [0][20]\t Batch [5200][5500]\t Training Loss 0.1311\t Accuracy 0.8824\n",
      "Epoch [0][20]\t Batch [5250][5500]\t Training Loss 0.1308\t Accuracy 0.8828\n",
      "Epoch [0][20]\t Batch [5300][5500]\t Training Loss 0.1305\t Accuracy 0.8829\n",
      "Epoch [0][20]\t Batch [5350][5500]\t Training Loss 0.1300\t Accuracy 0.8833\n",
      "Epoch [0][20]\t Batch [5400][5500]\t Training Loss 0.1296\t Accuracy 0.8836\n",
      "Epoch [0][20]\t Batch [5450][5500]\t Training Loss 0.1291\t Accuracy 0.8842\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1288\t Average training accuracy 0.8844\n",
      "Epoch [0]\t Average validation loss 0.0785\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [1][20]\t Batch [0][5500]\t Training Loss 0.0436\t Accuracy 1.0000\n",
      "Epoch [1][20]\t Batch [50][5500]\t Training Loss 0.0804\t Accuracy 0.9392\n",
      "Epoch [1][20]\t Batch [100][5500]\t Training Loss 0.0839\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [150][5500]\t Training Loss 0.0891\t Accuracy 0.9265\n",
      "Epoch [1][20]\t Batch [200][5500]\t Training Loss 0.0858\t Accuracy 0.9294\n",
      "Epoch [1][20]\t Batch [250][5500]\t Training Loss 0.0845\t Accuracy 0.9323\n",
      "Epoch [1][20]\t Batch [300][5500]\t Training Loss 0.0838\t Accuracy 0.9339\n",
      "Epoch [1][20]\t Batch [350][5500]\t Training Loss 0.0828\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [400][5500]\t Training Loss 0.0819\t Accuracy 0.9367\n",
      "Epoch [1][20]\t Batch [450][5500]\t Training Loss 0.0819\t Accuracy 0.9366\n",
      "Epoch [1][20]\t Batch [500][5500]\t Training Loss 0.0816\t Accuracy 0.9379\n",
      "Epoch [1][20]\t Batch [550][5500]\t Training Loss 0.0820\t Accuracy 0.9370\n",
      "Epoch [1][20]\t Batch [600][5500]\t Training Loss 0.0823\t Accuracy 0.9368\n",
      "Epoch [1][20]\t Batch [650][5500]\t Training Loss 0.0817\t Accuracy 0.9379\n",
      "Epoch [1][20]\t Batch [700][5500]\t Training Loss 0.0821\t Accuracy 0.9371\n",
      "Epoch [1][20]\t Batch [750][5500]\t Training Loss 0.0822\t Accuracy 0.9369\n",
      "Epoch [1][20]\t Batch [800][5500]\t Training Loss 0.0823\t Accuracy 0.9370\n",
      "Epoch [1][20]\t Batch [850][5500]\t Training Loss 0.0827\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [900][5500]\t Training Loss 0.0841\t Accuracy 0.9331\n",
      "Epoch [1][20]\t Batch [950][5500]\t Training Loss 0.0838\t Accuracy 0.9332\n",
      "Epoch [1][20]\t Batch [1000][5500]\t Training Loss 0.0830\t Accuracy 0.9344\n",
      "Epoch [1][20]\t Batch [1050][5500]\t Training Loss 0.0832\t Accuracy 0.9343\n",
      "Epoch [1][20]\t Batch [1100][5500]\t Training Loss 0.0829\t Accuracy 0.9349\n",
      "Epoch [1][20]\t Batch [1150][5500]\t Training Loss 0.0827\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [1200][5500]\t Training Loss 0.0830\t Accuracy 0.9349\n",
      "Epoch [1][20]\t Batch [1250][5500]\t Training Loss 0.0832\t Accuracy 0.9348\n",
      "Epoch [1][20]\t Batch [1300][5500]\t Training Loss 0.0840\t Accuracy 0.9338\n",
      "Epoch [1][20]\t Batch [1350][5500]\t Training Loss 0.0843\t Accuracy 0.9340\n",
      "Epoch [1][20]\t Batch [1400][5500]\t Training Loss 0.0844\t Accuracy 0.9338\n",
      "Epoch [1][20]\t Batch [1450][5500]\t Training Loss 0.0848\t Accuracy 0.9331\n",
      "Epoch [1][20]\t Batch [1500][5500]\t Training Loss 0.0856\t Accuracy 0.9323\n",
      "Epoch [1][20]\t Batch [1550][5500]\t Training Loss 0.0853\t Accuracy 0.9325\n",
      "Epoch [1][20]\t Batch [1600][5500]\t Training Loss 0.0855\t Accuracy 0.9320\n",
      "Epoch [1][20]\t Batch [1650][5500]\t Training Loss 0.0852\t Accuracy 0.9326\n",
      "Epoch [1][20]\t Batch [1700][5500]\t Training Loss 0.0853\t Accuracy 0.9322\n",
      "Epoch [1][20]\t Batch [1750][5500]\t Training Loss 0.0853\t Accuracy 0.9319\n",
      "Epoch [1][20]\t Batch [1800][5500]\t Training Loss 0.0855\t Accuracy 0.9318\n",
      "Epoch [1][20]\t Batch [1850][5500]\t Training Loss 0.0851\t Accuracy 0.9323\n",
      "Epoch [1][20]\t Batch [1900][5500]\t Training Loss 0.0847\t Accuracy 0.9330\n",
      "Epoch [1][20]\t Batch [1950][5500]\t Training Loss 0.0845\t Accuracy 0.9333\n",
      "Epoch [1][20]\t Batch [2000][5500]\t Training Loss 0.0843\t Accuracy 0.9338\n",
      "Epoch [1][20]\t Batch [2050][5500]\t Training Loss 0.0840\t Accuracy 0.9343\n",
      "Epoch [1][20]\t Batch [2100][5500]\t Training Loss 0.0842\t Accuracy 0.9338\n",
      "Epoch [1][20]\t Batch [2150][5500]\t Training Loss 0.0841\t Accuracy 0.9340\n",
      "Epoch [1][20]\t Batch [2200][5500]\t Training Loss 0.0839\t Accuracy 0.9343\n",
      "Epoch [1][20]\t Batch [2250][5500]\t Training Loss 0.0838\t Accuracy 0.9347\n",
      "Epoch [1][20]\t Batch [2300][5500]\t Training Loss 0.0839\t Accuracy 0.9345\n",
      "Epoch [1][20]\t Batch [2350][5500]\t Training Loss 0.0837\t Accuracy 0.9348\n",
      "Epoch [1][20]\t Batch [2400][5500]\t Training Loss 0.0837\t Accuracy 0.9347\n",
      "Epoch [1][20]\t Batch [2450][5500]\t Training Loss 0.0835\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [2500][5500]\t Training Loss 0.0835\t Accuracy 0.9351\n",
      "Epoch [1][20]\t Batch [2550][5500]\t Training Loss 0.0833\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [2600][5500]\t Training Loss 0.0832\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [2650][5500]\t Training Loss 0.0831\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [2700][5500]\t Training Loss 0.0833\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [2750][5500]\t Training Loss 0.0834\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [2800][5500]\t Training Loss 0.0832\t Accuracy 0.9357\n",
      "Epoch [1][20]\t Batch [2850][5500]\t Training Loss 0.0831\t Accuracy 0.9359\n",
      "Epoch [1][20]\t Batch [2900][5500]\t Training Loss 0.0829\t Accuracy 0.9359\n",
      "Epoch [1][20]\t Batch [2950][5500]\t Training Loss 0.0831\t Accuracy 0.9358\n",
      "Epoch [1][20]\t Batch [3000][5500]\t Training Loss 0.0832\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [3050][5500]\t Training Loss 0.0832\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [3100][5500]\t Training Loss 0.0833\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [3150][5500]\t Training Loss 0.0835\t Accuracy 0.9348\n",
      "Epoch [1][20]\t Batch [3200][5500]\t Training Loss 0.0836\t Accuracy 0.9346\n",
      "Epoch [1][20]\t Batch [3250][5500]\t Training Loss 0.0838\t Accuracy 0.9341\n",
      "Epoch [1][20]\t Batch [3300][5500]\t Training Loss 0.0837\t Accuracy 0.9343\n",
      "Epoch [1][20]\t Batch [3350][5500]\t Training Loss 0.0835\t Accuracy 0.9346\n",
      "Epoch [1][20]\t Batch [3400][5500]\t Training Loss 0.0831\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [3450][5500]\t Training Loss 0.0829\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [3500][5500]\t Training Loss 0.0829\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [3550][5500]\t Training Loss 0.0828\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [3600][5500]\t Training Loss 0.0827\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [3650][5500]\t Training Loss 0.0827\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [3700][5500]\t Training Loss 0.0826\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [3750][5500]\t Training Loss 0.0828\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [3800][5500]\t Training Loss 0.0829\t Accuracy 0.9350\n",
      "Epoch [1][20]\t Batch [3850][5500]\t Training Loss 0.0829\t Accuracy 0.9349\n",
      "Epoch [1][20]\t Batch [3900][5500]\t Training Loss 0.0827\t Accuracy 0.9351\n",
      "Epoch [1][20]\t Batch [3950][5500]\t Training Loss 0.0828\t Accuracy 0.9349\n",
      "Epoch [1][20]\t Batch [4000][5500]\t Training Loss 0.0828\t Accuracy 0.9350\n",
      "Epoch [1][20]\t Batch [4050][5500]\t Training Loss 0.0827\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [4100][5500]\t Training Loss 0.0825\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [4150][5500]\t Training Loss 0.0826\t Accuracy 0.9349\n",
      "Epoch [1][20]\t Batch [4200][5500]\t Training Loss 0.0825\t Accuracy 0.9351\n",
      "Epoch [1][20]\t Batch [4250][5500]\t Training Loss 0.0826\t Accuracy 0.9350\n",
      "Epoch [1][20]\t Batch [4300][5500]\t Training Loss 0.0826\t Accuracy 0.9350\n",
      "Epoch [1][20]\t Batch [4350][5500]\t Training Loss 0.0825\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [4400][5500]\t Training Loss 0.0824\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4450][5500]\t Training Loss 0.0824\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4500][5500]\t Training Loss 0.0822\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [4550][5500]\t Training Loss 0.0822\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [4600][5500]\t Training Loss 0.0822\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4650][5500]\t Training Loss 0.0824\t Accuracy 0.9352\n",
      "Epoch [1][20]\t Batch [4700][5500]\t Training Loss 0.0822\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4750][5500]\t Training Loss 0.0823\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4800][5500]\t Training Loss 0.0823\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [4850][5500]\t Training Loss 0.0822\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [4900][5500]\t Training Loss 0.0822\t Accuracy 0.9354\n",
      "Epoch [1][20]\t Batch [4950][5500]\t Training Loss 0.0822\t Accuracy 0.9355\n",
      "Epoch [1][20]\t Batch [5000][5500]\t Training Loss 0.0823\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [5050][5500]\t Training Loss 0.0824\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [5100][5500]\t Training Loss 0.0823\t Accuracy 0.9353\n",
      "Epoch [1][20]\t Batch [5150][5500]\t Training Loss 0.0822\t Accuracy 0.9356\n",
      "Epoch [1][20]\t Batch [5200][5500]\t Training Loss 0.0821\t Accuracy 0.9357\n",
      "Epoch [1][20]\t Batch [5250][5500]\t Training Loss 0.0820\t Accuracy 0.9357\n",
      "Epoch [1][20]\t Batch [5300][5500]\t Training Loss 0.0821\t Accuracy 0.9357\n",
      "Epoch [1][20]\t Batch [5350][5500]\t Training Loss 0.0820\t Accuracy 0.9358\n",
      "Epoch [1][20]\t Batch [5400][5500]\t Training Loss 0.0819\t Accuracy 0.9358\n",
      "Epoch [1][20]\t Batch [5450][5500]\t Training Loss 0.0817\t Accuracy 0.9360\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0817\t Average training accuracy 0.9361\n",
      "Epoch [1]\t Average validation loss 0.0692\t Average validation accuracy 0.9546\n",
      "\n",
      "Epoch [2][20]\t Batch [0][5500]\t Training Loss 0.0419\t Accuracy 1.0000\n",
      "Epoch [2][20]\t Batch [50][5500]\t Training Loss 0.0684\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [100][5500]\t Training Loss 0.0713\t Accuracy 0.9455\n",
      "Epoch [2][20]\t Batch [150][5500]\t Training Loss 0.0766\t Accuracy 0.9404\n",
      "Epoch [2][20]\t Batch [200][5500]\t Training Loss 0.0746\t Accuracy 0.9418\n",
      "Epoch [2][20]\t Batch [250][5500]\t Training Loss 0.0731\t Accuracy 0.9442\n",
      "Epoch [2][20]\t Batch [300][5500]\t Training Loss 0.0729\t Accuracy 0.9442\n",
      "Epoch [2][20]\t Batch [350][5500]\t Training Loss 0.0716\t Accuracy 0.9473\n",
      "Epoch [2][20]\t Batch [400][5500]\t Training Loss 0.0711\t Accuracy 0.9486\n",
      "Epoch [2][20]\t Batch [450][5500]\t Training Loss 0.0710\t Accuracy 0.9490\n",
      "Epoch [2][20]\t Batch [500][5500]\t Training Loss 0.0705\t Accuracy 0.9505\n",
      "Epoch [2][20]\t Batch [550][5500]\t Training Loss 0.0707\t Accuracy 0.9495\n",
      "Epoch [2][20]\t Batch [600][5500]\t Training Loss 0.0709\t Accuracy 0.9498\n",
      "Epoch [2][20]\t Batch [650][5500]\t Training Loss 0.0704\t Accuracy 0.9504\n",
      "Epoch [2][20]\t Batch [700][5500]\t Training Loss 0.0707\t Accuracy 0.9495\n",
      "Epoch [2][20]\t Batch [750][5500]\t Training Loss 0.0707\t Accuracy 0.9495\n",
      "Epoch [2][20]\t Batch [800][5500]\t Training Loss 0.0707\t Accuracy 0.9492\n",
      "Epoch [2][20]\t Batch [850][5500]\t Training Loss 0.0712\t Accuracy 0.9477\n",
      "Epoch [2][20]\t Batch [900][5500]\t Training Loss 0.0725\t Accuracy 0.9454\n",
      "Epoch [2][20]\t Batch [950][5500]\t Training Loss 0.0723\t Accuracy 0.9455\n",
      "Epoch [2][20]\t Batch [1000][5500]\t Training Loss 0.0717\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [1050][5500]\t Training Loss 0.0720\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [1100][5500]\t Training Loss 0.0719\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [1150][5500]\t Training Loss 0.0717\t Accuracy 0.9468\n",
      "Epoch [2][20]\t Batch [1200][5500]\t Training Loss 0.0722\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [1250][5500]\t Training Loss 0.0723\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [1300][5500]\t Training Loss 0.0730\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [1350][5500]\t Training Loss 0.0733\t Accuracy 0.9452\n",
      "Epoch [2][20]\t Batch [1400][5500]\t Training Loss 0.0735\t Accuracy 0.9452\n",
      "Epoch [2][20]\t Batch [1450][5500]\t Training Loss 0.0739\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [1500][5500]\t Training Loss 0.0746\t Accuracy 0.9450\n",
      "Epoch [2][20]\t Batch [1550][5500]\t Training Loss 0.0745\t Accuracy 0.9449\n",
      "Epoch [2][20]\t Batch [1600][5500]\t Training Loss 0.0748\t Accuracy 0.9447\n",
      "Epoch [2][20]\t Batch [1650][5500]\t Training Loss 0.0745\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [1700][5500]\t Training Loss 0.0746\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [1750][5500]\t Training Loss 0.0747\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [1800][5500]\t Training Loss 0.0749\t Accuracy 0.9448\n",
      "Epoch [2][20]\t Batch [1850][5500]\t Training Loss 0.0745\t Accuracy 0.9452\n",
      "Epoch [2][20]\t Batch [1900][5500]\t Training Loss 0.0743\t Accuracy 0.9456\n",
      "Epoch [2][20]\t Batch [1950][5500]\t Training Loss 0.0742\t Accuracy 0.9457\n",
      "Epoch [2][20]\t Batch [2000][5500]\t Training Loss 0.0739\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [2050][5500]\t Training Loss 0.0738\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [2100][5500]\t Training Loss 0.0740\t Accuracy 0.9458\n",
      "Epoch [2][20]\t Batch [2150][5500]\t Training Loss 0.0740\t Accuracy 0.9458\n",
      "Epoch [2][20]\t Batch [2200][5500]\t Training Loss 0.0739\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [2250][5500]\t Training Loss 0.0738\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [2300][5500]\t Training Loss 0.0739\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [2350][5500]\t Training Loss 0.0738\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [2400][5500]\t Training Loss 0.0738\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [2450][5500]\t Training Loss 0.0737\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [2500][5500]\t Training Loss 0.0736\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [2550][5500]\t Training Loss 0.0734\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [2600][5500]\t Training Loss 0.0733\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [2650][5500]\t Training Loss 0.0732\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [2700][5500]\t Training Loss 0.0735\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [2750][5500]\t Training Loss 0.0736\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [2800][5500]\t Training Loss 0.0734\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [2850][5500]\t Training Loss 0.0733\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [2900][5500]\t Training Loss 0.0732\t Accuracy 0.9465\n",
      "Epoch [2][20]\t Batch [2950][5500]\t Training Loss 0.0733\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [3000][5500]\t Training Loss 0.0734\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [3050][5500]\t Training Loss 0.0734\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [3100][5500]\t Training Loss 0.0735\t Accuracy 0.9457\n",
      "Epoch [2][20]\t Batch [3150][5500]\t Training Loss 0.0737\t Accuracy 0.9454\n",
      "Epoch [2][20]\t Batch [3200][5500]\t Training Loss 0.0738\t Accuracy 0.9454\n",
      "Epoch [2][20]\t Batch [3250][5500]\t Training Loss 0.0740\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [3300][5500]\t Training Loss 0.0739\t Accuracy 0.9453\n",
      "Epoch [2][20]\t Batch [3350][5500]\t Training Loss 0.0738\t Accuracy 0.9455\n",
      "Epoch [2][20]\t Batch [3400][5500]\t Training Loss 0.0734\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [3450][5500]\t Training Loss 0.0732\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [3500][5500]\t Training Loss 0.0733\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [3550][5500]\t Training Loss 0.0732\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [3600][5500]\t Training Loss 0.0731\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [3650][5500]\t Training Loss 0.0732\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [3700][5500]\t Training Loss 0.0731\t Accuracy 0.9466\n",
      "Epoch [2][20]\t Batch [3750][5500]\t Training Loss 0.0733\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [3800][5500]\t Training Loss 0.0734\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [3850][5500]\t Training Loss 0.0734\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [3900][5500]\t Training Loss 0.0733\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [3950][5500]\t Training Loss 0.0734\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4000][5500]\t Training Loss 0.0734\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [4050][5500]\t Training Loss 0.0733\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [4100][5500]\t Training Loss 0.0732\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [4150][5500]\t Training Loss 0.0734\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [4200][5500]\t Training Loss 0.0733\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4250][5500]\t Training Loss 0.0734\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [4300][5500]\t Training Loss 0.0735\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [4350][5500]\t Training Loss 0.0733\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [4400][5500]\t Training Loss 0.0732\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [4450][5500]\t Training Loss 0.0732\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4500][5500]\t Training Loss 0.0731\t Accuracy 0.9464\n",
      "Epoch [2][20]\t Batch [4550][5500]\t Training Loss 0.0731\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [4600][5500]\t Training Loss 0.0732\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4650][5500]\t Training Loss 0.0733\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [4700][5500]\t Training Loss 0.0732\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4750][5500]\t Training Loss 0.0733\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [4800][5500]\t Training Loss 0.0733\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [4850][5500]\t Training Loss 0.0732\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [4900][5500]\t Training Loss 0.0733\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [4950][5500]\t Training Loss 0.0733\t Accuracy 0.9463\n",
      "Epoch [2][20]\t Batch [5000][5500]\t Training Loss 0.0734\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [5050][5500]\t Training Loss 0.0734\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [5100][5500]\t Training Loss 0.0734\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [5150][5500]\t Training Loss 0.0733\t Accuracy 0.9460\n",
      "Epoch [2][20]\t Batch [5200][5500]\t Training Loss 0.0732\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [5250][5500]\t Training Loss 0.0731\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [5300][5500]\t Training Loss 0.0732\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [5350][5500]\t Training Loss 0.0731\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [5400][5500]\t Training Loss 0.0731\t Accuracy 0.9461\n",
      "Epoch [2][20]\t Batch [5450][5500]\t Training Loss 0.0730\t Accuracy 0.9464\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0730\t Average training accuracy 0.9463\n",
      "Epoch [2]\t Average validation loss 0.0647\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [3][20]\t Batch [0][5500]\t Training Loss 0.0346\t Accuracy 1.0000\n",
      "Epoch [3][20]\t Batch [50][5500]\t Training Loss 0.0626\t Accuracy 0.9510\n",
      "Epoch [3][20]\t Batch [100][5500]\t Training Loss 0.0650\t Accuracy 0.9505\n",
      "Epoch [3][20]\t Batch [150][5500]\t Training Loss 0.0706\t Accuracy 0.9470\n",
      "Epoch [3][20]\t Batch [200][5500]\t Training Loss 0.0686\t Accuracy 0.9473\n",
      "Epoch [3][20]\t Batch [250][5500]\t Training Loss 0.0671\t Accuracy 0.9502\n",
      "Epoch [3][20]\t Batch [300][5500]\t Training Loss 0.0669\t Accuracy 0.9505\n",
      "Epoch [3][20]\t Batch [350][5500]\t Training Loss 0.0657\t Accuracy 0.9536\n",
      "Epoch [3][20]\t Batch [400][5500]\t Training Loss 0.0653\t Accuracy 0.9554\n",
      "Epoch [3][20]\t Batch [450][5500]\t Training Loss 0.0652\t Accuracy 0.9550\n",
      "Epoch [3][20]\t Batch [500][5500]\t Training Loss 0.0645\t Accuracy 0.9567\n",
      "Epoch [3][20]\t Batch [550][5500]\t Training Loss 0.0649\t Accuracy 0.9554\n",
      "Epoch [3][20]\t Batch [600][5500]\t Training Loss 0.0650\t Accuracy 0.9554\n",
      "Epoch [3][20]\t Batch [650][5500]\t Training Loss 0.0647\t Accuracy 0.9564\n",
      "Epoch [3][20]\t Batch [700][5500]\t Training Loss 0.0648\t Accuracy 0.9561\n",
      "Epoch [3][20]\t Batch [750][5500]\t Training Loss 0.0648\t Accuracy 0.9567\n",
      "Epoch [3][20]\t Batch [800][5500]\t Training Loss 0.0649\t Accuracy 0.9563\n",
      "Epoch [3][20]\t Batch [850][5500]\t Training Loss 0.0654\t Accuracy 0.9552\n",
      "Epoch [3][20]\t Batch [900][5500]\t Training Loss 0.0666\t Accuracy 0.9529\n",
      "Epoch [3][20]\t Batch [950][5500]\t Training Loss 0.0664\t Accuracy 0.9531\n",
      "Epoch [3][20]\t Batch [1000][5500]\t Training Loss 0.0659\t Accuracy 0.9533\n",
      "Epoch [3][20]\t Batch [1050][5500]\t Training Loss 0.0665\t Accuracy 0.9527\n",
      "Epoch [3][20]\t Batch [1100][5500]\t Training Loss 0.0663\t Accuracy 0.9533\n",
      "Epoch [3][20]\t Batch [1150][5500]\t Training Loss 0.0661\t Accuracy 0.9536\n",
      "Epoch [3][20]\t Batch [1200][5500]\t Training Loss 0.0666\t Accuracy 0.9528\n",
      "Epoch [3][20]\t Batch [1250][5500]\t Training Loss 0.0667\t Accuracy 0.9525\n",
      "Epoch [3][20]\t Batch [1300][5500]\t Training Loss 0.0673\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [1350][5500]\t Training Loss 0.0676\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [1400][5500]\t Training Loss 0.0678\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [1450][5500]\t Training Loss 0.0681\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [1500][5500]\t Training Loss 0.0687\t Accuracy 0.9513\n",
      "Epoch [3][20]\t Batch [1550][5500]\t Training Loss 0.0687\t Accuracy 0.9511\n",
      "Epoch [3][20]\t Batch [1600][5500]\t Training Loss 0.0689\t Accuracy 0.9508\n",
      "Epoch [3][20]\t Batch [1650][5500]\t Training Loss 0.0686\t Accuracy 0.9514\n",
      "Epoch [3][20]\t Batch [1700][5500]\t Training Loss 0.0687\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [1750][5500]\t Training Loss 0.0688\t Accuracy 0.9513\n",
      "Epoch [3][20]\t Batch [1800][5500]\t Training Loss 0.0690\t Accuracy 0.9510\n",
      "Epoch [3][20]\t Batch [1850][5500]\t Training Loss 0.0688\t Accuracy 0.9512\n",
      "Epoch [3][20]\t Batch [1900][5500]\t Training Loss 0.0685\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [1950][5500]\t Training Loss 0.0684\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [2000][5500]\t Training Loss 0.0681\t Accuracy 0.9521\n",
      "Epoch [3][20]\t Batch [2050][5500]\t Training Loss 0.0681\t Accuracy 0.9524\n",
      "Epoch [3][20]\t Batch [2100][5500]\t Training Loss 0.0683\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [2150][5500]\t Training Loss 0.0683\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [2200][5500]\t Training Loss 0.0681\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [2250][5500]\t Training Loss 0.0680\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [2300][5500]\t Training Loss 0.0682\t Accuracy 0.9521\n",
      "Epoch [3][20]\t Batch [2350][5500]\t Training Loss 0.0680\t Accuracy 0.9524\n",
      "Epoch [3][20]\t Batch [2400][5500]\t Training Loss 0.0681\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [2450][5500]\t Training Loss 0.0680\t Accuracy 0.9523\n",
      "Epoch [3][20]\t Batch [2500][5500]\t Training Loss 0.0679\t Accuracy 0.9525\n",
      "Epoch [3][20]\t Batch [2550][5500]\t Training Loss 0.0677\t Accuracy 0.9525\n",
      "Epoch [3][20]\t Batch [2600][5500]\t Training Loss 0.0676\t Accuracy 0.9526\n",
      "Epoch [3][20]\t Batch [2650][5500]\t Training Loss 0.0676\t Accuracy 0.9526\n",
      "Epoch [3][20]\t Batch [2700][5500]\t Training Loss 0.0678\t Accuracy 0.9525\n",
      "Epoch [3][20]\t Batch [2750][5500]\t Training Loss 0.0679\t Accuracy 0.9524\n",
      "Epoch [3][20]\t Batch [2800][5500]\t Training Loss 0.0677\t Accuracy 0.9526\n",
      "Epoch [3][20]\t Batch [2850][5500]\t Training Loss 0.0677\t Accuracy 0.9525\n",
      "Epoch [3][20]\t Batch [2900][5500]\t Training Loss 0.0676\t Accuracy 0.9527\n",
      "Epoch [3][20]\t Batch [2950][5500]\t Training Loss 0.0677\t Accuracy 0.9524\n",
      "Epoch [3][20]\t Batch [3000][5500]\t Training Loss 0.0678\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [3050][5500]\t Training Loss 0.0678\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [3100][5500]\t Training Loss 0.0679\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [3150][5500]\t Training Loss 0.0681\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [3200][5500]\t Training Loss 0.0682\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [3250][5500]\t Training Loss 0.0684\t Accuracy 0.9512\n",
      "Epoch [3][20]\t Batch [3300][5500]\t Training Loss 0.0682\t Accuracy 0.9514\n",
      "Epoch [3][20]\t Batch [3350][5500]\t Training Loss 0.0681\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [3400][5500]\t Training Loss 0.0678\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [3450][5500]\t Training Loss 0.0676\t Accuracy 0.9521\n",
      "Epoch [3][20]\t Batch [3500][5500]\t Training Loss 0.0677\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [3550][5500]\t Training Loss 0.0677\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [3600][5500]\t Training Loss 0.0676\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [3650][5500]\t Training Loss 0.0676\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [3700][5500]\t Training Loss 0.0676\t Accuracy 0.9523\n",
      "Epoch [3][20]\t Batch [3750][5500]\t Training Loss 0.0678\t Accuracy 0.9521\n",
      "Epoch [3][20]\t Batch [3800][5500]\t Training Loss 0.0679\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [3850][5500]\t Training Loss 0.0679\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [3900][5500]\t Training Loss 0.0678\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [3950][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4000][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4050][5500]\t Training Loss 0.0678\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [4100][5500]\t Training Loss 0.0677\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [4150][5500]\t Training Loss 0.0679\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [4200][5500]\t Training Loss 0.0679\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [4250][5500]\t Training Loss 0.0680\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [4300][5500]\t Training Loss 0.0680\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [4350][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4400][5500]\t Training Loss 0.0678\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [4450][5500]\t Training Loss 0.0679\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [4500][5500]\t Training Loss 0.0678\t Accuracy 0.9519\n",
      "Epoch [3][20]\t Batch [4550][5500]\t Training Loss 0.0678\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4600][5500]\t Training Loss 0.0678\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4650][5500]\t Training Loss 0.0680\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [4700][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4750][5500]\t Training Loss 0.0680\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4800][5500]\t Training Loss 0.0680\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [4850][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "Epoch [3][20]\t Batch [4900][5500]\t Training Loss 0.0680\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [4950][5500]\t Training Loss 0.0680\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [5000][5500]\t Training Loss 0.0682\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [5050][5500]\t Training Loss 0.0682\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [5100][5500]\t Training Loss 0.0682\t Accuracy 0.9514\n",
      "Epoch [3][20]\t Batch [5150][5500]\t Training Loss 0.0681\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [5200][5500]\t Training Loss 0.0680\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [5250][5500]\t Training Loss 0.0680\t Accuracy 0.9517\n",
      "Epoch [3][20]\t Batch [5300][5500]\t Training Loss 0.0681\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [5350][5500]\t Training Loss 0.0680\t Accuracy 0.9516\n",
      "Epoch [3][20]\t Batch [5400][5500]\t Training Loss 0.0680\t Accuracy 0.9515\n",
      "Epoch [3][20]\t Batch [5450][5500]\t Training Loss 0.0679\t Accuracy 0.9518\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0679\t Average training accuracy 0.9516\n",
      "Epoch [3]\t Average validation loss 0.0615\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [4][20]\t Batch [0][5500]\t Training Loss 0.0311\t Accuracy 1.0000\n",
      "Epoch [4][20]\t Batch [50][5500]\t Training Loss 0.0587\t Accuracy 0.9569\n",
      "Epoch [4][20]\t Batch [100][5500]\t Training Loss 0.0613\t Accuracy 0.9574\n",
      "Epoch [4][20]\t Batch [150][5500]\t Training Loss 0.0673\t Accuracy 0.9523\n",
      "Epoch [4][20]\t Batch [200][5500]\t Training Loss 0.0652\t Accuracy 0.9537\n",
      "Epoch [4][20]\t Batch [250][5500]\t Training Loss 0.0636\t Accuracy 0.9566\n",
      "Epoch [4][20]\t Batch [300][5500]\t Training Loss 0.0635\t Accuracy 0.9568\n",
      "Epoch [4][20]\t Batch [350][5500]\t Training Loss 0.0625\t Accuracy 0.9590\n",
      "Epoch [4][20]\t Batch [400][5500]\t Training Loss 0.0621\t Accuracy 0.9591\n",
      "Epoch [4][20]\t Batch [450][5500]\t Training Loss 0.0620\t Accuracy 0.9590\n",
      "Epoch [4][20]\t Batch [500][5500]\t Training Loss 0.0613\t Accuracy 0.9605\n",
      "Epoch [4][20]\t Batch [550][5500]\t Training Loss 0.0616\t Accuracy 0.9590\n",
      "Epoch [4][20]\t Batch [600][5500]\t Training Loss 0.0617\t Accuracy 0.9587\n",
      "Epoch [4][20]\t Batch [650][5500]\t Training Loss 0.0614\t Accuracy 0.9590\n",
      "Epoch [4][20]\t Batch [700][5500]\t Training Loss 0.0616\t Accuracy 0.9591\n",
      "Epoch [4][20]\t Batch [750][5500]\t Training Loss 0.0615\t Accuracy 0.9597\n",
      "Epoch [4][20]\t Batch [800][5500]\t Training Loss 0.0617\t Accuracy 0.9597\n",
      "Epoch [4][20]\t Batch [850][5500]\t Training Loss 0.0621\t Accuracy 0.9583\n",
      "Epoch [4][20]\t Batch [900][5500]\t Training Loss 0.0634\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [950][5500]\t Training Loss 0.0632\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [1000][5500]\t Training Loss 0.0627\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [1050][5500]\t Training Loss 0.0632\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [1100][5500]\t Training Loss 0.0631\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [1150][5500]\t Training Loss 0.0630\t Accuracy 0.9565\n",
      "Epoch [4][20]\t Batch [1200][5500]\t Training Loss 0.0634\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [1250][5500]\t Training Loss 0.0634\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [1300][5500]\t Training Loss 0.0639\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [1350][5500]\t Training Loss 0.0642\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [1400][5500]\t Training Loss 0.0643\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [1450][5500]\t Training Loss 0.0646\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [1500][5500]\t Training Loss 0.0652\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [1550][5500]\t Training Loss 0.0651\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [1600][5500]\t Training Loss 0.0653\t Accuracy 0.9548\n",
      "Epoch [4][20]\t Batch [1650][5500]\t Training Loss 0.0651\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [1700][5500]\t Training Loss 0.0651\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [1750][5500]\t Training Loss 0.0651\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [1800][5500]\t Training Loss 0.0654\t Accuracy 0.9551\n",
      "Epoch [4][20]\t Batch [1850][5500]\t Training Loss 0.0652\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [1900][5500]\t Training Loss 0.0650\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [1950][5500]\t Training Loss 0.0649\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [2000][5500]\t Training Loss 0.0647\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [2050][5500]\t Training Loss 0.0646\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [2100][5500]\t Training Loss 0.0649\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [2150][5500]\t Training Loss 0.0648\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [2200][5500]\t Training Loss 0.0646\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [2250][5500]\t Training Loss 0.0646\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [2300][5500]\t Training Loss 0.0647\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [2350][5500]\t Training Loss 0.0646\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [2400][5500]\t Training Loss 0.0646\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [2450][5500]\t Training Loss 0.0646\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [2500][5500]\t Training Loss 0.0645\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [2550][5500]\t Training Loss 0.0644\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [2600][5500]\t Training Loss 0.0643\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [2650][5500]\t Training Loss 0.0643\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [2700][5500]\t Training Loss 0.0644\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [2750][5500]\t Training Loss 0.0645\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [2800][5500]\t Training Loss 0.0644\t Accuracy 0.9563\n",
      "Epoch [4][20]\t Batch [2850][5500]\t Training Loss 0.0643\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [2900][5500]\t Training Loss 0.0642\t Accuracy 0.9563\n",
      "Epoch [4][20]\t Batch [2950][5500]\t Training Loss 0.0644\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [3000][5500]\t Training Loss 0.0644\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3050][5500]\t Training Loss 0.0644\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [3100][5500]\t Training Loss 0.0645\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [3150][5500]\t Training Loss 0.0647\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [3200][5500]\t Training Loss 0.0648\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [3250][5500]\t Training Loss 0.0650\t Accuracy 0.9549\n",
      "Epoch [4][20]\t Batch [3300][5500]\t Training Loss 0.0649\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [3350][5500]\t Training Loss 0.0648\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [3400][5500]\t Training Loss 0.0645\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3450][5500]\t Training Loss 0.0643\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3500][5500]\t Training Loss 0.0644\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3550][5500]\t Training Loss 0.0644\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [3600][5500]\t Training Loss 0.0643\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [3650][5500]\t Training Loss 0.0644\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [3700][5500]\t Training Loss 0.0643\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [3750][5500]\t Training Loss 0.0645\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3800][5500]\t Training Loss 0.0646\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [3850][5500]\t Training Loss 0.0646\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [3900][5500]\t Training Loss 0.0645\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [3950][5500]\t Training Loss 0.0646\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [4000][5500]\t Training Loss 0.0647\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [4050][5500]\t Training Loss 0.0646\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [4100][5500]\t Training Loss 0.0645\t Accuracy 0.9557\n",
      "Epoch [4][20]\t Batch [4150][5500]\t Training Loss 0.0646\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [4200][5500]\t Training Loss 0.0646\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [4250][5500]\t Training Loss 0.0647\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [4300][5500]\t Training Loss 0.0647\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [4350][5500]\t Training Loss 0.0646\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [4400][5500]\t Training Loss 0.0645\t Accuracy 0.9556\n",
      "Epoch [4][20]\t Batch [4450][5500]\t Training Loss 0.0646\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [4500][5500]\t Training Loss 0.0646\t Accuracy 0.9555\n",
      "Epoch [4][20]\t Batch [4550][5500]\t Training Loss 0.0646\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [4600][5500]\t Training Loss 0.0646\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [4650][5500]\t Training Loss 0.0648\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [4700][5500]\t Training Loss 0.0647\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [4750][5500]\t Training Loss 0.0648\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [4800][5500]\t Training Loss 0.0648\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [4850][5500]\t Training Loss 0.0648\t Accuracy 0.9554\n",
      "Epoch [4][20]\t Batch [4900][5500]\t Training Loss 0.0648\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [4950][5500]\t Training Loss 0.0648\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [5000][5500]\t Training Loss 0.0650\t Accuracy 0.9551\n",
      "Epoch [4][20]\t Batch [5050][5500]\t Training Loss 0.0650\t Accuracy 0.9550\n",
      "Epoch [4][20]\t Batch [5100][5500]\t Training Loss 0.0650\t Accuracy 0.9550\n",
      "Epoch [4][20]\t Batch [5150][5500]\t Training Loss 0.0650\t Accuracy 0.9551\n",
      "Epoch [4][20]\t Batch [5200][5500]\t Training Loss 0.0649\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [5250][5500]\t Training Loss 0.0649\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [5300][5500]\t Training Loss 0.0649\t Accuracy 0.9551\n",
      "Epoch [4][20]\t Batch [5350][5500]\t Training Loss 0.0649\t Accuracy 0.9552\n",
      "Epoch [4][20]\t Batch [5400][5500]\t Training Loss 0.0649\t Accuracy 0.9551\n",
      "Epoch [4][20]\t Batch [5450][5500]\t Training Loss 0.0648\t Accuracy 0.9553\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0648\t Average training accuracy 0.9552\n",
      "Epoch [4]\t Average validation loss 0.0593\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [5][20]\t Batch [0][5500]\t Training Loss 0.0311\t Accuracy 1.0000\n",
      "Epoch [5][20]\t Batch [50][5500]\t Training Loss 0.0559\t Accuracy 0.9549\n",
      "Epoch [5][20]\t Batch [100][5500]\t Training Loss 0.0587\t Accuracy 0.9614\n",
      "Epoch [5][20]\t Batch [150][5500]\t Training Loss 0.0646\t Accuracy 0.9523\n",
      "Epoch [5][20]\t Batch [200][5500]\t Training Loss 0.0625\t Accuracy 0.9532\n",
      "Epoch [5][20]\t Batch [250][5500]\t Training Loss 0.0608\t Accuracy 0.9566\n",
      "Epoch [5][20]\t Batch [300][5500]\t Training Loss 0.0607\t Accuracy 0.9581\n",
      "Epoch [5][20]\t Batch [350][5500]\t Training Loss 0.0598\t Accuracy 0.9601\n",
      "Epoch [5][20]\t Batch [400][5500]\t Training Loss 0.0595\t Accuracy 0.9601\n",
      "Epoch [5][20]\t Batch [450][5500]\t Training Loss 0.0594\t Accuracy 0.9605\n",
      "Epoch [5][20]\t Batch [500][5500]\t Training Loss 0.0587\t Accuracy 0.9621\n",
      "Epoch [5][20]\t Batch [550][5500]\t Training Loss 0.0590\t Accuracy 0.9610\n",
      "Epoch [5][20]\t Batch [600][5500]\t Training Loss 0.0592\t Accuracy 0.9609\n",
      "Epoch [5][20]\t Batch [650][5500]\t Training Loss 0.0589\t Accuracy 0.9614\n",
      "Epoch [5][20]\t Batch [700][5500]\t Training Loss 0.0591\t Accuracy 0.9612\n",
      "Epoch [5][20]\t Batch [750][5500]\t Training Loss 0.0590\t Accuracy 0.9621\n",
      "Epoch [5][20]\t Batch [800][5500]\t Training Loss 0.0592\t Accuracy 0.9622\n",
      "Epoch [5][20]\t Batch [850][5500]\t Training Loss 0.0597\t Accuracy 0.9611\n",
      "Epoch [5][20]\t Batch [900][5500]\t Training Loss 0.0610\t Accuracy 0.9588\n",
      "Epoch [5][20]\t Batch [950][5500]\t Training Loss 0.0608\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [1000][5500]\t Training Loss 0.0603\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [1050][5500]\t Training Loss 0.0609\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [1100][5500]\t Training Loss 0.0607\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [1150][5500]\t Training Loss 0.0606\t Accuracy 0.9587\n",
      "Epoch [5][20]\t Batch [1200][5500]\t Training Loss 0.0610\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [1250][5500]\t Training Loss 0.0610\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [1300][5500]\t Training Loss 0.0615\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [1350][5500]\t Training Loss 0.0617\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [1400][5500]\t Training Loss 0.0619\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [1450][5500]\t Training Loss 0.0622\t Accuracy 0.9578\n",
      "Epoch [5][20]\t Batch [1500][5500]\t Training Loss 0.0627\t Accuracy 0.9576\n",
      "Epoch [5][20]\t Batch [1550][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [1600][5500]\t Training Loss 0.0629\t Accuracy 0.9573\n",
      "Epoch [5][20]\t Batch [1650][5500]\t Training Loss 0.0625\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [1700][5500]\t Training Loss 0.0625\t Accuracy 0.9582\n",
      "Epoch [5][20]\t Batch [1750][5500]\t Training Loss 0.0626\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [1800][5500]\t Training Loss 0.0628\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [1850][5500]\t Training Loss 0.0627\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [1900][5500]\t Training Loss 0.0625\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [1950][5500]\t Training Loss 0.0624\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [2000][5500]\t Training Loss 0.0622\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [2050][5500]\t Training Loss 0.0622\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [2100][5500]\t Training Loss 0.0624\t Accuracy 0.9582\n",
      "Epoch [5][20]\t Batch [2150][5500]\t Training Loss 0.0624\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [2200][5500]\t Training Loss 0.0622\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [2250][5500]\t Training Loss 0.0621\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [2300][5500]\t Training Loss 0.0623\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [2350][5500]\t Training Loss 0.0622\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [2400][5500]\t Training Loss 0.0622\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [2450][5500]\t Training Loss 0.0622\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [2500][5500]\t Training Loss 0.0621\t Accuracy 0.9587\n",
      "Epoch [5][20]\t Batch [2550][5500]\t Training Loss 0.0620\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [2600][5500]\t Training Loss 0.0619\t Accuracy 0.9587\n",
      "Epoch [5][20]\t Batch [2650][5500]\t Training Loss 0.0619\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [2700][5500]\t Training Loss 0.0621\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [2750][5500]\t Training Loss 0.0621\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [2800][5500]\t Training Loss 0.0620\t Accuracy 0.9587\n",
      "Epoch [5][20]\t Batch [2850][5500]\t Training Loss 0.0619\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [2900][5500]\t Training Loss 0.0618\t Accuracy 0.9587\n",
      "Epoch [5][20]\t Batch [2950][5500]\t Training Loss 0.0620\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3000][5500]\t Training Loss 0.0621\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [3050][5500]\t Training Loss 0.0621\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3100][5500]\t Training Loss 0.0621\t Accuracy 0.9582\n",
      "Epoch [5][20]\t Batch [3150][5500]\t Training Loss 0.0623\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [3200][5500]\t Training Loss 0.0625\t Accuracy 0.9578\n",
      "Epoch [5][20]\t Batch [3250][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [3300][5500]\t Training Loss 0.0625\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [3350][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [3400][5500]\t Training Loss 0.0621\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3450][5500]\t Training Loss 0.0620\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3500][5500]\t Training Loss 0.0621\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3550][5500]\t Training Loss 0.0621\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [3600][5500]\t Training Loss 0.0620\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [3650][5500]\t Training Loss 0.0621\t Accuracy 0.9585\n",
      "Epoch [5][20]\t Batch [3700][5500]\t Training Loss 0.0620\t Accuracy 0.9586\n",
      "Epoch [5][20]\t Batch [3750][5500]\t Training Loss 0.0622\t Accuracy 0.9584\n",
      "Epoch [5][20]\t Batch [3800][5500]\t Training Loss 0.0623\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [3850][5500]\t Training Loss 0.0623\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [3900][5500]\t Training Loss 0.0623\t Accuracy 0.9583\n",
      "Epoch [5][20]\t Batch [3950][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4000][5500]\t Training Loss 0.0624\t Accuracy 0.9581\n",
      "Epoch [5][20]\t Batch [4050][5500]\t Training Loss 0.0623\t Accuracy 0.9581\n",
      "Epoch [5][20]\t Batch [4100][5500]\t Training Loss 0.0622\t Accuracy 0.9582\n",
      "Epoch [5][20]\t Batch [4150][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4200][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4250][5500]\t Training Loss 0.0625\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [4300][5500]\t Training Loss 0.0625\t Accuracy 0.9578\n",
      "Epoch [5][20]\t Batch [4350][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4400][5500]\t Training Loss 0.0623\t Accuracy 0.9582\n",
      "Epoch [5][20]\t Batch [4450][5500]\t Training Loss 0.0624\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4500][5500]\t Training Loss 0.0623\t Accuracy 0.9580\n",
      "Epoch [5][20]\t Batch [4550][5500]\t Training Loss 0.0624\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [4600][5500]\t Training Loss 0.0624\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [4650][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [4700][5500]\t Training Loss 0.0625\t Accuracy 0.9579\n",
      "Epoch [5][20]\t Batch [4750][5500]\t Training Loss 0.0626\t Accuracy 0.9578\n",
      "Epoch [5][20]\t Batch [4800][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [4850][5500]\t Training Loss 0.0625\t Accuracy 0.9578\n",
      "Epoch [5][20]\t Batch [4900][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [4950][5500]\t Training Loss 0.0626\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [5000][5500]\t Training Loss 0.0628\t Accuracy 0.9575\n",
      "Epoch [5][20]\t Batch [5050][5500]\t Training Loss 0.0628\t Accuracy 0.9575\n",
      "Epoch [5][20]\t Batch [5100][5500]\t Training Loss 0.0628\t Accuracy 0.9575\n",
      "Epoch [5][20]\t Batch [5150][5500]\t Training Loss 0.0628\t Accuracy 0.9576\n",
      "Epoch [5][20]\t Batch [5200][5500]\t Training Loss 0.0627\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [5250][5500]\t Training Loss 0.0627\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [5300][5500]\t Training Loss 0.0627\t Accuracy 0.9576\n",
      "Epoch [5][20]\t Batch [5350][5500]\t Training Loss 0.0627\t Accuracy 0.9577\n",
      "Epoch [5][20]\t Batch [5400][5500]\t Training Loss 0.0627\t Accuracy 0.9576\n",
      "Epoch [5][20]\t Batch [5450][5500]\t Training Loss 0.0626\t Accuracy 0.9578\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0626\t Average training accuracy 0.9577\n",
      "Epoch [5]\t Average validation loss 0.0581\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [6][20]\t Batch [0][5500]\t Training Loss 0.0329\t Accuracy 1.0000\n",
      "Epoch [6][20]\t Batch [50][5500]\t Training Loss 0.0544\t Accuracy 0.9569\n",
      "Epoch [6][20]\t Batch [100][5500]\t Training Loss 0.0572\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [150][5500]\t Training Loss 0.0631\t Accuracy 0.9530\n",
      "Epoch [6][20]\t Batch [200][5500]\t Training Loss 0.0611\t Accuracy 0.9537\n",
      "Epoch [6][20]\t Batch [250][5500]\t Training Loss 0.0594\t Accuracy 0.9574\n",
      "Epoch [6][20]\t Batch [300][5500]\t Training Loss 0.0592\t Accuracy 0.9591\n",
      "Epoch [6][20]\t Batch [350][5500]\t Training Loss 0.0583\t Accuracy 0.9613\n",
      "Epoch [6][20]\t Batch [400][5500]\t Training Loss 0.0579\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [450][5500]\t Training Loss 0.0578\t Accuracy 0.9610\n",
      "Epoch [6][20]\t Batch [500][5500]\t Training Loss 0.0571\t Accuracy 0.9621\n",
      "Epoch [6][20]\t Batch [550][5500]\t Training Loss 0.0575\t Accuracy 0.9610\n",
      "Epoch [6][20]\t Batch [600][5500]\t Training Loss 0.0575\t Accuracy 0.9614\n",
      "Epoch [6][20]\t Batch [650][5500]\t Training Loss 0.0573\t Accuracy 0.9616\n",
      "Epoch [6][20]\t Batch [700][5500]\t Training Loss 0.0575\t Accuracy 0.9616\n",
      "Epoch [6][20]\t Batch [750][5500]\t Training Loss 0.0574\t Accuracy 0.9625\n",
      "Epoch [6][20]\t Batch [800][5500]\t Training Loss 0.0576\t Accuracy 0.9625\n",
      "Epoch [6][20]\t Batch [850][5500]\t Training Loss 0.0581\t Accuracy 0.9616\n",
      "Epoch [6][20]\t Batch [900][5500]\t Training Loss 0.0593\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [950][5500]\t Training Loss 0.0592\t Accuracy 0.9594\n",
      "Epoch [6][20]\t Batch [1000][5500]\t Training Loss 0.0587\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [1050][5500]\t Training Loss 0.0593\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [1100][5500]\t Training Loss 0.0591\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [1150][5500]\t Training Loss 0.0590\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [1200][5500]\t Training Loss 0.0594\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [1250][5500]\t Training Loss 0.0595\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [1300][5500]\t Training Loss 0.0598\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [1350][5500]\t Training Loss 0.0601\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [1400][5500]\t Training Loss 0.0603\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [1450][5500]\t Training Loss 0.0605\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [1500][5500]\t Training Loss 0.0610\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [1550][5500]\t Training Loss 0.0610\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [1600][5500]\t Training Loss 0.0612\t Accuracy 0.9593\n",
      "Epoch [6][20]\t Batch [1650][5500]\t Training Loss 0.0609\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [1700][5500]\t Training Loss 0.0608\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [1750][5500]\t Training Loss 0.0608\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [1800][5500]\t Training Loss 0.0611\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [1850][5500]\t Training Loss 0.0610\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [1900][5500]\t Training Loss 0.0607\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [1950][5500]\t Training Loss 0.0607\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [2000][5500]\t Training Loss 0.0605\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [2050][5500]\t Training Loss 0.0605\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [2100][5500]\t Training Loss 0.0607\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [2150][5500]\t Training Loss 0.0607\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [2200][5500]\t Training Loss 0.0605\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [2250][5500]\t Training Loss 0.0604\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [2300][5500]\t Training Loss 0.0606\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [2350][5500]\t Training Loss 0.0605\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [2400][5500]\t Training Loss 0.0605\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [2450][5500]\t Training Loss 0.0605\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [2500][5500]\t Training Loss 0.0604\t Accuracy 0.9606\n",
      "Epoch [6][20]\t Batch [2550][5500]\t Training Loss 0.0603\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [2600][5500]\t Training Loss 0.0602\t Accuracy 0.9606\n",
      "Epoch [6][20]\t Batch [2650][5500]\t Training Loss 0.0602\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [2700][5500]\t Training Loss 0.0604\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [2750][5500]\t Training Loss 0.0604\t Accuracy 0.9604\n",
      "Epoch [6][20]\t Batch [2800][5500]\t Training Loss 0.0603\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [2850][5500]\t Training Loss 0.0603\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [2900][5500]\t Training Loss 0.0602\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [2950][5500]\t Training Loss 0.0603\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3000][5500]\t Training Loss 0.0604\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [3050][5500]\t Training Loss 0.0604\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [3100][5500]\t Training Loss 0.0604\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [3150][5500]\t Training Loss 0.0606\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [3200][5500]\t Training Loss 0.0608\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [3250][5500]\t Training Loss 0.0609\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [3300][5500]\t Training Loss 0.0608\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [3350][5500]\t Training Loss 0.0607\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [3400][5500]\t Training Loss 0.0604\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3450][5500]\t Training Loss 0.0603\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3500][5500]\t Training Loss 0.0604\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3550][5500]\t Training Loss 0.0604\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [3600][5500]\t Training Loss 0.0604\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [3650][5500]\t Training Loss 0.0604\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [3700][5500]\t Training Loss 0.0604\t Accuracy 0.9605\n",
      "Epoch [6][20]\t Batch [3750][5500]\t Training Loss 0.0605\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3800][5500]\t Training Loss 0.0606\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3850][5500]\t Training Loss 0.0607\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [3900][5500]\t Training Loss 0.0606\t Accuracy 0.9603\n",
      "Epoch [6][20]\t Batch [3950][5500]\t Training Loss 0.0607\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [4000][5500]\t Training Loss 0.0607\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [4050][5500]\t Training Loss 0.0607\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [4100][5500]\t Training Loss 0.0606\t Accuracy 0.9602\n",
      "Epoch [6][20]\t Batch [4150][5500]\t Training Loss 0.0607\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [4200][5500]\t Training Loss 0.0607\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [4250][5500]\t Training Loss 0.0608\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [4300][5500]\t Training Loss 0.0608\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [4350][5500]\t Training Loss 0.0607\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [4400][5500]\t Training Loss 0.0606\t Accuracy 0.9601\n",
      "Epoch [6][20]\t Batch [4450][5500]\t Training Loss 0.0607\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [4500][5500]\t Training Loss 0.0607\t Accuracy 0.9599\n",
      "Epoch [6][20]\t Batch [4550][5500]\t Training Loss 0.0607\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [4600][5500]\t Training Loss 0.0608\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [4650][5500]\t Training Loss 0.0609\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [4700][5500]\t Training Loss 0.0608\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [4750][5500]\t Training Loss 0.0609\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [4800][5500]\t Training Loss 0.0609\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [4850][5500]\t Training Loss 0.0609\t Accuracy 0.9598\n",
      "Epoch [6][20]\t Batch [4900][5500]\t Training Loss 0.0609\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [4950][5500]\t Training Loss 0.0609\t Accuracy 0.9597\n",
      "Epoch [6][20]\t Batch [5000][5500]\t Training Loss 0.0611\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [5050][5500]\t Training Loss 0.0612\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [5100][5500]\t Training Loss 0.0612\t Accuracy 0.9594\n",
      "Epoch [6][20]\t Batch [5150][5500]\t Training Loss 0.0611\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [5200][5500]\t Training Loss 0.0611\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [5250][5500]\t Training Loss 0.0610\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [5300][5500]\t Training Loss 0.0611\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [5350][5500]\t Training Loss 0.0610\t Accuracy 0.9596\n",
      "Epoch [6][20]\t Batch [5400][5500]\t Training Loss 0.0610\t Accuracy 0.9595\n",
      "Epoch [6][20]\t Batch [5450][5500]\t Training Loss 0.0609\t Accuracy 0.9597\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0610\t Average training accuracy 0.9596\n",
      "Epoch [6]\t Average validation loss 0.0567\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [7][20]\t Batch [0][5500]\t Training Loss 0.0312\t Accuracy 1.0000\n",
      "Epoch [7][20]\t Batch [50][5500]\t Training Loss 0.0532\t Accuracy 0.9569\n",
      "Epoch [7][20]\t Batch [100][5500]\t Training Loss 0.0558\t Accuracy 0.9604\n",
      "Epoch [7][20]\t Batch [150][5500]\t Training Loss 0.0619\t Accuracy 0.9530\n",
      "Epoch [7][20]\t Batch [200][5500]\t Training Loss 0.0598\t Accuracy 0.9542\n",
      "Epoch [7][20]\t Batch [250][5500]\t Training Loss 0.0580\t Accuracy 0.9574\n",
      "Epoch [7][20]\t Batch [300][5500]\t Training Loss 0.0579\t Accuracy 0.9598\n",
      "Epoch [7][20]\t Batch [350][5500]\t Training Loss 0.0570\t Accuracy 0.9630\n",
      "Epoch [7][20]\t Batch [400][5500]\t Training Loss 0.0565\t Accuracy 0.9621\n",
      "Epoch [7][20]\t Batch [450][5500]\t Training Loss 0.0564\t Accuracy 0.9625\n",
      "Epoch [7][20]\t Batch [500][5500]\t Training Loss 0.0558\t Accuracy 0.9637\n",
      "Epoch [7][20]\t Batch [550][5500]\t Training Loss 0.0561\t Accuracy 0.9630\n",
      "Epoch [7][20]\t Batch [600][5500]\t Training Loss 0.0562\t Accuracy 0.9632\n",
      "Epoch [7][20]\t Batch [650][5500]\t Training Loss 0.0559\t Accuracy 0.9633\n",
      "Epoch [7][20]\t Batch [700][5500]\t Training Loss 0.0561\t Accuracy 0.9632\n",
      "Epoch [7][20]\t Batch [750][5500]\t Training Loss 0.0561\t Accuracy 0.9639\n",
      "Epoch [7][20]\t Batch [800][5500]\t Training Loss 0.0563\t Accuracy 0.9638\n",
      "Epoch [7][20]\t Batch [850][5500]\t Training Loss 0.0568\t Accuracy 0.9631\n",
      "Epoch [7][20]\t Batch [900][5500]\t Training Loss 0.0580\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [950][5500]\t Training Loss 0.0579\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [1000][5500]\t Training Loss 0.0574\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [1050][5500]\t Training Loss 0.0580\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [1100][5500]\t Training Loss 0.0578\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [1150][5500]\t Training Loss 0.0577\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [1200][5500]\t Training Loss 0.0582\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [1250][5500]\t Training Loss 0.0582\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [1300][5500]\t Training Loss 0.0586\t Accuracy 0.9615\n",
      "Epoch [7][20]\t Batch [1350][5500]\t Training Loss 0.0588\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [1400][5500]\t Training Loss 0.0590\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [1450][5500]\t Training Loss 0.0592\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [1500][5500]\t Training Loss 0.0596\t Accuracy 0.9609\n",
      "Epoch [7][20]\t Batch [1550][5500]\t Training Loss 0.0596\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [1600][5500]\t Training Loss 0.0598\t Accuracy 0.9608\n",
      "Epoch [7][20]\t Batch [1650][5500]\t Training Loss 0.0595\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [1700][5500]\t Training Loss 0.0594\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [1750][5500]\t Training Loss 0.0594\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [1800][5500]\t Training Loss 0.0597\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [1850][5500]\t Training Loss 0.0595\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [1900][5500]\t Training Loss 0.0593\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [1950][5500]\t Training Loss 0.0593\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [2000][5500]\t Training Loss 0.0591\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [2050][5500]\t Training Loss 0.0591\t Accuracy 0.9621\n",
      "Epoch [7][20]\t Batch [2100][5500]\t Training Loss 0.0593\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [2150][5500]\t Training Loss 0.0593\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [2200][5500]\t Training Loss 0.0591\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2250][5500]\t Training Loss 0.0591\t Accuracy 0.9621\n",
      "Epoch [7][20]\t Batch [2300][5500]\t Training Loss 0.0592\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2350][5500]\t Training Loss 0.0591\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2400][5500]\t Training Loss 0.0592\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2450][5500]\t Training Loss 0.0591\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2500][5500]\t Training Loss 0.0590\t Accuracy 0.9621\n",
      "Epoch [7][20]\t Batch [2550][5500]\t Training Loss 0.0589\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [2600][5500]\t Training Loss 0.0589\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [2650][5500]\t Training Loss 0.0589\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2700][5500]\t Training Loss 0.0590\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [2750][5500]\t Training Loss 0.0590\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [2800][5500]\t Training Loss 0.0589\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2850][5500]\t Training Loss 0.0589\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [2900][5500]\t Training Loss 0.0588\t Accuracy 0.9621\n",
      "Epoch [7][20]\t Batch [2950][5500]\t Training Loss 0.0589\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [3000][5500]\t Training Loss 0.0590\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3050][5500]\t Training Loss 0.0590\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3100][5500]\t Training Loss 0.0590\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3150][5500]\t Training Loss 0.0592\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [3200][5500]\t Training Loss 0.0594\t Accuracy 0.9613\n",
      "Epoch [7][20]\t Batch [3250][5500]\t Training Loss 0.0595\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [3300][5500]\t Training Loss 0.0594\t Accuracy 0.9615\n",
      "Epoch [7][20]\t Batch [3350][5500]\t Training Loss 0.0593\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [3400][5500]\t Training Loss 0.0591\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [3450][5500]\t Training Loss 0.0590\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [3500][5500]\t Training Loss 0.0590\t Accuracy 0.9619\n",
      "Epoch [7][20]\t Batch [3550][5500]\t Training Loss 0.0591\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3600][5500]\t Training Loss 0.0590\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [3650][5500]\t Training Loss 0.0590\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [3700][5500]\t Training Loss 0.0590\t Accuracy 0.9620\n",
      "Epoch [7][20]\t Batch [3750][5500]\t Training Loss 0.0591\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3800][5500]\t Training Loss 0.0593\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [3850][5500]\t Training Loss 0.0593\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [3900][5500]\t Training Loss 0.0592\t Accuracy 0.9618\n",
      "Epoch [7][20]\t Batch [3950][5500]\t Training Loss 0.0593\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [4000][5500]\t Training Loss 0.0594\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [4050][5500]\t Training Loss 0.0593\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [4100][5500]\t Training Loss 0.0592\t Accuracy 0.9617\n",
      "Epoch [7][20]\t Batch [4150][5500]\t Training Loss 0.0593\t Accuracy 0.9615\n",
      "Epoch [7][20]\t Batch [4200][5500]\t Training Loss 0.0593\t Accuracy 0.9615\n",
      "Epoch [7][20]\t Batch [4250][5500]\t Training Loss 0.0594\t Accuracy 0.9613\n",
      "Epoch [7][20]\t Batch [4300][5500]\t Training Loss 0.0595\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [4350][5500]\t Training Loss 0.0593\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [4400][5500]\t Training Loss 0.0593\t Accuracy 0.9616\n",
      "Epoch [7][20]\t Batch [4450][5500]\t Training Loss 0.0594\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [4500][5500]\t Training Loss 0.0593\t Accuracy 0.9615\n",
      "Epoch [7][20]\t Batch [4550][5500]\t Training Loss 0.0593\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [4600][5500]\t Training Loss 0.0594\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [4650][5500]\t Training Loss 0.0595\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [4700][5500]\t Training Loss 0.0594\t Accuracy 0.9613\n",
      "Epoch [7][20]\t Batch [4750][5500]\t Training Loss 0.0595\t Accuracy 0.9612\n",
      "Epoch [7][20]\t Batch [4800][5500]\t Training Loss 0.0595\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [4850][5500]\t Training Loss 0.0595\t Accuracy 0.9613\n",
      "Epoch [7][20]\t Batch [4900][5500]\t Training Loss 0.0595\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [4950][5500]\t Training Loss 0.0595\t Accuracy 0.9611\n",
      "Epoch [7][20]\t Batch [5000][5500]\t Training Loss 0.0597\t Accuracy 0.9609\n",
      "Epoch [7][20]\t Batch [5050][5500]\t Training Loss 0.0598\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5100][5500]\t Training Loss 0.0598\t Accuracy 0.9609\n",
      "Epoch [7][20]\t Batch [5150][5500]\t Training Loss 0.0597\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5200][5500]\t Training Loss 0.0597\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5250][5500]\t Training Loss 0.0596\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5300][5500]\t Training Loss 0.0597\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5350][5500]\t Training Loss 0.0597\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5400][5500]\t Training Loss 0.0597\t Accuracy 0.9610\n",
      "Epoch [7][20]\t Batch [5450][5500]\t Training Loss 0.0596\t Accuracy 0.9612\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0596\t Average training accuracy 0.9611\n",
      "Epoch [7]\t Average validation loss 0.0556\t Average validation accuracy 0.9686\n",
      "\n",
      "Epoch [8][20]\t Batch [0][5500]\t Training Loss 0.0307\t Accuracy 1.0000\n",
      "Epoch [8][20]\t Batch [50][5500]\t Training Loss 0.0515\t Accuracy 0.9608\n",
      "Epoch [8][20]\t Batch [100][5500]\t Training Loss 0.0542\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [150][5500]\t Training Loss 0.0607\t Accuracy 0.9543\n",
      "Epoch [8][20]\t Batch [200][5500]\t Training Loss 0.0585\t Accuracy 0.9562\n",
      "Epoch [8][20]\t Batch [250][5500]\t Training Loss 0.0568\t Accuracy 0.9594\n",
      "Epoch [8][20]\t Batch [300][5500]\t Training Loss 0.0566\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [350][5500]\t Training Loss 0.0557\t Accuracy 0.9650\n",
      "Epoch [8][20]\t Batch [400][5500]\t Training Loss 0.0552\t Accuracy 0.9638\n",
      "Epoch [8][20]\t Batch [450][5500]\t Training Loss 0.0550\t Accuracy 0.9641\n",
      "Epoch [8][20]\t Batch [500][5500]\t Training Loss 0.0545\t Accuracy 0.9647\n",
      "Epoch [8][20]\t Batch [550][5500]\t Training Loss 0.0548\t Accuracy 0.9639\n",
      "Epoch [8][20]\t Batch [600][5500]\t Training Loss 0.0549\t Accuracy 0.9642\n",
      "Epoch [8][20]\t Batch [650][5500]\t Training Loss 0.0546\t Accuracy 0.9642\n",
      "Epoch [8][20]\t Batch [700][5500]\t Training Loss 0.0548\t Accuracy 0.9641\n",
      "Epoch [8][20]\t Batch [750][5500]\t Training Loss 0.0548\t Accuracy 0.9648\n",
      "Epoch [8][20]\t Batch [800][5500]\t Training Loss 0.0551\t Accuracy 0.9648\n",
      "Epoch [8][20]\t Batch [850][5500]\t Training Loss 0.0555\t Accuracy 0.9639\n",
      "Epoch [8][20]\t Batch [900][5500]\t Training Loss 0.0567\t Accuracy 0.9619\n",
      "Epoch [8][20]\t Batch [950][5500]\t Training Loss 0.0566\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [1000][5500]\t Training Loss 0.0560\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [1050][5500]\t Training Loss 0.0567\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [1100][5500]\t Training Loss 0.0566\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [1150][5500]\t Training Loss 0.0564\t Accuracy 0.9627\n",
      "Epoch [8][20]\t Batch [1200][5500]\t Training Loss 0.0569\t Accuracy 0.9621\n",
      "Epoch [8][20]\t Batch [1250][5500]\t Training Loss 0.0570\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [1300][5500]\t Training Loss 0.0574\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [1350][5500]\t Training Loss 0.0576\t Accuracy 0.9627\n",
      "Epoch [8][20]\t Batch [1400][5500]\t Training Loss 0.0577\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [1450][5500]\t Training Loss 0.0579\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [1500][5500]\t Training Loss 0.0583\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [1550][5500]\t Training Loss 0.0583\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [1600][5500]\t Training Loss 0.0585\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [1650][5500]\t Training Loss 0.0582\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [1700][5500]\t Training Loss 0.0581\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [1750][5500]\t Training Loss 0.0581\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [1800][5500]\t Training Loss 0.0583\t Accuracy 0.9627\n",
      "Epoch [8][20]\t Batch [1850][5500]\t Training Loss 0.0582\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [1900][5500]\t Training Loss 0.0580\t Accuracy 0.9631\n",
      "Epoch [8][20]\t Batch [1950][5500]\t Training Loss 0.0580\t Accuracy 0.9631\n",
      "Epoch [8][20]\t Batch [2000][5500]\t Training Loss 0.0578\t Accuracy 0.9634\n",
      "Epoch [8][20]\t Batch [2050][5500]\t Training Loss 0.0577\t Accuracy 0.9635\n",
      "Epoch [8][20]\t Batch [2100][5500]\t Training Loss 0.0580\t Accuracy 0.9631\n",
      "Epoch [8][20]\t Batch [2150][5500]\t Training Loss 0.0579\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2200][5500]\t Training Loss 0.0578\t Accuracy 0.9632\n",
      "Epoch [8][20]\t Batch [2250][5500]\t Training Loss 0.0578\t Accuracy 0.9633\n",
      "Epoch [8][20]\t Batch [2300][5500]\t Training Loss 0.0579\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2350][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2400][5500]\t Training Loss 0.0578\t Accuracy 0.9629\n",
      "Epoch [8][20]\t Batch [2450][5500]\t Training Loss 0.0578\t Accuracy 0.9629\n",
      "Epoch [8][20]\t Batch [2500][5500]\t Training Loss 0.0577\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2550][5500]\t Training Loss 0.0576\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2600][5500]\t Training Loss 0.0576\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2650][5500]\t Training Loss 0.0576\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2700][5500]\t Training Loss 0.0578\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [2750][5500]\t Training Loss 0.0578\t Accuracy 0.9629\n",
      "Epoch [8][20]\t Batch [2800][5500]\t Training Loss 0.0577\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2850][5500]\t Training Loss 0.0576\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [2900][5500]\t Training Loss 0.0575\t Accuracy 0.9632\n",
      "Epoch [8][20]\t Batch [2950][5500]\t Training Loss 0.0577\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3000][5500]\t Training Loss 0.0578\t Accuracy 0.9629\n",
      "Epoch [8][20]\t Batch [3050][5500]\t Training Loss 0.0578\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3100][5500]\t Training Loss 0.0577\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3150][5500]\t Training Loss 0.0580\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [3200][5500]\t Training Loss 0.0581\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [3250][5500]\t Training Loss 0.0582\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [3300][5500]\t Training Loss 0.0581\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [3350][5500]\t Training Loss 0.0581\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [3400][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3450][5500]\t Training Loss 0.0577\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3500][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3550][5500]\t Training Loss 0.0578\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3600][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3650][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3700][5500]\t Training Loss 0.0578\t Accuracy 0.9630\n",
      "Epoch [8][20]\t Batch [3750][5500]\t Training Loss 0.0579\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3800][5500]\t Training Loss 0.0581\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3850][5500]\t Training Loss 0.0581\t Accuracy 0.9627\n",
      "Epoch [8][20]\t Batch [3900][5500]\t Training Loss 0.0580\t Accuracy 0.9628\n",
      "Epoch [8][20]\t Batch [3950][5500]\t Training Loss 0.0581\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [4000][5500]\t Training Loss 0.0582\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [4050][5500]\t Training Loss 0.0581\t Accuracy 0.9626\n",
      "Epoch [8][20]\t Batch [4100][5500]\t Training Loss 0.0580\t Accuracy 0.9627\n",
      "Epoch [8][20]\t Batch [4150][5500]\t Training Loss 0.0581\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4200][5500]\t Training Loss 0.0581\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [4250][5500]\t Training Loss 0.0582\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [4300][5500]\t Training Loss 0.0583\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [4350][5500]\t Training Loss 0.0582\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4400][5500]\t Training Loss 0.0581\t Accuracy 0.9625\n",
      "Epoch [8][20]\t Batch [4450][5500]\t Training Loss 0.0582\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4500][5500]\t Training Loss 0.0581\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4550][5500]\t Training Loss 0.0581\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [4600][5500]\t Training Loss 0.0582\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4650][5500]\t Training Loss 0.0583\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [4700][5500]\t Training Loss 0.0583\t Accuracy 0.9623\n",
      "Epoch [8][20]\t Batch [4750][5500]\t Training Loss 0.0583\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [4800][5500]\t Training Loss 0.0584\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [4850][5500]\t Training Loss 0.0583\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [4900][5500]\t Training Loss 0.0584\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [4950][5500]\t Training Loss 0.0584\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [5000][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5050][5500]\t Training Loss 0.0586\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5100][5500]\t Training Loss 0.0586\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5150][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5200][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5250][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5300][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5350][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5400][5500]\t Training Loss 0.0585\t Accuracy 0.9620\n",
      "Epoch [8][20]\t Batch [5450][5500]\t Training Loss 0.0584\t Accuracy 0.9621\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0584\t Average training accuracy 0.9621\n",
      "Epoch [8]\t Average validation loss 0.0552\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [9][20]\t Batch [0][5500]\t Training Loss 0.0328\t Accuracy 1.0000\n",
      "Epoch [9][20]\t Batch [50][5500]\t Training Loss 0.0504\t Accuracy 0.9608\n",
      "Epoch [9][20]\t Batch [100][5500]\t Training Loss 0.0528\t Accuracy 0.9624\n",
      "Epoch [9][20]\t Batch [150][5500]\t Training Loss 0.0596\t Accuracy 0.9536\n",
      "Epoch [9][20]\t Batch [200][5500]\t Training Loss 0.0574\t Accuracy 0.9562\n",
      "Epoch [9][20]\t Batch [250][5500]\t Training Loss 0.0559\t Accuracy 0.9602\n",
      "Epoch [9][20]\t Batch [300][5500]\t Training Loss 0.0557\t Accuracy 0.9628\n",
      "Epoch [9][20]\t Batch [350][5500]\t Training Loss 0.0549\t Accuracy 0.9647\n",
      "Epoch [9][20]\t Batch [400][5500]\t Training Loss 0.0544\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [450][5500]\t Training Loss 0.0541\t Accuracy 0.9647\n",
      "Epoch [9][20]\t Batch [500][5500]\t Training Loss 0.0537\t Accuracy 0.9655\n",
      "Epoch [9][20]\t Batch [550][5500]\t Training Loss 0.0540\t Accuracy 0.9652\n",
      "Epoch [9][20]\t Batch [600][5500]\t Training Loss 0.0541\t Accuracy 0.9652\n",
      "Epoch [9][20]\t Batch [650][5500]\t Training Loss 0.0538\t Accuracy 0.9653\n",
      "Epoch [9][20]\t Batch [700][5500]\t Training Loss 0.0540\t Accuracy 0.9653\n",
      "Epoch [9][20]\t Batch [750][5500]\t Training Loss 0.0539\t Accuracy 0.9660\n",
      "Epoch [9][20]\t Batch [800][5500]\t Training Loss 0.0542\t Accuracy 0.9659\n",
      "Epoch [9][20]\t Batch [850][5500]\t Training Loss 0.0547\t Accuracy 0.9652\n",
      "Epoch [9][20]\t Batch [900][5500]\t Training Loss 0.0558\t Accuracy 0.9634\n",
      "Epoch [9][20]\t Batch [950][5500]\t Training Loss 0.0557\t Accuracy 0.9632\n",
      "Epoch [9][20]\t Batch [1000][5500]\t Training Loss 0.0552\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [1050][5500]\t Training Loss 0.0558\t Accuracy 0.9634\n",
      "Epoch [9][20]\t Batch [1100][5500]\t Training Loss 0.0557\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [1150][5500]\t Training Loss 0.0556\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [1200][5500]\t Training Loss 0.0561\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [1250][5500]\t Training Loss 0.0561\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [1300][5500]\t Training Loss 0.0565\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [1350][5500]\t Training Loss 0.0568\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [1400][5500]\t Training Loss 0.0569\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [1450][5500]\t Training Loss 0.0570\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [1500][5500]\t Training Loss 0.0575\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [1550][5500]\t Training Loss 0.0575\t Accuracy 0.9637\n",
      "Epoch [9][20]\t Batch [1600][5500]\t Training Loss 0.0576\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [1650][5500]\t Training Loss 0.0573\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [1700][5500]\t Training Loss 0.0572\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [1750][5500]\t Training Loss 0.0572\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [1800][5500]\t Training Loss 0.0574\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [1850][5500]\t Training Loss 0.0573\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [1900][5500]\t Training Loss 0.0570\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [1950][5500]\t Training Loss 0.0571\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [2000][5500]\t Training Loss 0.0568\t Accuracy 0.9646\n",
      "Epoch [9][20]\t Batch [2050][5500]\t Training Loss 0.0568\t Accuracy 0.9647\n",
      "Epoch [9][20]\t Batch [2100][5500]\t Training Loss 0.0570\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [2150][5500]\t Training Loss 0.0570\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [2200][5500]\t Training Loss 0.0568\t Accuracy 0.9645\n",
      "Epoch [9][20]\t Batch [2250][5500]\t Training Loss 0.0568\t Accuracy 0.9645\n",
      "Epoch [9][20]\t Batch [2300][5500]\t Training Loss 0.0570\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [2350][5500]\t Training Loss 0.0569\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [2400][5500]\t Training Loss 0.0569\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2450][5500]\t Training Loss 0.0569\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2500][5500]\t Training Loss 0.0568\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [2550][5500]\t Training Loss 0.0567\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2600][5500]\t Training Loss 0.0567\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2650][5500]\t Training Loss 0.0567\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [2700][5500]\t Training Loss 0.0569\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [2750][5500]\t Training Loss 0.0569\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2800][5500]\t Training Loss 0.0567\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [2850][5500]\t Training Loss 0.0567\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [2900][5500]\t Training Loss 0.0566\t Accuracy 0.9645\n",
      "Epoch [9][20]\t Batch [2950][5500]\t Training Loss 0.0567\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [3000][5500]\t Training Loss 0.0568\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [3050][5500]\t Training Loss 0.0568\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [3100][5500]\t Training Loss 0.0568\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [3150][5500]\t Training Loss 0.0570\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [3200][5500]\t Training Loss 0.0572\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [3250][5500]\t Training Loss 0.0573\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [3300][5500]\t Training Loss 0.0571\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [3350][5500]\t Training Loss 0.0571\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [3400][5500]\t Training Loss 0.0568\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [3450][5500]\t Training Loss 0.0568\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [3500][5500]\t Training Loss 0.0568\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [3550][5500]\t Training Loss 0.0569\t Accuracy 0.9642\n",
      "Epoch [9][20]\t Batch [3600][5500]\t Training Loss 0.0568\t Accuracy 0.9644\n",
      "Epoch [9][20]\t Batch [3650][5500]\t Training Loss 0.0568\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [3700][5500]\t Training Loss 0.0568\t Accuracy 0.9643\n",
      "Epoch [9][20]\t Batch [3750][5500]\t Training Loss 0.0569\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [3800][5500]\t Training Loss 0.0571\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [3850][5500]\t Training Loss 0.0571\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [3900][5500]\t Training Loss 0.0570\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [3950][5500]\t Training Loss 0.0572\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [4000][5500]\t Training Loss 0.0572\t Accuracy 0.9639\n",
      "Epoch [9][20]\t Batch [4050][5500]\t Training Loss 0.0571\t Accuracy 0.9640\n",
      "Epoch [9][20]\t Batch [4100][5500]\t Training Loss 0.0570\t Accuracy 0.9641\n",
      "Epoch [9][20]\t Batch [4150][5500]\t Training Loss 0.0572\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [4200][5500]\t Training Loss 0.0572\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [4250][5500]\t Training Loss 0.0573\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [4300][5500]\t Training Loss 0.0573\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [4350][5500]\t Training Loss 0.0572\t Accuracy 0.9637\n",
      "Epoch [9][20]\t Batch [4400][5500]\t Training Loss 0.0571\t Accuracy 0.9638\n",
      "Epoch [9][20]\t Batch [4450][5500]\t Training Loss 0.0572\t Accuracy 0.9637\n",
      "Epoch [9][20]\t Batch [4500][5500]\t Training Loss 0.0571\t Accuracy 0.9637\n",
      "Epoch [9][20]\t Batch [4550][5500]\t Training Loss 0.0571\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [4600][5500]\t Training Loss 0.0572\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [4650][5500]\t Training Loss 0.0573\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [4700][5500]\t Training Loss 0.0573\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [4750][5500]\t Training Loss 0.0573\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [4800][5500]\t Training Loss 0.0574\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [4850][5500]\t Training Loss 0.0573\t Accuracy 0.9636\n",
      "Epoch [9][20]\t Batch [4900][5500]\t Training Loss 0.0574\t Accuracy 0.9634\n",
      "Epoch [9][20]\t Batch [4950][5500]\t Training Loss 0.0574\t Accuracy 0.9634\n",
      "Epoch [9][20]\t Batch [5000][5500]\t Training Loss 0.0575\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5050][5500]\t Training Loss 0.0576\t Accuracy 0.9632\n",
      "Epoch [9][20]\t Batch [5100][5500]\t Training Loss 0.0576\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5150][5500]\t Training Loss 0.0576\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5200][5500]\t Training Loss 0.0575\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5250][5500]\t Training Loss 0.0575\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5300][5500]\t Training Loss 0.0576\t Accuracy 0.9630\n",
      "Epoch [9][20]\t Batch [5350][5500]\t Training Loss 0.0575\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5400][5500]\t Training Loss 0.0575\t Accuracy 0.9631\n",
      "Epoch [9][20]\t Batch [5450][5500]\t Training Loss 0.0574\t Accuracy 0.9633\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0574\t Average training accuracy 0.9632\n",
      "Epoch [9]\t Average validation loss 0.0545\t Average validation accuracy 0.9680\n",
      "\n",
      "Epoch [10][20]\t Batch [0][5500]\t Training Loss 0.0336\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [50][5500]\t Training Loss 0.0503\t Accuracy 0.9588\n",
      "Epoch [10][20]\t Batch [100][5500]\t Training Loss 0.0522\t Accuracy 0.9634\n",
      "Epoch [10][20]\t Batch [150][5500]\t Training Loss 0.0588\t Accuracy 0.9550\n",
      "Epoch [10][20]\t Batch [200][5500]\t Training Loss 0.0566\t Accuracy 0.9567\n",
      "Epoch [10][20]\t Batch [250][5500]\t Training Loss 0.0552\t Accuracy 0.9602\n",
      "Epoch [10][20]\t Batch [300][5500]\t Training Loss 0.0550\t Accuracy 0.9625\n",
      "Epoch [10][20]\t Batch [350][5500]\t Training Loss 0.0540\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [400][5500]\t Training Loss 0.0537\t Accuracy 0.9638\n",
      "Epoch [10][20]\t Batch [450][5500]\t Training Loss 0.0534\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [500][5500]\t Training Loss 0.0530\t Accuracy 0.9651\n",
      "Epoch [10][20]\t Batch [550][5500]\t Training Loss 0.0533\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [600][5500]\t Training Loss 0.0533\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [650][5500]\t Training Loss 0.0530\t Accuracy 0.9651\n",
      "Epoch [10][20]\t Batch [700][5500]\t Training Loss 0.0531\t Accuracy 0.9653\n",
      "Epoch [10][20]\t Batch [750][5500]\t Training Loss 0.0531\t Accuracy 0.9662\n",
      "Epoch [10][20]\t Batch [800][5500]\t Training Loss 0.0534\t Accuracy 0.9662\n",
      "Epoch [10][20]\t Batch [850][5500]\t Training Loss 0.0539\t Accuracy 0.9656\n",
      "Epoch [10][20]\t Batch [900][5500]\t Training Loss 0.0550\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [950][5500]\t Training Loss 0.0549\t Accuracy 0.9635\n",
      "Epoch [10][20]\t Batch [1000][5500]\t Training Loss 0.0543\t Accuracy 0.9642\n",
      "Epoch [10][20]\t Batch [1050][5500]\t Training Loss 0.0549\t Accuracy 0.9638\n",
      "Epoch [10][20]\t Batch [1100][5500]\t Training Loss 0.0548\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [1150][5500]\t Training Loss 0.0547\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [1200][5500]\t Training Loss 0.0552\t Accuracy 0.9640\n",
      "Epoch [10][20]\t Batch [1250][5500]\t Training Loss 0.0553\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [1300][5500]\t Training Loss 0.0557\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [1350][5500]\t Training Loss 0.0560\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [1400][5500]\t Training Loss 0.0560\t Accuracy 0.9642\n",
      "Epoch [10][20]\t Batch [1450][5500]\t Training Loss 0.0562\t Accuracy 0.9640\n",
      "Epoch [10][20]\t Batch [1500][5500]\t Training Loss 0.0566\t Accuracy 0.9640\n",
      "Epoch [10][20]\t Batch [1550][5500]\t Training Loss 0.0566\t Accuracy 0.9640\n",
      "Epoch [10][20]\t Batch [1600][5500]\t Training Loss 0.0567\t Accuracy 0.9638\n",
      "Epoch [10][20]\t Batch [1650][5500]\t Training Loss 0.0564\t Accuracy 0.9641\n",
      "Epoch [10][20]\t Batch [1700][5500]\t Training Loss 0.0563\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [1750][5500]\t Training Loss 0.0563\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [1800][5500]\t Training Loss 0.0565\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [1850][5500]\t Training Loss 0.0564\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [1900][5500]\t Training Loss 0.0561\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [1950][5500]\t Training Loss 0.0561\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2000][5500]\t Training Loss 0.0559\t Accuracy 0.9649\n",
      "Epoch [10][20]\t Batch [2050][5500]\t Training Loss 0.0559\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [2100][5500]\t Training Loss 0.0561\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [2150][5500]\t Training Loss 0.0561\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [2200][5500]\t Training Loss 0.0559\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2250][5500]\t Training Loss 0.0559\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [2300][5500]\t Training Loss 0.0561\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [2350][5500]\t Training Loss 0.0560\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [2400][5500]\t Training Loss 0.0560\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [2450][5500]\t Training Loss 0.0560\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [2500][5500]\t Training Loss 0.0559\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2550][5500]\t Training Loss 0.0558\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [2600][5500]\t Training Loss 0.0558\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2650][5500]\t Training Loss 0.0558\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2700][5500]\t Training Loss 0.0560\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [2750][5500]\t Training Loss 0.0560\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2800][5500]\t Training Loss 0.0558\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [2850][5500]\t Training Loss 0.0558\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [2900][5500]\t Training Loss 0.0557\t Accuracy 0.9649\n",
      "Epoch [10][20]\t Batch [2950][5500]\t Training Loss 0.0559\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3000][5500]\t Training Loss 0.0559\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [3050][5500]\t Training Loss 0.0559\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [3100][5500]\t Training Loss 0.0559\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3150][5500]\t Training Loss 0.0561\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [3200][5500]\t Training Loss 0.0563\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [3250][5500]\t Training Loss 0.0563\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [3300][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [3350][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [3400][5500]\t Training Loss 0.0559\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [3450][5500]\t Training Loss 0.0558\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [3500][5500]\t Training Loss 0.0559\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [3550][5500]\t Training Loss 0.0559\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3600][5500]\t Training Loss 0.0559\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [3650][5500]\t Training Loss 0.0559\t Accuracy 0.9649\n",
      "Epoch [10][20]\t Batch [3700][5500]\t Training Loss 0.0558\t Accuracy 0.9650\n",
      "Epoch [10][20]\t Batch [3750][5500]\t Training Loss 0.0560\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3800][5500]\t Training Loss 0.0561\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3850][5500]\t Training Loss 0.0561\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [3900][5500]\t Training Loss 0.0561\t Accuracy 0.9649\n",
      "Epoch [10][20]\t Batch [3950][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [4000][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [4050][5500]\t Training Loss 0.0562\t Accuracy 0.9647\n",
      "Epoch [10][20]\t Batch [4100][5500]\t Training Loss 0.0561\t Accuracy 0.9648\n",
      "Epoch [10][20]\t Batch [4150][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [4200][5500]\t Training Loss 0.0562\t Accuracy 0.9646\n",
      "Epoch [10][20]\t Batch [4250][5500]\t Training Loss 0.0563\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4300][5500]\t Training Loss 0.0563\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [4350][5500]\t Training Loss 0.0562\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4400][5500]\t Training Loss 0.0561\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [4450][5500]\t Training Loss 0.0562\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4500][5500]\t Training Loss 0.0561\t Accuracy 0.9645\n",
      "Epoch [10][20]\t Batch [4550][5500]\t Training Loss 0.0562\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4600][5500]\t Training Loss 0.0562\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4650][5500]\t Training Loss 0.0564\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [4700][5500]\t Training Loss 0.0563\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4750][5500]\t Training Loss 0.0564\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [4800][5500]\t Training Loss 0.0564\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [4850][5500]\t Training Loss 0.0563\t Accuracy 0.9644\n",
      "Epoch [10][20]\t Batch [4900][5500]\t Training Loss 0.0564\t Accuracy 0.9643\n",
      "Epoch [10][20]\t Batch [4950][5500]\t Training Loss 0.0564\t Accuracy 0.9642\n",
      "Epoch [10][20]\t Batch [5000][5500]\t Training Loss 0.0566\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5050][5500]\t Training Loss 0.0566\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5100][5500]\t Training Loss 0.0567\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5150][5500]\t Training Loss 0.0566\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5200][5500]\t Training Loss 0.0566\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5250][5500]\t Training Loss 0.0565\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5300][5500]\t Training Loss 0.0566\t Accuracy 0.9638\n",
      "Epoch [10][20]\t Batch [5350][5500]\t Training Loss 0.0566\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5400][5500]\t Training Loss 0.0565\t Accuracy 0.9639\n",
      "Epoch [10][20]\t Batch [5450][5500]\t Training Loss 0.0565\t Accuracy 0.9640\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0565\t Average training accuracy 0.9639\n",
      "Epoch [10]\t Average validation loss 0.0538\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [11][20]\t Batch [0][5500]\t Training Loss 0.0397\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [50][5500]\t Training Loss 0.0498\t Accuracy 0.9686\n",
      "Epoch [11][20]\t Batch [100][5500]\t Training Loss 0.0516\t Accuracy 0.9693\n",
      "Epoch [11][20]\t Batch [150][5500]\t Training Loss 0.0580\t Accuracy 0.9609\n",
      "Epoch [11][20]\t Batch [200][5500]\t Training Loss 0.0556\t Accuracy 0.9622\n",
      "Epoch [11][20]\t Batch [250][5500]\t Training Loss 0.0542\t Accuracy 0.9649\n",
      "Epoch [11][20]\t Batch [300][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [350][5500]\t Training Loss 0.0532\t Accuracy 0.9675\n",
      "Epoch [11][20]\t Batch [400][5500]\t Training Loss 0.0527\t Accuracy 0.9668\n",
      "Epoch [11][20]\t Batch [450][5500]\t Training Loss 0.0525\t Accuracy 0.9670\n",
      "Epoch [11][20]\t Batch [500][5500]\t Training Loss 0.0521\t Accuracy 0.9673\n",
      "Epoch [11][20]\t Batch [550][5500]\t Training Loss 0.0523\t Accuracy 0.9666\n",
      "Epoch [11][20]\t Batch [600][5500]\t Training Loss 0.0524\t Accuracy 0.9664\n",
      "Epoch [11][20]\t Batch [650][5500]\t Training Loss 0.0521\t Accuracy 0.9668\n",
      "Epoch [11][20]\t Batch [700][5500]\t Training Loss 0.0522\t Accuracy 0.9670\n",
      "Epoch [11][20]\t Batch [750][5500]\t Training Loss 0.0521\t Accuracy 0.9679\n",
      "Epoch [11][20]\t Batch [800][5500]\t Training Loss 0.0524\t Accuracy 0.9678\n",
      "Epoch [11][20]\t Batch [850][5500]\t Training Loss 0.0529\t Accuracy 0.9671\n",
      "Epoch [11][20]\t Batch [900][5500]\t Training Loss 0.0540\t Accuracy 0.9655\n",
      "Epoch [11][20]\t Batch [950][5500]\t Training Loss 0.0539\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [1000][5500]\t Training Loss 0.0533\t Accuracy 0.9661\n",
      "Epoch [11][20]\t Batch [1050][5500]\t Training Loss 0.0538\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [1100][5500]\t Training Loss 0.0537\t Accuracy 0.9663\n",
      "Epoch [11][20]\t Batch [1150][5500]\t Training Loss 0.0537\t Accuracy 0.9666\n",
      "Epoch [11][20]\t Batch [1200][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [1250][5500]\t Training Loss 0.0542\t Accuracy 0.9660\n",
      "Epoch [11][20]\t Batch [1300][5500]\t Training Loss 0.0546\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [1350][5500]\t Training Loss 0.0549\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [1400][5500]\t Training Loss 0.0550\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [1450][5500]\t Training Loss 0.0551\t Accuracy 0.9655\n",
      "Epoch [11][20]\t Batch [1500][5500]\t Training Loss 0.0555\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [1550][5500]\t Training Loss 0.0555\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [1600][5500]\t Training Loss 0.0556\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [1650][5500]\t Training Loss 0.0553\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [1700][5500]\t Training Loss 0.0553\t Accuracy 0.9660\n",
      "Epoch [11][20]\t Batch [1750][5500]\t Training Loss 0.0553\t Accuracy 0.9660\n",
      "Epoch [11][20]\t Batch [1800][5500]\t Training Loss 0.0555\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [1850][5500]\t Training Loss 0.0554\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [1900][5500]\t Training Loss 0.0551\t Accuracy 0.9660\n",
      "Epoch [11][20]\t Batch [1950][5500]\t Training Loss 0.0551\t Accuracy 0.9661\n",
      "Epoch [11][20]\t Batch [2000][5500]\t Training Loss 0.0549\t Accuracy 0.9663\n",
      "Epoch [11][20]\t Batch [2050][5500]\t Training Loss 0.0549\t Accuracy 0.9663\n",
      "Epoch [11][20]\t Batch [2100][5500]\t Training Loss 0.0551\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2150][5500]\t Training Loss 0.0550\t Accuracy 0.9661\n",
      "Epoch [11][20]\t Batch [2200][5500]\t Training Loss 0.0549\t Accuracy 0.9661\n",
      "Epoch [11][20]\t Batch [2250][5500]\t Training Loss 0.0549\t Accuracy 0.9662\n",
      "Epoch [11][20]\t Batch [2300][5500]\t Training Loss 0.0551\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [2350][5500]\t Training Loss 0.0550\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2400][5500]\t Training Loss 0.0550\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [2450][5500]\t Training Loss 0.0550\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [2500][5500]\t Training Loss 0.0549\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2550][5500]\t Training Loss 0.0548\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2600][5500]\t Training Loss 0.0548\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [2650][5500]\t Training Loss 0.0548\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2700][5500]\t Training Loss 0.0550\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [2750][5500]\t Training Loss 0.0550\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [2800][5500]\t Training Loss 0.0548\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [2850][5500]\t Training Loss 0.0548\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [2900][5500]\t Training Loss 0.0547\t Accuracy 0.9659\n",
      "Epoch [11][20]\t Batch [2950][5500]\t Training Loss 0.0549\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3000][5500]\t Training Loss 0.0549\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3050][5500]\t Training Loss 0.0550\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3100][5500]\t Training Loss 0.0549\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3150][5500]\t Training Loss 0.0551\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [3200][5500]\t Training Loss 0.0552\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [3250][5500]\t Training Loss 0.0553\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [3300][5500]\t Training Loss 0.0552\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [3350][5500]\t Training Loss 0.0552\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [3400][5500]\t Training Loss 0.0549\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [3450][5500]\t Training Loss 0.0549\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3500][5500]\t Training Loss 0.0549\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3550][5500]\t Training Loss 0.0550\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3600][5500]\t Training Loss 0.0549\t Accuracy 0.9658\n",
      "Epoch [11][20]\t Batch [3650][5500]\t Training Loss 0.0549\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3700][5500]\t Training Loss 0.0548\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3750][5500]\t Training Loss 0.0550\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3800][5500]\t Training Loss 0.0551\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3850][5500]\t Training Loss 0.0551\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [3900][5500]\t Training Loss 0.0551\t Accuracy 0.9657\n",
      "Epoch [11][20]\t Batch [3950][5500]\t Training Loss 0.0552\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [4000][5500]\t Training Loss 0.0553\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [4050][5500]\t Training Loss 0.0552\t Accuracy 0.9654\n",
      "Epoch [11][20]\t Batch [4100][5500]\t Training Loss 0.0551\t Accuracy 0.9655\n",
      "Epoch [11][20]\t Batch [4150][5500]\t Training Loss 0.0552\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [4200][5500]\t Training Loss 0.0552\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [4250][5500]\t Training Loss 0.0553\t Accuracy 0.9651\n",
      "Epoch [11][20]\t Batch [4300][5500]\t Training Loss 0.0554\t Accuracy 0.9650\n",
      "Epoch [11][20]\t Batch [4350][5500]\t Training Loss 0.0552\t Accuracy 0.9651\n",
      "Epoch [11][20]\t Batch [4400][5500]\t Training Loss 0.0552\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4450][5500]\t Training Loss 0.0552\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4500][5500]\t Training Loss 0.0551\t Accuracy 0.9653\n",
      "Epoch [11][20]\t Batch [4550][5500]\t Training Loss 0.0552\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4600][5500]\t Training Loss 0.0552\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4650][5500]\t Training Loss 0.0554\t Accuracy 0.9651\n",
      "Epoch [11][20]\t Batch [4700][5500]\t Training Loss 0.0553\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4750][5500]\t Training Loss 0.0554\t Accuracy 0.9651\n",
      "Epoch [11][20]\t Batch [4800][5500]\t Training Loss 0.0554\t Accuracy 0.9650\n",
      "Epoch [11][20]\t Batch [4850][5500]\t Training Loss 0.0553\t Accuracy 0.9652\n",
      "Epoch [11][20]\t Batch [4900][5500]\t Training Loss 0.0554\t Accuracy 0.9650\n",
      "Epoch [11][20]\t Batch [4950][5500]\t Training Loss 0.0554\t Accuracy 0.9650\n",
      "Epoch [11][20]\t Batch [5000][5500]\t Training Loss 0.0556\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5050][5500]\t Training Loss 0.0556\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5100][5500]\t Training Loss 0.0557\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5150][5500]\t Training Loss 0.0556\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5200][5500]\t Training Loss 0.0556\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5250][5500]\t Training Loss 0.0556\t Accuracy 0.9648\n",
      "Epoch [11][20]\t Batch [5300][5500]\t Training Loss 0.0556\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [5350][5500]\t Training Loss 0.0556\t Accuracy 0.9648\n",
      "Epoch [11][20]\t Batch [5400][5500]\t Training Loss 0.0556\t Accuracy 0.9648\n",
      "Epoch [11][20]\t Batch [5450][5500]\t Training Loss 0.0555\t Accuracy 0.9649\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0555\t Average training accuracy 0.9649\n",
      "Epoch [11]\t Average validation loss 0.0532\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [12][20]\t Batch [0][5500]\t Training Loss 0.0410\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][5500]\t Training Loss 0.0489\t Accuracy 0.9686\n",
      "Epoch [12][20]\t Batch [100][5500]\t Training Loss 0.0507\t Accuracy 0.9703\n",
      "Epoch [12][20]\t Batch [150][5500]\t Training Loss 0.0570\t Accuracy 0.9609\n",
      "Epoch [12][20]\t Batch [200][5500]\t Training Loss 0.0545\t Accuracy 0.9627\n",
      "Epoch [12][20]\t Batch [250][5500]\t Training Loss 0.0531\t Accuracy 0.9661\n",
      "Epoch [12][20]\t Batch [300][5500]\t Training Loss 0.0529\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [350][5500]\t Training Loss 0.0519\t Accuracy 0.9689\n",
      "Epoch [12][20]\t Batch [400][5500]\t Training Loss 0.0516\t Accuracy 0.9681\n",
      "Epoch [12][20]\t Batch [450][5500]\t Training Loss 0.0514\t Accuracy 0.9683\n",
      "Epoch [12][20]\t Batch [500][5500]\t Training Loss 0.0510\t Accuracy 0.9687\n",
      "Epoch [12][20]\t Batch [550][5500]\t Training Loss 0.0513\t Accuracy 0.9682\n",
      "Epoch [12][20]\t Batch [600][5500]\t Training Loss 0.0514\t Accuracy 0.9677\n",
      "Epoch [12][20]\t Batch [650][5500]\t Training Loss 0.0511\t Accuracy 0.9680\n",
      "Epoch [12][20]\t Batch [700][5500]\t Training Loss 0.0513\t Accuracy 0.9682\n",
      "Epoch [12][20]\t Batch [750][5500]\t Training Loss 0.0512\t Accuracy 0.9690\n",
      "Epoch [12][20]\t Batch [800][5500]\t Training Loss 0.0515\t Accuracy 0.9690\n",
      "Epoch [12][20]\t Batch [850][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [12][20]\t Batch [900][5500]\t Training Loss 0.0530\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [950][5500]\t Training Loss 0.0528\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [1000][5500]\t Training Loss 0.0522\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [1050][5500]\t Training Loss 0.0527\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [1100][5500]\t Training Loss 0.0527\t Accuracy 0.9674\n",
      "Epoch [12][20]\t Batch [1150][5500]\t Training Loss 0.0526\t Accuracy 0.9675\n",
      "Epoch [12][20]\t Batch [1200][5500]\t Training Loss 0.0530\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [1250][5500]\t Training Loss 0.0531\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [1300][5500]\t Training Loss 0.0536\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [1350][5500]\t Training Loss 0.0539\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [1400][5500]\t Training Loss 0.0540\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [1450][5500]\t Training Loss 0.0541\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [1500][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [1550][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [1600][5500]\t Training Loss 0.0546\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [1650][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [1700][5500]\t Training Loss 0.0543\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [1750][5500]\t Training Loss 0.0543\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [1800][5500]\t Training Loss 0.0545\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [1850][5500]\t Training Loss 0.0544\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [1900][5500]\t Training Loss 0.0541\t Accuracy 0.9674\n",
      "Epoch [12][20]\t Batch [1950][5500]\t Training Loss 0.0541\t Accuracy 0.9675\n",
      "Epoch [12][20]\t Batch [2000][5500]\t Training Loss 0.0539\t Accuracy 0.9677\n",
      "Epoch [12][20]\t Batch [2050][5500]\t Training Loss 0.0539\t Accuracy 0.9677\n",
      "Epoch [12][20]\t Batch [2100][5500]\t Training Loss 0.0541\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [2150][5500]\t Training Loss 0.0540\t Accuracy 0.9674\n",
      "Epoch [12][20]\t Batch [2200][5500]\t Training Loss 0.0539\t Accuracy 0.9674\n",
      "Epoch [12][20]\t Batch [2250][5500]\t Training Loss 0.0539\t Accuracy 0.9675\n",
      "Epoch [12][20]\t Batch [2300][5500]\t Training Loss 0.0541\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2350][5500]\t Training Loss 0.0540\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [2400][5500]\t Training Loss 0.0540\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [2450][5500]\t Training Loss 0.0540\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [2500][5500]\t Training Loss 0.0539\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2550][5500]\t Training Loss 0.0539\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [2600][5500]\t Training Loss 0.0539\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2650][5500]\t Training Loss 0.0538\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2700][5500]\t Training Loss 0.0540\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [2750][5500]\t Training Loss 0.0540\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [2800][5500]\t Training Loss 0.0539\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2850][5500]\t Training Loss 0.0539\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [2900][5500]\t Training Loss 0.0538\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [2950][5500]\t Training Loss 0.0540\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3000][5500]\t Training Loss 0.0540\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3050][5500]\t Training Loss 0.0541\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3100][5500]\t Training Loss 0.0540\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3150][5500]\t Training Loss 0.0542\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [3200][5500]\t Training Loss 0.0543\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [3250][5500]\t Training Loss 0.0544\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [3300][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [3350][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [3400][5500]\t Training Loss 0.0540\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [3450][5500]\t Training Loss 0.0540\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [3500][5500]\t Training Loss 0.0540\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [3550][5500]\t Training Loss 0.0540\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3600][5500]\t Training Loss 0.0540\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [3650][5500]\t Training Loss 0.0540\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [3700][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [3750][5500]\t Training Loss 0.0541\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3800][5500]\t Training Loss 0.0542\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [3850][5500]\t Training Loss 0.0542\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [3900][5500]\t Training Loss 0.0542\t Accuracy 0.9672\n",
      "Epoch [12][20]\t Batch [3950][5500]\t Training Loss 0.0543\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4000][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [4050][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [4100][5500]\t Training Loss 0.0542\t Accuracy 0.9671\n",
      "Epoch [12][20]\t Batch [4150][5500]\t Training Loss 0.0543\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [4200][5500]\t Training Loss 0.0543\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4250][5500]\t Training Loss 0.0544\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4300][5500]\t Training Loss 0.0544\t Accuracy 0.9665\n",
      "Epoch [12][20]\t Batch [4350][5500]\t Training Loss 0.0543\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4400][5500]\t Training Loss 0.0542\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4450][5500]\t Training Loss 0.0543\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4500][5500]\t Training Loss 0.0542\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [4550][5500]\t Training Loss 0.0543\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4600][5500]\t Training Loss 0.0543\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4650][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4700][5500]\t Training Loss 0.0544\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [4750][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4800][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4850][5500]\t Training Loss 0.0544\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [4900][5500]\t Training Loss 0.0545\t Accuracy 0.9667\n",
      "Epoch [12][20]\t Batch [4950][5500]\t Training Loss 0.0545\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [5000][5500]\t Training Loss 0.0547\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5050][5500]\t Training Loss 0.0548\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5100][5500]\t Training Loss 0.0548\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5150][5500]\t Training Loss 0.0547\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5200][5500]\t Training Loss 0.0547\t Accuracy 0.9664\n",
      "Epoch [12][20]\t Batch [5250][5500]\t Training Loss 0.0547\t Accuracy 0.9664\n",
      "Epoch [12][20]\t Batch [5300][5500]\t Training Loss 0.0547\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5350][5500]\t Training Loss 0.0547\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5400][5500]\t Training Loss 0.0547\t Accuracy 0.9663\n",
      "Epoch [12][20]\t Batch [5450][5500]\t Training Loss 0.0546\t Accuracy 0.9664\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0546\t Average training accuracy 0.9664\n",
      "Epoch [12]\t Average validation loss 0.0526\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [13][20]\t Batch [0][5500]\t Training Loss 0.0380\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][5500]\t Training Loss 0.0472\t Accuracy 0.9667\n",
      "Epoch [13][20]\t Batch [100][5500]\t Training Loss 0.0495\t Accuracy 0.9683\n",
      "Epoch [13][20]\t Batch [150][5500]\t Training Loss 0.0557\t Accuracy 0.9609\n",
      "Epoch [13][20]\t Batch [200][5500]\t Training Loss 0.0532\t Accuracy 0.9627\n",
      "Epoch [13][20]\t Batch [250][5500]\t Training Loss 0.0520\t Accuracy 0.9661\n",
      "Epoch [13][20]\t Batch [300][5500]\t Training Loss 0.0519\t Accuracy 0.9671\n",
      "Epoch [13][20]\t Batch [350][5500]\t Training Loss 0.0510\t Accuracy 0.9689\n",
      "Epoch [13][20]\t Batch [400][5500]\t Training Loss 0.0506\t Accuracy 0.9686\n",
      "Epoch [13][20]\t Batch [450][5500]\t Training Loss 0.0505\t Accuracy 0.9692\n",
      "Epoch [13][20]\t Batch [500][5500]\t Training Loss 0.0502\t Accuracy 0.9695\n",
      "Epoch [13][20]\t Batch [550][5500]\t Training Loss 0.0505\t Accuracy 0.9688\n",
      "Epoch [13][20]\t Batch [600][5500]\t Training Loss 0.0507\t Accuracy 0.9682\n",
      "Epoch [13][20]\t Batch [650][5500]\t Training Loss 0.0504\t Accuracy 0.9687\n",
      "Epoch [13][20]\t Batch [700][5500]\t Training Loss 0.0505\t Accuracy 0.9689\n",
      "Epoch [13][20]\t Batch [750][5500]\t Training Loss 0.0505\t Accuracy 0.9695\n",
      "Epoch [13][20]\t Batch [800][5500]\t Training Loss 0.0507\t Accuracy 0.9699\n",
      "Epoch [13][20]\t Batch [850][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [13][20]\t Batch [900][5500]\t Training Loss 0.0522\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [950][5500]\t Training Loss 0.0520\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [1000][5500]\t Training Loss 0.0515\t Accuracy 0.9683\n",
      "Epoch [13][20]\t Batch [1050][5500]\t Training Loss 0.0519\t Accuracy 0.9682\n",
      "Epoch [13][20]\t Batch [1100][5500]\t Training Loss 0.0519\t Accuracy 0.9686\n",
      "Epoch [13][20]\t Batch [1150][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [13][20]\t Batch [1200][5500]\t Training Loss 0.0523\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [1250][5500]\t Training Loss 0.0524\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [1300][5500]\t Training Loss 0.0528\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [1350][5500]\t Training Loss 0.0531\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [1400][5500]\t Training Loss 0.0533\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [1450][5500]\t Training Loss 0.0534\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [1500][5500]\t Training Loss 0.0538\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [1550][5500]\t Training Loss 0.0538\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [1600][5500]\t Training Loss 0.0539\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [1650][5500]\t Training Loss 0.0536\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [1700][5500]\t Training Loss 0.0536\t Accuracy 0.9680\n",
      "Epoch [13][20]\t Batch [1750][5500]\t Training Loss 0.0535\t Accuracy 0.9681\n",
      "Epoch [13][20]\t Batch [1800][5500]\t Training Loss 0.0537\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [1850][5500]\t Training Loss 0.0536\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [1900][5500]\t Training Loss 0.0534\t Accuracy 0.9681\n",
      "Epoch [13][20]\t Batch [1950][5500]\t Training Loss 0.0534\t Accuracy 0.9682\n",
      "Epoch [13][20]\t Batch [2000][5500]\t Training Loss 0.0532\t Accuracy 0.9684\n",
      "Epoch [13][20]\t Batch [2050][5500]\t Training Loss 0.0531\t Accuracy 0.9683\n",
      "Epoch [13][20]\t Batch [2100][5500]\t Training Loss 0.0534\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [2150][5500]\t Training Loss 0.0533\t Accuracy 0.9680\n",
      "Epoch [13][20]\t Batch [2200][5500]\t Training Loss 0.0531\t Accuracy 0.9681\n",
      "Epoch [13][20]\t Batch [2250][5500]\t Training Loss 0.0531\t Accuracy 0.9682\n",
      "Epoch [13][20]\t Batch [2300][5500]\t Training Loss 0.0533\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [2350][5500]\t Training Loss 0.0532\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [2400][5500]\t Training Loss 0.0533\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [2450][5500]\t Training Loss 0.0533\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [2500][5500]\t Training Loss 0.0532\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [2550][5500]\t Training Loss 0.0531\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [2600][5500]\t Training Loss 0.0531\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [2650][5500]\t Training Loss 0.0531\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [2700][5500]\t Training Loss 0.0533\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [2750][5500]\t Training Loss 0.0533\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [2800][5500]\t Training Loss 0.0531\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [2850][5500]\t Training Loss 0.0531\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [2900][5500]\t Training Loss 0.0530\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [2950][5500]\t Training Loss 0.0532\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [3000][5500]\t Training Loss 0.0533\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [3050][5500]\t Training Loss 0.0533\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [3100][5500]\t Training Loss 0.0532\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [3150][5500]\t Training Loss 0.0534\t Accuracy 0.9674\n",
      "Epoch [13][20]\t Batch [3200][5500]\t Training Loss 0.0536\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [3250][5500]\t Training Loss 0.0537\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [3300][5500]\t Training Loss 0.0535\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [3350][5500]\t Training Loss 0.0535\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [3400][5500]\t Training Loss 0.0533\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [3450][5500]\t Training Loss 0.0532\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [3500][5500]\t Training Loss 0.0532\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [3550][5500]\t Training Loss 0.0533\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [3600][5500]\t Training Loss 0.0532\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [3650][5500]\t Training Loss 0.0532\t Accuracy 0.9680\n",
      "Epoch [13][20]\t Batch [3700][5500]\t Training Loss 0.0532\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [3750][5500]\t Training Loss 0.0533\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [3800][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [3850][5500]\t Training Loss 0.0535\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [3900][5500]\t Training Loss 0.0534\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [3950][5500]\t Training Loss 0.0536\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [4000][5500]\t Training Loss 0.0536\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [4050][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4100][5500]\t Training Loss 0.0534\t Accuracy 0.9679\n",
      "Epoch [13][20]\t Batch [4150][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4200][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4250][5500]\t Training Loss 0.0536\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [4300][5500]\t Training Loss 0.0536\t Accuracy 0.9674\n",
      "Epoch [13][20]\t Batch [4350][5500]\t Training Loss 0.0535\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [4400][5500]\t Training Loss 0.0534\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [4450][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4500][5500]\t Training Loss 0.0534\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [4550][5500]\t Training Loss 0.0534\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [4600][5500]\t Training Loss 0.0535\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4650][5500]\t Training Loss 0.0536\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [4700][5500]\t Training Loss 0.0536\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4750][5500]\t Training Loss 0.0536\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [4800][5500]\t Training Loss 0.0536\t Accuracy 0.9676\n",
      "Epoch [13][20]\t Batch [4850][5500]\t Training Loss 0.0536\t Accuracy 0.9677\n",
      "Epoch [13][20]\t Batch [4900][5500]\t Training Loss 0.0537\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [4950][5500]\t Training Loss 0.0537\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [5000][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5050][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5100][5500]\t Training Loss 0.0540\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5150][5500]\t Training Loss 0.0539\t Accuracy 0.9672\n",
      "Epoch [13][20]\t Batch [5200][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5250][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5300][5500]\t Training Loss 0.0539\t Accuracy 0.9672\n",
      "Epoch [13][20]\t Batch [5350][5500]\t Training Loss 0.0539\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5400][5500]\t Training Loss 0.0538\t Accuracy 0.9673\n",
      "Epoch [13][20]\t Batch [5450][5500]\t Training Loss 0.0538\t Accuracy 0.9674\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0538\t Average training accuracy 0.9673\n",
      "Epoch [13]\t Average validation loss 0.0513\t Average validation accuracy 0.9704\n",
      "\n",
      "Epoch [14][20]\t Batch [0][5500]\t Training Loss 0.0331\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][5500]\t Training Loss 0.0457\t Accuracy 0.9667\n",
      "Epoch [14][20]\t Batch [100][5500]\t Training Loss 0.0480\t Accuracy 0.9693\n",
      "Epoch [14][20]\t Batch [150][5500]\t Training Loss 0.0545\t Accuracy 0.9616\n",
      "Epoch [14][20]\t Batch [200][5500]\t Training Loss 0.0519\t Accuracy 0.9637\n",
      "Epoch [14][20]\t Batch [250][5500]\t Training Loss 0.0506\t Accuracy 0.9673\n",
      "Epoch [14][20]\t Batch [300][5500]\t Training Loss 0.0504\t Accuracy 0.9681\n",
      "Epoch [14][20]\t Batch [350][5500]\t Training Loss 0.0495\t Accuracy 0.9704\n",
      "Epoch [14][20]\t Batch [400][5500]\t Training Loss 0.0492\t Accuracy 0.9698\n",
      "Epoch [14][20]\t Batch [450][5500]\t Training Loss 0.0491\t Accuracy 0.9705\n",
      "Epoch [14][20]\t Batch [500][5500]\t Training Loss 0.0487\t Accuracy 0.9709\n",
      "Epoch [14][20]\t Batch [550][5500]\t Training Loss 0.0491\t Accuracy 0.9701\n",
      "Epoch [14][20]\t Batch [600][5500]\t Training Loss 0.0492\t Accuracy 0.9696\n",
      "Epoch [14][20]\t Batch [650][5500]\t Training Loss 0.0490\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [700][5500]\t Training Loss 0.0491\t Accuracy 0.9702\n",
      "Epoch [14][20]\t Batch [750][5500]\t Training Loss 0.0492\t Accuracy 0.9707\n",
      "Epoch [14][20]\t Batch [800][5500]\t Training Loss 0.0494\t Accuracy 0.9709\n",
      "Epoch [14][20]\t Batch [850][5500]\t Training Loss 0.0498\t Accuracy 0.9702\n",
      "Epoch [14][20]\t Batch [900][5500]\t Training Loss 0.0509\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [950][5500]\t Training Loss 0.0508\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [1000][5500]\t Training Loss 0.0502\t Accuracy 0.9692\n",
      "Epoch [14][20]\t Batch [1050][5500]\t Training Loss 0.0507\t Accuracy 0.9691\n",
      "Epoch [14][20]\t Batch [1100][5500]\t Training Loss 0.0507\t Accuracy 0.9695\n",
      "Epoch [14][20]\t Batch [1150][5500]\t Training Loss 0.0506\t Accuracy 0.9697\n",
      "Epoch [14][20]\t Batch [1200][5500]\t Training Loss 0.0511\t Accuracy 0.9688\n",
      "Epoch [14][20]\t Batch [1250][5500]\t Training Loss 0.0512\t Accuracy 0.9688\n",
      "Epoch [14][20]\t Batch [1300][5500]\t Training Loss 0.0516\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [1350][5500]\t Training Loss 0.0519\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [1400][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [1450][5500]\t Training Loss 0.0521\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [1500][5500]\t Training Loss 0.0525\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [1550][5500]\t Training Loss 0.0525\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [1600][5500]\t Training Loss 0.0526\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [1650][5500]\t Training Loss 0.0524\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [1700][5500]\t Training Loss 0.0524\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [1750][5500]\t Training Loss 0.0523\t Accuracy 0.9688\n",
      "Epoch [14][20]\t Batch [1800][5500]\t Training Loss 0.0525\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [1850][5500]\t Training Loss 0.0524\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [1900][5500]\t Training Loss 0.0521\t Accuracy 0.9688\n",
      "Epoch [14][20]\t Batch [1950][5500]\t Training Loss 0.0522\t Accuracy 0.9689\n",
      "Epoch [14][20]\t Batch [2000][5500]\t Training Loss 0.0520\t Accuracy 0.9691\n",
      "Epoch [14][20]\t Batch [2050][5500]\t Training Loss 0.0520\t Accuracy 0.9689\n",
      "Epoch [14][20]\t Batch [2100][5500]\t Training Loss 0.0522\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2150][5500]\t Training Loss 0.0521\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [2200][5500]\t Training Loss 0.0519\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [2250][5500]\t Training Loss 0.0520\t Accuracy 0.9689\n",
      "Epoch [14][20]\t Batch [2300][5500]\t Training Loss 0.0521\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [2350][5500]\t Training Loss 0.0520\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [2400][5500]\t Training Loss 0.0521\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [2450][5500]\t Training Loss 0.0521\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2500][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2550][5500]\t Training Loss 0.0520\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [2600][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2650][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2700][5500]\t Training Loss 0.0521\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [2750][5500]\t Training Loss 0.0521\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [2800][5500]\t Training Loss 0.0520\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [2850][5500]\t Training Loss 0.0520\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [2900][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [2950][5500]\t Training Loss 0.0521\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [3000][5500]\t Training Loss 0.0522\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [3050][5500]\t Training Loss 0.0522\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [3100][5500]\t Training Loss 0.0521\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [3150][5500]\t Training Loss 0.0523\t Accuracy 0.9681\n",
      "Epoch [14][20]\t Batch [3200][5500]\t Training Loss 0.0525\t Accuracy 0.9680\n",
      "Epoch [14][20]\t Batch [3250][5500]\t Training Loss 0.0525\t Accuracy 0.9681\n",
      "Epoch [14][20]\t Batch [3300][5500]\t Training Loss 0.0524\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [3350][5500]\t Training Loss 0.0524\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [3400][5500]\t Training Loss 0.0522\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [3450][5500]\t Training Loss 0.0521\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [3500][5500]\t Training Loss 0.0522\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [3550][5500]\t Training Loss 0.0522\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [3600][5500]\t Training Loss 0.0521\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [3650][5500]\t Training Loss 0.0521\t Accuracy 0.9687\n",
      "Epoch [14][20]\t Batch [3700][5500]\t Training Loss 0.0521\t Accuracy 0.9686\n",
      "Epoch [14][20]\t Batch [3750][5500]\t Training Loss 0.0523\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [3800][5500]\t Training Loss 0.0524\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [3850][5500]\t Training Loss 0.0524\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [3900][5500]\t Training Loss 0.0524\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [3950][5500]\t Training Loss 0.0525\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4000][5500]\t Training Loss 0.0525\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4050][5500]\t Training Loss 0.0525\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4100][5500]\t Training Loss 0.0524\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [4150][5500]\t Training Loss 0.0525\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4200][5500]\t Training Loss 0.0525\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4250][5500]\t Training Loss 0.0526\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4300][5500]\t Training Loss 0.0526\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4350][5500]\t Training Loss 0.0525\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [4400][5500]\t Training Loss 0.0524\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4450][5500]\t Training Loss 0.0524\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4500][5500]\t Training Loss 0.0524\t Accuracy 0.9685\n",
      "Epoch [14][20]\t Batch [4550][5500]\t Training Loss 0.0524\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4600][5500]\t Training Loss 0.0524\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4650][5500]\t Training Loss 0.0526\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4700][5500]\t Training Loss 0.0525\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [4750][5500]\t Training Loss 0.0526\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4800][5500]\t Training Loss 0.0526\t Accuracy 0.9683\n",
      "Epoch [14][20]\t Batch [4850][5500]\t Training Loss 0.0525\t Accuracy 0.9684\n",
      "Epoch [14][20]\t Batch [4900][5500]\t Training Loss 0.0526\t Accuracy 0.9682\n",
      "Epoch [14][20]\t Batch [4950][5500]\t Training Loss 0.0526\t Accuracy 0.9681\n",
      "Epoch [14][20]\t Batch [5000][5500]\t Training Loss 0.0528\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5050][5500]\t Training Loss 0.0529\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5100][5500]\t Training Loss 0.0529\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5150][5500]\t Training Loss 0.0529\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5200][5500]\t Training Loss 0.0528\t Accuracy 0.9680\n",
      "Epoch [14][20]\t Batch [5250][5500]\t Training Loss 0.0528\t Accuracy 0.9680\n",
      "Epoch [14][20]\t Batch [5300][5500]\t Training Loss 0.0529\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5350][5500]\t Training Loss 0.0528\t Accuracy 0.9680\n",
      "Epoch [14][20]\t Batch [5400][5500]\t Training Loss 0.0528\t Accuracy 0.9679\n",
      "Epoch [14][20]\t Batch [5450][5500]\t Training Loss 0.0527\t Accuracy 0.9680\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0527\t Average training accuracy 0.9680\n",
      "Epoch [14]\t Average validation loss 0.0507\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [15][20]\t Batch [0][5500]\t Training Loss 0.0283\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][5500]\t Training Loss 0.0445\t Accuracy 0.9686\n",
      "Epoch [15][20]\t Batch [100][5500]\t Training Loss 0.0471\t Accuracy 0.9713\n",
      "Epoch [15][20]\t Batch [150][5500]\t Training Loss 0.0535\t Accuracy 0.9629\n",
      "Epoch [15][20]\t Batch [200][5500]\t Training Loss 0.0509\t Accuracy 0.9647\n",
      "Epoch [15][20]\t Batch [250][5500]\t Training Loss 0.0497\t Accuracy 0.9685\n",
      "Epoch [15][20]\t Batch [300][5500]\t Training Loss 0.0496\t Accuracy 0.9694\n",
      "Epoch [15][20]\t Batch [350][5500]\t Training Loss 0.0487\t Accuracy 0.9715\n",
      "Epoch [15][20]\t Batch [400][5500]\t Training Loss 0.0484\t Accuracy 0.9706\n",
      "Epoch [15][20]\t Batch [450][5500]\t Training Loss 0.0483\t Accuracy 0.9712\n",
      "Epoch [15][20]\t Batch [500][5500]\t Training Loss 0.0480\t Accuracy 0.9713\n",
      "Epoch [15][20]\t Batch [550][5500]\t Training Loss 0.0483\t Accuracy 0.9706\n",
      "Epoch [15][20]\t Batch [600][5500]\t Training Loss 0.0485\t Accuracy 0.9699\n",
      "Epoch [15][20]\t Batch [650][5500]\t Training Loss 0.0482\t Accuracy 0.9704\n",
      "Epoch [15][20]\t Batch [700][5500]\t Training Loss 0.0484\t Accuracy 0.9709\n",
      "Epoch [15][20]\t Batch [750][5500]\t Training Loss 0.0484\t Accuracy 0.9714\n",
      "Epoch [15][20]\t Batch [800][5500]\t Training Loss 0.0487\t Accuracy 0.9715\n",
      "Epoch [15][20]\t Batch [850][5500]\t Training Loss 0.0491\t Accuracy 0.9709\n",
      "Epoch [15][20]\t Batch [900][5500]\t Training Loss 0.0501\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [950][5500]\t Training Loss 0.0500\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [1000][5500]\t Training Loss 0.0495\t Accuracy 0.9699\n",
      "Epoch [15][20]\t Batch [1050][5500]\t Training Loss 0.0499\t Accuracy 0.9696\n",
      "Epoch [15][20]\t Batch [1100][5500]\t Training Loss 0.0499\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [1150][5500]\t Training Loss 0.0499\t Accuracy 0.9702\n",
      "Epoch [15][20]\t Batch [1200][5500]\t Training Loss 0.0503\t Accuracy 0.9696\n",
      "Epoch [15][20]\t Batch [1250][5500]\t Training Loss 0.0505\t Accuracy 0.9697\n",
      "Epoch [15][20]\t Batch [1300][5500]\t Training Loss 0.0509\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [1350][5500]\t Training Loss 0.0511\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [1400][5500]\t Training Loss 0.0512\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [1450][5500]\t Training Loss 0.0513\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [1500][5500]\t Training Loss 0.0517\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [1550][5500]\t Training Loss 0.0517\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [1600][5500]\t Training Loss 0.0517\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [1650][5500]\t Training Loss 0.0515\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [1700][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [1750][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [1800][5500]\t Training Loss 0.0516\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [1850][5500]\t Training Loss 0.0515\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [1900][5500]\t Training Loss 0.0512\t Accuracy 0.9694\n",
      "Epoch [15][20]\t Batch [1950][5500]\t Training Loss 0.0513\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [2000][5500]\t Training Loss 0.0511\t Accuracy 0.9696\n",
      "Epoch [15][20]\t Batch [2050][5500]\t Training Loss 0.0511\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [2100][5500]\t Training Loss 0.0513\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [2150][5500]\t Training Loss 0.0512\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2200][5500]\t Training Loss 0.0510\t Accuracy 0.9694\n",
      "Epoch [15][20]\t Batch [2250][5500]\t Training Loss 0.0510\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [2300][5500]\t Training Loss 0.0512\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2350][5500]\t Training Loss 0.0511\t Accuracy 0.9695\n",
      "Epoch [15][20]\t Batch [2400][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2450][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [2500][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2550][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [2600][5500]\t Training Loss 0.0510\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2650][5500]\t Training Loss 0.0510\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [2700][5500]\t Training Loss 0.0512\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [2750][5500]\t Training Loss 0.0512\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [2800][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [2850][5500]\t Training Loss 0.0510\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [2900][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [2950][5500]\t Training Loss 0.0511\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [3000][5500]\t Training Loss 0.0512\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [3050][5500]\t Training Loss 0.0512\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [3100][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3150][5500]\t Training Loss 0.0513\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [3200][5500]\t Training Loss 0.0515\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [3250][5500]\t Training Loss 0.0516\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [3300][5500]\t Training Loss 0.0514\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [3350][5500]\t Training Loss 0.0514\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [3400][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3450][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3500][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3550][5500]\t Training Loss 0.0513\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [3600][5500]\t Training Loss 0.0512\t Accuracy 0.9694\n",
      "Epoch [15][20]\t Batch [3650][5500]\t Training Loss 0.0512\t Accuracy 0.9694\n",
      "Epoch [15][20]\t Batch [3700][5500]\t Training Loss 0.0512\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [3750][5500]\t Training Loss 0.0514\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [3800][5500]\t Training Loss 0.0515\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [3850][5500]\t Training Loss 0.0515\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3900][5500]\t Training Loss 0.0515\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [3950][5500]\t Training Loss 0.0516\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [4000][5500]\t Training Loss 0.0516\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [4050][5500]\t Training Loss 0.0515\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [4100][5500]\t Training Loss 0.0514\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4150][5500]\t Training Loss 0.0515\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [4200][5500]\t Training Loss 0.0515\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [4250][5500]\t Training Loss 0.0516\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [4300][5500]\t Training Loss 0.0517\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [4350][5500]\t Training Loss 0.0516\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [4400][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4450][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4500][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4550][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4600][5500]\t Training Loss 0.0515\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4650][5500]\t Training Loss 0.0516\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [4700][5500]\t Training Loss 0.0516\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [4750][5500]\t Training Loss 0.0517\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [4800][5500]\t Training Loss 0.0517\t Accuracy 0.9692\n",
      "Epoch [15][20]\t Batch [4850][5500]\t Training Loss 0.0516\t Accuracy 0.9693\n",
      "Epoch [15][20]\t Batch [4900][5500]\t Training Loss 0.0517\t Accuracy 0.9691\n",
      "Epoch [15][20]\t Batch [4950][5500]\t Training Loss 0.0517\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [5000][5500]\t Training Loss 0.0519\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [5050][5500]\t Training Loss 0.0520\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [5100][5500]\t Training Loss 0.0520\t Accuracy 0.9688\n",
      "Epoch [15][20]\t Batch [5150][5500]\t Training Loss 0.0520\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [5200][5500]\t Training Loss 0.0519\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [5250][5500]\t Training Loss 0.0519\t Accuracy 0.9690\n",
      "Epoch [15][20]\t Batch [5300][5500]\t Training Loss 0.0520\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [5350][5500]\t Training Loss 0.0519\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [5400][5500]\t Training Loss 0.0519\t Accuracy 0.9689\n",
      "Epoch [15][20]\t Batch [5450][5500]\t Training Loss 0.0518\t Accuracy 0.9690\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0518\t Average training accuracy 0.9690\n",
      "Epoch [15]\t Average validation loss 0.0504\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [16][20]\t Batch [0][5500]\t Training Loss 0.0278\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][5500]\t Training Loss 0.0439\t Accuracy 0.9745\n",
      "Epoch [16][20]\t Batch [100][5500]\t Training Loss 0.0462\t Accuracy 0.9743\n",
      "Epoch [16][20]\t Batch [150][5500]\t Training Loss 0.0526\t Accuracy 0.9636\n",
      "Epoch [16][20]\t Batch [200][5500]\t Training Loss 0.0500\t Accuracy 0.9657\n",
      "Epoch [16][20]\t Batch [250][5500]\t Training Loss 0.0488\t Accuracy 0.9693\n",
      "Epoch [16][20]\t Batch [300][5500]\t Training Loss 0.0487\t Accuracy 0.9694\n",
      "Epoch [16][20]\t Batch [350][5500]\t Training Loss 0.0477\t Accuracy 0.9715\n",
      "Epoch [16][20]\t Batch [400][5500]\t Training Loss 0.0474\t Accuracy 0.9716\n",
      "Epoch [16][20]\t Batch [450][5500]\t Training Loss 0.0474\t Accuracy 0.9718\n",
      "Epoch [16][20]\t Batch [500][5500]\t Training Loss 0.0471\t Accuracy 0.9723\n",
      "Epoch [16][20]\t Batch [550][5500]\t Training Loss 0.0474\t Accuracy 0.9715\n",
      "Epoch [16][20]\t Batch [600][5500]\t Training Loss 0.0476\t Accuracy 0.9709\n",
      "Epoch [16][20]\t Batch [650][5500]\t Training Loss 0.0474\t Accuracy 0.9714\n",
      "Epoch [16][20]\t Batch [700][5500]\t Training Loss 0.0475\t Accuracy 0.9720\n",
      "Epoch [16][20]\t Batch [750][5500]\t Training Loss 0.0475\t Accuracy 0.9723\n",
      "Epoch [16][20]\t Batch [800][5500]\t Training Loss 0.0477\t Accuracy 0.9725\n",
      "Epoch [16][20]\t Batch [850][5500]\t Training Loss 0.0481\t Accuracy 0.9719\n",
      "Epoch [16][20]\t Batch [900][5500]\t Training Loss 0.0492\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [950][5500]\t Training Loss 0.0491\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [1000][5500]\t Training Loss 0.0486\t Accuracy 0.9711\n",
      "Epoch [16][20]\t Batch [1050][5500]\t Training Loss 0.0490\t Accuracy 0.9708\n",
      "Epoch [16][20]\t Batch [1100][5500]\t Training Loss 0.0490\t Accuracy 0.9710\n",
      "Epoch [16][20]\t Batch [1150][5500]\t Training Loss 0.0489\t Accuracy 0.9712\n",
      "Epoch [16][20]\t Batch [1200][5500]\t Training Loss 0.0494\t Accuracy 0.9706\n",
      "Epoch [16][20]\t Batch [1250][5500]\t Training Loss 0.0496\t Accuracy 0.9707\n",
      "Epoch [16][20]\t Batch [1300][5500]\t Training Loss 0.0500\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [1350][5500]\t Training Loss 0.0503\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [1400][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [1450][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [1500][5500]\t Training Loss 0.0508\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [1550][5500]\t Training Loss 0.0508\t Accuracy 0.9699\n",
      "Epoch [16][20]\t Batch [1600][5500]\t Training Loss 0.0509\t Accuracy 0.9699\n",
      "Epoch [16][20]\t Batch [1650][5500]\t Training Loss 0.0506\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [1700][5500]\t Training Loss 0.0507\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [1750][5500]\t Training Loss 0.0506\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [1800][5500]\t Training Loss 0.0508\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [1850][5500]\t Training Loss 0.0506\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [1900][5500]\t Training Loss 0.0504\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [1950][5500]\t Training Loss 0.0504\t Accuracy 0.9706\n",
      "Epoch [16][20]\t Batch [2000][5500]\t Training Loss 0.0502\t Accuracy 0.9708\n",
      "Epoch [16][20]\t Batch [2050][5500]\t Training Loss 0.0502\t Accuracy 0.9706\n",
      "Epoch [16][20]\t Batch [2100][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [2150][5500]\t Training Loss 0.0503\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [2200][5500]\t Training Loss 0.0501\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [2250][5500]\t Training Loss 0.0501\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [2300][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [2350][5500]\t Training Loss 0.0502\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [2400][5500]\t Training Loss 0.0502\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [2450][5500]\t Training Loss 0.0503\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [2500][5500]\t Training Loss 0.0502\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [2550][5500]\t Training Loss 0.0502\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [2600][5500]\t Training Loss 0.0502\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [2650][5500]\t Training Loss 0.0501\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [2700][5500]\t Training Loss 0.0503\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [2750][5500]\t Training Loss 0.0503\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [2800][5500]\t Training Loss 0.0502\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [2850][5500]\t Training Loss 0.0501\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [2900][5500]\t Training Loss 0.0501\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [2950][5500]\t Training Loss 0.0502\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [3000][5500]\t Training Loss 0.0503\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [3050][5500]\t Training Loss 0.0503\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [3100][5500]\t Training Loss 0.0503\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [3150][5500]\t Training Loss 0.0504\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [3200][5500]\t Training Loss 0.0506\t Accuracy 0.9699\n",
      "Epoch [16][20]\t Batch [3250][5500]\t Training Loss 0.0506\t Accuracy 0.9699\n",
      "Epoch [16][20]\t Batch [3300][5500]\t Training Loss 0.0505\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [3350][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [3400][5500]\t Training Loss 0.0503\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [3450][5500]\t Training Loss 0.0502\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [3500][5500]\t Training Loss 0.0503\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [3550][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [3600][5500]\t Training Loss 0.0503\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [3650][5500]\t Training Loss 0.0503\t Accuracy 0.9706\n",
      "Epoch [16][20]\t Batch [3700][5500]\t Training Loss 0.0503\t Accuracy 0.9705\n",
      "Epoch [16][20]\t Batch [3750][5500]\t Training Loss 0.0505\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [3800][5500]\t Training Loss 0.0506\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [3850][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [3900][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [3950][5500]\t Training Loss 0.0507\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [4000][5500]\t Training Loss 0.0507\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [4050][5500]\t Training Loss 0.0507\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [4100][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [4150][5500]\t Training Loss 0.0507\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [4200][5500]\t Training Loss 0.0507\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [4250][5500]\t Training Loss 0.0508\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [4300][5500]\t Training Loss 0.0508\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [4350][5500]\t Training Loss 0.0507\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [4400][5500]\t Training Loss 0.0506\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [4450][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [4500][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [4550][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [4600][5500]\t Training Loss 0.0507\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [4650][5500]\t Training Loss 0.0508\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [4700][5500]\t Training Loss 0.0507\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [4750][5500]\t Training Loss 0.0508\t Accuracy 0.9702\n",
      "Epoch [16][20]\t Batch [4800][5500]\t Training Loss 0.0508\t Accuracy 0.9703\n",
      "Epoch [16][20]\t Batch [4850][5500]\t Training Loss 0.0507\t Accuracy 0.9704\n",
      "Epoch [16][20]\t Batch [4900][5500]\t Training Loss 0.0508\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [4950][5500]\t Training Loss 0.0509\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [5000][5500]\t Training Loss 0.0510\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5050][5500]\t Training Loss 0.0511\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5100][5500]\t Training Loss 0.0511\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5150][5500]\t Training Loss 0.0511\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5200][5500]\t Training Loss 0.0510\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [5250][5500]\t Training Loss 0.0510\t Accuracy 0.9701\n",
      "Epoch [16][20]\t Batch [5300][5500]\t Training Loss 0.0511\t Accuracy 0.9699\n",
      "Epoch [16][20]\t Batch [5350][5500]\t Training Loss 0.0510\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5400][5500]\t Training Loss 0.0510\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [5450][5500]\t Training Loss 0.0509\t Accuracy 0.9701\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0510\t Average training accuracy 0.9701\n",
      "Epoch [16]\t Average validation loss 0.0502\t Average validation accuracy 0.9728\n",
      "\n",
      "Epoch [17][20]\t Batch [0][5500]\t Training Loss 0.0297\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][5500]\t Training Loss 0.0435\t Accuracy 0.9745\n",
      "Epoch [17][20]\t Batch [100][5500]\t Training Loss 0.0454\t Accuracy 0.9752\n",
      "Epoch [17][20]\t Batch [150][5500]\t Training Loss 0.0517\t Accuracy 0.9649\n",
      "Epoch [17][20]\t Batch [200][5500]\t Training Loss 0.0492\t Accuracy 0.9657\n",
      "Epoch [17][20]\t Batch [250][5500]\t Training Loss 0.0481\t Accuracy 0.9689\n",
      "Epoch [17][20]\t Batch [300][5500]\t Training Loss 0.0479\t Accuracy 0.9698\n",
      "Epoch [17][20]\t Batch [350][5500]\t Training Loss 0.0470\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [400][5500]\t Training Loss 0.0467\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [450][5500]\t Training Loss 0.0467\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [500][5500]\t Training Loss 0.0464\t Accuracy 0.9723\n",
      "Epoch [17][20]\t Batch [550][5500]\t Training Loss 0.0467\t Accuracy 0.9719\n",
      "Epoch [17][20]\t Batch [600][5500]\t Training Loss 0.0469\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [650][5500]\t Training Loss 0.0467\t Accuracy 0.9719\n",
      "Epoch [17][20]\t Batch [700][5500]\t Training Loss 0.0469\t Accuracy 0.9725\n",
      "Epoch [17][20]\t Batch [750][5500]\t Training Loss 0.0469\t Accuracy 0.9727\n",
      "Epoch [17][20]\t Batch [800][5500]\t Training Loss 0.0471\t Accuracy 0.9732\n",
      "Epoch [17][20]\t Batch [850][5500]\t Training Loss 0.0474\t Accuracy 0.9726\n",
      "Epoch [17][20]\t Batch [900][5500]\t Training Loss 0.0484\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [950][5500]\t Training Loss 0.0483\t Accuracy 0.9709\n",
      "Epoch [17][20]\t Batch [1000][5500]\t Training Loss 0.0478\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [1050][5500]\t Training Loss 0.0482\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [1100][5500]\t Training Loss 0.0483\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [1150][5500]\t Training Loss 0.0482\t Accuracy 0.9721\n",
      "Epoch [17][20]\t Batch [1200][5500]\t Training Loss 0.0487\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [1250][5500]\t Training Loss 0.0489\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [1300][5500]\t Training Loss 0.0493\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [1350][5500]\t Training Loss 0.0495\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [1400][5500]\t Training Loss 0.0496\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [1450][5500]\t Training Loss 0.0497\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [1500][5500]\t Training Loss 0.0500\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [1550][5500]\t Training Loss 0.0500\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [1600][5500]\t Training Loss 0.0501\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [1650][5500]\t Training Loss 0.0499\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [1700][5500]\t Training Loss 0.0499\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [1750][5500]\t Training Loss 0.0498\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [1800][5500]\t Training Loss 0.0500\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [1850][5500]\t Training Loss 0.0499\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [1900][5500]\t Training Loss 0.0496\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [1950][5500]\t Training Loss 0.0496\t Accuracy 0.9720\n",
      "Epoch [17][20]\t Batch [2000][5500]\t Training Loss 0.0494\t Accuracy 0.9722\n",
      "Epoch [17][20]\t Batch [2050][5500]\t Training Loss 0.0494\t Accuracy 0.9720\n",
      "Epoch [17][20]\t Batch [2100][5500]\t Training Loss 0.0496\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [2150][5500]\t Training Loss 0.0496\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [2200][5500]\t Training Loss 0.0494\t Accuracy 0.9719\n",
      "Epoch [17][20]\t Batch [2250][5500]\t Training Loss 0.0494\t Accuracy 0.9719\n",
      "Epoch [17][20]\t Batch [2300][5500]\t Training Loss 0.0495\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [2350][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [2400][5500]\t Training Loss 0.0495\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [2450][5500]\t Training Loss 0.0495\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [2500][5500]\t Training Loss 0.0495\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [2550][5500]\t Training Loss 0.0494\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [2600][5500]\t Training Loss 0.0494\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [2650][5500]\t Training Loss 0.0493\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [2700][5500]\t Training Loss 0.0495\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [2750][5500]\t Training Loss 0.0495\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [2800][5500]\t Training Loss 0.0494\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [2850][5500]\t Training Loss 0.0494\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [2900][5500]\t Training Loss 0.0493\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [2950][5500]\t Training Loss 0.0494\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [3000][5500]\t Training Loss 0.0495\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [3050][5500]\t Training Loss 0.0495\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [3100][5500]\t Training Loss 0.0495\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [3150][5500]\t Training Loss 0.0496\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [3200][5500]\t Training Loss 0.0498\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [3250][5500]\t Training Loss 0.0498\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [3300][5500]\t Training Loss 0.0497\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [3350][5500]\t Training Loss 0.0497\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [3400][5500]\t Training Loss 0.0495\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [3450][5500]\t Training Loss 0.0495\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [3500][5500]\t Training Loss 0.0495\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [3550][5500]\t Training Loss 0.0496\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [3600][5500]\t Training Loss 0.0495\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [3650][5500]\t Training Loss 0.0495\t Accuracy 0.9718\n",
      "Epoch [17][20]\t Batch [3700][5500]\t Training Loss 0.0495\t Accuracy 0.9717\n",
      "Epoch [17][20]\t Batch [3750][5500]\t Training Loss 0.0497\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [3800][5500]\t Training Loss 0.0498\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [3850][5500]\t Training Loss 0.0498\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [3900][5500]\t Training Loss 0.0498\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [3950][5500]\t Training Loss 0.0499\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [4000][5500]\t Training Loss 0.0499\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [4050][5500]\t Training Loss 0.0499\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4100][5500]\t Training Loss 0.0498\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [4150][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4200][5500]\t Training Loss 0.0499\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4250][5500]\t Training Loss 0.0500\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [4300][5500]\t Training Loss 0.0500\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [4350][5500]\t Training Loss 0.0499\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4400][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4450][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4500][5500]\t Training Loss 0.0498\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4550][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4600][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4650][5500]\t Training Loss 0.0500\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4700][5500]\t Training Loss 0.0499\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4750][5500]\t Training Loss 0.0500\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4800][5500]\t Training Loss 0.0500\t Accuracy 0.9714\n",
      "Epoch [17][20]\t Batch [4850][5500]\t Training Loss 0.0500\t Accuracy 0.9715\n",
      "Epoch [17][20]\t Batch [4900][5500]\t Training Loss 0.0501\t Accuracy 0.9713\n",
      "Epoch [17][20]\t Batch [4950][5500]\t Training Loss 0.0501\t Accuracy 0.9712\n",
      "Epoch [17][20]\t Batch [5000][5500]\t Training Loss 0.0502\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [5050][5500]\t Training Loss 0.0503\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [5100][5500]\t Training Loss 0.0503\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [5150][5500]\t Training Loss 0.0503\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [5200][5500]\t Training Loss 0.0502\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [5250][5500]\t Training Loss 0.0502\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [5300][5500]\t Training Loss 0.0503\t Accuracy 0.9710\n",
      "Epoch [17][20]\t Batch [5350][5500]\t Training Loss 0.0502\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [5400][5500]\t Training Loss 0.0502\t Accuracy 0.9711\n",
      "Epoch [17][20]\t Batch [5450][5500]\t Training Loss 0.0502\t Accuracy 0.9711\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0502\t Average training accuracy 0.9711\n",
      "Epoch [17]\t Average validation loss 0.0493\t Average validation accuracy 0.9734\n",
      "\n",
      "Epoch [18][20]\t Batch [0][5500]\t Training Loss 0.0275\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][5500]\t Training Loss 0.0426\t Accuracy 0.9745\n",
      "Epoch [18][20]\t Batch [100][5500]\t Training Loss 0.0447\t Accuracy 0.9752\n",
      "Epoch [18][20]\t Batch [150][5500]\t Training Loss 0.0509\t Accuracy 0.9662\n",
      "Epoch [18][20]\t Batch [200][5500]\t Training Loss 0.0484\t Accuracy 0.9672\n",
      "Epoch [18][20]\t Batch [250][5500]\t Training Loss 0.0473\t Accuracy 0.9701\n",
      "Epoch [18][20]\t Batch [300][5500]\t Training Loss 0.0471\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [350][5500]\t Training Loss 0.0462\t Accuracy 0.9735\n",
      "Epoch [18][20]\t Batch [400][5500]\t Training Loss 0.0460\t Accuracy 0.9731\n",
      "Epoch [18][20]\t Batch [450][5500]\t Training Loss 0.0460\t Accuracy 0.9734\n",
      "Epoch [18][20]\t Batch [500][5500]\t Training Loss 0.0457\t Accuracy 0.9741\n",
      "Epoch [18][20]\t Batch [550][5500]\t Training Loss 0.0460\t Accuracy 0.9731\n",
      "Epoch [18][20]\t Batch [600][5500]\t Training Loss 0.0462\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [650][5500]\t Training Loss 0.0460\t Accuracy 0.9728\n",
      "Epoch [18][20]\t Batch [700][5500]\t Training Loss 0.0462\t Accuracy 0.9733\n",
      "Epoch [18][20]\t Batch [750][5500]\t Training Loss 0.0462\t Accuracy 0.9735\n",
      "Epoch [18][20]\t Batch [800][5500]\t Training Loss 0.0464\t Accuracy 0.9740\n",
      "Epoch [18][20]\t Batch [850][5500]\t Training Loss 0.0467\t Accuracy 0.9733\n",
      "Epoch [18][20]\t Batch [900][5500]\t Training Loss 0.0478\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [950][5500]\t Training Loss 0.0477\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [1000][5500]\t Training Loss 0.0472\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [1050][5500]\t Training Loss 0.0476\t Accuracy 0.9723\n",
      "Epoch [18][20]\t Batch [1100][5500]\t Training Loss 0.0476\t Accuracy 0.9726\n",
      "Epoch [18][20]\t Batch [1150][5500]\t Training Loss 0.0476\t Accuracy 0.9729\n",
      "Epoch [18][20]\t Batch [1200][5500]\t Training Loss 0.0481\t Accuracy 0.9723\n",
      "Epoch [18][20]\t Batch [1250][5500]\t Training Loss 0.0483\t Accuracy 0.9723\n",
      "Epoch [18][20]\t Batch [1300][5500]\t Training Loss 0.0487\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [1350][5500]\t Training Loss 0.0489\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [1400][5500]\t Training Loss 0.0490\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [1450][5500]\t Training Loss 0.0491\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [1500][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [1550][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [1600][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [1650][5500]\t Training Loss 0.0492\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [1700][5500]\t Training Loss 0.0492\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [1750][5500]\t Training Loss 0.0491\t Accuracy 0.9726\n",
      "Epoch [18][20]\t Batch [1800][5500]\t Training Loss 0.0493\t Accuracy 0.9723\n",
      "Epoch [18][20]\t Batch [1850][5500]\t Training Loss 0.0492\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [1900][5500]\t Training Loss 0.0490\t Accuracy 0.9725\n",
      "Epoch [18][20]\t Batch [1950][5500]\t Training Loss 0.0490\t Accuracy 0.9725\n",
      "Epoch [18][20]\t Batch [2000][5500]\t Training Loss 0.0488\t Accuracy 0.9727\n",
      "Epoch [18][20]\t Batch [2050][5500]\t Training Loss 0.0488\t Accuracy 0.9726\n",
      "Epoch [18][20]\t Batch [2100][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [2150][5500]\t Training Loss 0.0489\t Accuracy 0.9723\n",
      "Epoch [18][20]\t Batch [2200][5500]\t Training Loss 0.0488\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [2250][5500]\t Training Loss 0.0488\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [2300][5500]\t Training Loss 0.0489\t Accuracy 0.9722\n",
      "Epoch [18][20]\t Batch [2350][5500]\t Training Loss 0.0488\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [2400][5500]\t Training Loss 0.0488\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [2450][5500]\t Training Loss 0.0488\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [2500][5500]\t Training Loss 0.0488\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [2550][5500]\t Training Loss 0.0488\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [2600][5500]\t Training Loss 0.0488\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [2650][5500]\t Training Loss 0.0487\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [2700][5500]\t Training Loss 0.0489\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [2750][5500]\t Training Loss 0.0489\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [2800][5500]\t Training Loss 0.0488\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [2850][5500]\t Training Loss 0.0488\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [2900][5500]\t Training Loss 0.0487\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [2950][5500]\t Training Loss 0.0488\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [3000][5500]\t Training Loss 0.0489\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [3050][5500]\t Training Loss 0.0489\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [3100][5500]\t Training Loss 0.0489\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [3150][5500]\t Training Loss 0.0490\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [3200][5500]\t Training Loss 0.0491\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [3250][5500]\t Training Loss 0.0492\t Accuracy 0.9715\n",
      "Epoch [18][20]\t Batch [3300][5500]\t Training Loss 0.0491\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [3350][5500]\t Training Loss 0.0491\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [3400][5500]\t Training Loss 0.0489\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [3450][5500]\t Training Loss 0.0488\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [3500][5500]\t Training Loss 0.0489\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [3550][5500]\t Training Loss 0.0490\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [3600][5500]\t Training Loss 0.0489\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [3650][5500]\t Training Loss 0.0489\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [3700][5500]\t Training Loss 0.0489\t Accuracy 0.9721\n",
      "Epoch [18][20]\t Batch [3750][5500]\t Training Loss 0.0491\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [3800][5500]\t Training Loss 0.0492\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [3850][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [3900][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [3950][5500]\t Training Loss 0.0493\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [4000][5500]\t Training Loss 0.0493\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [4050][5500]\t Training Loss 0.0492\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4100][5500]\t Training Loss 0.0492\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [4150][5500]\t Training Loss 0.0492\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4200][5500]\t Training Loss 0.0492\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4250][5500]\t Training Loss 0.0493\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [4300][5500]\t Training Loss 0.0494\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [4350][5500]\t Training Loss 0.0493\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4400][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4450][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4500][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4550][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4600][5500]\t Training Loss 0.0492\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4650][5500]\t Training Loss 0.0494\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [4700][5500]\t Training Loss 0.0493\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4750][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4800][5500]\t Training Loss 0.0494\t Accuracy 0.9719\n",
      "Epoch [18][20]\t Batch [4850][5500]\t Training Loss 0.0493\t Accuracy 0.9720\n",
      "Epoch [18][20]\t Batch [4900][5500]\t Training Loss 0.0494\t Accuracy 0.9718\n",
      "Epoch [18][20]\t Batch [4950][5500]\t Training Loss 0.0494\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [5000][5500]\t Training Loss 0.0496\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [5050][5500]\t Training Loss 0.0497\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [5100][5500]\t Training Loss 0.0497\t Accuracy 0.9715\n",
      "Epoch [18][20]\t Batch [5150][5500]\t Training Loss 0.0496\t Accuracy 0.9715\n",
      "Epoch [18][20]\t Batch [5200][5500]\t Training Loss 0.0496\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [5250][5500]\t Training Loss 0.0496\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [5300][5500]\t Training Loss 0.0497\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [5350][5500]\t Training Loss 0.0496\t Accuracy 0.9717\n",
      "Epoch [18][20]\t Batch [5400][5500]\t Training Loss 0.0496\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [5450][5500]\t Training Loss 0.0495\t Accuracy 0.9717\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0495\t Average training accuracy 0.9717\n",
      "Epoch [18]\t Average validation loss 0.0490\t Average validation accuracy 0.9736\n",
      "\n",
      "Epoch [19][20]\t Batch [0][5500]\t Training Loss 0.0268\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][5500]\t Training Loss 0.0419\t Accuracy 0.9745\n",
      "Epoch [19][20]\t Batch [100][5500]\t Training Loss 0.0440\t Accuracy 0.9762\n",
      "Epoch [19][20]\t Batch [150][5500]\t Training Loss 0.0500\t Accuracy 0.9675\n",
      "Epoch [19][20]\t Batch [200][5500]\t Training Loss 0.0476\t Accuracy 0.9682\n",
      "Epoch [19][20]\t Batch [250][5500]\t Training Loss 0.0466\t Accuracy 0.9709\n",
      "Epoch [19][20]\t Batch [300][5500]\t Training Loss 0.0463\t Accuracy 0.9721\n",
      "Epoch [19][20]\t Batch [350][5500]\t Training Loss 0.0455\t Accuracy 0.9738\n",
      "Epoch [19][20]\t Batch [400][5500]\t Training Loss 0.0452\t Accuracy 0.9733\n",
      "Epoch [19][20]\t Batch [450][5500]\t Training Loss 0.0453\t Accuracy 0.9736\n",
      "Epoch [19][20]\t Batch [500][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [19][20]\t Batch [550][5500]\t Training Loss 0.0454\t Accuracy 0.9735\n",
      "Epoch [19][20]\t Batch [600][5500]\t Training Loss 0.0456\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [650][5500]\t Training Loss 0.0455\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [700][5500]\t Training Loss 0.0456\t Accuracy 0.9739\n",
      "Epoch [19][20]\t Batch [750][5500]\t Training Loss 0.0457\t Accuracy 0.9742\n",
      "Epoch [19][20]\t Batch [800][5500]\t Training Loss 0.0458\t Accuracy 0.9747\n",
      "Epoch [19][20]\t Batch [850][5500]\t Training Loss 0.0461\t Accuracy 0.9741\n",
      "Epoch [19][20]\t Batch [900][5500]\t Training Loss 0.0471\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [950][5500]\t Training Loss 0.0470\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [1000][5500]\t Training Loss 0.0466\t Accuracy 0.9734\n",
      "Epoch [19][20]\t Batch [1050][5500]\t Training Loss 0.0470\t Accuracy 0.9732\n",
      "Epoch [19][20]\t Batch [1100][5500]\t Training Loss 0.0470\t Accuracy 0.9734\n",
      "Epoch [19][20]\t Batch [1150][5500]\t Training Loss 0.0469\t Accuracy 0.9736\n",
      "Epoch [19][20]\t Batch [1200][5500]\t Training Loss 0.0475\t Accuracy 0.9730\n",
      "Epoch [19][20]\t Batch [1250][5500]\t Training Loss 0.0476\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [1300][5500]\t Training Loss 0.0480\t Accuracy 0.9729\n",
      "Epoch [19][20]\t Batch [1350][5500]\t Training Loss 0.0482\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [1400][5500]\t Training Loss 0.0483\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [1450][5500]\t Training Loss 0.0484\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [1500][5500]\t Training Loss 0.0487\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [1550][5500]\t Training Loss 0.0487\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [1600][5500]\t Training Loss 0.0488\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [1650][5500]\t Training Loss 0.0485\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [1700][5500]\t Training Loss 0.0486\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [1750][5500]\t Training Loss 0.0485\t Accuracy 0.9732\n",
      "Epoch [19][20]\t Batch [1800][5500]\t Training Loss 0.0487\t Accuracy 0.9730\n",
      "Epoch [19][20]\t Batch [1850][5500]\t Training Loss 0.0485\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [1900][5500]\t Training Loss 0.0483\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [1950][5500]\t Training Loss 0.0483\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [2000][5500]\t Training Loss 0.0481\t Accuracy 0.9733\n",
      "Epoch [19][20]\t Batch [2050][5500]\t Training Loss 0.0481\t Accuracy 0.9732\n",
      "Epoch [19][20]\t Batch [2100][5500]\t Training Loss 0.0483\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [2150][5500]\t Training Loss 0.0483\t Accuracy 0.9729\n",
      "Epoch [19][20]\t Batch [2200][5500]\t Training Loss 0.0481\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [2250][5500]\t Training Loss 0.0481\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [2300][5500]\t Training Loss 0.0483\t Accuracy 0.9729\n",
      "Epoch [19][20]\t Batch [2350][5500]\t Training Loss 0.0481\t Accuracy 0.9731\n",
      "Epoch [19][20]\t Batch [2400][5500]\t Training Loss 0.0482\t Accuracy 0.9729\n",
      "Epoch [19][20]\t Batch [2450][5500]\t Training Loss 0.0482\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [2500][5500]\t Training Loss 0.0482\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [2550][5500]\t Training Loss 0.0481\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [2600][5500]\t Training Loss 0.0481\t Accuracy 0.9729\n",
      "Epoch [19][20]\t Batch [2650][5500]\t Training Loss 0.0481\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [2700][5500]\t Training Loss 0.0483\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [2750][5500]\t Training Loss 0.0483\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [2800][5500]\t Training Loss 0.0482\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [2850][5500]\t Training Loss 0.0481\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [2900][5500]\t Training Loss 0.0481\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [2950][5500]\t Training Loss 0.0482\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3000][5500]\t Training Loss 0.0483\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3050][5500]\t Training Loss 0.0483\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3100][5500]\t Training Loss 0.0482\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [3150][5500]\t Training Loss 0.0484\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [3200][5500]\t Training Loss 0.0485\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [3250][5500]\t Training Loss 0.0486\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [3300][5500]\t Training Loss 0.0485\t Accuracy 0.9723\n",
      "Epoch [19][20]\t Batch [3350][5500]\t Training Loss 0.0484\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [3400][5500]\t Training Loss 0.0483\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [3450][5500]\t Training Loss 0.0482\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [3500][5500]\t Training Loss 0.0483\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [3550][5500]\t Training Loss 0.0483\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [3600][5500]\t Training Loss 0.0483\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [3650][5500]\t Training Loss 0.0483\t Accuracy 0.9728\n",
      "Epoch [19][20]\t Batch [3700][5500]\t Training Loss 0.0483\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [3750][5500]\t Training Loss 0.0485\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3800][5500]\t Training Loss 0.0486\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3850][5500]\t Training Loss 0.0486\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [3900][5500]\t Training Loss 0.0486\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [3950][5500]\t Training Loss 0.0487\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [4000][5500]\t Training Loss 0.0487\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [4050][5500]\t Training Loss 0.0486\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [4100][5500]\t Training Loss 0.0485\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4150][5500]\t Training Loss 0.0486\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4200][5500]\t Training Loss 0.0486\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4250][5500]\t Training Loss 0.0487\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [4300][5500]\t Training Loss 0.0488\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [4350][5500]\t Training Loss 0.0487\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4400][5500]\t Training Loss 0.0486\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4450][5500]\t Training Loss 0.0486\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4500][5500]\t Training Loss 0.0486\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4550][5500]\t Training Loss 0.0486\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4600][5500]\t Training Loss 0.0486\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4650][5500]\t Training Loss 0.0487\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [4700][5500]\t Training Loss 0.0487\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4750][5500]\t Training Loss 0.0487\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [4800][5500]\t Training Loss 0.0488\t Accuracy 0.9726\n",
      "Epoch [19][20]\t Batch [4850][5500]\t Training Loss 0.0487\t Accuracy 0.9727\n",
      "Epoch [19][20]\t Batch [4900][5500]\t Training Loss 0.0488\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [4950][5500]\t Training Loss 0.0488\t Accuracy 0.9724\n",
      "Epoch [19][20]\t Batch [5000][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5050][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5100][5500]\t Training Loss 0.0491\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5150][5500]\t Training Loss 0.0490\t Accuracy 0.9721\n",
      "Epoch [19][20]\t Batch [5200][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5250][5500]\t Training Loss 0.0490\t Accuracy 0.9723\n",
      "Epoch [19][20]\t Batch [5300][5500]\t Training Loss 0.0491\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5350][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5400][5500]\t Training Loss 0.0490\t Accuracy 0.9722\n",
      "Epoch [19][20]\t Batch [5450][5500]\t Training Loss 0.0489\t Accuracy 0.9724\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0489\t Average training accuracy 0.9723\n",
      "Epoch [19]\t Average validation loss 0.0487\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [0][20]\t Batch [0][5500]\t Training Loss 2.9670\t Accuracy 0.0000\n",
      "Epoch [0][20]\t Batch [50][5500]\t Training Loss 2.5407\t Accuracy 0.0725\n",
      "Epoch [0][20]\t Batch [100][5500]\t Training Loss 2.4562\t Accuracy 0.0713\n",
      "Epoch [0][20]\t Batch [150][5500]\t Training Loss 2.3956\t Accuracy 0.0841\n",
      "Epoch [0][20]\t Batch [200][5500]\t Training Loss 2.3494\t Accuracy 0.0970\n",
      "Epoch [0][20]\t Batch [250][5500]\t Training Loss 2.3205\t Accuracy 0.1052\n",
      "Epoch [0][20]\t Batch [300][5500]\t Training Loss 2.2945\t Accuracy 0.1282\n",
      "Epoch [0][20]\t Batch [350][5500]\t Training Loss 2.2727\t Accuracy 0.1510\n",
      "Epoch [0][20]\t Batch [400][5500]\t Training Loss 2.2530\t Accuracy 0.1731\n",
      "Epoch [0][20]\t Batch [450][5500]\t Training Loss 2.2338\t Accuracy 0.1949\n",
      "Epoch [0][20]\t Batch [500][5500]\t Training Loss 2.2158\t Accuracy 0.2168\n",
      "Epoch [0][20]\t Batch [550][5500]\t Training Loss 2.1990\t Accuracy 0.2377\n",
      "Epoch [0][20]\t Batch [600][5500]\t Training Loss 2.1830\t Accuracy 0.2601\n",
      "Epoch [0][20]\t Batch [650][5500]\t Training Loss 2.1659\t Accuracy 0.2813\n",
      "Epoch [0][20]\t Batch [700][5500]\t Training Loss 2.1526\t Accuracy 0.2970\n",
      "Epoch [0][20]\t Batch [750][5500]\t Training Loss 2.1425\t Accuracy 0.3084\n",
      "Epoch [0][20]\t Batch [800][5500]\t Training Loss 2.1296\t Accuracy 0.3226\n",
      "Epoch [0][20]\t Batch [850][5500]\t Training Loss 2.1172\t Accuracy 0.3374\n",
      "Epoch [0][20]\t Batch [900][5500]\t Training Loss 2.1045\t Accuracy 0.3504\n",
      "Epoch [0][20]\t Batch [950][5500]\t Training Loss 2.0924\t Accuracy 0.3642\n",
      "Epoch [0][20]\t Batch [1000][5500]\t Training Loss 2.0802\t Accuracy 0.3770\n",
      "Epoch [0][20]\t Batch [1050][5500]\t Training Loss 2.0676\t Accuracy 0.3891\n",
      "Epoch [0][20]\t Batch [1100][5500]\t Training Loss 2.0546\t Accuracy 0.4018\n",
      "Epoch [0][20]\t Batch [1150][5500]\t Training Loss 2.0425\t Accuracy 0.4122\n",
      "Epoch [0][20]\t Batch [1200][5500]\t Training Loss 2.0321\t Accuracy 0.4202\n",
      "Epoch [0][20]\t Batch [1250][5500]\t Training Loss 2.0217\t Accuracy 0.4302\n",
      "Epoch [0][20]\t Batch [1300][5500]\t Training Loss 2.0122\t Accuracy 0.4365\n",
      "Epoch [0][20]\t Batch [1350][5500]\t Training Loss 2.0017\t Accuracy 0.4457\n",
      "Epoch [0][20]\t Batch [1400][5500]\t Training Loss 1.9919\t Accuracy 0.4537\n",
      "Epoch [0][20]\t Batch [1450][5500]\t Training Loss 1.9827\t Accuracy 0.4603\n",
      "Epoch [0][20]\t Batch [1500][5500]\t Training Loss 1.9747\t Accuracy 0.4672\n",
      "Epoch [0][20]\t Batch [1550][5500]\t Training Loss 1.9648\t Accuracy 0.4753\n",
      "Epoch [0][20]\t Batch [1600][5500]\t Training Loss 1.9560\t Accuracy 0.4811\n",
      "Epoch [0][20]\t Batch [1650][5500]\t Training Loss 1.9459\t Accuracy 0.4887\n",
      "Epoch [0][20]\t Batch [1700][5500]\t Training Loss 1.9373\t Accuracy 0.4941\n",
      "Epoch [0][20]\t Batch [1750][5500]\t Training Loss 1.9284\t Accuracy 0.4994\n",
      "Epoch [0][20]\t Batch [1800][5500]\t Training Loss 1.9210\t Accuracy 0.5037\n",
      "Epoch [0][20]\t Batch [1850][5500]\t Training Loss 1.9116\t Accuracy 0.5096\n",
      "Epoch [0][20]\t Batch [1900][5500]\t Training Loss 1.9021\t Accuracy 0.5163\n",
      "Epoch [0][20]\t Batch [1950][5500]\t Training Loss 1.8927\t Accuracy 0.5213\n",
      "Epoch [0][20]\t Batch [2000][5500]\t Training Loss 1.8825\t Accuracy 0.5271\n",
      "Epoch [0][20]\t Batch [2050][5500]\t Training Loss 1.8736\t Accuracy 0.5317\n",
      "Epoch [0][20]\t Batch [2100][5500]\t Training Loss 1.8648\t Accuracy 0.5371\n",
      "Epoch [0][20]\t Batch [2150][5500]\t Training Loss 1.8556\t Accuracy 0.5418\n",
      "Epoch [0][20]\t Batch [2200][5500]\t Training Loss 1.8457\t Accuracy 0.5468\n",
      "Epoch [0][20]\t Batch [2250][5500]\t Training Loss 1.8383\t Accuracy 0.5510\n",
      "Epoch [0][20]\t Batch [2300][5500]\t Training Loss 1.8298\t Accuracy 0.5554\n",
      "Epoch [0][20]\t Batch [2350][5500]\t Training Loss 1.8208\t Accuracy 0.5599\n",
      "Epoch [0][20]\t Batch [2400][5500]\t Training Loss 1.8130\t Accuracy 0.5643\n",
      "Epoch [0][20]\t Batch [2450][5500]\t Training Loss 1.8045\t Accuracy 0.5682\n",
      "Epoch [0][20]\t Batch [2500][5500]\t Training Loss 1.7980\t Accuracy 0.5710\n",
      "Epoch [0][20]\t Batch [2550][5500]\t Training Loss 1.7888\t Accuracy 0.5750\n",
      "Epoch [0][20]\t Batch [2600][5500]\t Training Loss 1.7803\t Accuracy 0.5790\n",
      "Epoch [0][20]\t Batch [2650][5500]\t Training Loss 1.7727\t Accuracy 0.5821\n",
      "Epoch [0][20]\t Batch [2700][5500]\t Training Loss 1.7658\t Accuracy 0.5848\n",
      "Epoch [0][20]\t Batch [2750][5500]\t Training Loss 1.7587\t Accuracy 0.5876\n",
      "Epoch [0][20]\t Batch [2800][5500]\t Training Loss 1.7507\t Accuracy 0.5912\n",
      "Epoch [0][20]\t Batch [2850][5500]\t Training Loss 1.7421\t Accuracy 0.5950\n",
      "Epoch [0][20]\t Batch [2900][5500]\t Training Loss 1.7343\t Accuracy 0.5979\n",
      "Epoch [0][20]\t Batch [2950][5500]\t Training Loss 1.7267\t Accuracy 0.6008\n",
      "Epoch [0][20]\t Batch [3000][5500]\t Training Loss 1.7208\t Accuracy 0.6032\n",
      "Epoch [0][20]\t Batch [3050][5500]\t Training Loss 1.7149\t Accuracy 0.6056\n",
      "Epoch [0][20]\t Batch [3100][5500]\t Training Loss 1.7092\t Accuracy 0.6081\n",
      "Epoch [0][20]\t Batch [3150][5500]\t Training Loss 1.7033\t Accuracy 0.6103\n",
      "Epoch [0][20]\t Batch [3200][5500]\t Training Loss 1.6975\t Accuracy 0.6122\n",
      "Epoch [0][20]\t Batch [3250][5500]\t Training Loss 1.6923\t Accuracy 0.6140\n",
      "Epoch [0][20]\t Batch [3300][5500]\t Training Loss 1.6853\t Accuracy 0.6170\n",
      "Epoch [0][20]\t Batch [3350][5500]\t Training Loss 1.6794\t Accuracy 0.6190\n",
      "Epoch [0][20]\t Batch [3400][5500]\t Training Loss 1.6717\t Accuracy 0.6222\n",
      "Epoch [0][20]\t Batch [3450][5500]\t Training Loss 1.6651\t Accuracy 0.6250\n",
      "Epoch [0][20]\t Batch [3500][5500]\t Training Loss 1.6590\t Accuracy 0.6272\n",
      "Epoch [0][20]\t Batch [3550][5500]\t Training Loss 1.6527\t Accuracy 0.6299\n",
      "Epoch [0][20]\t Batch [3600][5500]\t Training Loss 1.6464\t Accuracy 0.6322\n",
      "Epoch [0][20]\t Batch [3650][5500]\t Training Loss 1.6402\t Accuracy 0.6342\n",
      "Epoch [0][20]\t Batch [3700][5500]\t Training Loss 1.6327\t Accuracy 0.6370\n",
      "Epoch [0][20]\t Batch [3750][5500]\t Training Loss 1.6266\t Accuracy 0.6389\n",
      "Epoch [0][20]\t Batch [3800][5500]\t Training Loss 1.6207\t Accuracy 0.6408\n",
      "Epoch [0][20]\t Batch [3850][5500]\t Training Loss 1.6146\t Accuracy 0.6429\n",
      "Epoch [0][20]\t Batch [3900][5500]\t Training Loss 1.6081\t Accuracy 0.6452\n",
      "Epoch [0][20]\t Batch [3950][5500]\t Training Loss 1.6018\t Accuracy 0.6473\n",
      "Epoch [0][20]\t Batch [4000][5500]\t Training Loss 1.5966\t Accuracy 0.6489\n",
      "Epoch [0][20]\t Batch [4050][5500]\t Training Loss 1.5903\t Accuracy 0.6509\n",
      "Epoch [0][20]\t Batch [4100][5500]\t Training Loss 1.5840\t Accuracy 0.6531\n",
      "Epoch [0][20]\t Batch [4150][5500]\t Training Loss 1.5793\t Accuracy 0.6543\n",
      "Epoch [0][20]\t Batch [4200][5500]\t Training Loss 1.5742\t Accuracy 0.6562\n",
      "Epoch [0][20]\t Batch [4250][5500]\t Training Loss 1.5695\t Accuracy 0.6573\n",
      "Epoch [0][20]\t Batch [4300][5500]\t Training Loss 1.5637\t Accuracy 0.6590\n",
      "Epoch [0][20]\t Batch [4350][5500]\t Training Loss 1.5577\t Accuracy 0.6610\n",
      "Epoch [0][20]\t Batch [4400][5500]\t Training Loss 1.5522\t Accuracy 0.6628\n",
      "Epoch [0][20]\t Batch [4450][5500]\t Training Loss 1.5475\t Accuracy 0.6641\n",
      "Epoch [0][20]\t Batch [4500][5500]\t Training Loss 1.5417\t Accuracy 0.6661\n",
      "Epoch [0][20]\t Batch [4550][5500]\t Training Loss 1.5366\t Accuracy 0.6676\n",
      "Epoch [0][20]\t Batch [4600][5500]\t Training Loss 1.5318\t Accuracy 0.6690\n",
      "Epoch [0][20]\t Batch [4650][5500]\t Training Loss 1.5272\t Accuracy 0.6702\n",
      "Epoch [0][20]\t Batch [4700][5500]\t Training Loss 1.5211\t Accuracy 0.6719\n",
      "Epoch [0][20]\t Batch [4750][5500]\t Training Loss 1.5163\t Accuracy 0.6732\n",
      "Epoch [0][20]\t Batch [4800][5500]\t Training Loss 1.5109\t Accuracy 0.6746\n",
      "Epoch [0][20]\t Batch [4850][5500]\t Training Loss 1.5046\t Accuracy 0.6766\n",
      "Epoch [0][20]\t Batch [4900][5500]\t Training Loss 1.4993\t Accuracy 0.6780\n",
      "Epoch [0][20]\t Batch [4950][5500]\t Training Loss 1.4948\t Accuracy 0.6789\n",
      "Epoch [0][20]\t Batch [5000][5500]\t Training Loss 1.4912\t Accuracy 0.6798\n",
      "Epoch [0][20]\t Batch [5050][5500]\t Training Loss 1.4866\t Accuracy 0.6811\n",
      "Epoch [0][20]\t Batch [5100][5500]\t Training Loss 1.4818\t Accuracy 0.6823\n",
      "Epoch [0][20]\t Batch [5150][5500]\t Training Loss 1.4765\t Accuracy 0.6839\n",
      "Epoch [0][20]\t Batch [5200][5500]\t Training Loss 1.4713\t Accuracy 0.6857\n",
      "Epoch [0][20]\t Batch [5250][5500]\t Training Loss 1.4668\t Accuracy 0.6868\n",
      "Epoch [0][20]\t Batch [5300][5500]\t Training Loss 1.4624\t Accuracy 0.6879\n",
      "Epoch [0][20]\t Batch [5350][5500]\t Training Loss 1.4575\t Accuracy 0.6891\n",
      "Epoch [0][20]\t Batch [5400][5500]\t Training Loss 1.4531\t Accuracy 0.6903\n",
      "Epoch [0][20]\t Batch [5450][5500]\t Training Loss 1.4483\t Accuracy 0.6916\n",
      "\n",
      "Epoch [0]\t Average training loss 1.4438\t Average training accuracy 0.6928\n",
      "Epoch [0]\t Average validation loss 0.8777\t Average validation accuracy 0.8642\n",
      "\n",
      "Epoch [1][20]\t Batch [0][5500]\t Training Loss 0.7883\t Accuracy 1.0000\n",
      "Epoch [1][20]\t Batch [50][5500]\t Training Loss 0.9059\t Accuracy 0.8353\n",
      "Epoch [1][20]\t Batch [100][5500]\t Training Loss 0.9511\t Accuracy 0.8168\n",
      "Epoch [1][20]\t Batch [150][5500]\t Training Loss 0.9588\t Accuracy 0.8126\n",
      "Epoch [1][20]\t Batch [200][5500]\t Training Loss 0.9355\t Accuracy 0.8214\n",
      "Epoch [1][20]\t Batch [250][5500]\t Training Loss 0.9224\t Accuracy 0.8223\n",
      "Epoch [1][20]\t Batch [300][5500]\t Training Loss 0.9145\t Accuracy 0.8252\n",
      "Epoch [1][20]\t Batch [350][5500]\t Training Loss 0.9173\t Accuracy 0.8279\n",
      "Epoch [1][20]\t Batch [400][5500]\t Training Loss 0.9133\t Accuracy 0.8299\n",
      "Epoch [1][20]\t Batch [450][5500]\t Training Loss 0.9104\t Accuracy 0.8315\n",
      "Epoch [1][20]\t Batch [500][5500]\t Training Loss 0.9044\t Accuracy 0.8341\n",
      "Epoch [1][20]\t Batch [550][5500]\t Training Loss 0.9004\t Accuracy 0.8338\n",
      "Epoch [1][20]\t Batch [600][5500]\t Training Loss 0.8979\t Accuracy 0.8353\n",
      "Epoch [1][20]\t Batch [650][5500]\t Training Loss 0.8900\t Accuracy 0.8366\n",
      "Epoch [1][20]\t Batch [700][5500]\t Training Loss 0.8879\t Accuracy 0.8364\n",
      "Epoch [1][20]\t Batch [750][5500]\t Training Loss 0.8948\t Accuracy 0.8352\n",
      "Epoch [1][20]\t Batch [800][5500]\t Training Loss 0.8961\t Accuracy 0.8335\n",
      "Epoch [1][20]\t Batch [850][5500]\t Training Loss 0.8974\t Accuracy 0.8325\n",
      "Epoch [1][20]\t Batch [900][5500]\t Training Loss 0.8982\t Accuracy 0.8305\n",
      "Epoch [1][20]\t Batch [950][5500]\t Training Loss 0.8947\t Accuracy 0.8313\n",
      "Epoch [1][20]\t Batch [1000][5500]\t Training Loss 0.8902\t Accuracy 0.8325\n",
      "Epoch [1][20]\t Batch [1050][5500]\t Training Loss 0.8854\t Accuracy 0.8331\n",
      "Epoch [1][20]\t Batch [1100][5500]\t Training Loss 0.8808\t Accuracy 0.8350\n",
      "Epoch [1][20]\t Batch [1150][5500]\t Training Loss 0.8770\t Accuracy 0.8356\n",
      "Epoch [1][20]\t Batch [1200][5500]\t Training Loss 0.8786\t Accuracy 0.8338\n",
      "Epoch [1][20]\t Batch [1250][5500]\t Training Loss 0.8779\t Accuracy 0.8337\n",
      "Epoch [1][20]\t Batch [1300][5500]\t Training Loss 0.8787\t Accuracy 0.8329\n",
      "Epoch [1][20]\t Batch [1350][5500]\t Training Loss 0.8783\t Accuracy 0.8324\n",
      "Epoch [1][20]\t Batch [1400][5500]\t Training Loss 0.8783\t Accuracy 0.8308\n",
      "Epoch [1][20]\t Batch [1450][5500]\t Training Loss 0.8792\t Accuracy 0.8298\n",
      "Epoch [1][20]\t Batch [1500][5500]\t Training Loss 0.8817\t Accuracy 0.8282\n",
      "Epoch [1][20]\t Batch [1550][5500]\t Training Loss 0.8789\t Accuracy 0.8297\n",
      "Epoch [1][20]\t Batch [1600][5500]\t Training Loss 0.8795\t Accuracy 0.8284\n",
      "Epoch [1][20]\t Batch [1650][5500]\t Training Loss 0.8771\t Accuracy 0.8286\n",
      "Epoch [1][20]\t Batch [1700][5500]\t Training Loss 0.8770\t Accuracy 0.8282\n",
      "Epoch [1][20]\t Batch [1750][5500]\t Training Loss 0.8765\t Accuracy 0.8277\n",
      "Epoch [1][20]\t Batch [1800][5500]\t Training Loss 0.8779\t Accuracy 0.8269\n",
      "Epoch [1][20]\t Batch [1850][5500]\t Training Loss 0.8750\t Accuracy 0.8277\n",
      "Epoch [1][20]\t Batch [1900][5500]\t Training Loss 0.8718\t Accuracy 0.8294\n",
      "Epoch [1][20]\t Batch [1950][5500]\t Training Loss 0.8701\t Accuracy 0.8296\n",
      "Epoch [1][20]\t Batch [2000][5500]\t Training Loss 0.8664\t Accuracy 0.8301\n",
      "Epoch [1][20]\t Batch [2050][5500]\t Training Loss 0.8649\t Accuracy 0.8306\n",
      "Epoch [1][20]\t Batch [2100][5500]\t Training Loss 0.8638\t Accuracy 0.8299\n",
      "Epoch [1][20]\t Batch [2150][5500]\t Training Loss 0.8609\t Accuracy 0.8310\n",
      "Epoch [1][20]\t Batch [2200][5500]\t Training Loss 0.8572\t Accuracy 0.8319\n",
      "Epoch [1][20]\t Batch [2250][5500]\t Training Loss 0.8567\t Accuracy 0.8319\n",
      "Epoch [1][20]\t Batch [2300][5500]\t Training Loss 0.8551\t Accuracy 0.8319\n",
      "Epoch [1][20]\t Batch [2350][5500]\t Training Loss 0.8527\t Accuracy 0.8322\n",
      "Epoch [1][20]\t Batch [2400][5500]\t Training Loss 0.8515\t Accuracy 0.8324\n",
      "Epoch [1][20]\t Batch [2450][5500]\t Training Loss 0.8496\t Accuracy 0.8326\n",
      "Epoch [1][20]\t Batch [2500][5500]\t Training Loss 0.8498\t Accuracy 0.8320\n",
      "Epoch [1][20]\t Batch [2550][5500]\t Training Loss 0.8467\t Accuracy 0.8326\n",
      "Epoch [1][20]\t Batch [2600][5500]\t Training Loss 0.8444\t Accuracy 0.8334\n",
      "Epoch [1][20]\t Batch [2650][5500]\t Training Loss 0.8429\t Accuracy 0.8338\n",
      "Epoch [1][20]\t Batch [2700][5500]\t Training Loss 0.8424\t Accuracy 0.8338\n",
      "Epoch [1][20]\t Batch [2750][5500]\t Training Loss 0.8414\t Accuracy 0.8337\n",
      "Epoch [1][20]\t Batch [2800][5500]\t Training Loss 0.8391\t Accuracy 0.8342\n",
      "Epoch [1][20]\t Batch [2850][5500]\t Training Loss 0.8365\t Accuracy 0.8349\n",
      "Epoch [1][20]\t Batch [2900][5500]\t Training Loss 0.8349\t Accuracy 0.8349\n",
      "Epoch [1][20]\t Batch [2950][5500]\t Training Loss 0.8335\t Accuracy 0.8346\n",
      "Epoch [1][20]\t Batch [3000][5500]\t Training Loss 0.8337\t Accuracy 0.8340\n",
      "Epoch [1][20]\t Batch [3050][5500]\t Training Loss 0.8337\t Accuracy 0.8336\n",
      "Epoch [1][20]\t Batch [3100][5500]\t Training Loss 0.8338\t Accuracy 0.8335\n",
      "Epoch [1][20]\t Batch [3150][5500]\t Training Loss 0.8338\t Accuracy 0.8332\n",
      "Epoch [1][20]\t Batch [3200][5500]\t Training Loss 0.8336\t Accuracy 0.8328\n",
      "Epoch [1][20]\t Batch [3250][5500]\t Training Loss 0.8343\t Accuracy 0.8322\n",
      "Epoch [1][20]\t Batch [3300][5500]\t Training Loss 0.8326\t Accuracy 0.8327\n",
      "Epoch [1][20]\t Batch [3350][5500]\t Training Loss 0.8322\t Accuracy 0.8326\n",
      "Epoch [1][20]\t Batch [3400][5500]\t Training Loss 0.8291\t Accuracy 0.8336\n",
      "Epoch [1][20]\t Batch [3450][5500]\t Training Loss 0.8273\t Accuracy 0.8343\n",
      "Epoch [1][20]\t Batch [3500][5500]\t Training Loss 0.8266\t Accuracy 0.8341\n",
      "Epoch [1][20]\t Batch [3550][5500]\t Training Loss 0.8249\t Accuracy 0.8345\n",
      "Epoch [1][20]\t Batch [3600][5500]\t Training Loss 0.8235\t Accuracy 0.8347\n",
      "Epoch [1][20]\t Batch [3650][5500]\t Training Loss 0.8221\t Accuracy 0.8349\n",
      "Epoch [1][20]\t Batch [3700][5500]\t Training Loss 0.8193\t Accuracy 0.8355\n",
      "Epoch [1][20]\t Batch [3750][5500]\t Training Loss 0.8187\t Accuracy 0.8352\n",
      "Epoch [1][20]\t Batch [3800][5500]\t Training Loss 0.8175\t Accuracy 0.8353\n",
      "Epoch [1][20]\t Batch [3850][5500]\t Training Loss 0.8161\t Accuracy 0.8356\n",
      "Epoch [1][20]\t Batch [3900][5500]\t Training Loss 0.8143\t Accuracy 0.8360\n",
      "Epoch [1][20]\t Batch [3950][5500]\t Training Loss 0.8129\t Accuracy 0.8364\n",
      "Epoch [1][20]\t Batch [4000][5500]\t Training Loss 0.8124\t Accuracy 0.8364\n",
      "Epoch [1][20]\t Batch [4050][5500]\t Training Loss 0.8107\t Accuracy 0.8367\n",
      "Epoch [1][20]\t Batch [4100][5500]\t Training Loss 0.8087\t Accuracy 0.8374\n",
      "Epoch [1][20]\t Batch [4150][5500]\t Training Loss 0.8086\t Accuracy 0.8371\n",
      "Epoch [1][20]\t Batch [4200][5500]\t Training Loss 0.8077\t Accuracy 0.8372\n",
      "Epoch [1][20]\t Batch [4250][5500]\t Training Loss 0.8077\t Accuracy 0.8366\n",
      "Epoch [1][20]\t Batch [4300][5500]\t Training Loss 0.8063\t Accuracy 0.8368\n",
      "Epoch [1][20]\t Batch [4350][5500]\t Training Loss 0.8044\t Accuracy 0.8372\n",
      "Epoch [1][20]\t Batch [4400][5500]\t Training Loss 0.8030\t Accuracy 0.8373\n",
      "Epoch [1][20]\t Batch [4450][5500]\t Training Loss 0.8026\t Accuracy 0.8370\n",
      "Epoch [1][20]\t Batch [4500][5500]\t Training Loss 0.8008\t Accuracy 0.8375\n",
      "Epoch [1][20]\t Batch [4550][5500]\t Training Loss 0.7999\t Accuracy 0.8375\n",
      "Epoch [1][20]\t Batch [4600][5500]\t Training Loss 0.7991\t Accuracy 0.8376\n",
      "Epoch [1][20]\t Batch [4650][5500]\t Training Loss 0.7986\t Accuracy 0.8375\n",
      "Epoch [1][20]\t Batch [4700][5500]\t Training Loss 0.7964\t Accuracy 0.8379\n",
      "Epoch [1][20]\t Batch [4750][5500]\t Training Loss 0.7955\t Accuracy 0.8380\n",
      "Epoch [1][20]\t Batch [4800][5500]\t Training Loss 0.7942\t Accuracy 0.8380\n",
      "Epoch [1][20]\t Batch [4850][5500]\t Training Loss 0.7918\t Accuracy 0.8385\n",
      "Epoch [1][20]\t Batch [4900][5500]\t Training Loss 0.7904\t Accuracy 0.8387\n",
      "Epoch [1][20]\t Batch [4950][5500]\t Training Loss 0.7898\t Accuracy 0.8384\n",
      "Epoch [1][20]\t Batch [5000][5500]\t Training Loss 0.7899\t Accuracy 0.8384\n",
      "Epoch [1][20]\t Batch [5050][5500]\t Training Loss 0.7893\t Accuracy 0.8383\n",
      "Epoch [1][20]\t Batch [5100][5500]\t Training Loss 0.7881\t Accuracy 0.8384\n",
      "Epoch [1][20]\t Batch [5150][5500]\t Training Loss 0.7864\t Accuracy 0.8386\n",
      "Epoch [1][20]\t Batch [5200][5500]\t Training Loss 0.7847\t Accuracy 0.8391\n",
      "Epoch [1][20]\t Batch [5250][5500]\t Training Loss 0.7839\t Accuracy 0.8391\n",
      "Epoch [1][20]\t Batch [5300][5500]\t Training Loss 0.7831\t Accuracy 0.8390\n",
      "Epoch [1][20]\t Batch [5350][5500]\t Training Loss 0.7818\t Accuracy 0.8392\n",
      "Epoch [1][20]\t Batch [5400][5500]\t Training Loss 0.7807\t Accuracy 0.8394\n",
      "Epoch [1][20]\t Batch [5450][5500]\t Training Loss 0.7793\t Accuracy 0.8396\n",
      "\n",
      "Epoch [1]\t Average training loss 0.7782\t Average training accuracy 0.8397\n",
      "Epoch [1]\t Average validation loss 0.5738\t Average validation accuracy 0.8942\n",
      "\n",
      "Epoch [2][20]\t Batch [0][5500]\t Training Loss 0.4241\t Accuracy 1.0000\n",
      "Epoch [2][20]\t Batch [50][5500]\t Training Loss 0.6230\t Accuracy 0.8569\n",
      "Epoch [2][20]\t Batch [100][5500]\t Training Loss 0.6669\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [150][5500]\t Training Loss 0.6759\t Accuracy 0.8430\n",
      "Epoch [2][20]\t Batch [200][5500]\t Training Loss 0.6510\t Accuracy 0.8507\n",
      "Epoch [2][20]\t Batch [250][5500]\t Training Loss 0.6384\t Accuracy 0.8542\n",
      "Epoch [2][20]\t Batch [300][5500]\t Training Loss 0.6316\t Accuracy 0.8575\n",
      "Epoch [2][20]\t Batch [350][5500]\t Training Loss 0.6332\t Accuracy 0.8604\n",
      "Epoch [2][20]\t Batch [400][5500]\t Training Loss 0.6306\t Accuracy 0.8638\n",
      "Epoch [2][20]\t Batch [450][5500]\t Training Loss 0.6288\t Accuracy 0.8652\n",
      "Epoch [2][20]\t Batch [500][5500]\t Training Loss 0.6250\t Accuracy 0.8655\n",
      "Epoch [2][20]\t Batch [550][5500]\t Training Loss 0.6237\t Accuracy 0.8655\n",
      "Epoch [2][20]\t Batch [600][5500]\t Training Loss 0.6220\t Accuracy 0.8674\n",
      "Epoch [2][20]\t Batch [650][5500]\t Training Loss 0.6160\t Accuracy 0.8688\n",
      "Epoch [2][20]\t Batch [700][5500]\t Training Loss 0.6146\t Accuracy 0.8679\n",
      "Epoch [2][20]\t Batch [750][5500]\t Training Loss 0.6212\t Accuracy 0.8670\n",
      "Epoch [2][20]\t Batch [800][5500]\t Training Loss 0.6245\t Accuracy 0.8655\n",
      "Epoch [2][20]\t Batch [850][5500]\t Training Loss 0.6275\t Accuracy 0.8646\n",
      "Epoch [2][20]\t Batch [900][5500]\t Training Loss 0.6317\t Accuracy 0.8623\n",
      "Epoch [2][20]\t Batch [950][5500]\t Training Loss 0.6295\t Accuracy 0.8638\n",
      "Epoch [2][20]\t Batch [1000][5500]\t Training Loss 0.6261\t Accuracy 0.8643\n",
      "Epoch [2][20]\t Batch [1050][5500]\t Training Loss 0.6226\t Accuracy 0.8652\n",
      "Epoch [2][20]\t Batch [1100][5500]\t Training Loss 0.6190\t Accuracy 0.8670\n",
      "Epoch [2][20]\t Batch [1150][5500]\t Training Loss 0.6163\t Accuracy 0.8673\n",
      "Epoch [2][20]\t Batch [1200][5500]\t Training Loss 0.6196\t Accuracy 0.8651\n",
      "Epoch [2][20]\t Batch [1250][5500]\t Training Loss 0.6201\t Accuracy 0.8647\n",
      "Epoch [2][20]\t Batch [1300][5500]\t Training Loss 0.6224\t Accuracy 0.8638\n",
      "Epoch [2][20]\t Batch [1350][5500]\t Training Loss 0.6236\t Accuracy 0.8632\n",
      "Epoch [2][20]\t Batch [1400][5500]\t Training Loss 0.6249\t Accuracy 0.8616\n",
      "Epoch [2][20]\t Batch [1450][5500]\t Training Loss 0.6275\t Accuracy 0.8604\n",
      "Epoch [2][20]\t Batch [1500][5500]\t Training Loss 0.6315\t Accuracy 0.8586\n",
      "Epoch [2][20]\t Batch [1550][5500]\t Training Loss 0.6295\t Accuracy 0.8596\n",
      "Epoch [2][20]\t Batch [1600][5500]\t Training Loss 0.6312\t Accuracy 0.8585\n",
      "Epoch [2][20]\t Batch [1650][5500]\t Training Loss 0.6299\t Accuracy 0.8587\n",
      "Epoch [2][20]\t Batch [1700][5500]\t Training Loss 0.6308\t Accuracy 0.8582\n",
      "Epoch [2][20]\t Batch [1750][5500]\t Training Loss 0.6312\t Accuracy 0.8577\n",
      "Epoch [2][20]\t Batch [1800][5500]\t Training Loss 0.6336\t Accuracy 0.8569\n",
      "Epoch [2][20]\t Batch [1850][5500]\t Training Loss 0.6313\t Accuracy 0.8575\n",
      "Epoch [2][20]\t Batch [1900][5500]\t Training Loss 0.6288\t Accuracy 0.8589\n",
      "Epoch [2][20]\t Batch [1950][5500]\t Training Loss 0.6282\t Accuracy 0.8588\n",
      "Epoch [2][20]\t Batch [2000][5500]\t Training Loss 0.6258\t Accuracy 0.8593\n",
      "Epoch [2][20]\t Batch [2050][5500]\t Training Loss 0.6251\t Accuracy 0.8596\n",
      "Epoch [2][20]\t Batch [2100][5500]\t Training Loss 0.6256\t Accuracy 0.8587\n",
      "Epoch [2][20]\t Batch [2150][5500]\t Training Loss 0.6236\t Accuracy 0.8595\n",
      "Epoch [2][20]\t Batch [2200][5500]\t Training Loss 0.6210\t Accuracy 0.8601\n",
      "Epoch [2][20]\t Batch [2250][5500]\t Training Loss 0.6212\t Accuracy 0.8603\n",
      "Epoch [2][20]\t Batch [2300][5500]\t Training Loss 0.6206\t Accuracy 0.8601\n",
      "Epoch [2][20]\t Batch [2350][5500]\t Training Loss 0.6193\t Accuracy 0.8604\n",
      "Epoch [2][20]\t Batch [2400][5500]\t Training Loss 0.6191\t Accuracy 0.8607\n",
      "Epoch [2][20]\t Batch [2450][5500]\t Training Loss 0.6183\t Accuracy 0.8609\n",
      "Epoch [2][20]\t Batch [2500][5500]\t Training Loss 0.6193\t Accuracy 0.8601\n",
      "Epoch [2][20]\t Batch [2550][5500]\t Training Loss 0.6174\t Accuracy 0.8606\n",
      "Epoch [2][20]\t Batch [2600][5500]\t Training Loss 0.6160\t Accuracy 0.8610\n",
      "Epoch [2][20]\t Batch [2650][5500]\t Training Loss 0.6155\t Accuracy 0.8615\n",
      "Epoch [2][20]\t Batch [2700][5500]\t Training Loss 0.6160\t Accuracy 0.8613\n",
      "Epoch [2][20]\t Batch [2750][5500]\t Training Loss 0.6158\t Accuracy 0.8614\n",
      "Epoch [2][20]\t Batch [2800][5500]\t Training Loss 0.6145\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [2850][5500]\t Training Loss 0.6129\t Accuracy 0.8623\n",
      "Epoch [2][20]\t Batch [2900][5500]\t Training Loss 0.6123\t Accuracy 0.8624\n",
      "Epoch [2][20]\t Batch [2950][5500]\t Training Loss 0.6121\t Accuracy 0.8619\n",
      "Epoch [2][20]\t Batch [3000][5500]\t Training Loss 0.6131\t Accuracy 0.8615\n",
      "Epoch [2][20]\t Batch [3050][5500]\t Training Loss 0.6139\t Accuracy 0.8611\n",
      "Epoch [2][20]\t Batch [3100][5500]\t Training Loss 0.6148\t Accuracy 0.8607\n",
      "Epoch [2][20]\t Batch [3150][5500]\t Training Loss 0.6158\t Accuracy 0.8605\n",
      "Epoch [2][20]\t Batch [3200][5500]\t Training Loss 0.6164\t Accuracy 0.8600\n",
      "Epoch [2][20]\t Batch [3250][5500]\t Training Loss 0.6179\t Accuracy 0.8592\n",
      "Epoch [2][20]\t Batch [3300][5500]\t Training Loss 0.6170\t Accuracy 0.8595\n",
      "Epoch [2][20]\t Batch [3350][5500]\t Training Loss 0.6173\t Accuracy 0.8593\n",
      "Epoch [2][20]\t Batch [3400][5500]\t Training Loss 0.6150\t Accuracy 0.8601\n",
      "Epoch [2][20]\t Batch [3450][5500]\t Training Loss 0.6138\t Accuracy 0.8606\n",
      "Epoch [2][20]\t Batch [3500][5500]\t Training Loss 0.6140\t Accuracy 0.8603\n",
      "Epoch [2][20]\t Batch [3550][5500]\t Training Loss 0.6130\t Accuracy 0.8603\n",
      "Epoch [2][20]\t Batch [3600][5500]\t Training Loss 0.6123\t Accuracy 0.8603\n",
      "Epoch [2][20]\t Batch [3650][5500]\t Training Loss 0.6116\t Accuracy 0.8604\n",
      "Epoch [2][20]\t Batch [3700][5500]\t Training Loss 0.6097\t Accuracy 0.8610\n",
      "Epoch [2][20]\t Batch [3750][5500]\t Training Loss 0.6103\t Accuracy 0.8606\n",
      "Epoch [2][20]\t Batch [3800][5500]\t Training Loss 0.6099\t Accuracy 0.8607\n",
      "Epoch [2][20]\t Batch [3850][5500]\t Training Loss 0.6092\t Accuracy 0.8608\n",
      "Epoch [2][20]\t Batch [3900][5500]\t Training Loss 0.6082\t Accuracy 0.8612\n",
      "Epoch [2][20]\t Batch [3950][5500]\t Training Loss 0.6077\t Accuracy 0.8613\n",
      "Epoch [2][20]\t Batch [4000][5500]\t Training Loss 0.6080\t Accuracy 0.8613\n",
      "Epoch [2][20]\t Batch [4050][5500]\t Training Loss 0.6070\t Accuracy 0.8614\n",
      "Epoch [2][20]\t Batch [4100][5500]\t Training Loss 0.6057\t Accuracy 0.8620\n",
      "Epoch [2][20]\t Batch [4150][5500]\t Training Loss 0.6063\t Accuracy 0.8618\n",
      "Epoch [2][20]\t Batch [4200][5500]\t Training Loss 0.6061\t Accuracy 0.8618\n",
      "Epoch [2][20]\t Batch [4250][5500]\t Training Loss 0.6069\t Accuracy 0.8612\n",
      "Epoch [2][20]\t Batch [4300][5500]\t Training Loss 0.6063\t Accuracy 0.8613\n",
      "Epoch [2][20]\t Batch [4350][5500]\t Training Loss 0.6051\t Accuracy 0.8615\n",
      "Epoch [2][20]\t Batch [4400][5500]\t Training Loss 0.6044\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [4450][5500]\t Training Loss 0.6047\t Accuracy 0.8614\n",
      "Epoch [2][20]\t Batch [4500][5500]\t Training Loss 0.6035\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [4550][5500]\t Training Loss 0.6034\t Accuracy 0.8616\n",
      "Epoch [2][20]\t Batch [4600][5500]\t Training Loss 0.6033\t Accuracy 0.8616\n",
      "Epoch [2][20]\t Batch [4650][5500]\t Training Loss 0.6035\t Accuracy 0.8614\n",
      "Epoch [2][20]\t Batch [4700][5500]\t Training Loss 0.6021\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [4750][5500]\t Training Loss 0.6019\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [4800][5500]\t Training Loss 0.6013\t Accuracy 0.8617\n",
      "Epoch [2][20]\t Batch [4850][5500]\t Training Loss 0.5998\t Accuracy 0.8622\n",
      "Epoch [2][20]\t Batch [4900][5500]\t Training Loss 0.5991\t Accuracy 0.8623\n",
      "Epoch [2][20]\t Batch [4950][5500]\t Training Loss 0.5992\t Accuracy 0.8619\n",
      "Epoch [2][20]\t Batch [5000][5500]\t Training Loss 0.5999\t Accuracy 0.8618\n",
      "Epoch [2][20]\t Batch [5050][5500]\t Training Loss 0.6001\t Accuracy 0.8618\n",
      "Epoch [2][20]\t Batch [5100][5500]\t Training Loss 0.5995\t Accuracy 0.8618\n",
      "Epoch [2][20]\t Batch [5150][5500]\t Training Loss 0.5985\t Accuracy 0.8619\n",
      "Epoch [2][20]\t Batch [5200][5500]\t Training Loss 0.5974\t Accuracy 0.8623\n",
      "Epoch [2][20]\t Batch [5250][5500]\t Training Loss 0.5973\t Accuracy 0.8622\n",
      "Epoch [2][20]\t Batch [5300][5500]\t Training Loss 0.5973\t Accuracy 0.8620\n",
      "Epoch [2][20]\t Batch [5350][5500]\t Training Loss 0.5965\t Accuracy 0.8623\n",
      "Epoch [2][20]\t Batch [5400][5500]\t Training Loss 0.5961\t Accuracy 0.8622\n",
      "Epoch [2][20]\t Batch [5450][5500]\t Training Loss 0.5953\t Accuracy 0.8624\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5948\t Average training accuracy 0.8624\n",
      "Epoch [2]\t Average validation loss 0.4578\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [3][20]\t Batch [0][5500]\t Training Loss 0.2874\t Accuracy 1.0000\n",
      "Epoch [3][20]\t Batch [50][5500]\t Training Loss 0.5134\t Accuracy 0.8765\n",
      "Epoch [3][20]\t Batch [100][5500]\t Training Loss 0.5545\t Accuracy 0.8614\n",
      "Epoch [3][20]\t Batch [150][5500]\t Training Loss 0.5636\t Accuracy 0.8609\n",
      "Epoch [3][20]\t Batch [200][5500]\t Training Loss 0.5382\t Accuracy 0.8692\n",
      "Epoch [3][20]\t Batch [250][5500]\t Training Loss 0.5255\t Accuracy 0.8733\n",
      "Epoch [3][20]\t Batch [300][5500]\t Training Loss 0.5189\t Accuracy 0.8757\n",
      "Epoch [3][20]\t Batch [350][5500]\t Training Loss 0.5189\t Accuracy 0.8781\n",
      "Epoch [3][20]\t Batch [400][5500]\t Training Loss 0.5169\t Accuracy 0.8803\n",
      "Epoch [3][20]\t Batch [450][5500]\t Training Loss 0.5154\t Accuracy 0.8816\n",
      "Epoch [3][20]\t Batch [500][5500]\t Training Loss 0.5124\t Accuracy 0.8810\n",
      "Epoch [3][20]\t Batch [550][5500]\t Training Loss 0.5122\t Accuracy 0.8811\n",
      "Epoch [3][20]\t Batch [600][5500]\t Training Loss 0.5106\t Accuracy 0.8824\n",
      "Epoch [3][20]\t Batch [650][5500]\t Training Loss 0.5053\t Accuracy 0.8840\n",
      "Epoch [3][20]\t Batch [700][5500]\t Training Loss 0.5043\t Accuracy 0.8830\n",
      "Epoch [3][20]\t Batch [750][5500]\t Training Loss 0.5101\t Accuracy 0.8823\n",
      "Epoch [3][20]\t Batch [800][5500]\t Training Loss 0.5136\t Accuracy 0.8813\n",
      "Epoch [3][20]\t Batch [850][5500]\t Training Loss 0.5170\t Accuracy 0.8803\n",
      "Epoch [3][20]\t Batch [900][5500]\t Training Loss 0.5226\t Accuracy 0.8776\n",
      "Epoch [3][20]\t Batch [950][5500]\t Training Loss 0.5211\t Accuracy 0.8788\n",
      "Epoch [3][20]\t Batch [1000][5500]\t Training Loss 0.5181\t Accuracy 0.8792\n",
      "Epoch [3][20]\t Batch [1050][5500]\t Training Loss 0.5150\t Accuracy 0.8798\n",
      "Epoch [3][20]\t Batch [1100][5500]\t Training Loss 0.5117\t Accuracy 0.8812\n",
      "Epoch [3][20]\t Batch [1150][5500]\t Training Loss 0.5094\t Accuracy 0.8816\n",
      "Epoch [3][20]\t Batch [1200][5500]\t Training Loss 0.5131\t Accuracy 0.8796\n",
      "Epoch [3][20]\t Batch [1250][5500]\t Training Loss 0.5139\t Accuracy 0.8789\n",
      "Epoch [3][20]\t Batch [1300][5500]\t Training Loss 0.5167\t Accuracy 0.8779\n",
      "Epoch [3][20]\t Batch [1350][5500]\t Training Loss 0.5183\t Accuracy 0.8771\n",
      "Epoch [3][20]\t Batch [1400][5500]\t Training Loss 0.5199\t Accuracy 0.8755\n",
      "Epoch [3][20]\t Batch [1450][5500]\t Training Loss 0.5231\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [1500][5500]\t Training Loss 0.5274\t Accuracy 0.8728\n",
      "Epoch [3][20]\t Batch [1550][5500]\t Training Loss 0.5256\t Accuracy 0.8741\n",
      "Epoch [3][20]\t Batch [1600][5500]\t Training Loss 0.5275\t Accuracy 0.8733\n",
      "Epoch [3][20]\t Batch [1650][5500]\t Training Loss 0.5263\t Accuracy 0.8735\n",
      "Epoch [3][20]\t Batch [1700][5500]\t Training Loss 0.5274\t Accuracy 0.8730\n",
      "Epoch [3][20]\t Batch [1750][5500]\t Training Loss 0.5281\t Accuracy 0.8722\n",
      "Epoch [3][20]\t Batch [1800][5500]\t Training Loss 0.5305\t Accuracy 0.8713\n",
      "Epoch [3][20]\t Batch [1850][5500]\t Training Loss 0.5283\t Accuracy 0.8724\n",
      "Epoch [3][20]\t Batch [1900][5500]\t Training Loss 0.5260\t Accuracy 0.8734\n",
      "Epoch [3][20]\t Batch [1950][5500]\t Training Loss 0.5257\t Accuracy 0.8734\n",
      "Epoch [3][20]\t Batch [2000][5500]\t Training Loss 0.5237\t Accuracy 0.8739\n",
      "Epoch [3][20]\t Batch [2050][5500]\t Training Loss 0.5232\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [2100][5500]\t Training Loss 0.5243\t Accuracy 0.8733\n",
      "Epoch [3][20]\t Batch [2150][5500]\t Training Loss 0.5225\t Accuracy 0.8742\n",
      "Epoch [3][20]\t Batch [2200][5500]\t Training Loss 0.5203\t Accuracy 0.8749\n",
      "Epoch [3][20]\t Batch [2250][5500]\t Training Loss 0.5206\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [2300][5500]\t Training Loss 0.5203\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [2350][5500]\t Training Loss 0.5194\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [2400][5500]\t Training Loss 0.5194\t Accuracy 0.8751\n",
      "Epoch [3][20]\t Batch [2450][5500]\t Training Loss 0.5189\t Accuracy 0.8751\n",
      "Epoch [3][20]\t Batch [2500][5500]\t Training Loss 0.5202\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [2550][5500]\t Training Loss 0.5186\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [2600][5500]\t Training Loss 0.5175\t Accuracy 0.8751\n",
      "Epoch [3][20]\t Batch [2650][5500]\t Training Loss 0.5173\t Accuracy 0.8756\n",
      "Epoch [3][20]\t Batch [2700][5500]\t Training Loss 0.5181\t Accuracy 0.8755\n",
      "Epoch [3][20]\t Batch [2750][5500]\t Training Loss 0.5181\t Accuracy 0.8755\n",
      "Epoch [3][20]\t Batch [2800][5500]\t Training Loss 0.5170\t Accuracy 0.8757\n",
      "Epoch [3][20]\t Batch [2850][5500]\t Training Loss 0.5159\t Accuracy 0.8762\n",
      "Epoch [3][20]\t Batch [2900][5500]\t Training Loss 0.5156\t Accuracy 0.8762\n",
      "Epoch [3][20]\t Batch [2950][5500]\t Training Loss 0.5157\t Accuracy 0.8758\n",
      "Epoch [3][20]\t Batch [3000][5500]\t Training Loss 0.5169\t Accuracy 0.8753\n",
      "Epoch [3][20]\t Batch [3050][5500]\t Training Loss 0.5178\t Accuracy 0.8749\n",
      "Epoch [3][20]\t Batch [3100][5500]\t Training Loss 0.5189\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [3150][5500]\t Training Loss 0.5202\t Accuracy 0.8742\n",
      "Epoch [3][20]\t Batch [3200][5500]\t Training Loss 0.5210\t Accuracy 0.8736\n",
      "Epoch [3][20]\t Batch [3250][5500]\t Training Loss 0.5227\t Accuracy 0.8726\n",
      "Epoch [3][20]\t Batch [3300][5500]\t Training Loss 0.5221\t Accuracy 0.8729\n",
      "Epoch [3][20]\t Batch [3350][5500]\t Training Loss 0.5225\t Accuracy 0.8726\n",
      "Epoch [3][20]\t Batch [3400][5500]\t Training Loss 0.5204\t Accuracy 0.8735\n",
      "Epoch [3][20]\t Batch [3450][5500]\t Training Loss 0.5194\t Accuracy 0.8740\n",
      "Epoch [3][20]\t Batch [3500][5500]\t Training Loss 0.5199\t Accuracy 0.8737\n",
      "Epoch [3][20]\t Batch [3550][5500]\t Training Loss 0.5191\t Accuracy 0.8738\n",
      "Epoch [3][20]\t Batch [3600][5500]\t Training Loss 0.5185\t Accuracy 0.8737\n",
      "Epoch [3][20]\t Batch [3650][5500]\t Training Loss 0.5181\t Accuracy 0.8738\n",
      "Epoch [3][20]\t Batch [3700][5500]\t Training Loss 0.5164\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [3750][5500]\t Training Loss 0.5175\t Accuracy 0.8740\n",
      "Epoch [3][20]\t Batch [3800][5500]\t Training Loss 0.5174\t Accuracy 0.8740\n",
      "Epoch [3][20]\t Batch [3850][5500]\t Training Loss 0.5168\t Accuracy 0.8741\n",
      "Epoch [3][20]\t Batch [3900][5500]\t Training Loss 0.5162\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [3950][5500]\t Training Loss 0.5160\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [4000][5500]\t Training Loss 0.5164\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [4050][5500]\t Training Loss 0.5156\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [4100][5500]\t Training Loss 0.5147\t Accuracy 0.8749\n",
      "Epoch [3][20]\t Batch [4150][5500]\t Training Loss 0.5154\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [4200][5500]\t Training Loss 0.5153\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [4250][5500]\t Training Loss 0.5164\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [4300][5500]\t Training Loss 0.5162\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [4350][5500]\t Training Loss 0.5151\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [4400][5500]\t Training Loss 0.5147\t Accuracy 0.8747\n",
      "Epoch [3][20]\t Batch [4450][5500]\t Training Loss 0.5151\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [4500][5500]\t Training Loss 0.5142\t Accuracy 0.8747\n",
      "Epoch [3][20]\t Batch [4550][5500]\t Training Loss 0.5143\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [4600][5500]\t Training Loss 0.5145\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [4650][5500]\t Training Loss 0.5149\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [4700][5500]\t Training Loss 0.5137\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [4750][5500]\t Training Loss 0.5137\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [4800][5500]\t Training Loss 0.5134\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [4850][5500]\t Training Loss 0.5122\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [4900][5500]\t Training Loss 0.5117\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [4950][5500]\t Training Loss 0.5120\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [5000][5500]\t Training Loss 0.5129\t Accuracy 0.8743\n",
      "Epoch [3][20]\t Batch [5050][5500]\t Training Loss 0.5134\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [5100][5500]\t Training Loss 0.5130\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [5150][5500]\t Training Loss 0.5123\t Accuracy 0.8745\n",
      "Epoch [3][20]\t Batch [5200][5500]\t Training Loss 0.5114\t Accuracy 0.8748\n",
      "Epoch [3][20]\t Batch [5250][5500]\t Training Loss 0.5115\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [5300][5500]\t Training Loss 0.5118\t Accuracy 0.8744\n",
      "Epoch [3][20]\t Batch [5350][5500]\t Training Loss 0.5112\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [5400][5500]\t Training Loss 0.5110\t Accuracy 0.8746\n",
      "Epoch [3][20]\t Batch [5450][5500]\t Training Loss 0.5104\t Accuracy 0.8747\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5102\t Average training accuracy 0.8748\n",
      "Epoch [3]\t Average validation loss 0.3971\t Average validation accuracy 0.9134\n",
      "\n",
      "Epoch [4][20]\t Batch [0][5500]\t Training Loss 0.2196\t Accuracy 1.0000\n",
      "Epoch [4][20]\t Batch [50][5500]\t Training Loss 0.4544\t Accuracy 0.8941\n",
      "Epoch [4][20]\t Batch [100][5500]\t Training Loss 0.4935\t Accuracy 0.8752\n",
      "Epoch [4][20]\t Batch [150][5500]\t Training Loss 0.5029\t Accuracy 0.8748\n",
      "Epoch [4][20]\t Batch [200][5500]\t Training Loss 0.4774\t Accuracy 0.8836\n",
      "Epoch [4][20]\t Batch [250][5500]\t Training Loss 0.4645\t Accuracy 0.8869\n",
      "Epoch [4][20]\t Batch [300][5500]\t Training Loss 0.4578\t Accuracy 0.8894\n",
      "Epoch [4][20]\t Batch [350][5500]\t Training Loss 0.4566\t Accuracy 0.8903\n",
      "Epoch [4][20]\t Batch [400][5500]\t Training Loss 0.4549\t Accuracy 0.8925\n",
      "Epoch [4][20]\t Batch [450][5500]\t Training Loss 0.4535\t Accuracy 0.8936\n",
      "Epoch [4][20]\t Batch [500][5500]\t Training Loss 0.4509\t Accuracy 0.8926\n",
      "Epoch [4][20]\t Batch [550][5500]\t Training Loss 0.4513\t Accuracy 0.8918\n",
      "Epoch [4][20]\t Batch [600][5500]\t Training Loss 0.4499\t Accuracy 0.8918\n",
      "Epoch [4][20]\t Batch [650][5500]\t Training Loss 0.4451\t Accuracy 0.8932\n",
      "Epoch [4][20]\t Batch [700][5500]\t Training Loss 0.4443\t Accuracy 0.8926\n",
      "Epoch [4][20]\t Batch [750][5500]\t Training Loss 0.4493\t Accuracy 0.8915\n",
      "Epoch [4][20]\t Batch [800][5500]\t Training Loss 0.4528\t Accuracy 0.8901\n",
      "Epoch [4][20]\t Batch [850][5500]\t Training Loss 0.4564\t Accuracy 0.8895\n",
      "Epoch [4][20]\t Batch [900][5500]\t Training Loss 0.4627\t Accuracy 0.8869\n",
      "Epoch [4][20]\t Batch [950][5500]\t Training Loss 0.4616\t Accuracy 0.8877\n",
      "Epoch [4][20]\t Batch [1000][5500]\t Training Loss 0.4589\t Accuracy 0.8883\n",
      "Epoch [4][20]\t Batch [1050][5500]\t Training Loss 0.4561\t Accuracy 0.8889\n",
      "Epoch [4][20]\t Batch [1100][5500]\t Training Loss 0.4529\t Accuracy 0.8901\n",
      "Epoch [4][20]\t Batch [1150][5500]\t Training Loss 0.4507\t Accuracy 0.8907\n",
      "Epoch [4][20]\t Batch [1200][5500]\t Training Loss 0.4547\t Accuracy 0.8888\n",
      "Epoch [4][20]\t Batch [1250][5500]\t Training Loss 0.4555\t Accuracy 0.8882\n",
      "Epoch [4][20]\t Batch [1300][5500]\t Training Loss 0.4586\t Accuracy 0.8874\n",
      "Epoch [4][20]\t Batch [1350][5500]\t Training Loss 0.4603\t Accuracy 0.8863\n",
      "Epoch [4][20]\t Batch [1400][5500]\t Training Loss 0.4621\t Accuracy 0.8849\n",
      "Epoch [4][20]\t Batch [1450][5500]\t Training Loss 0.4654\t Accuracy 0.8841\n",
      "Epoch [4][20]\t Batch [1500][5500]\t Training Loss 0.4697\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [1550][5500]\t Training Loss 0.4681\t Accuracy 0.8838\n",
      "Epoch [4][20]\t Batch [1600][5500]\t Training Loss 0.4700\t Accuracy 0.8829\n",
      "Epoch [4][20]\t Batch [1650][5500]\t Training Loss 0.4689\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [1700][5500]\t Training Loss 0.4700\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [1750][5500]\t Training Loss 0.4706\t Accuracy 0.8822\n",
      "Epoch [4][20]\t Batch [1800][5500]\t Training Loss 0.4730\t Accuracy 0.8815\n",
      "Epoch [4][20]\t Batch [1850][5500]\t Training Loss 0.4708\t Accuracy 0.8824\n",
      "Epoch [4][20]\t Batch [1900][5500]\t Training Loss 0.4687\t Accuracy 0.8834\n",
      "Epoch [4][20]\t Batch [1950][5500]\t Training Loss 0.4685\t Accuracy 0.8833\n",
      "Epoch [4][20]\t Batch [2000][5500]\t Training Loss 0.4666\t Accuracy 0.8836\n",
      "Epoch [4][20]\t Batch [2050][5500]\t Training Loss 0.4661\t Accuracy 0.8840\n",
      "Epoch [4][20]\t Batch [2100][5500]\t Training Loss 0.4676\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [2150][5500]\t Training Loss 0.4659\t Accuracy 0.8841\n",
      "Epoch [4][20]\t Batch [2200][5500]\t Training Loss 0.4640\t Accuracy 0.8847\n",
      "Epoch [4][20]\t Batch [2250][5500]\t Training Loss 0.4641\t Accuracy 0.8847\n",
      "Epoch [4][20]\t Batch [2300][5500]\t Training Loss 0.4640\t Accuracy 0.8844\n",
      "Epoch [4][20]\t Batch [2350][5500]\t Training Loss 0.4632\t Accuracy 0.8844\n",
      "Epoch [4][20]\t Batch [2400][5500]\t Training Loss 0.4634\t Accuracy 0.8846\n",
      "Epoch [4][20]\t Batch [2450][5500]\t Training Loss 0.4630\t Accuracy 0.8845\n",
      "Epoch [4][20]\t Batch [2500][5500]\t Training Loss 0.4644\t Accuracy 0.8838\n",
      "Epoch [4][20]\t Batch [2550][5500]\t Training Loss 0.4630\t Accuracy 0.8840\n",
      "Epoch [4][20]\t Batch [2600][5500]\t Training Loss 0.4620\t Accuracy 0.8844\n",
      "Epoch [4][20]\t Batch [2650][5500]\t Training Loss 0.4619\t Accuracy 0.8847\n",
      "Epoch [4][20]\t Batch [2700][5500]\t Training Loss 0.4629\t Accuracy 0.8846\n",
      "Epoch [4][20]\t Batch [2750][5500]\t Training Loss 0.4630\t Accuracy 0.8844\n",
      "Epoch [4][20]\t Batch [2800][5500]\t Training Loss 0.4620\t Accuracy 0.8845\n",
      "Epoch [4][20]\t Batch [2850][5500]\t Training Loss 0.4611\t Accuracy 0.8849\n",
      "Epoch [4][20]\t Batch [2900][5500]\t Training Loss 0.4609\t Accuracy 0.8850\n",
      "Epoch [4][20]\t Batch [2950][5500]\t Training Loss 0.4612\t Accuracy 0.8846\n",
      "Epoch [4][20]\t Batch [3000][5500]\t Training Loss 0.4624\t Accuracy 0.8841\n",
      "Epoch [4][20]\t Batch [3050][5500]\t Training Loss 0.4632\t Accuracy 0.8838\n",
      "Epoch [4][20]\t Batch [3100][5500]\t Training Loss 0.4645\t Accuracy 0.8836\n",
      "Epoch [4][20]\t Batch [3150][5500]\t Training Loss 0.4659\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [3200][5500]\t Training Loss 0.4668\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [3250][5500]\t Training Loss 0.4685\t Accuracy 0.8818\n",
      "Epoch [4][20]\t Batch [3300][5500]\t Training Loss 0.4681\t Accuracy 0.8819\n",
      "Epoch [4][20]\t Batch [3350][5500]\t Training Loss 0.4684\t Accuracy 0.8818\n",
      "Epoch [4][20]\t Batch [3400][5500]\t Training Loss 0.4665\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [3450][5500]\t Training Loss 0.4655\t Accuracy 0.8830\n",
      "Epoch [4][20]\t Batch [3500][5500]\t Training Loss 0.4661\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [3550][5500]\t Training Loss 0.4654\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [3600][5500]\t Training Loss 0.4648\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [3650][5500]\t Training Loss 0.4645\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [3700][5500]\t Training Loss 0.4630\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [3750][5500]\t Training Loss 0.4644\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [3800][5500]\t Training Loss 0.4644\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [3850][5500]\t Training Loss 0.4639\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [3900][5500]\t Training Loss 0.4633\t Accuracy 0.8829\n",
      "Epoch [4][20]\t Batch [3950][5500]\t Training Loss 0.4634\t Accuracy 0.8829\n",
      "Epoch [4][20]\t Batch [4000][5500]\t Training Loss 0.4638\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4050][5500]\t Training Loss 0.4631\t Accuracy 0.8830\n",
      "Epoch [4][20]\t Batch [4100][5500]\t Training Loss 0.4622\t Accuracy 0.8833\n",
      "Epoch [4][20]\t Batch [4150][5500]\t Training Loss 0.4630\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [4200][5500]\t Training Loss 0.4630\t Accuracy 0.8832\n",
      "Epoch [4][20]\t Batch [4250][5500]\t Training Loss 0.4642\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4300][5500]\t Training Loss 0.4641\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4350][5500]\t Training Loss 0.4632\t Accuracy 0.8830\n",
      "Epoch [4][20]\t Batch [4400][5500]\t Training Loss 0.4629\t Accuracy 0.8830\n",
      "Epoch [4][20]\t Batch [4450][5500]\t Training Loss 0.4633\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4500][5500]\t Training Loss 0.4626\t Accuracy 0.8830\n",
      "Epoch [4][20]\t Batch [4550][5500]\t Training Loss 0.4628\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4600][5500]\t Training Loss 0.4630\t Accuracy 0.8827\n",
      "Epoch [4][20]\t Batch [4650][5500]\t Training Loss 0.4636\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [4700][5500]\t Training Loss 0.4625\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [4750][5500]\t Training Loss 0.4626\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [4800][5500]\t Training Loss 0.4624\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [4850][5500]\t Training Loss 0.4614\t Accuracy 0.8829\n",
      "Epoch [4][20]\t Batch [4900][5500]\t Training Loss 0.4609\t Accuracy 0.8829\n",
      "Epoch [4][20]\t Batch [4950][5500]\t Training Loss 0.4613\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [5000][5500]\t Training Loss 0.4623\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [5050][5500]\t Training Loss 0.4630\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [5100][5500]\t Training Loss 0.4627\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [5150][5500]\t Training Loss 0.4621\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [5200][5500]\t Training Loss 0.4613\t Accuracy 0.8828\n",
      "Epoch [4][20]\t Batch [5250][5500]\t Training Loss 0.4615\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [5300][5500]\t Training Loss 0.4620\t Accuracy 0.8824\n",
      "Epoch [4][20]\t Batch [5350][5500]\t Training Loss 0.4614\t Accuracy 0.8826\n",
      "Epoch [4][20]\t Batch [5400][5500]\t Training Loss 0.4613\t Accuracy 0.8825\n",
      "Epoch [4][20]\t Batch [5450][5500]\t Training Loss 0.4608\t Accuracy 0.8826\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4607\t Average training accuracy 0.8826\n",
      "Epoch [4]\t Average validation loss 0.3599\t Average validation accuracy 0.9180\n",
      "\n",
      "Epoch [5][20]\t Batch [0][5500]\t Training Loss 0.1799\t Accuracy 1.0000\n",
      "Epoch [5][20]\t Batch [50][5500]\t Training Loss 0.4170\t Accuracy 0.8980\n",
      "Epoch [5][20]\t Batch [100][5500]\t Training Loss 0.4548\t Accuracy 0.8792\n",
      "Epoch [5][20]\t Batch [150][5500]\t Training Loss 0.4648\t Accuracy 0.8781\n",
      "Epoch [5][20]\t Batch [200][5500]\t Training Loss 0.4393\t Accuracy 0.8871\n",
      "Epoch [5][20]\t Batch [250][5500]\t Training Loss 0.4261\t Accuracy 0.8904\n",
      "Epoch [5][20]\t Batch [300][5500]\t Training Loss 0.4193\t Accuracy 0.8927\n",
      "Epoch [5][20]\t Batch [350][5500]\t Training Loss 0.4172\t Accuracy 0.8943\n",
      "Epoch [5][20]\t Batch [400][5500]\t Training Loss 0.4156\t Accuracy 0.8960\n",
      "Epoch [5][20]\t Batch [450][5500]\t Training Loss 0.4144\t Accuracy 0.8971\n",
      "Epoch [5][20]\t Batch [500][5500]\t Training Loss 0.4119\t Accuracy 0.8968\n",
      "Epoch [5][20]\t Batch [550][5500]\t Training Loss 0.4128\t Accuracy 0.8962\n",
      "Epoch [5][20]\t Batch [600][5500]\t Training Loss 0.4114\t Accuracy 0.8965\n",
      "Epoch [5][20]\t Batch [650][5500]\t Training Loss 0.4069\t Accuracy 0.8985\n",
      "Epoch [5][20]\t Batch [700][5500]\t Training Loss 0.4063\t Accuracy 0.8974\n",
      "Epoch [5][20]\t Batch [750][5500]\t Training Loss 0.4109\t Accuracy 0.8965\n",
      "Epoch [5][20]\t Batch [800][5500]\t Training Loss 0.4143\t Accuracy 0.8953\n",
      "Epoch [5][20]\t Batch [850][5500]\t Training Loss 0.4178\t Accuracy 0.8946\n",
      "Epoch [5][20]\t Batch [900][5500]\t Training Loss 0.4246\t Accuracy 0.8918\n",
      "Epoch [5][20]\t Batch [950][5500]\t Training Loss 0.4238\t Accuracy 0.8922\n",
      "Epoch [5][20]\t Batch [1000][5500]\t Training Loss 0.4212\t Accuracy 0.8929\n",
      "Epoch [5][20]\t Batch [1050][5500]\t Training Loss 0.4187\t Accuracy 0.8935\n",
      "Epoch [5][20]\t Batch [1100][5500]\t Training Loss 0.4155\t Accuracy 0.8950\n",
      "Epoch [5][20]\t Batch [1150][5500]\t Training Loss 0.4134\t Accuracy 0.8955\n",
      "Epoch [5][20]\t Batch [1200][5500]\t Training Loss 0.4175\t Accuracy 0.8938\n",
      "Epoch [5][20]\t Batch [1250][5500]\t Training Loss 0.4184\t Accuracy 0.8933\n",
      "Epoch [5][20]\t Batch [1300][5500]\t Training Loss 0.4216\t Accuracy 0.8922\n",
      "Epoch [5][20]\t Batch [1350][5500]\t Training Loss 0.4234\t Accuracy 0.8912\n",
      "Epoch [5][20]\t Batch [1400][5500]\t Training Loss 0.4252\t Accuracy 0.8900\n",
      "Epoch [5][20]\t Batch [1450][5500]\t Training Loss 0.4286\t Accuracy 0.8892\n",
      "Epoch [5][20]\t Batch [1500][5500]\t Training Loss 0.4329\t Accuracy 0.8876\n",
      "Epoch [5][20]\t Batch [1550][5500]\t Training Loss 0.4314\t Accuracy 0.8887\n",
      "Epoch [5][20]\t Batch [1600][5500]\t Training Loss 0.4332\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [1650][5500]\t Training Loss 0.4321\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [1700][5500]\t Training Loss 0.4332\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [1750][5500]\t Training Loss 0.4338\t Accuracy 0.8875\n",
      "Epoch [5][20]\t Batch [1800][5500]\t Training Loss 0.4360\t Accuracy 0.8867\n",
      "Epoch [5][20]\t Batch [1850][5500]\t Training Loss 0.4338\t Accuracy 0.8877\n",
      "Epoch [5][20]\t Batch [1900][5500]\t Training Loss 0.4318\t Accuracy 0.8885\n",
      "Epoch [5][20]\t Batch [1950][5500]\t Training Loss 0.4316\t Accuracy 0.8886\n",
      "Epoch [5][20]\t Batch [2000][5500]\t Training Loss 0.4299\t Accuracy 0.8891\n",
      "Epoch [5][20]\t Batch [2050][5500]\t Training Loss 0.4293\t Accuracy 0.8897\n",
      "Epoch [5][20]\t Batch [2100][5500]\t Training Loss 0.4311\t Accuracy 0.8890\n",
      "Epoch [5][20]\t Batch [2150][5500]\t Training Loss 0.4294\t Accuracy 0.8899\n",
      "Epoch [5][20]\t Batch [2200][5500]\t Training Loss 0.4276\t Accuracy 0.8906\n",
      "Epoch [5][20]\t Batch [2250][5500]\t Training Loss 0.4277\t Accuracy 0.8904\n",
      "Epoch [5][20]\t Batch [2300][5500]\t Training Loss 0.4276\t Accuracy 0.8901\n",
      "Epoch [5][20]\t Batch [2350][5500]\t Training Loss 0.4270\t Accuracy 0.8903\n",
      "Epoch [5][20]\t Batch [2400][5500]\t Training Loss 0.4272\t Accuracy 0.8903\n",
      "Epoch [5][20]\t Batch [2450][5500]\t Training Loss 0.4269\t Accuracy 0.8902\n",
      "Epoch [5][20]\t Batch [2500][5500]\t Training Loss 0.4283\t Accuracy 0.8896\n",
      "Epoch [5][20]\t Batch [2550][5500]\t Training Loss 0.4271\t Accuracy 0.8898\n",
      "Epoch [5][20]\t Batch [2600][5500]\t Training Loss 0.4262\t Accuracy 0.8902\n",
      "Epoch [5][20]\t Batch [2650][5500]\t Training Loss 0.4261\t Accuracy 0.8906\n",
      "Epoch [5][20]\t Batch [2700][5500]\t Training Loss 0.4272\t Accuracy 0.8904\n",
      "Epoch [5][20]\t Batch [2750][5500]\t Training Loss 0.4273\t Accuracy 0.8902\n",
      "Epoch [5][20]\t Batch [2800][5500]\t Training Loss 0.4264\t Accuracy 0.8903\n",
      "Epoch [5][20]\t Batch [2850][5500]\t Training Loss 0.4256\t Accuracy 0.8906\n",
      "Epoch [5][20]\t Batch [2900][5500]\t Training Loss 0.4255\t Accuracy 0.8907\n",
      "Epoch [5][20]\t Batch [2950][5500]\t Training Loss 0.4258\t Accuracy 0.8903\n",
      "Epoch [5][20]\t Batch [3000][5500]\t Training Loss 0.4270\t Accuracy 0.8898\n",
      "Epoch [5][20]\t Batch [3050][5500]\t Training Loss 0.4278\t Accuracy 0.8895\n",
      "Epoch [5][20]\t Batch [3100][5500]\t Training Loss 0.4290\t Accuracy 0.8893\n",
      "Epoch [5][20]\t Batch [3150][5500]\t Training Loss 0.4306\t Accuracy 0.8890\n",
      "Epoch [5][20]\t Batch [3200][5500]\t Training Loss 0.4315\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [3250][5500]\t Training Loss 0.4332\t Accuracy 0.8875\n",
      "Epoch [5][20]\t Batch [3300][5500]\t Training Loss 0.4328\t Accuracy 0.8875\n",
      "Epoch [5][20]\t Batch [3350][5500]\t Training Loss 0.4331\t Accuracy 0.8874\n",
      "Epoch [5][20]\t Batch [3400][5500]\t Training Loss 0.4312\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [3450][5500]\t Training Loss 0.4304\t Accuracy 0.8884\n",
      "Epoch [5][20]\t Batch [3500][5500]\t Training Loss 0.4310\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [3550][5500]\t Training Loss 0.4303\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [3600][5500]\t Training Loss 0.4297\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [3650][5500]\t Training Loss 0.4294\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [3700][5500]\t Training Loss 0.4281\t Accuracy 0.8885\n",
      "Epoch [5][20]\t Batch [3750][5500]\t Training Loss 0.4297\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [3800][5500]\t Training Loss 0.4297\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [3850][5500]\t Training Loss 0.4292\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [3900][5500]\t Training Loss 0.4288\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [3950][5500]\t Training Loss 0.4289\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [4000][5500]\t Training Loss 0.4293\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4050][5500]\t Training Loss 0.4287\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [4100][5500]\t Training Loss 0.4279\t Accuracy 0.8888\n",
      "Epoch [5][20]\t Batch [4150][5500]\t Training Loss 0.4286\t Accuracy 0.8886\n",
      "Epoch [5][20]\t Batch [4200][5500]\t Training Loss 0.4287\t Accuracy 0.8886\n",
      "Epoch [5][20]\t Batch [4250][5500]\t Training Loss 0.4299\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [4300][5500]\t Training Loss 0.4300\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4350][5500]\t Training Loss 0.4291\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [4400][5500]\t Training Loss 0.4288\t Accuracy 0.8884\n",
      "Epoch [5][20]\t Batch [4450][5500]\t Training Loss 0.4293\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4500][5500]\t Training Loss 0.4286\t Accuracy 0.8883\n",
      "Epoch [5][20]\t Batch [4550][5500]\t Training Loss 0.4289\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4600][5500]\t Training Loss 0.4292\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [4650][5500]\t Training Loss 0.4298\t Accuracy 0.8879\n",
      "Epoch [5][20]\t Batch [4700][5500]\t Training Loss 0.4288\t Accuracy 0.8881\n",
      "Epoch [5][20]\t Batch [4750][5500]\t Training Loss 0.4289\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [4800][5500]\t Training Loss 0.4288\t Accuracy 0.8879\n",
      "Epoch [5][20]\t Batch [4850][5500]\t Training Loss 0.4279\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4900][5500]\t Training Loss 0.4275\t Accuracy 0.8882\n",
      "Epoch [5][20]\t Batch [4950][5500]\t Training Loss 0.4279\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [5000][5500]\t Training Loss 0.4289\t Accuracy 0.8879\n",
      "Epoch [5][20]\t Batch [5050][5500]\t Training Loss 0.4296\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [5100][5500]\t Training Loss 0.4294\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [5150][5500]\t Training Loss 0.4289\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [5200][5500]\t Training Loss 0.4282\t Accuracy 0.8880\n",
      "Epoch [5][20]\t Batch [5250][5500]\t Training Loss 0.4285\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [5300][5500]\t Training Loss 0.4290\t Accuracy 0.8876\n",
      "Epoch [5][20]\t Batch [5350][5500]\t Training Loss 0.4285\t Accuracy 0.8879\n",
      "Epoch [5][20]\t Batch [5400][5500]\t Training Loss 0.4285\t Accuracy 0.8878\n",
      "Epoch [5][20]\t Batch [5450][5500]\t Training Loss 0.4280\t Accuracy 0.8879\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4279\t Average training accuracy 0.8878\n",
      "Epoch [5]\t Average validation loss 0.3348\t Average validation accuracy 0.9208\n",
      "\n",
      "Epoch [6][20]\t Batch [0][5500]\t Training Loss 0.1536\t Accuracy 1.0000\n",
      "Epoch [6][20]\t Batch [50][5500]\t Training Loss 0.3910\t Accuracy 0.9000\n",
      "Epoch [6][20]\t Batch [100][5500]\t Training Loss 0.4278\t Accuracy 0.8842\n",
      "Epoch [6][20]\t Batch [150][5500]\t Training Loss 0.4385\t Accuracy 0.8841\n",
      "Epoch [6][20]\t Batch [200][5500]\t Training Loss 0.4130\t Accuracy 0.8930\n",
      "Epoch [6][20]\t Batch [250][5500]\t Training Loss 0.3996\t Accuracy 0.8956\n",
      "Epoch [6][20]\t Batch [300][5500]\t Training Loss 0.3927\t Accuracy 0.8977\n",
      "Epoch [6][20]\t Batch [350][5500]\t Training Loss 0.3898\t Accuracy 0.8991\n",
      "Epoch [6][20]\t Batch [400][5500]\t Training Loss 0.3884\t Accuracy 0.9010\n",
      "Epoch [6][20]\t Batch [450][5500]\t Training Loss 0.3873\t Accuracy 0.9018\n",
      "Epoch [6][20]\t Batch [500][5500]\t Training Loss 0.3849\t Accuracy 0.9014\n",
      "Epoch [6][20]\t Batch [550][5500]\t Training Loss 0.3860\t Accuracy 0.9011\n",
      "Epoch [6][20]\t Batch [600][5500]\t Training Loss 0.3848\t Accuracy 0.9012\n",
      "Epoch [6][20]\t Batch [650][5500]\t Training Loss 0.3805\t Accuracy 0.9029\n",
      "Epoch [6][20]\t Batch [700][5500]\t Training Loss 0.3799\t Accuracy 0.9017\n",
      "Epoch [6][20]\t Batch [750][5500]\t Training Loss 0.3841\t Accuracy 0.9008\n",
      "Epoch [6][20]\t Batch [800][5500]\t Training Loss 0.3874\t Accuracy 0.8998\n",
      "Epoch [6][20]\t Batch [850][5500]\t Training Loss 0.3910\t Accuracy 0.8988\n",
      "Epoch [6][20]\t Batch [900][5500]\t Training Loss 0.3980\t Accuracy 0.8960\n",
      "Epoch [6][20]\t Batch [950][5500]\t Training Loss 0.3975\t Accuracy 0.8962\n",
      "Epoch [6][20]\t Batch [1000][5500]\t Training Loss 0.3950\t Accuracy 0.8967\n",
      "Epoch [6][20]\t Batch [1050][5500]\t Training Loss 0.3927\t Accuracy 0.8972\n",
      "Epoch [6][20]\t Batch [1100][5500]\t Training Loss 0.3895\t Accuracy 0.8990\n",
      "Epoch [6][20]\t Batch [1150][5500]\t Training Loss 0.3874\t Accuracy 0.8997\n",
      "Epoch [6][20]\t Batch [1200][5500]\t Training Loss 0.3916\t Accuracy 0.8980\n",
      "Epoch [6][20]\t Batch [1250][5500]\t Training Loss 0.3925\t Accuracy 0.8974\n",
      "Epoch [6][20]\t Batch [1300][5500]\t Training Loss 0.3959\t Accuracy 0.8964\n",
      "Epoch [6][20]\t Batch [1350][5500]\t Training Loss 0.3977\t Accuracy 0.8955\n",
      "Epoch [6][20]\t Batch [1400][5500]\t Training Loss 0.3995\t Accuracy 0.8948\n",
      "Epoch [6][20]\t Batch [1450][5500]\t Training Loss 0.4029\t Accuracy 0.8939\n",
      "Epoch [6][20]\t Batch [1500][5500]\t Training Loss 0.4071\t Accuracy 0.8925\n",
      "Epoch [6][20]\t Batch [1550][5500]\t Training Loss 0.4057\t Accuracy 0.8934\n",
      "Epoch [6][20]\t Batch [1600][5500]\t Training Loss 0.4074\t Accuracy 0.8927\n",
      "Epoch [6][20]\t Batch [1650][5500]\t Training Loss 0.4063\t Accuracy 0.8930\n",
      "Epoch [6][20]\t Batch [1700][5500]\t Training Loss 0.4074\t Accuracy 0.8925\n",
      "Epoch [6][20]\t Batch [1750][5500]\t Training Loss 0.4079\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [1800][5500]\t Training Loss 0.4101\t Accuracy 0.8914\n",
      "Epoch [6][20]\t Batch [1850][5500]\t Training Loss 0.4078\t Accuracy 0.8923\n",
      "Epoch [6][20]\t Batch [1900][5500]\t Training Loss 0.4059\t Accuracy 0.8932\n",
      "Epoch [6][20]\t Batch [1950][5500]\t Training Loss 0.4057\t Accuracy 0.8932\n",
      "Epoch [6][20]\t Batch [2000][5500]\t Training Loss 0.4041\t Accuracy 0.8936\n",
      "Epoch [6][20]\t Batch [2050][5500]\t Training Loss 0.4034\t Accuracy 0.8941\n",
      "Epoch [6][20]\t Batch [2100][5500]\t Training Loss 0.4054\t Accuracy 0.8934\n",
      "Epoch [6][20]\t Batch [2150][5500]\t Training Loss 0.4038\t Accuracy 0.8942\n",
      "Epoch [6][20]\t Batch [2200][5500]\t Training Loss 0.4021\t Accuracy 0.8949\n",
      "Epoch [6][20]\t Batch [2250][5500]\t Training Loss 0.4021\t Accuracy 0.8947\n",
      "Epoch [6][20]\t Batch [2300][5500]\t Training Loss 0.4021\t Accuracy 0.8944\n",
      "Epoch [6][20]\t Batch [2350][5500]\t Training Loss 0.4015\t Accuracy 0.8944\n",
      "Epoch [6][20]\t Batch [2400][5500]\t Training Loss 0.4017\t Accuracy 0.8944\n",
      "Epoch [6][20]\t Batch [2450][5500]\t Training Loss 0.4014\t Accuracy 0.8942\n",
      "Epoch [6][20]\t Batch [2500][5500]\t Training Loss 0.4029\t Accuracy 0.8936\n",
      "Epoch [6][20]\t Batch [2550][5500]\t Training Loss 0.4017\t Accuracy 0.8938\n",
      "Epoch [6][20]\t Batch [2600][5500]\t Training Loss 0.4009\t Accuracy 0.8940\n",
      "Epoch [6][20]\t Batch [2650][5500]\t Training Loss 0.4008\t Accuracy 0.8944\n",
      "Epoch [6][20]\t Batch [2700][5500]\t Training Loss 0.4019\t Accuracy 0.8942\n",
      "Epoch [6][20]\t Batch [2750][5500]\t Training Loss 0.4021\t Accuracy 0.8940\n",
      "Epoch [6][20]\t Batch [2800][5500]\t Training Loss 0.4013\t Accuracy 0.8941\n",
      "Epoch [6][20]\t Batch [2850][5500]\t Training Loss 0.4006\t Accuracy 0.8945\n",
      "Epoch [6][20]\t Batch [2900][5500]\t Training Loss 0.4004\t Accuracy 0.8945\n",
      "Epoch [6][20]\t Batch [2950][5500]\t Training Loss 0.4009\t Accuracy 0.8942\n",
      "Epoch [6][20]\t Batch [3000][5500]\t Training Loss 0.4020\t Accuracy 0.8937\n",
      "Epoch [6][20]\t Batch [3050][5500]\t Training Loss 0.4028\t Accuracy 0.8934\n",
      "Epoch [6][20]\t Batch [3100][5500]\t Training Loss 0.4040\t Accuracy 0.8932\n",
      "Epoch [6][20]\t Batch [3150][5500]\t Training Loss 0.4055\t Accuracy 0.8928\n",
      "Epoch [6][20]\t Batch [3200][5500]\t Training Loss 0.4065\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [3250][5500]\t Training Loss 0.4081\t Accuracy 0.8914\n",
      "Epoch [6][20]\t Batch [3300][5500]\t Training Loss 0.4078\t Accuracy 0.8915\n",
      "Epoch [6][20]\t Batch [3350][5500]\t Training Loss 0.4081\t Accuracy 0.8914\n",
      "Epoch [6][20]\t Batch [3400][5500]\t Training Loss 0.4062\t Accuracy 0.8920\n",
      "Epoch [6][20]\t Batch [3450][5500]\t Training Loss 0.4054\t Accuracy 0.8924\n",
      "Epoch [6][20]\t Batch [3500][5500]\t Training Loss 0.4060\t Accuracy 0.8920\n",
      "Epoch [6][20]\t Batch [3550][5500]\t Training Loss 0.4054\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [3600][5500]\t Training Loss 0.4048\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [3650][5500]\t Training Loss 0.4046\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [3700][5500]\t Training Loss 0.4033\t Accuracy 0.8926\n",
      "Epoch [6][20]\t Batch [3750][5500]\t Training Loss 0.4050\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [3800][5500]\t Training Loss 0.4051\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [3850][5500]\t Training Loss 0.4046\t Accuracy 0.8923\n",
      "Epoch [6][20]\t Batch [3900][5500]\t Training Loss 0.4042\t Accuracy 0.8924\n",
      "Epoch [6][20]\t Batch [3950][5500]\t Training Loss 0.4044\t Accuracy 0.8923\n",
      "Epoch [6][20]\t Batch [4000][5500]\t Training Loss 0.4048\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [4050][5500]\t Training Loss 0.4042\t Accuracy 0.8924\n",
      "Epoch [6][20]\t Batch [4100][5500]\t Training Loss 0.4034\t Accuracy 0.8928\n",
      "Epoch [6][20]\t Batch [4150][5500]\t Training Loss 0.4042\t Accuracy 0.8926\n",
      "Epoch [6][20]\t Batch [4200][5500]\t Training Loss 0.4042\t Accuracy 0.8926\n",
      "Epoch [6][20]\t Batch [4250][5500]\t Training Loss 0.4055\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [4300][5500]\t Training Loss 0.4056\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [4350][5500]\t Training Loss 0.4047\t Accuracy 0.8923\n",
      "Epoch [6][20]\t Batch [4400][5500]\t Training Loss 0.4045\t Accuracy 0.8924\n",
      "Epoch [6][20]\t Batch [4450][5500]\t Training Loss 0.4050\t Accuracy 0.8923\n",
      "Epoch [6][20]\t Batch [4500][5500]\t Training Loss 0.4044\t Accuracy 0.8924\n",
      "Epoch [6][20]\t Batch [4550][5500]\t Training Loss 0.4047\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [4600][5500]\t Training Loss 0.4050\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [4650][5500]\t Training Loss 0.4057\t Accuracy 0.8919\n",
      "Epoch [6][20]\t Batch [4700][5500]\t Training Loss 0.4047\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [4750][5500]\t Training Loss 0.4048\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [4800][5500]\t Training Loss 0.4048\t Accuracy 0.8919\n",
      "Epoch [6][20]\t Batch [4850][5500]\t Training Loss 0.4039\t Accuracy 0.8922\n",
      "Epoch [6][20]\t Batch [4900][5500]\t Training Loss 0.4036\t Accuracy 0.8921\n",
      "Epoch [6][20]\t Batch [4950][5500]\t Training Loss 0.4040\t Accuracy 0.8919\n",
      "Epoch [6][20]\t Batch [5000][5500]\t Training Loss 0.4050\t Accuracy 0.8918\n",
      "Epoch [6][20]\t Batch [5050][5500]\t Training Loss 0.4058\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5100][5500]\t Training Loss 0.4056\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5150][5500]\t Training Loss 0.4051\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5200][5500]\t Training Loss 0.4044\t Accuracy 0.8919\n",
      "Epoch [6][20]\t Batch [5250][5500]\t Training Loss 0.4048\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5300][5500]\t Training Loss 0.4054\t Accuracy 0.8915\n",
      "Epoch [6][20]\t Batch [5350][5500]\t Training Loss 0.4049\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5400][5500]\t Training Loss 0.4049\t Accuracy 0.8917\n",
      "Epoch [6][20]\t Batch [5450][5500]\t Training Loss 0.4045\t Accuracy 0.8917\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4044\t Average training accuracy 0.8916\n",
      "Epoch [6]\t Average validation loss 0.3166\t Average validation accuracy 0.9236\n",
      "\n",
      "Epoch [7][20]\t Batch [0][5500]\t Training Loss 0.1347\t Accuracy 1.0000\n",
      "Epoch [7][20]\t Batch [50][5500]\t Training Loss 0.3716\t Accuracy 0.9020\n",
      "Epoch [7][20]\t Batch [100][5500]\t Training Loss 0.4076\t Accuracy 0.8871\n",
      "Epoch [7][20]\t Batch [150][5500]\t Training Loss 0.4192\t Accuracy 0.8881\n",
      "Epoch [7][20]\t Batch [200][5500]\t Training Loss 0.3937\t Accuracy 0.8965\n",
      "Epoch [7][20]\t Batch [250][5500]\t Training Loss 0.3801\t Accuracy 0.8980\n",
      "Epoch [7][20]\t Batch [300][5500]\t Training Loss 0.3731\t Accuracy 0.9003\n",
      "Epoch [7][20]\t Batch [350][5500]\t Training Loss 0.3697\t Accuracy 0.9017\n",
      "Epoch [7][20]\t Batch [400][5500]\t Training Loss 0.3682\t Accuracy 0.9032\n",
      "Epoch [7][20]\t Batch [450][5500]\t Training Loss 0.3672\t Accuracy 0.9040\n",
      "Epoch [7][20]\t Batch [500][5500]\t Training Loss 0.3648\t Accuracy 0.9038\n",
      "Epoch [7][20]\t Batch [550][5500]\t Training Loss 0.3662\t Accuracy 0.9038\n",
      "Epoch [7][20]\t Batch [600][5500]\t Training Loss 0.3650\t Accuracy 0.9043\n",
      "Epoch [7][20]\t Batch [650][5500]\t Training Loss 0.3609\t Accuracy 0.9058\n",
      "Epoch [7][20]\t Batch [700][5500]\t Training Loss 0.3604\t Accuracy 0.9047\n",
      "Epoch [7][20]\t Batch [750][5500]\t Training Loss 0.3643\t Accuracy 0.9040\n",
      "Epoch [7][20]\t Batch [800][5500]\t Training Loss 0.3675\t Accuracy 0.9031\n",
      "Epoch [7][20]\t Batch [850][5500]\t Training Loss 0.3711\t Accuracy 0.9020\n",
      "Epoch [7][20]\t Batch [900][5500]\t Training Loss 0.3783\t Accuracy 0.8994\n",
      "Epoch [7][20]\t Batch [950][5500]\t Training Loss 0.3780\t Accuracy 0.8997\n",
      "Epoch [7][20]\t Batch [1000][5500]\t Training Loss 0.3756\t Accuracy 0.9001\n",
      "Epoch [7][20]\t Batch [1050][5500]\t Training Loss 0.3734\t Accuracy 0.9004\n",
      "Epoch [7][20]\t Batch [1100][5500]\t Training Loss 0.3702\t Accuracy 0.9021\n",
      "Epoch [7][20]\t Batch [1150][5500]\t Training Loss 0.3681\t Accuracy 0.9027\n",
      "Epoch [7][20]\t Batch [1200][5500]\t Training Loss 0.3724\t Accuracy 0.9011\n",
      "Epoch [7][20]\t Batch [1250][5500]\t Training Loss 0.3734\t Accuracy 0.9005\n",
      "Epoch [7][20]\t Batch [1300][5500]\t Training Loss 0.3768\t Accuracy 0.8995\n",
      "Epoch [7][20]\t Batch [1350][5500]\t Training Loss 0.3786\t Accuracy 0.8987\n",
      "Epoch [7][20]\t Batch [1400][5500]\t Training Loss 0.3803\t Accuracy 0.8979\n",
      "Epoch [7][20]\t Batch [1450][5500]\t Training Loss 0.3838\t Accuracy 0.8972\n",
      "Epoch [7][20]\t Batch [1500][5500]\t Training Loss 0.3880\t Accuracy 0.8959\n",
      "Epoch [7][20]\t Batch [1550][5500]\t Training Loss 0.3866\t Accuracy 0.8968\n",
      "Epoch [7][20]\t Batch [1600][5500]\t Training Loss 0.3882\t Accuracy 0.8961\n",
      "Epoch [7][20]\t Batch [1650][5500]\t Training Loss 0.3871\t Accuracy 0.8965\n",
      "Epoch [7][20]\t Batch [1700][5500]\t Training Loss 0.3881\t Accuracy 0.8961\n",
      "Epoch [7][20]\t Batch [1750][5500]\t Training Loss 0.3886\t Accuracy 0.8957\n",
      "Epoch [7][20]\t Batch [1800][5500]\t Training Loss 0.3907\t Accuracy 0.8950\n",
      "Epoch [7][20]\t Batch [1850][5500]\t Training Loss 0.3884\t Accuracy 0.8959\n",
      "Epoch [7][20]\t Batch [1900][5500]\t Training Loss 0.3865\t Accuracy 0.8967\n",
      "Epoch [7][20]\t Batch [1950][5500]\t Training Loss 0.3863\t Accuracy 0.8967\n",
      "Epoch [7][20]\t Batch [2000][5500]\t Training Loss 0.3848\t Accuracy 0.8971\n",
      "Epoch [7][20]\t Batch [2050][5500]\t Training Loss 0.3841\t Accuracy 0.8976\n",
      "Epoch [7][20]\t Batch [2100][5500]\t Training Loss 0.3862\t Accuracy 0.8969\n",
      "Epoch [7][20]\t Batch [2150][5500]\t Training Loss 0.3847\t Accuracy 0.8976\n",
      "Epoch [7][20]\t Batch [2200][5500]\t Training Loss 0.3830\t Accuracy 0.8983\n",
      "Epoch [7][20]\t Batch [2250][5500]\t Training Loss 0.3830\t Accuracy 0.8981\n",
      "Epoch [7][20]\t Batch [2300][5500]\t Training Loss 0.3829\t Accuracy 0.8978\n",
      "Epoch [7][20]\t Batch [2350][5500]\t Training Loss 0.3824\t Accuracy 0.8977\n",
      "Epoch [7][20]\t Batch [2400][5500]\t Training Loss 0.3827\t Accuracy 0.8978\n",
      "Epoch [7][20]\t Batch [2450][5500]\t Training Loss 0.3824\t Accuracy 0.8977\n",
      "Epoch [7][20]\t Batch [2500][5500]\t Training Loss 0.3839\t Accuracy 0.8972\n",
      "Epoch [7][20]\t Batch [2550][5500]\t Training Loss 0.3828\t Accuracy 0.8973\n",
      "Epoch [7][20]\t Batch [2600][5500]\t Training Loss 0.3819\t Accuracy 0.8975\n",
      "Epoch [7][20]\t Batch [2650][5500]\t Training Loss 0.3819\t Accuracy 0.8979\n",
      "Epoch [7][20]\t Batch [2700][5500]\t Training Loss 0.3831\t Accuracy 0.8976\n",
      "Epoch [7][20]\t Batch [2750][5500]\t Training Loss 0.3833\t Accuracy 0.8975\n",
      "Epoch [7][20]\t Batch [2800][5500]\t Training Loss 0.3825\t Accuracy 0.8976\n",
      "Epoch [7][20]\t Batch [2850][5500]\t Training Loss 0.3818\t Accuracy 0.8979\n",
      "Epoch [7][20]\t Batch [2900][5500]\t Training Loss 0.3817\t Accuracy 0.8980\n",
      "Epoch [7][20]\t Batch [2950][5500]\t Training Loss 0.3821\t Accuracy 0.8977\n",
      "Epoch [7][20]\t Batch [3000][5500]\t Training Loss 0.3833\t Accuracy 0.8972\n",
      "Epoch [7][20]\t Batch [3050][5500]\t Training Loss 0.3839\t Accuracy 0.8968\n",
      "Epoch [7][20]\t Batch [3100][5500]\t Training Loss 0.3851\t Accuracy 0.8967\n",
      "Epoch [7][20]\t Batch [3150][5500]\t Training Loss 0.3867\t Accuracy 0.8963\n",
      "Epoch [7][20]\t Batch [3200][5500]\t Training Loss 0.3876\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [3250][5500]\t Training Loss 0.3893\t Accuracy 0.8949\n",
      "Epoch [7][20]\t Batch [3300][5500]\t Training Loss 0.3890\t Accuracy 0.8949\n",
      "Epoch [7][20]\t Batch [3350][5500]\t Training Loss 0.3893\t Accuracy 0.8948\n",
      "Epoch [7][20]\t Batch [3400][5500]\t Training Loss 0.3874\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [3450][5500]\t Training Loss 0.3866\t Accuracy 0.8957\n",
      "Epoch [7][20]\t Batch [3500][5500]\t Training Loss 0.3872\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [3550][5500]\t Training Loss 0.3866\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [3600][5500]\t Training Loss 0.3860\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [3650][5500]\t Training Loss 0.3858\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [3700][5500]\t Training Loss 0.3846\t Accuracy 0.8960\n",
      "Epoch [7][20]\t Batch [3750][5500]\t Training Loss 0.3864\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [3800][5500]\t Training Loss 0.3865\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [3850][5500]\t Training Loss 0.3860\t Accuracy 0.8957\n",
      "Epoch [7][20]\t Batch [3900][5500]\t Training Loss 0.3856\t Accuracy 0.8957\n",
      "Epoch [7][20]\t Batch [3950][5500]\t Training Loss 0.3859\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [4000][5500]\t Training Loss 0.3863\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [4050][5500]\t Training Loss 0.3857\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [4100][5500]\t Training Loss 0.3850\t Accuracy 0.8960\n",
      "Epoch [7][20]\t Batch [4150][5500]\t Training Loss 0.3857\t Accuracy 0.8958\n",
      "Epoch [7][20]\t Batch [4200][5500]\t Training Loss 0.3857\t Accuracy 0.8957\n",
      "Epoch [7][20]\t Batch [4250][5500]\t Training Loss 0.3871\t Accuracy 0.8953\n",
      "Epoch [7][20]\t Batch [4300][5500]\t Training Loss 0.3872\t Accuracy 0.8953\n",
      "Epoch [7][20]\t Batch [4350][5500]\t Training Loss 0.3863\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [4400][5500]\t Training Loss 0.3862\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [4450][5500]\t Training Loss 0.3866\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [4500][5500]\t Training Loss 0.3860\t Accuracy 0.8956\n",
      "Epoch [7][20]\t Batch [4550][5500]\t Training Loss 0.3864\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [4600][5500]\t Training Loss 0.3867\t Accuracy 0.8952\n",
      "Epoch [7][20]\t Batch [4650][5500]\t Training Loss 0.3874\t Accuracy 0.8951\n",
      "Epoch [7][20]\t Batch [4700][5500]\t Training Loss 0.3865\t Accuracy 0.8954\n",
      "Epoch [7][20]\t Batch [4750][5500]\t Training Loss 0.3866\t Accuracy 0.8953\n",
      "Epoch [7][20]\t Batch [4800][5500]\t Training Loss 0.3866\t Accuracy 0.8952\n",
      "Epoch [7][20]\t Batch [4850][5500]\t Training Loss 0.3858\t Accuracy 0.8955\n",
      "Epoch [7][20]\t Batch [4900][5500]\t Training Loss 0.3854\t Accuracy 0.8953\n",
      "Epoch [7][20]\t Batch [4950][5500]\t Training Loss 0.3859\t Accuracy 0.8952\n",
      "Epoch [7][20]\t Batch [5000][5500]\t Training Loss 0.3869\t Accuracy 0.8951\n",
      "Epoch [7][20]\t Batch [5050][5500]\t Training Loss 0.3877\t Accuracy 0.8950\n",
      "Epoch [7][20]\t Batch [5100][5500]\t Training Loss 0.3876\t Accuracy 0.8949\n",
      "Epoch [7][20]\t Batch [5150][5500]\t Training Loss 0.3871\t Accuracy 0.8950\n",
      "Epoch [7][20]\t Batch [5200][5500]\t Training Loss 0.3864\t Accuracy 0.8952\n",
      "Epoch [7][20]\t Batch [5250][5500]\t Training Loss 0.3868\t Accuracy 0.8950\n",
      "Epoch [7][20]\t Batch [5300][5500]\t Training Loss 0.3874\t Accuracy 0.8948\n",
      "Epoch [7][20]\t Batch [5350][5500]\t Training Loss 0.3869\t Accuracy 0.8950\n",
      "Epoch [7][20]\t Batch [5400][5500]\t Training Loss 0.3870\t Accuracy 0.8949\n",
      "Epoch [7][20]\t Batch [5450][5500]\t Training Loss 0.3866\t Accuracy 0.8950\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3866\t Average training accuracy 0.8949\n",
      "Epoch [7]\t Average validation loss 0.3027\t Average validation accuracy 0.9238\n",
      "\n",
      "Epoch [8][20]\t Batch [0][5500]\t Training Loss 0.1202\t Accuracy 1.0000\n",
      "Epoch [8][20]\t Batch [50][5500]\t Training Loss 0.3564\t Accuracy 0.9078\n",
      "Epoch [8][20]\t Batch [100][5500]\t Training Loss 0.3919\t Accuracy 0.8921\n",
      "Epoch [8][20]\t Batch [150][5500]\t Training Loss 0.4043\t Accuracy 0.8907\n",
      "Epoch [8][20]\t Batch [200][5500]\t Training Loss 0.3787\t Accuracy 0.8985\n",
      "Epoch [8][20]\t Batch [250][5500]\t Training Loss 0.3650\t Accuracy 0.9008\n",
      "Epoch [8][20]\t Batch [300][5500]\t Training Loss 0.3578\t Accuracy 0.9043\n",
      "Epoch [8][20]\t Batch [350][5500]\t Training Loss 0.3540\t Accuracy 0.9060\n",
      "Epoch [8][20]\t Batch [400][5500]\t Training Loss 0.3526\t Accuracy 0.9077\n",
      "Epoch [8][20]\t Batch [450][5500]\t Training Loss 0.3517\t Accuracy 0.9080\n",
      "Epoch [8][20]\t Batch [500][5500]\t Training Loss 0.3493\t Accuracy 0.9078\n",
      "Epoch [8][20]\t Batch [550][5500]\t Training Loss 0.3508\t Accuracy 0.9078\n",
      "Epoch [8][20]\t Batch [600][5500]\t Training Loss 0.3497\t Accuracy 0.9082\n",
      "Epoch [8][20]\t Batch [650][5500]\t Training Loss 0.3457\t Accuracy 0.9095\n",
      "Epoch [8][20]\t Batch [700][5500]\t Training Loss 0.3452\t Accuracy 0.9087\n",
      "Epoch [8][20]\t Batch [750][5500]\t Training Loss 0.3490\t Accuracy 0.9081\n",
      "Epoch [8][20]\t Batch [800][5500]\t Training Loss 0.3521\t Accuracy 0.9072\n",
      "Epoch [8][20]\t Batch [850][5500]\t Training Loss 0.3556\t Accuracy 0.9062\n",
      "Epoch [8][20]\t Batch [900][5500]\t Training Loss 0.3629\t Accuracy 0.9038\n",
      "Epoch [8][20]\t Batch [950][5500]\t Training Loss 0.3629\t Accuracy 0.9039\n",
      "Epoch [8][20]\t Batch [1000][5500]\t Training Loss 0.3605\t Accuracy 0.9044\n",
      "Epoch [8][20]\t Batch [1050][5500]\t Training Loss 0.3584\t Accuracy 0.9046\n",
      "Epoch [8][20]\t Batch [1100][5500]\t Training Loss 0.3553\t Accuracy 0.9063\n",
      "Epoch [8][20]\t Batch [1150][5500]\t Training Loss 0.3532\t Accuracy 0.9070\n",
      "Epoch [8][20]\t Batch [1200][5500]\t Training Loss 0.3575\t Accuracy 0.9054\n",
      "Epoch [8][20]\t Batch [1250][5500]\t Training Loss 0.3584\t Accuracy 0.9047\n",
      "Epoch [8][20]\t Batch [1300][5500]\t Training Loss 0.3619\t Accuracy 0.9036\n",
      "Epoch [8][20]\t Batch [1350][5500]\t Training Loss 0.3637\t Accuracy 0.9030\n",
      "Epoch [8][20]\t Batch [1400][5500]\t Training Loss 0.3654\t Accuracy 0.9021\n",
      "Epoch [8][20]\t Batch [1450][5500]\t Training Loss 0.3688\t Accuracy 0.9012\n",
      "Epoch [8][20]\t Batch [1500][5500]\t Training Loss 0.3730\t Accuracy 0.9001\n",
      "Epoch [8][20]\t Batch [1550][5500]\t Training Loss 0.3717\t Accuracy 0.9008\n",
      "Epoch [8][20]\t Batch [1600][5500]\t Training Loss 0.3732\t Accuracy 0.9001\n",
      "Epoch [8][20]\t Batch [1650][5500]\t Training Loss 0.3721\t Accuracy 0.9004\n",
      "Epoch [8][20]\t Batch [1700][5500]\t Training Loss 0.3731\t Accuracy 0.8999\n",
      "Epoch [8][20]\t Batch [1750][5500]\t Training Loss 0.3736\t Accuracy 0.8995\n",
      "Epoch [8][20]\t Batch [1800][5500]\t Training Loss 0.3755\t Accuracy 0.8987\n",
      "Epoch [8][20]\t Batch [1850][5500]\t Training Loss 0.3733\t Accuracy 0.8995\n",
      "Epoch [8][20]\t Batch [1900][5500]\t Training Loss 0.3714\t Accuracy 0.9003\n",
      "Epoch [8][20]\t Batch [1950][5500]\t Training Loss 0.3712\t Accuracy 0.9003\n",
      "Epoch [8][20]\t Batch [2000][5500]\t Training Loss 0.3697\t Accuracy 0.9007\n",
      "Epoch [8][20]\t Batch [2050][5500]\t Training Loss 0.3690\t Accuracy 0.9012\n",
      "Epoch [8][20]\t Batch [2100][5500]\t Training Loss 0.3712\t Accuracy 0.9005\n",
      "Epoch [8][20]\t Batch [2150][5500]\t Training Loss 0.3697\t Accuracy 0.9013\n",
      "Epoch [8][20]\t Batch [2200][5500]\t Training Loss 0.3681\t Accuracy 0.9019\n",
      "Epoch [8][20]\t Batch [2250][5500]\t Training Loss 0.3680\t Accuracy 0.9019\n",
      "Epoch [8][20]\t Batch [2300][5500]\t Training Loss 0.3680\t Accuracy 0.9015\n",
      "Epoch [8][20]\t Batch [2350][5500]\t Training Loss 0.3675\t Accuracy 0.9015\n",
      "Epoch [8][20]\t Batch [2400][5500]\t Training Loss 0.3678\t Accuracy 0.9015\n",
      "Epoch [8][20]\t Batch [2450][5500]\t Training Loss 0.3675\t Accuracy 0.9013\n",
      "Epoch [8][20]\t Batch [2500][5500]\t Training Loss 0.3690\t Accuracy 0.9008\n",
      "Epoch [8][20]\t Batch [2550][5500]\t Training Loss 0.3679\t Accuracy 0.9009\n",
      "Epoch [8][20]\t Batch [2600][5500]\t Training Loss 0.3671\t Accuracy 0.9012\n",
      "Epoch [8][20]\t Batch [2650][5500]\t Training Loss 0.3671\t Accuracy 0.9015\n",
      "Epoch [8][20]\t Batch [2700][5500]\t Training Loss 0.3683\t Accuracy 0.9013\n",
      "Epoch [8][20]\t Batch [2750][5500]\t Training Loss 0.3685\t Accuracy 0.9011\n",
      "Epoch [8][20]\t Batch [2800][5500]\t Training Loss 0.3677\t Accuracy 0.9012\n",
      "Epoch [8][20]\t Batch [2850][5500]\t Training Loss 0.3671\t Accuracy 0.9015\n",
      "Epoch [8][20]\t Batch [2900][5500]\t Training Loss 0.3670\t Accuracy 0.9016\n",
      "Epoch [8][20]\t Batch [2950][5500]\t Training Loss 0.3674\t Accuracy 0.9013\n",
      "Epoch [8][20]\t Batch [3000][5500]\t Training Loss 0.3686\t Accuracy 0.9008\n",
      "Epoch [8][20]\t Batch [3050][5500]\t Training Loss 0.3691\t Accuracy 0.9004\n",
      "Epoch [8][20]\t Batch [3100][5500]\t Training Loss 0.3703\t Accuracy 0.9001\n",
      "Epoch [8][20]\t Batch [3150][5500]\t Training Loss 0.3719\t Accuracy 0.8997\n",
      "Epoch [8][20]\t Batch [3200][5500]\t Training Loss 0.3729\t Accuracy 0.8990\n",
      "Epoch [8][20]\t Batch [3250][5500]\t Training Loss 0.3744\t Accuracy 0.8982\n",
      "Epoch [8][20]\t Batch [3300][5500]\t Training Loss 0.3742\t Accuracy 0.8984\n",
      "Epoch [8][20]\t Batch [3350][5500]\t Training Loss 0.3744\t Accuracy 0.8983\n",
      "Epoch [8][20]\t Batch [3400][5500]\t Training Loss 0.3726\t Accuracy 0.8988\n",
      "Epoch [8][20]\t Batch [3450][5500]\t Training Loss 0.3718\t Accuracy 0.8992\n",
      "Epoch [8][20]\t Batch [3500][5500]\t Training Loss 0.3725\t Accuracy 0.8988\n",
      "Epoch [8][20]\t Batch [3550][5500]\t Training Loss 0.3718\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [3600][5500]\t Training Loss 0.3713\t Accuracy 0.8990\n",
      "Epoch [8][20]\t Batch [3650][5500]\t Training Loss 0.3710\t Accuracy 0.8991\n",
      "Epoch [8][20]\t Batch [3700][5500]\t Training Loss 0.3699\t Accuracy 0.8994\n",
      "Epoch [8][20]\t Batch [3750][5500]\t Training Loss 0.3717\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [3800][5500]\t Training Loss 0.3719\t Accuracy 0.8988\n",
      "Epoch [8][20]\t Batch [3850][5500]\t Training Loss 0.3714\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [3900][5500]\t Training Loss 0.3710\t Accuracy 0.8990\n",
      "Epoch [8][20]\t Batch [3950][5500]\t Training Loss 0.3714\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [4000][5500]\t Training Loss 0.3717\t Accuracy 0.8987\n",
      "Epoch [8][20]\t Batch [4050][5500]\t Training Loss 0.3711\t Accuracy 0.8988\n",
      "Epoch [8][20]\t Batch [4100][5500]\t Training Loss 0.3704\t Accuracy 0.8992\n",
      "Epoch [8][20]\t Batch [4150][5500]\t Training Loss 0.3711\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [4200][5500]\t Training Loss 0.3712\t Accuracy 0.8989\n",
      "Epoch [8][20]\t Batch [4250][5500]\t Training Loss 0.3725\t Accuracy 0.8984\n",
      "Epoch [8][20]\t Batch [4300][5500]\t Training Loss 0.3727\t Accuracy 0.8984\n",
      "Epoch [8][20]\t Batch [4350][5500]\t Training Loss 0.3718\t Accuracy 0.8986\n",
      "Epoch [8][20]\t Batch [4400][5500]\t Training Loss 0.3717\t Accuracy 0.8986\n",
      "Epoch [8][20]\t Batch [4450][5500]\t Training Loss 0.3721\t Accuracy 0.8986\n",
      "Epoch [8][20]\t Batch [4500][5500]\t Training Loss 0.3716\t Accuracy 0.8987\n",
      "Epoch [8][20]\t Batch [4550][5500]\t Training Loss 0.3719\t Accuracy 0.8985\n",
      "Epoch [8][20]\t Batch [4600][5500]\t Training Loss 0.3723\t Accuracy 0.8983\n",
      "Epoch [8][20]\t Batch [4650][5500]\t Training Loss 0.3730\t Accuracy 0.8982\n",
      "Epoch [8][20]\t Batch [4700][5500]\t Training Loss 0.3721\t Accuracy 0.8984\n",
      "Epoch [8][20]\t Batch [4750][5500]\t Training Loss 0.3722\t Accuracy 0.8983\n",
      "Epoch [8][20]\t Batch [4800][5500]\t Training Loss 0.3723\t Accuracy 0.8983\n",
      "Epoch [8][20]\t Batch [4850][5500]\t Training Loss 0.3714\t Accuracy 0.8986\n",
      "Epoch [8][20]\t Batch [4900][5500]\t Training Loss 0.3711\t Accuracy 0.8984\n",
      "Epoch [8][20]\t Batch [4950][5500]\t Training Loss 0.3716\t Accuracy 0.8983\n",
      "Epoch [8][20]\t Batch [5000][5500]\t Training Loss 0.3726\t Accuracy 0.8982\n",
      "Epoch [8][20]\t Batch [5050][5500]\t Training Loss 0.3734\t Accuracy 0.8981\n",
      "Epoch [8][20]\t Batch [5100][5500]\t Training Loss 0.3733\t Accuracy 0.8980\n",
      "Epoch [8][20]\t Batch [5150][5500]\t Training Loss 0.3728\t Accuracy 0.8980\n",
      "Epoch [8][20]\t Batch [5200][5500]\t Training Loss 0.3722\t Accuracy 0.8982\n",
      "Epoch [8][20]\t Batch [5250][5500]\t Training Loss 0.3726\t Accuracy 0.8981\n",
      "Epoch [8][20]\t Batch [5300][5500]\t Training Loss 0.3733\t Accuracy 0.8979\n",
      "Epoch [8][20]\t Batch [5350][5500]\t Training Loss 0.3727\t Accuracy 0.8981\n",
      "Epoch [8][20]\t Batch [5400][5500]\t Training Loss 0.3729\t Accuracy 0.8980\n",
      "Epoch [8][20]\t Batch [5450][5500]\t Training Loss 0.3725\t Accuracy 0.8980\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3725\t Average training accuracy 0.8979\n",
      "Epoch [8]\t Average validation loss 0.2918\t Average validation accuracy 0.9260\n",
      "\n",
      "Epoch [9][20]\t Batch [0][5500]\t Training Loss 0.1086\t Accuracy 1.0000\n",
      "Epoch [9][20]\t Batch [50][5500]\t Training Loss 0.3441\t Accuracy 0.9078\n",
      "Epoch [9][20]\t Batch [100][5500]\t Training Loss 0.3791\t Accuracy 0.8931\n",
      "Epoch [9][20]\t Batch [150][5500]\t Training Loss 0.3924\t Accuracy 0.8921\n",
      "Epoch [9][20]\t Batch [200][5500]\t Training Loss 0.3667\t Accuracy 0.9000\n",
      "Epoch [9][20]\t Batch [250][5500]\t Training Loss 0.3528\t Accuracy 0.9028\n",
      "Epoch [9][20]\t Batch [300][5500]\t Training Loss 0.3456\t Accuracy 0.9063\n",
      "Epoch [9][20]\t Batch [350][5500]\t Training Loss 0.3415\t Accuracy 0.9080\n",
      "Epoch [9][20]\t Batch [400][5500]\t Training Loss 0.3400\t Accuracy 0.9102\n",
      "Epoch [9][20]\t Batch [450][5500]\t Training Loss 0.3392\t Accuracy 0.9102\n",
      "Epoch [9][20]\t Batch [500][5500]\t Training Loss 0.3368\t Accuracy 0.9098\n",
      "Epoch [9][20]\t Batch [550][5500]\t Training Loss 0.3384\t Accuracy 0.9096\n",
      "Epoch [9][20]\t Batch [600][5500]\t Training Loss 0.3374\t Accuracy 0.9098\n",
      "Epoch [9][20]\t Batch [650][5500]\t Training Loss 0.3334\t Accuracy 0.9112\n",
      "Epoch [9][20]\t Batch [700][5500]\t Training Loss 0.3330\t Accuracy 0.9104\n",
      "Epoch [9][20]\t Batch [750][5500]\t Training Loss 0.3366\t Accuracy 0.9101\n",
      "Epoch [9][20]\t Batch [800][5500]\t Training Loss 0.3396\t Accuracy 0.9096\n",
      "Epoch [9][20]\t Batch [850][5500]\t Training Loss 0.3432\t Accuracy 0.9085\n",
      "Epoch [9][20]\t Batch [900][5500]\t Training Loss 0.3506\t Accuracy 0.9061\n",
      "Epoch [9][20]\t Batch [950][5500]\t Training Loss 0.3507\t Accuracy 0.9061\n",
      "Epoch [9][20]\t Batch [1000][5500]\t Training Loss 0.3483\t Accuracy 0.9065\n",
      "Epoch [9][20]\t Batch [1050][5500]\t Training Loss 0.3463\t Accuracy 0.9068\n",
      "Epoch [9][20]\t Batch [1100][5500]\t Training Loss 0.3432\t Accuracy 0.9084\n",
      "Epoch [9][20]\t Batch [1150][5500]\t Training Loss 0.3411\t Accuracy 0.9092\n",
      "Epoch [9][20]\t Batch [1200][5500]\t Training Loss 0.3455\t Accuracy 0.9078\n",
      "Epoch [9][20]\t Batch [1250][5500]\t Training Loss 0.3464\t Accuracy 0.9071\n",
      "Epoch [9][20]\t Batch [1300][5500]\t Training Loss 0.3499\t Accuracy 0.9059\n",
      "Epoch [9][20]\t Batch [1350][5500]\t Training Loss 0.3517\t Accuracy 0.9053\n",
      "Epoch [9][20]\t Batch [1400][5500]\t Training Loss 0.3534\t Accuracy 0.9046\n",
      "Epoch [9][20]\t Batch [1450][5500]\t Training Loss 0.3568\t Accuracy 0.9039\n",
      "Epoch [9][20]\t Batch [1500][5500]\t Training Loss 0.3608\t Accuracy 0.9027\n",
      "Epoch [9][20]\t Batch [1550][5500]\t Training Loss 0.3596\t Accuracy 0.9034\n",
      "Epoch [9][20]\t Batch [1600][5500]\t Training Loss 0.3610\t Accuracy 0.9028\n",
      "Epoch [9][20]\t Batch [1650][5500]\t Training Loss 0.3600\t Accuracy 0.9031\n",
      "Epoch [9][20]\t Batch [1700][5500]\t Training Loss 0.3609\t Accuracy 0.9025\n",
      "Epoch [9][20]\t Batch [1750][5500]\t Training Loss 0.3614\t Accuracy 0.9023\n",
      "Epoch [9][20]\t Batch [1800][5500]\t Training Loss 0.3632\t Accuracy 0.9013\n",
      "Epoch [9][20]\t Batch [1850][5500]\t Training Loss 0.3610\t Accuracy 0.9021\n",
      "Epoch [9][20]\t Batch [1900][5500]\t Training Loss 0.3592\t Accuracy 0.9029\n",
      "Epoch [9][20]\t Batch [1950][5500]\t Training Loss 0.3589\t Accuracy 0.9030\n",
      "Epoch [9][20]\t Batch [2000][5500]\t Training Loss 0.3574\t Accuracy 0.9034\n",
      "Epoch [9][20]\t Batch [2050][5500]\t Training Loss 0.3567\t Accuracy 0.9039\n",
      "Epoch [9][20]\t Batch [2100][5500]\t Training Loss 0.3590\t Accuracy 0.9032\n",
      "Epoch [9][20]\t Batch [2150][5500]\t Training Loss 0.3575\t Accuracy 0.9040\n",
      "Epoch [9][20]\t Batch [2200][5500]\t Training Loss 0.3560\t Accuracy 0.9046\n",
      "Epoch [9][20]\t Batch [2250][5500]\t Training Loss 0.3559\t Accuracy 0.9046\n",
      "Epoch [9][20]\t Batch [2300][5500]\t Training Loss 0.3559\t Accuracy 0.9042\n",
      "Epoch [9][20]\t Batch [2350][5500]\t Training Loss 0.3554\t Accuracy 0.9043\n",
      "Epoch [9][20]\t Batch [2400][5500]\t Training Loss 0.3557\t Accuracy 0.9042\n",
      "Epoch [9][20]\t Batch [2450][5500]\t Training Loss 0.3554\t Accuracy 0.9041\n",
      "Epoch [9][20]\t Batch [2500][5500]\t Training Loss 0.3569\t Accuracy 0.9035\n",
      "Epoch [9][20]\t Batch [2550][5500]\t Training Loss 0.3558\t Accuracy 0.9036\n",
      "Epoch [9][20]\t Batch [2600][5500]\t Training Loss 0.3551\t Accuracy 0.9039\n",
      "Epoch [9][20]\t Batch [2650][5500]\t Training Loss 0.3550\t Accuracy 0.9042\n",
      "Epoch [9][20]\t Batch [2700][5500]\t Training Loss 0.3563\t Accuracy 0.9039\n",
      "Epoch [9][20]\t Batch [2750][5500]\t Training Loss 0.3565\t Accuracy 0.9037\n",
      "Epoch [9][20]\t Batch [2800][5500]\t Training Loss 0.3558\t Accuracy 0.9038\n",
      "Epoch [9][20]\t Batch [2850][5500]\t Training Loss 0.3552\t Accuracy 0.9041\n",
      "Epoch [9][20]\t Batch [2900][5500]\t Training Loss 0.3551\t Accuracy 0.9041\n",
      "Epoch [9][20]\t Batch [2950][5500]\t Training Loss 0.3555\t Accuracy 0.9039\n",
      "Epoch [9][20]\t Batch [3000][5500]\t Training Loss 0.3566\t Accuracy 0.9033\n",
      "Epoch [9][20]\t Batch [3050][5500]\t Training Loss 0.3571\t Accuracy 0.9029\n",
      "Epoch [9][20]\t Batch [3100][5500]\t Training Loss 0.3583\t Accuracy 0.9026\n",
      "Epoch [9][20]\t Batch [3150][5500]\t Training Loss 0.3599\t Accuracy 0.9022\n",
      "Epoch [9][20]\t Batch [3200][5500]\t Training Loss 0.3608\t Accuracy 0.9015\n",
      "Epoch [9][20]\t Batch [3250][5500]\t Training Loss 0.3623\t Accuracy 0.9008\n",
      "Epoch [9][20]\t Batch [3300][5500]\t Training Loss 0.3622\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [3350][5500]\t Training Loss 0.3624\t Accuracy 0.9008\n",
      "Epoch [9][20]\t Batch [3400][5500]\t Training Loss 0.3606\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [3450][5500]\t Training Loss 0.3598\t Accuracy 0.9017\n",
      "Epoch [9][20]\t Batch [3500][5500]\t Training Loss 0.3604\t Accuracy 0.9013\n",
      "Epoch [9][20]\t Batch [3550][5500]\t Training Loss 0.3598\t Accuracy 0.9015\n",
      "Epoch [9][20]\t Batch [3600][5500]\t Training Loss 0.3592\t Accuracy 0.9016\n",
      "Epoch [9][20]\t Batch [3650][5500]\t Training Loss 0.3590\t Accuracy 0.9017\n",
      "Epoch [9][20]\t Batch [3700][5500]\t Training Loss 0.3579\t Accuracy 0.9020\n",
      "Epoch [9][20]\t Batch [3750][5500]\t Training Loss 0.3598\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [3800][5500]\t Training Loss 0.3600\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [3850][5500]\t Training Loss 0.3594\t Accuracy 0.9015\n",
      "Epoch [9][20]\t Batch [3900][5500]\t Training Loss 0.3591\t Accuracy 0.9015\n",
      "Epoch [9][20]\t Batch [3950][5500]\t Training Loss 0.3595\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [4000][5500]\t Training Loss 0.3598\t Accuracy 0.9012\n",
      "Epoch [9][20]\t Batch [4050][5500]\t Training Loss 0.3592\t Accuracy 0.9013\n",
      "Epoch [9][20]\t Batch [4100][5500]\t Training Loss 0.3585\t Accuracy 0.9017\n",
      "Epoch [9][20]\t Batch [4150][5500]\t Training Loss 0.3593\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [4200][5500]\t Training Loss 0.3593\t Accuracy 0.9014\n",
      "Epoch [9][20]\t Batch [4250][5500]\t Training Loss 0.3606\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [4300][5500]\t Training Loss 0.3608\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [4350][5500]\t Training Loss 0.3600\t Accuracy 0.9011\n",
      "Epoch [9][20]\t Batch [4400][5500]\t Training Loss 0.3599\t Accuracy 0.9012\n",
      "Epoch [9][20]\t Batch [4450][5500]\t Training Loss 0.3603\t Accuracy 0.9011\n",
      "Epoch [9][20]\t Batch [4500][5500]\t Training Loss 0.3598\t Accuracy 0.9012\n",
      "Epoch [9][20]\t Batch [4550][5500]\t Training Loss 0.3602\t Accuracy 0.9010\n",
      "Epoch [9][20]\t Batch [4600][5500]\t Training Loss 0.3605\t Accuracy 0.9008\n",
      "Epoch [9][20]\t Batch [4650][5500]\t Training Loss 0.3613\t Accuracy 0.9008\n",
      "Epoch [9][20]\t Batch [4700][5500]\t Training Loss 0.3604\t Accuracy 0.9010\n",
      "Epoch [9][20]\t Batch [4750][5500]\t Training Loss 0.3605\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [4800][5500]\t Training Loss 0.3605\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [4850][5500]\t Training Loss 0.3597\t Accuracy 0.9012\n",
      "Epoch [9][20]\t Batch [4900][5500]\t Training Loss 0.3594\t Accuracy 0.9011\n",
      "Epoch [9][20]\t Batch [4950][5500]\t Training Loss 0.3599\t Accuracy 0.9010\n",
      "Epoch [9][20]\t Batch [5000][5500]\t Training Loss 0.3609\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [5050][5500]\t Training Loss 0.3617\t Accuracy 0.9007\n",
      "Epoch [9][20]\t Batch [5100][5500]\t Training Loss 0.3616\t Accuracy 0.9007\n",
      "Epoch [9][20]\t Batch [5150][5500]\t Training Loss 0.3612\t Accuracy 0.9006\n",
      "Epoch [9][20]\t Batch [5200][5500]\t Training Loss 0.3606\t Accuracy 0.9009\n",
      "Epoch [9][20]\t Batch [5250][5500]\t Training Loss 0.3610\t Accuracy 0.9007\n",
      "Epoch [9][20]\t Batch [5300][5500]\t Training Loss 0.3616\t Accuracy 0.9005\n",
      "Epoch [9][20]\t Batch [5350][5500]\t Training Loss 0.3611\t Accuracy 0.9007\n",
      "Epoch [9][20]\t Batch [5400][5500]\t Training Loss 0.3613\t Accuracy 0.9005\n",
      "Epoch [9][20]\t Batch [5450][5500]\t Training Loss 0.3609\t Accuracy 0.9006\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3609\t Average training accuracy 0.9005\n",
      "Epoch [9]\t Average validation loss 0.2829\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [10][20]\t Batch [0][5500]\t Training Loss 0.0992\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [50][5500]\t Training Loss 0.3339\t Accuracy 0.9157\n",
      "Epoch [10][20]\t Batch [100][5500]\t Training Loss 0.3684\t Accuracy 0.8980\n",
      "Epoch [10][20]\t Batch [150][5500]\t Training Loss 0.3826\t Accuracy 0.8947\n",
      "Epoch [10][20]\t Batch [200][5500]\t Training Loss 0.3568\t Accuracy 0.9020\n",
      "Epoch [10][20]\t Batch [250][5500]\t Training Loss 0.3428\t Accuracy 0.9044\n",
      "Epoch [10][20]\t Batch [300][5500]\t Training Loss 0.3355\t Accuracy 0.9090\n",
      "Epoch [10][20]\t Batch [350][5500]\t Training Loss 0.3311\t Accuracy 0.9100\n",
      "Epoch [10][20]\t Batch [400][5500]\t Training Loss 0.3295\t Accuracy 0.9120\n",
      "Epoch [10][20]\t Batch [450][5500]\t Training Loss 0.3288\t Accuracy 0.9120\n",
      "Epoch [10][20]\t Batch [500][5500]\t Training Loss 0.3264\t Accuracy 0.9116\n",
      "Epoch [10][20]\t Batch [550][5500]\t Training Loss 0.3282\t Accuracy 0.9114\n",
      "Epoch [10][20]\t Batch [600][5500]\t Training Loss 0.3271\t Accuracy 0.9120\n",
      "Epoch [10][20]\t Batch [650][5500]\t Training Loss 0.3233\t Accuracy 0.9132\n",
      "Epoch [10][20]\t Batch [700][5500]\t Training Loss 0.3229\t Accuracy 0.9128\n",
      "Epoch [10][20]\t Batch [750][5500]\t Training Loss 0.3264\t Accuracy 0.9125\n",
      "Epoch [10][20]\t Batch [800][5500]\t Training Loss 0.3293\t Accuracy 0.9121\n",
      "Epoch [10][20]\t Batch [850][5500]\t Training Loss 0.3329\t Accuracy 0.9109\n",
      "Epoch [10][20]\t Batch [900][5500]\t Training Loss 0.3403\t Accuracy 0.9087\n",
      "Epoch [10][20]\t Batch [950][5500]\t Training Loss 0.3405\t Accuracy 0.9085\n",
      "Epoch [10][20]\t Batch [1000][5500]\t Training Loss 0.3381\t Accuracy 0.9089\n",
      "Epoch [10][20]\t Batch [1050][5500]\t Training Loss 0.3363\t Accuracy 0.9092\n",
      "Epoch [10][20]\t Batch [1100][5500]\t Training Loss 0.3332\t Accuracy 0.9107\n",
      "Epoch [10][20]\t Batch [1150][5500]\t Training Loss 0.3311\t Accuracy 0.9115\n",
      "Epoch [10][20]\t Batch [1200][5500]\t Training Loss 0.3355\t Accuracy 0.9099\n",
      "Epoch [10][20]\t Batch [1250][5500]\t Training Loss 0.3364\t Accuracy 0.9095\n",
      "Epoch [10][20]\t Batch [1300][5500]\t Training Loss 0.3399\t Accuracy 0.9086\n",
      "Epoch [10][20]\t Batch [1350][5500]\t Training Loss 0.3417\t Accuracy 0.9080\n",
      "Epoch [10][20]\t Batch [1400][5500]\t Training Loss 0.3434\t Accuracy 0.9071\n",
      "Epoch [10][20]\t Batch [1450][5500]\t Training Loss 0.3467\t Accuracy 0.9064\n",
      "Epoch [10][20]\t Batch [1500][5500]\t Training Loss 0.3508\t Accuracy 0.9051\n",
      "Epoch [10][20]\t Batch [1550][5500]\t Training Loss 0.3496\t Accuracy 0.9057\n",
      "Epoch [10][20]\t Batch [1600][5500]\t Training Loss 0.3509\t Accuracy 0.9052\n",
      "Epoch [10][20]\t Batch [1650][5500]\t Training Loss 0.3499\t Accuracy 0.9055\n",
      "Epoch [10][20]\t Batch [1700][5500]\t Training Loss 0.3508\t Accuracy 0.9049\n",
      "Epoch [10][20]\t Batch [1750][5500]\t Training Loss 0.3512\t Accuracy 0.9046\n",
      "Epoch [10][20]\t Batch [1800][5500]\t Training Loss 0.3530\t Accuracy 0.9036\n",
      "Epoch [10][20]\t Batch [1850][5500]\t Training Loss 0.3508\t Accuracy 0.9043\n",
      "Epoch [10][20]\t Batch [1900][5500]\t Training Loss 0.3490\t Accuracy 0.9051\n",
      "Epoch [10][20]\t Batch [1950][5500]\t Training Loss 0.3487\t Accuracy 0.9052\n",
      "Epoch [10][20]\t Batch [2000][5500]\t Training Loss 0.3472\t Accuracy 0.9056\n",
      "Epoch [10][20]\t Batch [2050][5500]\t Training Loss 0.3465\t Accuracy 0.9062\n",
      "Epoch [10][20]\t Batch [2100][5500]\t Training Loss 0.3488\t Accuracy 0.9056\n",
      "Epoch [10][20]\t Batch [2150][5500]\t Training Loss 0.3474\t Accuracy 0.9064\n",
      "Epoch [10][20]\t Batch [2200][5500]\t Training Loss 0.3459\t Accuracy 0.9069\n",
      "Epoch [10][20]\t Batch [2250][5500]\t Training Loss 0.3458\t Accuracy 0.9068\n",
      "Epoch [10][20]\t Batch [2300][5500]\t Training Loss 0.3458\t Accuracy 0.9064\n",
      "Epoch [10][20]\t Batch [2350][5500]\t Training Loss 0.3453\t Accuracy 0.9064\n",
      "Epoch [10][20]\t Batch [2400][5500]\t Training Loss 0.3456\t Accuracy 0.9064\n",
      "Epoch [10][20]\t Batch [2450][5500]\t Training Loss 0.3454\t Accuracy 0.9062\n",
      "Epoch [10][20]\t Batch [2500][5500]\t Training Loss 0.3469\t Accuracy 0.9056\n",
      "Epoch [10][20]\t Batch [2550][5500]\t Training Loss 0.3458\t Accuracy 0.9057\n",
      "Epoch [10][20]\t Batch [2600][5500]\t Training Loss 0.3450\t Accuracy 0.9060\n",
      "Epoch [10][20]\t Batch [2650][5500]\t Training Loss 0.3450\t Accuracy 0.9063\n",
      "Epoch [10][20]\t Batch [2700][5500]\t Training Loss 0.3463\t Accuracy 0.9060\n",
      "Epoch [10][20]\t Batch [2750][5500]\t Training Loss 0.3465\t Accuracy 0.9058\n",
      "Epoch [10][20]\t Batch [2800][5500]\t Training Loss 0.3458\t Accuracy 0.9059\n",
      "Epoch [10][20]\t Batch [2850][5500]\t Training Loss 0.3452\t Accuracy 0.9061\n",
      "Epoch [10][20]\t Batch [2900][5500]\t Training Loss 0.3451\t Accuracy 0.9063\n",
      "Epoch [10][20]\t Batch [2950][5500]\t Training Loss 0.3455\t Accuracy 0.9061\n",
      "Epoch [10][20]\t Batch [3000][5500]\t Training Loss 0.3467\t Accuracy 0.9055\n",
      "Epoch [10][20]\t Batch [3050][5500]\t Training Loss 0.3471\t Accuracy 0.9051\n",
      "Epoch [10][20]\t Batch [3100][5500]\t Training Loss 0.3482\t Accuracy 0.9048\n",
      "Epoch [10][20]\t Batch [3150][5500]\t Training Loss 0.3499\t Accuracy 0.9043\n",
      "Epoch [10][20]\t Batch [3200][5500]\t Training Loss 0.3507\t Accuracy 0.9037\n",
      "Epoch [10][20]\t Batch [3250][5500]\t Training Loss 0.3522\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [3300][5500]\t Training Loss 0.3521\t Accuracy 0.9031\n",
      "Epoch [10][20]\t Batch [3350][5500]\t Training Loss 0.3523\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [3400][5500]\t Training Loss 0.3505\t Accuracy 0.9034\n",
      "Epoch [10][20]\t Batch [3450][5500]\t Training Loss 0.3498\t Accuracy 0.9038\n",
      "Epoch [10][20]\t Batch [3500][5500]\t Training Loss 0.3503\t Accuracy 0.9034\n",
      "Epoch [10][20]\t Batch [3550][5500]\t Training Loss 0.3498\t Accuracy 0.9035\n",
      "Epoch [10][20]\t Batch [3600][5500]\t Training Loss 0.3491\t Accuracy 0.9037\n",
      "Epoch [10][20]\t Batch [3650][5500]\t Training Loss 0.3489\t Accuracy 0.9037\n",
      "Epoch [10][20]\t Batch [3700][5500]\t Training Loss 0.3479\t Accuracy 0.9041\n",
      "Epoch [10][20]\t Batch [3750][5500]\t Training Loss 0.3498\t Accuracy 0.9034\n",
      "Epoch [10][20]\t Batch [3800][5500]\t Training Loss 0.3500\t Accuracy 0.9034\n",
      "Epoch [10][20]\t Batch [3850][5500]\t Training Loss 0.3495\t Accuracy 0.9036\n",
      "Epoch [10][20]\t Batch [3900][5500]\t Training Loss 0.3492\t Accuracy 0.9037\n",
      "Epoch [10][20]\t Batch [3950][5500]\t Training Loss 0.3496\t Accuracy 0.9036\n",
      "Epoch [10][20]\t Batch [4000][5500]\t Training Loss 0.3498\t Accuracy 0.9034\n",
      "Epoch [10][20]\t Batch [4050][5500]\t Training Loss 0.3493\t Accuracy 0.9035\n",
      "Epoch [10][20]\t Batch [4100][5500]\t Training Loss 0.3486\t Accuracy 0.9039\n",
      "Epoch [10][20]\t Batch [4150][5500]\t Training Loss 0.3493\t Accuracy 0.9036\n",
      "Epoch [10][20]\t Batch [4200][5500]\t Training Loss 0.3493\t Accuracy 0.9035\n",
      "Epoch [10][20]\t Batch [4250][5500]\t Training Loss 0.3507\t Accuracy 0.9031\n",
      "Epoch [10][20]\t Batch [4300][5500]\t Training Loss 0.3509\t Accuracy 0.9030\n",
      "Epoch [10][20]\t Batch [4350][5500]\t Training Loss 0.3501\t Accuracy 0.9032\n",
      "Epoch [10][20]\t Batch [4400][5500]\t Training Loss 0.3500\t Accuracy 0.9033\n",
      "Epoch [10][20]\t Batch [4450][5500]\t Training Loss 0.3504\t Accuracy 0.9032\n",
      "Epoch [10][20]\t Batch [4500][5500]\t Training Loss 0.3499\t Accuracy 0.9033\n",
      "Epoch [10][20]\t Batch [4550][5500]\t Training Loss 0.3503\t Accuracy 0.9031\n",
      "Epoch [10][20]\t Batch [4600][5500]\t Training Loss 0.3506\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [4650][5500]\t Training Loss 0.3514\t Accuracy 0.9028\n",
      "Epoch [10][20]\t Batch [4700][5500]\t Training Loss 0.3505\t Accuracy 0.9031\n",
      "Epoch [10][20]\t Batch [4750][5500]\t Training Loss 0.3506\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [4800][5500]\t Training Loss 0.3507\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [4850][5500]\t Training Loss 0.3499\t Accuracy 0.9032\n",
      "Epoch [10][20]\t Batch [4900][5500]\t Training Loss 0.3496\t Accuracy 0.9031\n",
      "Epoch [10][20]\t Batch [4950][5500]\t Training Loss 0.3501\t Accuracy 0.9030\n",
      "Epoch [10][20]\t Batch [5000][5500]\t Training Loss 0.3510\t Accuracy 0.9029\n",
      "Epoch [10][20]\t Batch [5050][5500]\t Training Loss 0.3519\t Accuracy 0.9027\n",
      "Epoch [10][20]\t Batch [5100][5500]\t Training Loss 0.3518\t Accuracy 0.9026\n",
      "Epoch [10][20]\t Batch [5150][5500]\t Training Loss 0.3514\t Accuracy 0.9026\n",
      "Epoch [10][20]\t Batch [5200][5500]\t Training Loss 0.3508\t Accuracy 0.9028\n",
      "Epoch [10][20]\t Batch [5250][5500]\t Training Loss 0.3512\t Accuracy 0.9027\n",
      "Epoch [10][20]\t Batch [5300][5500]\t Training Loss 0.3519\t Accuracy 0.9024\n",
      "Epoch [10][20]\t Batch [5350][5500]\t Training Loss 0.3514\t Accuracy 0.9026\n",
      "Epoch [10][20]\t Batch [5400][5500]\t Training Loss 0.3515\t Accuracy 0.9025\n",
      "Epoch [10][20]\t Batch [5450][5500]\t Training Loss 0.3512\t Accuracy 0.9025\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3512\t Average training accuracy 0.9024\n",
      "Epoch [10]\t Average validation loss 0.2754\t Average validation accuracy 0.9286\n",
      "\n",
      "Epoch [11][20]\t Batch [0][5500]\t Training Loss 0.0913\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [50][5500]\t Training Loss 0.3252\t Accuracy 0.9176\n",
      "Epoch [11][20]\t Batch [100][5500]\t Training Loss 0.3593\t Accuracy 0.9020\n",
      "Epoch [11][20]\t Batch [150][5500]\t Training Loss 0.3743\t Accuracy 0.8974\n",
      "Epoch [11][20]\t Batch [200][5500]\t Training Loss 0.3484\t Accuracy 0.9045\n",
      "Epoch [11][20]\t Batch [250][5500]\t Training Loss 0.3342\t Accuracy 0.9068\n",
      "Epoch [11][20]\t Batch [300][5500]\t Training Loss 0.3269\t Accuracy 0.9110\n",
      "Epoch [11][20]\t Batch [350][5500]\t Training Loss 0.3223\t Accuracy 0.9120\n",
      "Epoch [11][20]\t Batch [400][5500]\t Training Loss 0.3207\t Accuracy 0.9135\n",
      "Epoch [11][20]\t Batch [450][5500]\t Training Loss 0.3201\t Accuracy 0.9135\n",
      "Epoch [11][20]\t Batch [500][5500]\t Training Loss 0.3176\t Accuracy 0.9130\n",
      "Epoch [11][20]\t Batch [550][5500]\t Training Loss 0.3194\t Accuracy 0.9127\n",
      "Epoch [11][20]\t Batch [600][5500]\t Training Loss 0.3185\t Accuracy 0.9133\n",
      "Epoch [11][20]\t Batch [650][5500]\t Training Loss 0.3147\t Accuracy 0.9146\n",
      "Epoch [11][20]\t Batch [700][5500]\t Training Loss 0.3144\t Accuracy 0.9143\n",
      "Epoch [11][20]\t Batch [750][5500]\t Training Loss 0.3177\t Accuracy 0.9141\n",
      "Epoch [11][20]\t Batch [800][5500]\t Training Loss 0.3206\t Accuracy 0.9135\n",
      "Epoch [11][20]\t Batch [850][5500]\t Training Loss 0.3241\t Accuracy 0.9125\n",
      "Epoch [11][20]\t Batch [900][5500]\t Training Loss 0.3316\t Accuracy 0.9102\n",
      "Epoch [11][20]\t Batch [950][5500]\t Training Loss 0.3319\t Accuracy 0.9101\n",
      "Epoch [11][20]\t Batch [1000][5500]\t Training Loss 0.3295\t Accuracy 0.9105\n",
      "Epoch [11][20]\t Batch [1050][5500]\t Training Loss 0.3278\t Accuracy 0.9108\n",
      "Epoch [11][20]\t Batch [1100][5500]\t Training Loss 0.3247\t Accuracy 0.9124\n",
      "Epoch [11][20]\t Batch [1150][5500]\t Training Loss 0.3226\t Accuracy 0.9130\n",
      "Epoch [11][20]\t Batch [1200][5500]\t Training Loss 0.3270\t Accuracy 0.9114\n",
      "Epoch [11][20]\t Batch [1250][5500]\t Training Loss 0.3279\t Accuracy 0.9110\n",
      "Epoch [11][20]\t Batch [1300][5500]\t Training Loss 0.3314\t Accuracy 0.9100\n",
      "Epoch [11][20]\t Batch [1350][5500]\t Training Loss 0.3332\t Accuracy 0.9093\n",
      "Epoch [11][20]\t Batch [1400][5500]\t Training Loss 0.3349\t Accuracy 0.9084\n",
      "Epoch [11][20]\t Batch [1450][5500]\t Training Loss 0.3382\t Accuracy 0.9076\n",
      "Epoch [11][20]\t Batch [1500][5500]\t Training Loss 0.3422\t Accuracy 0.9065\n",
      "Epoch [11][20]\t Batch [1550][5500]\t Training Loss 0.3411\t Accuracy 0.9070\n",
      "Epoch [11][20]\t Batch [1600][5500]\t Training Loss 0.3423\t Accuracy 0.9064\n",
      "Epoch [11][20]\t Batch [1650][5500]\t Training Loss 0.3413\t Accuracy 0.9067\n",
      "Epoch [11][20]\t Batch [1700][5500]\t Training Loss 0.3422\t Accuracy 0.9060\n",
      "Epoch [11][20]\t Batch [1750][5500]\t Training Loss 0.3425\t Accuracy 0.9057\n",
      "Epoch [11][20]\t Batch [1800][5500]\t Training Loss 0.3443\t Accuracy 0.9048\n",
      "Epoch [11][20]\t Batch [1850][5500]\t Training Loss 0.3421\t Accuracy 0.9056\n",
      "Epoch [11][20]\t Batch [1900][5500]\t Training Loss 0.3403\t Accuracy 0.9063\n",
      "Epoch [11][20]\t Batch [1950][5500]\t Training Loss 0.3400\t Accuracy 0.9065\n",
      "Epoch [11][20]\t Batch [2000][5500]\t Training Loss 0.3385\t Accuracy 0.9069\n",
      "Epoch [11][20]\t Batch [2050][5500]\t Training Loss 0.3378\t Accuracy 0.9076\n",
      "Epoch [11][20]\t Batch [2100][5500]\t Training Loss 0.3402\t Accuracy 0.9070\n",
      "Epoch [11][20]\t Batch [2150][5500]\t Training Loss 0.3388\t Accuracy 0.9079\n",
      "Epoch [11][20]\t Batch [2200][5500]\t Training Loss 0.3374\t Accuracy 0.9083\n",
      "Epoch [11][20]\t Batch [2250][5500]\t Training Loss 0.3372\t Accuracy 0.9082\n",
      "Epoch [11][20]\t Batch [2300][5500]\t Training Loss 0.3372\t Accuracy 0.9077\n",
      "Epoch [11][20]\t Batch [2350][5500]\t Training Loss 0.3368\t Accuracy 0.9077\n",
      "Epoch [11][20]\t Batch [2400][5500]\t Training Loss 0.3371\t Accuracy 0.9077\n",
      "Epoch [11][20]\t Batch [2450][5500]\t Training Loss 0.3368\t Accuracy 0.9075\n",
      "Epoch [11][20]\t Batch [2500][5500]\t Training Loss 0.3383\t Accuracy 0.9069\n",
      "Epoch [11][20]\t Batch [2550][5500]\t Training Loss 0.3372\t Accuracy 0.9070\n",
      "Epoch [11][20]\t Batch [2600][5500]\t Training Loss 0.3365\t Accuracy 0.9073\n",
      "Epoch [11][20]\t Batch [2650][5500]\t Training Loss 0.3365\t Accuracy 0.9076\n",
      "Epoch [11][20]\t Batch [2700][5500]\t Training Loss 0.3377\t Accuracy 0.9072\n",
      "Epoch [11][20]\t Batch [2750][5500]\t Training Loss 0.3380\t Accuracy 0.9071\n",
      "Epoch [11][20]\t Batch [2800][5500]\t Training Loss 0.3373\t Accuracy 0.9071\n",
      "Epoch [11][20]\t Batch [2850][5500]\t Training Loss 0.3368\t Accuracy 0.9074\n",
      "Epoch [11][20]\t Batch [2900][5500]\t Training Loss 0.3366\t Accuracy 0.9076\n",
      "Epoch [11][20]\t Batch [2950][5500]\t Training Loss 0.3370\t Accuracy 0.9075\n",
      "Epoch [11][20]\t Batch [3000][5500]\t Training Loss 0.3381\t Accuracy 0.9070\n",
      "Epoch [11][20]\t Batch [3050][5500]\t Training Loss 0.3385\t Accuracy 0.9067\n",
      "Epoch [11][20]\t Batch [3100][5500]\t Training Loss 0.3396\t Accuracy 0.9064\n",
      "Epoch [11][20]\t Batch [3150][5500]\t Training Loss 0.3413\t Accuracy 0.9060\n",
      "Epoch [11][20]\t Batch [3200][5500]\t Training Loss 0.3422\t Accuracy 0.9054\n",
      "Epoch [11][20]\t Batch [3250][5500]\t Training Loss 0.3436\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [3300][5500]\t Training Loss 0.3435\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [3350][5500]\t Training Loss 0.3436\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [3400][5500]\t Training Loss 0.3419\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [3450][5500]\t Training Loss 0.3412\t Accuracy 0.9057\n",
      "Epoch [11][20]\t Batch [3500][5500]\t Training Loss 0.3417\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [3550][5500]\t Training Loss 0.3412\t Accuracy 0.9054\n",
      "Epoch [11][20]\t Batch [3600][5500]\t Training Loss 0.3405\t Accuracy 0.9056\n",
      "Epoch [11][20]\t Batch [3650][5500]\t Training Loss 0.3403\t Accuracy 0.9056\n",
      "Epoch [11][20]\t Batch [3700][5500]\t Training Loss 0.3393\t Accuracy 0.9060\n",
      "Epoch [11][20]\t Batch [3750][5500]\t Training Loss 0.3413\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [3800][5500]\t Training Loss 0.3415\t Accuracy 0.9052\n",
      "Epoch [11][20]\t Batch [3850][5500]\t Training Loss 0.3410\t Accuracy 0.9054\n",
      "Epoch [11][20]\t Batch [3900][5500]\t Training Loss 0.3407\t Accuracy 0.9055\n",
      "Epoch [11][20]\t Batch [3950][5500]\t Training Loss 0.3411\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [4000][5500]\t Training Loss 0.3413\t Accuracy 0.9052\n",
      "Epoch [11][20]\t Batch [4050][5500]\t Training Loss 0.3408\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [4100][5500]\t Training Loss 0.3401\t Accuracy 0.9057\n",
      "Epoch [11][20]\t Batch [4150][5500]\t Training Loss 0.3408\t Accuracy 0.9054\n",
      "Epoch [11][20]\t Batch [4200][5500]\t Training Loss 0.3408\t Accuracy 0.9053\n",
      "Epoch [11][20]\t Batch [4250][5500]\t Training Loss 0.3422\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [4300][5500]\t Training Loss 0.3424\t Accuracy 0.9048\n",
      "Epoch [11][20]\t Batch [4350][5500]\t Training Loss 0.3416\t Accuracy 0.9050\n",
      "Epoch [11][20]\t Batch [4400][5500]\t Training Loss 0.3415\t Accuracy 0.9051\n",
      "Epoch [11][20]\t Batch [4450][5500]\t Training Loss 0.3419\t Accuracy 0.9050\n",
      "Epoch [11][20]\t Batch [4500][5500]\t Training Loss 0.3414\t Accuracy 0.9051\n",
      "Epoch [11][20]\t Batch [4550][5500]\t Training Loss 0.3418\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [4600][5500]\t Training Loss 0.3422\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [4650][5500]\t Training Loss 0.3430\t Accuracy 0.9046\n",
      "Epoch [11][20]\t Batch [4700][5500]\t Training Loss 0.3421\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [4750][5500]\t Training Loss 0.3422\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [4800][5500]\t Training Loss 0.3423\t Accuracy 0.9046\n",
      "Epoch [11][20]\t Batch [4850][5500]\t Training Loss 0.3415\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [4900][5500]\t Training Loss 0.3412\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [4950][5500]\t Training Loss 0.3417\t Accuracy 0.9049\n",
      "Epoch [11][20]\t Batch [5000][5500]\t Training Loss 0.3426\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [5050][5500]\t Training Loss 0.3435\t Accuracy 0.9045\n",
      "Epoch [11][20]\t Batch [5100][5500]\t Training Loss 0.3434\t Accuracy 0.9045\n",
      "Epoch [11][20]\t Batch [5150][5500]\t Training Loss 0.3430\t Accuracy 0.9045\n",
      "Epoch [11][20]\t Batch [5200][5500]\t Training Loss 0.3424\t Accuracy 0.9047\n",
      "Epoch [11][20]\t Batch [5250][5500]\t Training Loss 0.3428\t Accuracy 0.9045\n",
      "Epoch [11][20]\t Batch [5300][5500]\t Training Loss 0.3435\t Accuracy 0.9043\n",
      "Epoch [11][20]\t Batch [5350][5500]\t Training Loss 0.3430\t Accuracy 0.9044\n",
      "Epoch [11][20]\t Batch [5400][5500]\t Training Loss 0.3432\t Accuracy 0.9044\n",
      "Epoch [11][20]\t Batch [5450][5500]\t Training Loss 0.3428\t Accuracy 0.9044\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3429\t Average training accuracy 0.9043\n",
      "Epoch [11]\t Average validation loss 0.2691\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [12][20]\t Batch [0][5500]\t Training Loss 0.0847\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][5500]\t Training Loss 0.3177\t Accuracy 0.9176\n",
      "Epoch [12][20]\t Batch [100][5500]\t Training Loss 0.3514\t Accuracy 0.9020\n",
      "Epoch [12][20]\t Batch [150][5500]\t Training Loss 0.3671\t Accuracy 0.8987\n",
      "Epoch [12][20]\t Batch [200][5500]\t Training Loss 0.3411\t Accuracy 0.9065\n",
      "Epoch [12][20]\t Batch [250][5500]\t Training Loss 0.3268\t Accuracy 0.9096\n",
      "Epoch [12][20]\t Batch [300][5500]\t Training Loss 0.3195\t Accuracy 0.9136\n",
      "Epoch [12][20]\t Batch [350][5500]\t Training Loss 0.3147\t Accuracy 0.9151\n",
      "Epoch [12][20]\t Batch [400][5500]\t Training Loss 0.3130\t Accuracy 0.9165\n",
      "Epoch [12][20]\t Batch [450][5500]\t Training Loss 0.3125\t Accuracy 0.9162\n",
      "Epoch [12][20]\t Batch [500][5500]\t Training Loss 0.3100\t Accuracy 0.9158\n",
      "Epoch [12][20]\t Batch [550][5500]\t Training Loss 0.3119\t Accuracy 0.9152\n",
      "Epoch [12][20]\t Batch [600][5500]\t Training Loss 0.3110\t Accuracy 0.9156\n",
      "Epoch [12][20]\t Batch [650][5500]\t Training Loss 0.3073\t Accuracy 0.9171\n",
      "Epoch [12][20]\t Batch [700][5500]\t Training Loss 0.3070\t Accuracy 0.9167\n",
      "Epoch [12][20]\t Batch [750][5500]\t Training Loss 0.3102\t Accuracy 0.9166\n",
      "Epoch [12][20]\t Batch [800][5500]\t Training Loss 0.3131\t Accuracy 0.9157\n",
      "Epoch [12][20]\t Batch [850][5500]\t Training Loss 0.3166\t Accuracy 0.9145\n",
      "Epoch [12][20]\t Batch [900][5500]\t Training Loss 0.3241\t Accuracy 0.9121\n",
      "Epoch [12][20]\t Batch [950][5500]\t Training Loss 0.3245\t Accuracy 0.9119\n",
      "Epoch [12][20]\t Batch [1000][5500]\t Training Loss 0.3221\t Accuracy 0.9124\n",
      "Epoch [12][20]\t Batch [1050][5500]\t Training Loss 0.3204\t Accuracy 0.9127\n",
      "Epoch [12][20]\t Batch [1100][5500]\t Training Loss 0.3174\t Accuracy 0.9142\n",
      "Epoch [12][20]\t Batch [1150][5500]\t Training Loss 0.3153\t Accuracy 0.9148\n",
      "Epoch [12][20]\t Batch [1200][5500]\t Training Loss 0.3196\t Accuracy 0.9132\n",
      "Epoch [12][20]\t Batch [1250][5500]\t Training Loss 0.3206\t Accuracy 0.9127\n",
      "Epoch [12][20]\t Batch [1300][5500]\t Training Loss 0.3241\t Accuracy 0.9119\n",
      "Epoch [12][20]\t Batch [1350][5500]\t Training Loss 0.3258\t Accuracy 0.9113\n",
      "Epoch [12][20]\t Batch [1400][5500]\t Training Loss 0.3275\t Accuracy 0.9105\n",
      "Epoch [12][20]\t Batch [1450][5500]\t Training Loss 0.3307\t Accuracy 0.9096\n",
      "Epoch [12][20]\t Batch [1500][5500]\t Training Loss 0.3347\t Accuracy 0.9084\n",
      "Epoch [12][20]\t Batch [1550][5500]\t Training Loss 0.3337\t Accuracy 0.9088\n",
      "Epoch [12][20]\t Batch [1600][5500]\t Training Loss 0.3348\t Accuracy 0.9083\n",
      "Epoch [12][20]\t Batch [1650][5500]\t Training Loss 0.3338\t Accuracy 0.9085\n",
      "Epoch [12][20]\t Batch [1700][5500]\t Training Loss 0.3346\t Accuracy 0.9079\n",
      "Epoch [12][20]\t Batch [1750][5500]\t Training Loss 0.3350\t Accuracy 0.9077\n",
      "Epoch [12][20]\t Batch [1800][5500]\t Training Loss 0.3367\t Accuracy 0.9069\n",
      "Epoch [12][20]\t Batch [1850][5500]\t Training Loss 0.3345\t Accuracy 0.9077\n",
      "Epoch [12][20]\t Batch [1900][5500]\t Training Loss 0.3328\t Accuracy 0.9084\n",
      "Epoch [12][20]\t Batch [1950][5500]\t Training Loss 0.3324\t Accuracy 0.9085\n",
      "Epoch [12][20]\t Batch [2000][5500]\t Training Loss 0.3310\t Accuracy 0.9090\n",
      "Epoch [12][20]\t Batch [2050][5500]\t Training Loss 0.3302\t Accuracy 0.9097\n",
      "Epoch [12][20]\t Batch [2100][5500]\t Training Loss 0.3327\t Accuracy 0.9091\n",
      "Epoch [12][20]\t Batch [2150][5500]\t Training Loss 0.3314\t Accuracy 0.9099\n",
      "Epoch [12][20]\t Batch [2200][5500]\t Training Loss 0.3299\t Accuracy 0.9104\n",
      "Epoch [12][20]\t Batch [2250][5500]\t Training Loss 0.3297\t Accuracy 0.9103\n",
      "Epoch [12][20]\t Batch [2300][5500]\t Training Loss 0.3297\t Accuracy 0.9098\n",
      "Epoch [12][20]\t Batch [2350][5500]\t Training Loss 0.3293\t Accuracy 0.9098\n",
      "Epoch [12][20]\t Batch [2400][5500]\t Training Loss 0.3296\t Accuracy 0.9097\n",
      "Epoch [12][20]\t Batch [2450][5500]\t Training Loss 0.3293\t Accuracy 0.9095\n",
      "Epoch [12][20]\t Batch [2500][5500]\t Training Loss 0.3309\t Accuracy 0.9089\n",
      "Epoch [12][20]\t Batch [2550][5500]\t Training Loss 0.3298\t Accuracy 0.9090\n",
      "Epoch [12][20]\t Batch [2600][5500]\t Training Loss 0.3291\t Accuracy 0.9093\n",
      "Epoch [12][20]\t Batch [2650][5500]\t Training Loss 0.3291\t Accuracy 0.9096\n",
      "Epoch [12][20]\t Batch [2700][5500]\t Training Loss 0.3303\t Accuracy 0.9091\n",
      "Epoch [12][20]\t Batch [2750][5500]\t Training Loss 0.3306\t Accuracy 0.9090\n",
      "Epoch [12][20]\t Batch [2800][5500]\t Training Loss 0.3299\t Accuracy 0.9091\n",
      "Epoch [12][20]\t Batch [2850][5500]\t Training Loss 0.3294\t Accuracy 0.9094\n",
      "Epoch [12][20]\t Batch [2900][5500]\t Training Loss 0.3293\t Accuracy 0.9096\n",
      "Epoch [12][20]\t Batch [2950][5500]\t Training Loss 0.3296\t Accuracy 0.9094\n",
      "Epoch [12][20]\t Batch [3000][5500]\t Training Loss 0.3307\t Accuracy 0.9090\n",
      "Epoch [12][20]\t Batch [3050][5500]\t Training Loss 0.3311\t Accuracy 0.9088\n",
      "Epoch [12][20]\t Batch [3100][5500]\t Training Loss 0.3322\t Accuracy 0.9085\n",
      "Epoch [12][20]\t Batch [3150][5500]\t Training Loss 0.3339\t Accuracy 0.9081\n",
      "Epoch [12][20]\t Batch [3200][5500]\t Training Loss 0.3347\t Accuracy 0.9075\n",
      "Epoch [12][20]\t Batch [3250][5500]\t Training Loss 0.3361\t Accuracy 0.9067\n",
      "Epoch [12][20]\t Batch [3300][5500]\t Training Loss 0.3360\t Accuracy 0.9068\n",
      "Epoch [12][20]\t Batch [3350][5500]\t Training Loss 0.3361\t Accuracy 0.9067\n",
      "Epoch [12][20]\t Batch [3400][5500]\t Training Loss 0.3344\t Accuracy 0.9073\n",
      "Epoch [12][20]\t Batch [3450][5500]\t Training Loss 0.3337\t Accuracy 0.9076\n",
      "Epoch [12][20]\t Batch [3500][5500]\t Training Loss 0.3342\t Accuracy 0.9072\n",
      "Epoch [12][20]\t Batch [3550][5500]\t Training Loss 0.3337\t Accuracy 0.9074\n",
      "Epoch [12][20]\t Batch [3600][5500]\t Training Loss 0.3331\t Accuracy 0.9075\n",
      "Epoch [12][20]\t Batch [3650][5500]\t Training Loss 0.3329\t Accuracy 0.9075\n",
      "Epoch [12][20]\t Batch [3700][5500]\t Training Loss 0.3318\t Accuracy 0.9078\n",
      "Epoch [12][20]\t Batch [3750][5500]\t Training Loss 0.3339\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [3800][5500]\t Training Loss 0.3341\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [3850][5500]\t Training Loss 0.3335\t Accuracy 0.9072\n",
      "Epoch [12][20]\t Batch [3900][5500]\t Training Loss 0.3333\t Accuracy 0.9073\n",
      "Epoch [12][20]\t Batch [3950][5500]\t Training Loss 0.3337\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [4000][5500]\t Training Loss 0.3339\t Accuracy 0.9070\n",
      "Epoch [12][20]\t Batch [4050][5500]\t Training Loss 0.3334\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [4100][5500]\t Training Loss 0.3327\t Accuracy 0.9074\n",
      "Epoch [12][20]\t Batch [4150][5500]\t Training Loss 0.3334\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [4200][5500]\t Training Loss 0.3334\t Accuracy 0.9071\n",
      "Epoch [12][20]\t Batch [4250][5500]\t Training Loss 0.3348\t Accuracy 0.9067\n",
      "Epoch [12][20]\t Batch [4300][5500]\t Training Loss 0.3350\t Accuracy 0.9066\n",
      "Epoch [12][20]\t Batch [4350][5500]\t Training Loss 0.3342\t Accuracy 0.9068\n",
      "Epoch [12][20]\t Batch [4400][5500]\t Training Loss 0.3342\t Accuracy 0.9068\n",
      "Epoch [12][20]\t Batch [4450][5500]\t Training Loss 0.3345\t Accuracy 0.9068\n",
      "Epoch [12][20]\t Batch [4500][5500]\t Training Loss 0.3340\t Accuracy 0.9068\n",
      "Epoch [12][20]\t Batch [4550][5500]\t Training Loss 0.3345\t Accuracy 0.9066\n",
      "Epoch [12][20]\t Batch [4600][5500]\t Training Loss 0.3348\t Accuracy 0.9064\n",
      "Epoch [12][20]\t Batch [4650][5500]\t Training Loss 0.3357\t Accuracy 0.9062\n",
      "Epoch [12][20]\t Batch [4700][5500]\t Training Loss 0.3348\t Accuracy 0.9065\n",
      "Epoch [12][20]\t Batch [4750][5500]\t Training Loss 0.3349\t Accuracy 0.9063\n",
      "Epoch [12][20]\t Batch [4800][5500]\t Training Loss 0.3350\t Accuracy 0.9063\n",
      "Epoch [12][20]\t Batch [4850][5500]\t Training Loss 0.3342\t Accuracy 0.9066\n",
      "Epoch [12][20]\t Batch [4900][5500]\t Training Loss 0.3339\t Accuracy 0.9065\n",
      "Epoch [12][20]\t Batch [4950][5500]\t Training Loss 0.3343\t Accuracy 0.9065\n",
      "Epoch [12][20]\t Batch [5000][5500]\t Training Loss 0.3353\t Accuracy 0.9063\n",
      "Epoch [12][20]\t Batch [5050][5500]\t Training Loss 0.3362\t Accuracy 0.9061\n",
      "Epoch [12][20]\t Batch [5100][5500]\t Training Loss 0.3361\t Accuracy 0.9061\n",
      "Epoch [12][20]\t Batch [5150][5500]\t Training Loss 0.3357\t Accuracy 0.9061\n",
      "Epoch [12][20]\t Batch [5200][5500]\t Training Loss 0.3351\t Accuracy 0.9063\n",
      "Epoch [12][20]\t Batch [5250][5500]\t Training Loss 0.3355\t Accuracy 0.9061\n",
      "Epoch [12][20]\t Batch [5300][5500]\t Training Loss 0.3363\t Accuracy 0.9059\n",
      "Epoch [12][20]\t Batch [5350][5500]\t Training Loss 0.3357\t Accuracy 0.9061\n",
      "Epoch [12][20]\t Batch [5400][5500]\t Training Loss 0.3359\t Accuracy 0.9060\n",
      "Epoch [12][20]\t Batch [5450][5500]\t Training Loss 0.3356\t Accuracy 0.9060\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3356\t Average training accuracy 0.9060\n",
      "Epoch [12]\t Average validation loss 0.2635\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [13][20]\t Batch [0][5500]\t Training Loss 0.0790\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][5500]\t Training Loss 0.3111\t Accuracy 0.9196\n",
      "Epoch [13][20]\t Batch [100][5500]\t Training Loss 0.3444\t Accuracy 0.9030\n",
      "Epoch [13][20]\t Batch [150][5500]\t Training Loss 0.3608\t Accuracy 0.8974\n",
      "Epoch [13][20]\t Batch [200][5500]\t Training Loss 0.3347\t Accuracy 0.9065\n",
      "Epoch [13][20]\t Batch [250][5500]\t Training Loss 0.3203\t Accuracy 0.9096\n",
      "Epoch [13][20]\t Batch [300][5500]\t Training Loss 0.3130\t Accuracy 0.9140\n",
      "Epoch [13][20]\t Batch [350][5500]\t Training Loss 0.3081\t Accuracy 0.9154\n",
      "Epoch [13][20]\t Batch [400][5500]\t Training Loss 0.3063\t Accuracy 0.9170\n",
      "Epoch [13][20]\t Batch [450][5500]\t Training Loss 0.3059\t Accuracy 0.9169\n",
      "Epoch [13][20]\t Batch [500][5500]\t Training Loss 0.3034\t Accuracy 0.9166\n",
      "Epoch [13][20]\t Batch [550][5500]\t Training Loss 0.3053\t Accuracy 0.9163\n",
      "Epoch [13][20]\t Batch [600][5500]\t Training Loss 0.3044\t Accuracy 0.9166\n",
      "Epoch [13][20]\t Batch [650][5500]\t Training Loss 0.3007\t Accuracy 0.9183\n",
      "Epoch [13][20]\t Batch [700][5500]\t Training Loss 0.3005\t Accuracy 0.9178\n",
      "Epoch [13][20]\t Batch [750][5500]\t Training Loss 0.3036\t Accuracy 0.9178\n",
      "Epoch [13][20]\t Batch [800][5500]\t Training Loss 0.3064\t Accuracy 0.9169\n",
      "Epoch [13][20]\t Batch [850][5500]\t Training Loss 0.3100\t Accuracy 0.9156\n",
      "Epoch [13][20]\t Batch [900][5500]\t Training Loss 0.3174\t Accuracy 0.9133\n",
      "Epoch [13][20]\t Batch [950][5500]\t Training Loss 0.3179\t Accuracy 0.9130\n",
      "Epoch [13][20]\t Batch [1000][5500]\t Training Loss 0.3155\t Accuracy 0.9135\n",
      "Epoch [13][20]\t Batch [1050][5500]\t Training Loss 0.3140\t Accuracy 0.9137\n",
      "Epoch [13][20]\t Batch [1100][5500]\t Training Loss 0.3109\t Accuracy 0.9152\n",
      "Epoch [13][20]\t Batch [1150][5500]\t Training Loss 0.3088\t Accuracy 0.9157\n",
      "Epoch [13][20]\t Batch [1200][5500]\t Training Loss 0.3132\t Accuracy 0.9141\n",
      "Epoch [13][20]\t Batch [1250][5500]\t Training Loss 0.3141\t Accuracy 0.9137\n",
      "Epoch [13][20]\t Batch [1300][5500]\t Training Loss 0.3176\t Accuracy 0.9127\n",
      "Epoch [13][20]\t Batch [1350][5500]\t Training Loss 0.3194\t Accuracy 0.9121\n",
      "Epoch [13][20]\t Batch [1400][5500]\t Training Loss 0.3210\t Accuracy 0.9113\n",
      "Epoch [13][20]\t Batch [1450][5500]\t Training Loss 0.3242\t Accuracy 0.9104\n",
      "Epoch [13][20]\t Batch [1500][5500]\t Training Loss 0.3281\t Accuracy 0.9094\n",
      "Epoch [13][20]\t Batch [1550][5500]\t Training Loss 0.3271\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [1600][5500]\t Training Loss 0.3283\t Accuracy 0.9094\n",
      "Epoch [13][20]\t Batch [1650][5500]\t Training Loss 0.3272\t Accuracy 0.9097\n",
      "Epoch [13][20]\t Batch [1700][5500]\t Training Loss 0.3280\t Accuracy 0.9091\n",
      "Epoch [13][20]\t Batch [1750][5500]\t Training Loss 0.3284\t Accuracy 0.9089\n",
      "Epoch [13][20]\t Batch [1800][5500]\t Training Loss 0.3300\t Accuracy 0.9081\n",
      "Epoch [13][20]\t Batch [1850][5500]\t Training Loss 0.3279\t Accuracy 0.9089\n",
      "Epoch [13][20]\t Batch [1900][5500]\t Training Loss 0.3261\t Accuracy 0.9095\n",
      "Epoch [13][20]\t Batch [1950][5500]\t Training Loss 0.3258\t Accuracy 0.9095\n",
      "Epoch [13][20]\t Batch [2000][5500]\t Training Loss 0.3243\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [2050][5500]\t Training Loss 0.3235\t Accuracy 0.9106\n",
      "Epoch [13][20]\t Batch [2100][5500]\t Training Loss 0.3261\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [2150][5500]\t Training Loss 0.3248\t Accuracy 0.9109\n",
      "Epoch [13][20]\t Batch [2200][5500]\t Training Loss 0.3234\t Accuracy 0.9113\n",
      "Epoch [13][20]\t Batch [2250][5500]\t Training Loss 0.3232\t Accuracy 0.9112\n",
      "Epoch [13][20]\t Batch [2300][5500]\t Training Loss 0.3231\t Accuracy 0.9107\n",
      "Epoch [13][20]\t Batch [2350][5500]\t Training Loss 0.3228\t Accuracy 0.9107\n",
      "Epoch [13][20]\t Batch [2400][5500]\t Training Loss 0.3231\t Accuracy 0.9105\n",
      "Epoch [13][20]\t Batch [2450][5500]\t Training Loss 0.3228\t Accuracy 0.9104\n",
      "Epoch [13][20]\t Batch [2500][5500]\t Training Loss 0.3243\t Accuracy 0.9098\n",
      "Epoch [13][20]\t Batch [2550][5500]\t Training Loss 0.3233\t Accuracy 0.9099\n",
      "Epoch [13][20]\t Batch [2600][5500]\t Training Loss 0.3226\t Accuracy 0.9102\n",
      "Epoch [13][20]\t Batch [2650][5500]\t Training Loss 0.3225\t Accuracy 0.9106\n",
      "Epoch [13][20]\t Batch [2700][5500]\t Training Loss 0.3238\t Accuracy 0.9101\n",
      "Epoch [13][20]\t Batch [2750][5500]\t Training Loss 0.3241\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [2800][5500]\t Training Loss 0.3234\t Accuracy 0.9101\n",
      "Epoch [13][20]\t Batch [2850][5500]\t Training Loss 0.3229\t Accuracy 0.9104\n",
      "Epoch [13][20]\t Batch [2900][5500]\t Training Loss 0.3228\t Accuracy 0.9105\n",
      "Epoch [13][20]\t Batch [2950][5500]\t Training Loss 0.3231\t Accuracy 0.9105\n",
      "Epoch [13][20]\t Batch [3000][5500]\t Training Loss 0.3242\t Accuracy 0.9101\n",
      "Epoch [13][20]\t Batch [3050][5500]\t Training Loss 0.3245\t Accuracy 0.9099\n",
      "Epoch [13][20]\t Batch [3100][5500]\t Training Loss 0.3256\t Accuracy 0.9096\n",
      "Epoch [13][20]\t Batch [3150][5500]\t Training Loss 0.3273\t Accuracy 0.9093\n",
      "Epoch [13][20]\t Batch [3200][5500]\t Training Loss 0.3281\t Accuracy 0.9087\n",
      "Epoch [13][20]\t Batch [3250][5500]\t Training Loss 0.3295\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [3300][5500]\t Training Loss 0.3293\t Accuracy 0.9080\n",
      "Epoch [13][20]\t Batch [3350][5500]\t Training Loss 0.3295\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [3400][5500]\t Training Loss 0.3278\t Accuracy 0.9084\n",
      "Epoch [13][20]\t Batch [3450][5500]\t Training Loss 0.3271\t Accuracy 0.9088\n",
      "Epoch [13][20]\t Batch [3500][5500]\t Training Loss 0.3276\t Accuracy 0.9084\n",
      "Epoch [13][20]\t Batch [3550][5500]\t Training Loss 0.3271\t Accuracy 0.9085\n",
      "Epoch [13][20]\t Batch [3600][5500]\t Training Loss 0.3264\t Accuracy 0.9086\n",
      "Epoch [13][20]\t Batch [3650][5500]\t Training Loss 0.3262\t Accuracy 0.9087\n",
      "Epoch [13][20]\t Batch [3700][5500]\t Training Loss 0.3252\t Accuracy 0.9090\n",
      "Epoch [13][20]\t Batch [3750][5500]\t Training Loss 0.3273\t Accuracy 0.9082\n",
      "Epoch [13][20]\t Batch [3800][5500]\t Training Loss 0.3276\t Accuracy 0.9082\n",
      "Epoch [13][20]\t Batch [3850][5500]\t Training Loss 0.3270\t Accuracy 0.9084\n",
      "Epoch [13][20]\t Batch [3900][5500]\t Training Loss 0.3267\t Accuracy 0.9084\n",
      "Epoch [13][20]\t Batch [3950][5500]\t Training Loss 0.3272\t Accuracy 0.9082\n",
      "Epoch [13][20]\t Batch [4000][5500]\t Training Loss 0.3274\t Accuracy 0.9081\n",
      "Epoch [13][20]\t Batch [4050][5500]\t Training Loss 0.3268\t Accuracy 0.9082\n",
      "Epoch [13][20]\t Batch [4100][5500]\t Training Loss 0.3262\t Accuracy 0.9086\n",
      "Epoch [13][20]\t Batch [4150][5500]\t Training Loss 0.3269\t Accuracy 0.9083\n",
      "Epoch [13][20]\t Batch [4200][5500]\t Training Loss 0.3269\t Accuracy 0.9083\n",
      "Epoch [13][20]\t Batch [4250][5500]\t Training Loss 0.3282\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [4300][5500]\t Training Loss 0.3285\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [4350][5500]\t Training Loss 0.3277\t Accuracy 0.9080\n",
      "Epoch [13][20]\t Batch [4400][5500]\t Training Loss 0.3276\t Accuracy 0.9080\n",
      "Epoch [13][20]\t Batch [4450][5500]\t Training Loss 0.3280\t Accuracy 0.9079\n",
      "Epoch [13][20]\t Batch [4500][5500]\t Training Loss 0.3275\t Accuracy 0.9080\n",
      "Epoch [13][20]\t Batch [4550][5500]\t Training Loss 0.3280\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [4600][5500]\t Training Loss 0.3283\t Accuracy 0.9076\n",
      "Epoch [13][20]\t Batch [4650][5500]\t Training Loss 0.3292\t Accuracy 0.9074\n",
      "Epoch [13][20]\t Batch [4700][5500]\t Training Loss 0.3283\t Accuracy 0.9077\n",
      "Epoch [13][20]\t Batch [4750][5500]\t Training Loss 0.3284\t Accuracy 0.9075\n",
      "Epoch [13][20]\t Batch [4800][5500]\t Training Loss 0.3285\t Accuracy 0.9075\n",
      "Epoch [13][20]\t Batch [4850][5500]\t Training Loss 0.3277\t Accuracy 0.9078\n",
      "Epoch [13][20]\t Batch [4900][5500]\t Training Loss 0.3275\t Accuracy 0.9077\n",
      "Epoch [13][20]\t Batch [4950][5500]\t Training Loss 0.3278\t Accuracy 0.9077\n",
      "Epoch [13][20]\t Batch [5000][5500]\t Training Loss 0.3288\t Accuracy 0.9075\n",
      "Epoch [13][20]\t Batch [5050][5500]\t Training Loss 0.3297\t Accuracy 0.9073\n",
      "Epoch [13][20]\t Batch [5100][5500]\t Training Loss 0.3296\t Accuracy 0.9073\n",
      "Epoch [13][20]\t Batch [5150][5500]\t Training Loss 0.3292\t Accuracy 0.9073\n",
      "Epoch [13][20]\t Batch [5200][5500]\t Training Loss 0.3286\t Accuracy 0.9075\n",
      "Epoch [13][20]\t Batch [5250][5500]\t Training Loss 0.3290\t Accuracy 0.9073\n",
      "Epoch [13][20]\t Batch [5300][5500]\t Training Loss 0.3298\t Accuracy 0.9071\n",
      "Epoch [13][20]\t Batch [5350][5500]\t Training Loss 0.3293\t Accuracy 0.9072\n",
      "Epoch [13][20]\t Batch [5400][5500]\t Training Loss 0.3295\t Accuracy 0.9071\n",
      "Epoch [13][20]\t Batch [5450][5500]\t Training Loss 0.3291\t Accuracy 0.9072\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3292\t Average training accuracy 0.9071\n",
      "Epoch [13]\t Average validation loss 0.2587\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [14][20]\t Batch [0][5500]\t Training Loss 0.0741\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][5500]\t Training Loss 0.3053\t Accuracy 0.9196\n",
      "Epoch [14][20]\t Batch [100][5500]\t Training Loss 0.3381\t Accuracy 0.9050\n",
      "Epoch [14][20]\t Batch [150][5500]\t Training Loss 0.3552\t Accuracy 0.8987\n",
      "Epoch [14][20]\t Batch [200][5500]\t Training Loss 0.3290\t Accuracy 0.9080\n",
      "Epoch [14][20]\t Batch [250][5500]\t Training Loss 0.3146\t Accuracy 0.9112\n",
      "Epoch [14][20]\t Batch [300][5500]\t Training Loss 0.3072\t Accuracy 0.9153\n",
      "Epoch [14][20]\t Batch [350][5500]\t Training Loss 0.3022\t Accuracy 0.9168\n",
      "Epoch [14][20]\t Batch [400][5500]\t Training Loss 0.3003\t Accuracy 0.9185\n",
      "Epoch [14][20]\t Batch [450][5500]\t Training Loss 0.3000\t Accuracy 0.9188\n",
      "Epoch [14][20]\t Batch [500][5500]\t Training Loss 0.2975\t Accuracy 0.9188\n",
      "Epoch [14][20]\t Batch [550][5500]\t Training Loss 0.2994\t Accuracy 0.9187\n",
      "Epoch [14][20]\t Batch [600][5500]\t Training Loss 0.2985\t Accuracy 0.9190\n",
      "Epoch [14][20]\t Batch [650][5500]\t Training Loss 0.2949\t Accuracy 0.9206\n",
      "Epoch [14][20]\t Batch [700][5500]\t Training Loss 0.2947\t Accuracy 0.9200\n",
      "Epoch [14][20]\t Batch [750][5500]\t Training Loss 0.2978\t Accuracy 0.9197\n",
      "Epoch [14][20]\t Batch [800][5500]\t Training Loss 0.3006\t Accuracy 0.9187\n",
      "Epoch [14][20]\t Batch [850][5500]\t Training Loss 0.3041\t Accuracy 0.9175\n",
      "Epoch [14][20]\t Batch [900][5500]\t Training Loss 0.3115\t Accuracy 0.9152\n",
      "Epoch [14][20]\t Batch [950][5500]\t Training Loss 0.3121\t Accuracy 0.9149\n",
      "Epoch [14][20]\t Batch [1000][5500]\t Training Loss 0.3097\t Accuracy 0.9153\n",
      "Epoch [14][20]\t Batch [1050][5500]\t Training Loss 0.3082\t Accuracy 0.9154\n",
      "Epoch [14][20]\t Batch [1100][5500]\t Training Loss 0.3052\t Accuracy 0.9168\n",
      "Epoch [14][20]\t Batch [1150][5500]\t Training Loss 0.3031\t Accuracy 0.9173\n",
      "Epoch [14][20]\t Batch [1200][5500]\t Training Loss 0.3074\t Accuracy 0.9157\n",
      "Epoch [14][20]\t Batch [1250][5500]\t Training Loss 0.3083\t Accuracy 0.9153\n",
      "Epoch [14][20]\t Batch [1300][5500]\t Training Loss 0.3119\t Accuracy 0.9143\n",
      "Epoch [14][20]\t Batch [1350][5500]\t Training Loss 0.3136\t Accuracy 0.9136\n",
      "Epoch [14][20]\t Batch [1400][5500]\t Training Loss 0.3151\t Accuracy 0.9129\n",
      "Epoch [14][20]\t Batch [1450][5500]\t Training Loss 0.3183\t Accuracy 0.9119\n",
      "Epoch [14][20]\t Batch [1500][5500]\t Training Loss 0.3222\t Accuracy 0.9109\n",
      "Epoch [14][20]\t Batch [1550][5500]\t Training Loss 0.3213\t Accuracy 0.9114\n",
      "Epoch [14][20]\t Batch [1600][5500]\t Training Loss 0.3224\t Accuracy 0.9108\n",
      "Epoch [14][20]\t Batch [1650][5500]\t Training Loss 0.3213\t Accuracy 0.9110\n",
      "Epoch [14][20]\t Batch [1700][5500]\t Training Loss 0.3221\t Accuracy 0.9105\n",
      "Epoch [14][20]\t Batch [1750][5500]\t Training Loss 0.3224\t Accuracy 0.9102\n",
      "Epoch [14][20]\t Batch [1800][5500]\t Training Loss 0.3240\t Accuracy 0.9095\n",
      "Epoch [14][20]\t Batch [1850][5500]\t Training Loss 0.3219\t Accuracy 0.9103\n",
      "Epoch [14][20]\t Batch [1900][5500]\t Training Loss 0.3202\t Accuracy 0.9109\n",
      "Epoch [14][20]\t Batch [1950][5500]\t Training Loss 0.3198\t Accuracy 0.9110\n",
      "Epoch [14][20]\t Batch [2000][5500]\t Training Loss 0.3184\t Accuracy 0.9114\n",
      "Epoch [14][20]\t Batch [2050][5500]\t Training Loss 0.3176\t Accuracy 0.9120\n",
      "Epoch [14][20]\t Batch [2100][5500]\t Training Loss 0.3202\t Accuracy 0.9114\n",
      "Epoch [14][20]\t Batch [2150][5500]\t Training Loss 0.3189\t Accuracy 0.9122\n",
      "Epoch [14][20]\t Batch [2200][5500]\t Training Loss 0.3175\t Accuracy 0.9126\n",
      "Epoch [14][20]\t Batch [2250][5500]\t Training Loss 0.3173\t Accuracy 0.9125\n",
      "Epoch [14][20]\t Batch [2300][5500]\t Training Loss 0.3173\t Accuracy 0.9120\n",
      "Epoch [14][20]\t Batch [2350][5500]\t Training Loss 0.3169\t Accuracy 0.9120\n",
      "Epoch [14][20]\t Batch [2400][5500]\t Training Loss 0.3172\t Accuracy 0.9118\n",
      "Epoch [14][20]\t Batch [2450][5500]\t Training Loss 0.3169\t Accuracy 0.9117\n",
      "Epoch [14][20]\t Batch [2500][5500]\t Training Loss 0.3184\t Accuracy 0.9111\n",
      "Epoch [14][20]\t Batch [2550][5500]\t Training Loss 0.3174\t Accuracy 0.9113\n",
      "Epoch [14][20]\t Batch [2600][5500]\t Training Loss 0.3167\t Accuracy 0.9116\n",
      "Epoch [14][20]\t Batch [2650][5500]\t Training Loss 0.3167\t Accuracy 0.9119\n",
      "Epoch [14][20]\t Batch [2700][5500]\t Training Loss 0.3180\t Accuracy 0.9115\n",
      "Epoch [14][20]\t Batch [2750][5500]\t Training Loss 0.3183\t Accuracy 0.9113\n",
      "Epoch [14][20]\t Batch [2800][5500]\t Training Loss 0.3176\t Accuracy 0.9114\n",
      "Epoch [14][20]\t Batch [2850][5500]\t Training Loss 0.3171\t Accuracy 0.9117\n",
      "Epoch [14][20]\t Batch [2900][5500]\t Training Loss 0.3170\t Accuracy 0.9119\n",
      "Epoch [14][20]\t Batch [2950][5500]\t Training Loss 0.3173\t Accuracy 0.9119\n",
      "Epoch [14][20]\t Batch [3000][5500]\t Training Loss 0.3184\t Accuracy 0.9115\n",
      "Epoch [14][20]\t Batch [3050][5500]\t Training Loss 0.3187\t Accuracy 0.9112\n",
      "Epoch [14][20]\t Batch [3100][5500]\t Training Loss 0.3197\t Accuracy 0.9110\n",
      "Epoch [14][20]\t Batch [3150][5500]\t Training Loss 0.3214\t Accuracy 0.9106\n",
      "Epoch [14][20]\t Batch [3200][5500]\t Training Loss 0.3221\t Accuracy 0.9101\n",
      "Epoch [14][20]\t Batch [3250][5500]\t Training Loss 0.3235\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [3300][5500]\t Training Loss 0.3234\t Accuracy 0.9095\n",
      "Epoch [14][20]\t Batch [3350][5500]\t Training Loss 0.3235\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [3400][5500]\t Training Loss 0.3218\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [3450][5500]\t Training Loss 0.3211\t Accuracy 0.9103\n",
      "Epoch [14][20]\t Batch [3500][5500]\t Training Loss 0.3217\t Accuracy 0.9099\n",
      "Epoch [14][20]\t Batch [3550][5500]\t Training Loss 0.3212\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [3600][5500]\t Training Loss 0.3205\t Accuracy 0.9101\n",
      "Epoch [14][20]\t Batch [3650][5500]\t Training Loss 0.3203\t Accuracy 0.9102\n",
      "Epoch [14][20]\t Batch [3700][5500]\t Training Loss 0.3193\t Accuracy 0.9105\n",
      "Epoch [14][20]\t Batch [3750][5500]\t Training Loss 0.3215\t Accuracy 0.9097\n",
      "Epoch [14][20]\t Batch [3800][5500]\t Training Loss 0.3217\t Accuracy 0.9096\n",
      "Epoch [14][20]\t Batch [3850][5500]\t Training Loss 0.3211\t Accuracy 0.9098\n",
      "Epoch [14][20]\t Batch [3900][5500]\t Training Loss 0.3209\t Accuracy 0.9098\n",
      "Epoch [14][20]\t Batch [3950][5500]\t Training Loss 0.3214\t Accuracy 0.9097\n",
      "Epoch [14][20]\t Batch [4000][5500]\t Training Loss 0.3215\t Accuracy 0.9096\n",
      "Epoch [14][20]\t Batch [4050][5500]\t Training Loss 0.3210\t Accuracy 0.9097\n",
      "Epoch [14][20]\t Batch [4100][5500]\t Training Loss 0.3203\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [4150][5500]\t Training Loss 0.3210\t Accuracy 0.9097\n",
      "Epoch [14][20]\t Batch [4200][5500]\t Training Loss 0.3210\t Accuracy 0.9097\n",
      "Epoch [14][20]\t Batch [4250][5500]\t Training Loss 0.3224\t Accuracy 0.9093\n",
      "Epoch [14][20]\t Batch [4300][5500]\t Training Loss 0.3227\t Accuracy 0.9093\n",
      "Epoch [14][20]\t Batch [4350][5500]\t Training Loss 0.3219\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [4400][5500]\t Training Loss 0.3218\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [4450][5500]\t Training Loss 0.3221\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [4500][5500]\t Training Loss 0.3217\t Accuracy 0.9094\n",
      "Epoch [14][20]\t Batch [4550][5500]\t Training Loss 0.3221\t Accuracy 0.9093\n",
      "Epoch [14][20]\t Batch [4600][5500]\t Training Loss 0.3225\t Accuracy 0.9091\n",
      "Epoch [14][20]\t Batch [4650][5500]\t Training Loss 0.3234\t Accuracy 0.9089\n",
      "Epoch [14][20]\t Batch [4700][5500]\t Training Loss 0.3224\t Accuracy 0.9092\n",
      "Epoch [14][20]\t Batch [4750][5500]\t Training Loss 0.3225\t Accuracy 0.9090\n",
      "Epoch [14][20]\t Batch [4800][5500]\t Training Loss 0.3227\t Accuracy 0.9090\n",
      "Epoch [14][20]\t Batch [4850][5500]\t Training Loss 0.3219\t Accuracy 0.9093\n",
      "Epoch [14][20]\t Batch [4900][5500]\t Training Loss 0.3217\t Accuracy 0.9092\n",
      "Epoch [14][20]\t Batch [4950][5500]\t Training Loss 0.3220\t Accuracy 0.9092\n",
      "Epoch [14][20]\t Batch [5000][5500]\t Training Loss 0.3230\t Accuracy 0.9090\n",
      "Epoch [14][20]\t Batch [5050][5500]\t Training Loss 0.3239\t Accuracy 0.9088\n",
      "Epoch [14][20]\t Batch [5100][5500]\t Training Loss 0.3238\t Accuracy 0.9087\n",
      "Epoch [14][20]\t Batch [5150][5500]\t Training Loss 0.3234\t Accuracy 0.9087\n",
      "Epoch [14][20]\t Batch [5200][5500]\t Training Loss 0.3228\t Accuracy 0.9090\n",
      "Epoch [14][20]\t Batch [5250][5500]\t Training Loss 0.3232\t Accuracy 0.9088\n",
      "Epoch [14][20]\t Batch [5300][5500]\t Training Loss 0.3240\t Accuracy 0.9085\n",
      "Epoch [14][20]\t Batch [5350][5500]\t Training Loss 0.3235\t Accuracy 0.9087\n",
      "Epoch [14][20]\t Batch [5400][5500]\t Training Loss 0.3237\t Accuracy 0.9086\n",
      "Epoch [14][20]\t Batch [5450][5500]\t Training Loss 0.3233\t Accuracy 0.9086\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3234\t Average training accuracy 0.9086\n",
      "Epoch [14]\t Average validation loss 0.2543\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [15][20]\t Batch [0][5500]\t Training Loss 0.0699\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][5500]\t Training Loss 0.3000\t Accuracy 0.9196\n",
      "Epoch [15][20]\t Batch [100][5500]\t Training Loss 0.3324\t Accuracy 0.9059\n",
      "Epoch [15][20]\t Batch [150][5500]\t Training Loss 0.3502\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [200][5500]\t Training Loss 0.3239\t Accuracy 0.9090\n",
      "Epoch [15][20]\t Batch [250][5500]\t Training Loss 0.3093\t Accuracy 0.9124\n",
      "Epoch [15][20]\t Batch [300][5500]\t Training Loss 0.3019\t Accuracy 0.9163\n",
      "Epoch [15][20]\t Batch [350][5500]\t Training Loss 0.2969\t Accuracy 0.9177\n",
      "Epoch [15][20]\t Batch [400][5500]\t Training Loss 0.2949\t Accuracy 0.9195\n",
      "Epoch [15][20]\t Batch [450][5500]\t Training Loss 0.2947\t Accuracy 0.9200\n",
      "Epoch [15][20]\t Batch [500][5500]\t Training Loss 0.2922\t Accuracy 0.9202\n",
      "Epoch [15][20]\t Batch [550][5500]\t Training Loss 0.2941\t Accuracy 0.9201\n",
      "Epoch [15][20]\t Batch [600][5500]\t Training Loss 0.2933\t Accuracy 0.9203\n",
      "Epoch [15][20]\t Batch [650][5500]\t Training Loss 0.2897\t Accuracy 0.9218\n",
      "Epoch [15][20]\t Batch [700][5500]\t Training Loss 0.2895\t Accuracy 0.9210\n",
      "Epoch [15][20]\t Batch [750][5500]\t Training Loss 0.2925\t Accuracy 0.9206\n",
      "Epoch [15][20]\t Batch [800][5500]\t Training Loss 0.2953\t Accuracy 0.9196\n",
      "Epoch [15][20]\t Batch [850][5500]\t Training Loss 0.2988\t Accuracy 0.9186\n",
      "Epoch [15][20]\t Batch [900][5500]\t Training Loss 0.3062\t Accuracy 0.9165\n",
      "Epoch [15][20]\t Batch [950][5500]\t Training Loss 0.3068\t Accuracy 0.9163\n",
      "Epoch [15][20]\t Batch [1000][5500]\t Training Loss 0.3044\t Accuracy 0.9167\n",
      "Epoch [15][20]\t Batch [1050][5500]\t Training Loss 0.3030\t Accuracy 0.9167\n",
      "Epoch [15][20]\t Batch [1100][5500]\t Training Loss 0.3000\t Accuracy 0.9180\n",
      "Epoch [15][20]\t Batch [1150][5500]\t Training Loss 0.2979\t Accuracy 0.9185\n",
      "Epoch [15][20]\t Batch [1200][5500]\t Training Loss 0.3022\t Accuracy 0.9167\n",
      "Epoch [15][20]\t Batch [1250][5500]\t Training Loss 0.3031\t Accuracy 0.9163\n",
      "Epoch [15][20]\t Batch [1300][5500]\t Training Loss 0.3066\t Accuracy 0.9153\n",
      "Epoch [15][20]\t Batch [1350][5500]\t Training Loss 0.3083\t Accuracy 0.9147\n",
      "Epoch [15][20]\t Batch [1400][5500]\t Training Loss 0.3099\t Accuracy 0.9141\n",
      "Epoch [15][20]\t Batch [1450][5500]\t Training Loss 0.3130\t Accuracy 0.9131\n",
      "Epoch [15][20]\t Batch [1500][5500]\t Training Loss 0.3169\t Accuracy 0.9121\n",
      "Epoch [15][20]\t Batch [1550][5500]\t Training Loss 0.3160\t Accuracy 0.9126\n",
      "Epoch [15][20]\t Batch [1600][5500]\t Training Loss 0.3170\t Accuracy 0.9120\n",
      "Epoch [15][20]\t Batch [1650][5500]\t Training Loss 0.3160\t Accuracy 0.9122\n",
      "Epoch [15][20]\t Batch [1700][5500]\t Training Loss 0.3168\t Accuracy 0.9116\n",
      "Epoch [15][20]\t Batch [1750][5500]\t Training Loss 0.3171\t Accuracy 0.9114\n",
      "Epoch [15][20]\t Batch [1800][5500]\t Training Loss 0.3186\t Accuracy 0.9105\n",
      "Epoch [15][20]\t Batch [1850][5500]\t Training Loss 0.3165\t Accuracy 0.9113\n",
      "Epoch [15][20]\t Batch [1900][5500]\t Training Loss 0.3148\t Accuracy 0.9120\n",
      "Epoch [15][20]\t Batch [1950][5500]\t Training Loss 0.3144\t Accuracy 0.9121\n",
      "Epoch [15][20]\t Batch [2000][5500]\t Training Loss 0.3130\t Accuracy 0.9126\n",
      "Epoch [15][20]\t Batch [2050][5500]\t Training Loss 0.3122\t Accuracy 0.9131\n",
      "Epoch [15][20]\t Batch [2100][5500]\t Training Loss 0.3148\t Accuracy 0.9125\n",
      "Epoch [15][20]\t Batch [2150][5500]\t Training Loss 0.3136\t Accuracy 0.9133\n",
      "Epoch [15][20]\t Batch [2200][5500]\t Training Loss 0.3123\t Accuracy 0.9138\n",
      "Epoch [15][20]\t Batch [2250][5500]\t Training Loss 0.3120\t Accuracy 0.9137\n",
      "Epoch [15][20]\t Batch [2300][5500]\t Training Loss 0.3120\t Accuracy 0.9133\n",
      "Epoch [15][20]\t Batch [2350][5500]\t Training Loss 0.3116\t Accuracy 0.9133\n",
      "Epoch [15][20]\t Batch [2400][5500]\t Training Loss 0.3119\t Accuracy 0.9132\n",
      "Epoch [15][20]\t Batch [2450][5500]\t Training Loss 0.3116\t Accuracy 0.9130\n",
      "Epoch [15][20]\t Batch [2500][5500]\t Training Loss 0.3131\t Accuracy 0.9124\n",
      "Epoch [15][20]\t Batch [2550][5500]\t Training Loss 0.3121\t Accuracy 0.9126\n",
      "Epoch [15][20]\t Batch [2600][5500]\t Training Loss 0.3114\t Accuracy 0.9130\n",
      "Epoch [15][20]\t Batch [2650][5500]\t Training Loss 0.3114\t Accuracy 0.9133\n",
      "Epoch [15][20]\t Batch [2700][5500]\t Training Loss 0.3127\t Accuracy 0.9129\n",
      "Epoch [15][20]\t Batch [2750][5500]\t Training Loss 0.3130\t Accuracy 0.9128\n",
      "Epoch [15][20]\t Batch [2800][5500]\t Training Loss 0.3124\t Accuracy 0.9128\n",
      "Epoch [15][20]\t Batch [2850][5500]\t Training Loss 0.3119\t Accuracy 0.9131\n",
      "Epoch [15][20]\t Batch [2900][5500]\t Training Loss 0.3117\t Accuracy 0.9132\n",
      "Epoch [15][20]\t Batch [2950][5500]\t Training Loss 0.3120\t Accuracy 0.9132\n",
      "Epoch [15][20]\t Batch [3000][5500]\t Training Loss 0.3131\t Accuracy 0.9128\n",
      "Epoch [15][20]\t Batch [3050][5500]\t Training Loss 0.3133\t Accuracy 0.9126\n",
      "Epoch [15][20]\t Batch [3100][5500]\t Training Loss 0.3143\t Accuracy 0.9124\n",
      "Epoch [15][20]\t Batch [3150][5500]\t Training Loss 0.3161\t Accuracy 0.9120\n",
      "Epoch [15][20]\t Batch [3200][5500]\t Training Loss 0.3168\t Accuracy 0.9115\n",
      "Epoch [15][20]\t Batch [3250][5500]\t Training Loss 0.3181\t Accuracy 0.9107\n",
      "Epoch [15][20]\t Batch [3300][5500]\t Training Loss 0.3180\t Accuracy 0.9108\n",
      "Epoch [15][20]\t Batch [3350][5500]\t Training Loss 0.3182\t Accuracy 0.9108\n",
      "Epoch [15][20]\t Batch [3400][5500]\t Training Loss 0.3165\t Accuracy 0.9113\n",
      "Epoch [15][20]\t Batch [3450][5500]\t Training Loss 0.3158\t Accuracy 0.9116\n",
      "Epoch [15][20]\t Batch [3500][5500]\t Training Loss 0.3163\t Accuracy 0.9112\n",
      "Epoch [15][20]\t Batch [3550][5500]\t Training Loss 0.3159\t Accuracy 0.9113\n",
      "Epoch [15][20]\t Batch [3600][5500]\t Training Loss 0.3152\t Accuracy 0.9115\n",
      "Epoch [15][20]\t Batch [3650][5500]\t Training Loss 0.3150\t Accuracy 0.9115\n",
      "Epoch [15][20]\t Batch [3700][5500]\t Training Loss 0.3140\t Accuracy 0.9118\n",
      "Epoch [15][20]\t Batch [3750][5500]\t Training Loss 0.3162\t Accuracy 0.9110\n",
      "Epoch [15][20]\t Batch [3800][5500]\t Training Loss 0.3164\t Accuracy 0.9110\n",
      "Epoch [15][20]\t Batch [3850][5500]\t Training Loss 0.3158\t Accuracy 0.9112\n",
      "Epoch [15][20]\t Batch [3900][5500]\t Training Loss 0.3156\t Accuracy 0.9112\n",
      "Epoch [15][20]\t Batch [3950][5500]\t Training Loss 0.3161\t Accuracy 0.9111\n",
      "Epoch [15][20]\t Batch [4000][5500]\t Training Loss 0.3162\t Accuracy 0.9109\n",
      "Epoch [15][20]\t Batch [4050][5500]\t Training Loss 0.3157\t Accuracy 0.9111\n",
      "Epoch [15][20]\t Batch [4100][5500]\t Training Loss 0.3150\t Accuracy 0.9114\n",
      "Epoch [15][20]\t Batch [4150][5500]\t Training Loss 0.3157\t Accuracy 0.9111\n",
      "Epoch [15][20]\t Batch [4200][5500]\t Training Loss 0.3157\t Accuracy 0.9111\n",
      "Epoch [15][20]\t Batch [4250][5500]\t Training Loss 0.3171\t Accuracy 0.9107\n",
      "Epoch [15][20]\t Batch [4300][5500]\t Training Loss 0.3173\t Accuracy 0.9106\n",
      "Epoch [15][20]\t Batch [4350][5500]\t Training Loss 0.3166\t Accuracy 0.9108\n",
      "Epoch [15][20]\t Batch [4400][5500]\t Training Loss 0.3165\t Accuracy 0.9108\n",
      "Epoch [15][20]\t Batch [4450][5500]\t Training Loss 0.3168\t Accuracy 0.9108\n",
      "Epoch [15][20]\t Batch [4500][5500]\t Training Loss 0.3164\t Accuracy 0.9109\n",
      "Epoch [15][20]\t Batch [4550][5500]\t Training Loss 0.3168\t Accuracy 0.9107\n",
      "Epoch [15][20]\t Batch [4600][5500]\t Training Loss 0.3172\t Accuracy 0.9105\n",
      "Epoch [15][20]\t Batch [4650][5500]\t Training Loss 0.3181\t Accuracy 0.9103\n",
      "Epoch [15][20]\t Batch [4700][5500]\t Training Loss 0.3172\t Accuracy 0.9106\n",
      "Epoch [15][20]\t Batch [4750][5500]\t Training Loss 0.3173\t Accuracy 0.9104\n",
      "Epoch [15][20]\t Batch [4800][5500]\t Training Loss 0.3174\t Accuracy 0.9104\n",
      "Epoch [15][20]\t Batch [4850][5500]\t Training Loss 0.3166\t Accuracy 0.9107\n",
      "Epoch [15][20]\t Batch [4900][5500]\t Training Loss 0.3164\t Accuracy 0.9106\n",
      "Epoch [15][20]\t Batch [4950][5500]\t Training Loss 0.3168\t Accuracy 0.9106\n",
      "Epoch [15][20]\t Batch [5000][5500]\t Training Loss 0.3177\t Accuracy 0.9104\n",
      "Epoch [15][20]\t Batch [5050][5500]\t Training Loss 0.3186\t Accuracy 0.9102\n",
      "Epoch [15][20]\t Batch [5100][5500]\t Training Loss 0.3185\t Accuracy 0.9101\n",
      "Epoch [15][20]\t Batch [5150][5500]\t Training Loss 0.3181\t Accuracy 0.9101\n",
      "Epoch [15][20]\t Batch [5200][5500]\t Training Loss 0.3176\t Accuracy 0.9104\n",
      "Epoch [15][20]\t Batch [5250][5500]\t Training Loss 0.3180\t Accuracy 0.9103\n",
      "Epoch [15][20]\t Batch [5300][5500]\t Training Loss 0.3188\t Accuracy 0.9100\n",
      "Epoch [15][20]\t Batch [5350][5500]\t Training Loss 0.3182\t Accuracy 0.9101\n",
      "Epoch [15][20]\t Batch [5400][5500]\t Training Loss 0.3185\t Accuracy 0.9100\n",
      "Epoch [15][20]\t Batch [5450][5500]\t Training Loss 0.3181\t Accuracy 0.9101\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3181\t Average training accuracy 0.9100\n",
      "Epoch [15]\t Average validation loss 0.2504\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [16][20]\t Batch [0][5500]\t Training Loss 0.0661\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][5500]\t Training Loss 0.2953\t Accuracy 0.9196\n",
      "Epoch [16][20]\t Batch [100][5500]\t Training Loss 0.3272\t Accuracy 0.9050\n",
      "Epoch [16][20]\t Batch [150][5500]\t Training Loss 0.3456\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [200][5500]\t Training Loss 0.3192\t Accuracy 0.9095\n",
      "Epoch [16][20]\t Batch [250][5500]\t Training Loss 0.3046\t Accuracy 0.9131\n",
      "Epoch [16][20]\t Batch [300][5500]\t Training Loss 0.2972\t Accuracy 0.9173\n",
      "Epoch [16][20]\t Batch [350][5500]\t Training Loss 0.2920\t Accuracy 0.9185\n",
      "Epoch [16][20]\t Batch [400][5500]\t Training Loss 0.2900\t Accuracy 0.9202\n",
      "Epoch [16][20]\t Batch [450][5500]\t Training Loss 0.2899\t Accuracy 0.9211\n",
      "Epoch [16][20]\t Batch [500][5500]\t Training Loss 0.2873\t Accuracy 0.9212\n",
      "Epoch [16][20]\t Batch [550][5500]\t Training Loss 0.2893\t Accuracy 0.9212\n",
      "Epoch [16][20]\t Batch [600][5500]\t Training Loss 0.2885\t Accuracy 0.9218\n",
      "Epoch [16][20]\t Batch [650][5500]\t Training Loss 0.2850\t Accuracy 0.9232\n",
      "Epoch [16][20]\t Batch [700][5500]\t Training Loss 0.2848\t Accuracy 0.9223\n",
      "Epoch [16][20]\t Batch [750][5500]\t Training Loss 0.2878\t Accuracy 0.9218\n",
      "Epoch [16][20]\t Batch [800][5500]\t Training Loss 0.2904\t Accuracy 0.9207\n",
      "Epoch [16][20]\t Batch [850][5500]\t Training Loss 0.2939\t Accuracy 0.9197\n",
      "Epoch [16][20]\t Batch [900][5500]\t Training Loss 0.3014\t Accuracy 0.9180\n",
      "Epoch [16][20]\t Batch [950][5500]\t Training Loss 0.3020\t Accuracy 0.9177\n",
      "Epoch [16][20]\t Batch [1000][5500]\t Training Loss 0.2996\t Accuracy 0.9180\n",
      "Epoch [16][20]\t Batch [1050][5500]\t Training Loss 0.2982\t Accuracy 0.9180\n",
      "Epoch [16][20]\t Batch [1100][5500]\t Training Loss 0.2953\t Accuracy 0.9192\n",
      "Epoch [16][20]\t Batch [1150][5500]\t Training Loss 0.2931\t Accuracy 0.9196\n",
      "Epoch [16][20]\t Batch [1200][5500]\t Training Loss 0.2975\t Accuracy 0.9179\n",
      "Epoch [16][20]\t Batch [1250][5500]\t Training Loss 0.2983\t Accuracy 0.9174\n",
      "Epoch [16][20]\t Batch [1300][5500]\t Training Loss 0.3019\t Accuracy 0.9165\n",
      "Epoch [16][20]\t Batch [1350][5500]\t Training Loss 0.3035\t Accuracy 0.9160\n",
      "Epoch [16][20]\t Batch [1400][5500]\t Training Loss 0.3051\t Accuracy 0.9153\n",
      "Epoch [16][20]\t Batch [1450][5500]\t Training Loss 0.3082\t Accuracy 0.9143\n",
      "Epoch [16][20]\t Batch [1500][5500]\t Training Loss 0.3120\t Accuracy 0.9133\n",
      "Epoch [16][20]\t Batch [1550][5500]\t Training Loss 0.3112\t Accuracy 0.9138\n",
      "Epoch [16][20]\t Batch [1600][5500]\t Training Loss 0.3122\t Accuracy 0.9132\n",
      "Epoch [16][20]\t Batch [1650][5500]\t Training Loss 0.3111\t Accuracy 0.9134\n",
      "Epoch [16][20]\t Batch [1700][5500]\t Training Loss 0.3119\t Accuracy 0.9128\n",
      "Epoch [16][20]\t Batch [1750][5500]\t Training Loss 0.3122\t Accuracy 0.9126\n",
      "Epoch [16][20]\t Batch [1800][5500]\t Training Loss 0.3137\t Accuracy 0.9118\n",
      "Epoch [16][20]\t Batch [1850][5500]\t Training Loss 0.3116\t Accuracy 0.9127\n",
      "Epoch [16][20]\t Batch [1900][5500]\t Training Loss 0.3099\t Accuracy 0.9134\n",
      "Epoch [16][20]\t Batch [1950][5500]\t Training Loss 0.3095\t Accuracy 0.9135\n",
      "Epoch [16][20]\t Batch [2000][5500]\t Training Loss 0.3081\t Accuracy 0.9138\n",
      "Epoch [16][20]\t Batch [2050][5500]\t Training Loss 0.3073\t Accuracy 0.9144\n",
      "Epoch [16][20]\t Batch [2100][5500]\t Training Loss 0.3099\t Accuracy 0.9138\n",
      "Epoch [16][20]\t Batch [2150][5500]\t Training Loss 0.3087\t Accuracy 0.9146\n",
      "Epoch [16][20]\t Batch [2200][5500]\t Training Loss 0.3074\t Accuracy 0.9149\n",
      "Epoch [16][20]\t Batch [2250][5500]\t Training Loss 0.3072\t Accuracy 0.9149\n",
      "Epoch [16][20]\t Batch [2300][5500]\t Training Loss 0.3071\t Accuracy 0.9145\n",
      "Epoch [16][20]\t Batch [2350][5500]\t Training Loss 0.3068\t Accuracy 0.9145\n",
      "Epoch [16][20]\t Batch [2400][5500]\t Training Loss 0.3071\t Accuracy 0.9143\n",
      "Epoch [16][20]\t Batch [2450][5500]\t Training Loss 0.3068\t Accuracy 0.9142\n",
      "Epoch [16][20]\t Batch [2500][5500]\t Training Loss 0.3083\t Accuracy 0.9135\n",
      "Epoch [16][20]\t Batch [2550][5500]\t Training Loss 0.3073\t Accuracy 0.9137\n",
      "Epoch [16][20]\t Batch [2600][5500]\t Training Loss 0.3066\t Accuracy 0.9141\n",
      "Epoch [16][20]\t Batch [2650][5500]\t Training Loss 0.3066\t Accuracy 0.9144\n",
      "Epoch [16][20]\t Batch [2700][5500]\t Training Loss 0.3079\t Accuracy 0.9141\n",
      "Epoch [16][20]\t Batch [2750][5500]\t Training Loss 0.3082\t Accuracy 0.9140\n",
      "Epoch [16][20]\t Batch [2800][5500]\t Training Loss 0.3076\t Accuracy 0.9140\n",
      "Epoch [16][20]\t Batch [2850][5500]\t Training Loss 0.3071\t Accuracy 0.9142\n",
      "Epoch [16][20]\t Batch [2900][5500]\t Training Loss 0.3069\t Accuracy 0.9143\n",
      "Epoch [16][20]\t Batch [2950][5500]\t Training Loss 0.3072\t Accuracy 0.9143\n",
      "Epoch [16][20]\t Batch [3000][5500]\t Training Loss 0.3083\t Accuracy 0.9138\n",
      "Epoch [16][20]\t Batch [3050][5500]\t Training Loss 0.3085\t Accuracy 0.9136\n",
      "Epoch [16][20]\t Batch [3100][5500]\t Training Loss 0.3095\t Accuracy 0.9134\n",
      "Epoch [16][20]\t Batch [3150][5500]\t Training Loss 0.3112\t Accuracy 0.9130\n",
      "Epoch [16][20]\t Batch [3200][5500]\t Training Loss 0.3119\t Accuracy 0.9125\n",
      "Epoch [16][20]\t Batch [3250][5500]\t Training Loss 0.3132\t Accuracy 0.9118\n",
      "Epoch [16][20]\t Batch [3300][5500]\t Training Loss 0.3131\t Accuracy 0.9119\n",
      "Epoch [16][20]\t Batch [3350][5500]\t Training Loss 0.3132\t Accuracy 0.9118\n",
      "Epoch [16][20]\t Batch [3400][5500]\t Training Loss 0.3115\t Accuracy 0.9123\n",
      "Epoch [16][20]\t Batch [3450][5500]\t Training Loss 0.3109\t Accuracy 0.9126\n",
      "Epoch [16][20]\t Batch [3500][5500]\t Training Loss 0.3114\t Accuracy 0.9123\n",
      "Epoch [16][20]\t Batch [3550][5500]\t Training Loss 0.3110\t Accuracy 0.9124\n",
      "Epoch [16][20]\t Batch [3600][5500]\t Training Loss 0.3103\t Accuracy 0.9126\n",
      "Epoch [16][20]\t Batch [3650][5500]\t Training Loss 0.3101\t Accuracy 0.9126\n",
      "Epoch [16][20]\t Batch [3700][5500]\t Training Loss 0.3091\t Accuracy 0.9129\n",
      "Epoch [16][20]\t Batch [3750][5500]\t Training Loss 0.3113\t Accuracy 0.9121\n",
      "Epoch [16][20]\t Batch [3800][5500]\t Training Loss 0.3116\t Accuracy 0.9121\n",
      "Epoch [16][20]\t Batch [3850][5500]\t Training Loss 0.3110\t Accuracy 0.9123\n",
      "Epoch [16][20]\t Batch [3900][5500]\t Training Loss 0.3107\t Accuracy 0.9123\n",
      "Epoch [16][20]\t Batch [3950][5500]\t Training Loss 0.3113\t Accuracy 0.9121\n",
      "Epoch [16][20]\t Batch [4000][5500]\t Training Loss 0.3113\t Accuracy 0.9120\n",
      "Epoch [16][20]\t Batch [4050][5500]\t Training Loss 0.3108\t Accuracy 0.9122\n",
      "Epoch [16][20]\t Batch [4100][5500]\t Training Loss 0.3102\t Accuracy 0.9125\n",
      "Epoch [16][20]\t Batch [4150][5500]\t Training Loss 0.3109\t Accuracy 0.9121\n",
      "Epoch [16][20]\t Batch [4200][5500]\t Training Loss 0.3109\t Accuracy 0.9121\n",
      "Epoch [16][20]\t Batch [4250][5500]\t Training Loss 0.3122\t Accuracy 0.9117\n",
      "Epoch [16][20]\t Batch [4300][5500]\t Training Loss 0.3125\t Accuracy 0.9116\n",
      "Epoch [16][20]\t Batch [4350][5500]\t Training Loss 0.3117\t Accuracy 0.9119\n",
      "Epoch [16][20]\t Batch [4400][5500]\t Training Loss 0.3117\t Accuracy 0.9118\n",
      "Epoch [16][20]\t Batch [4450][5500]\t Training Loss 0.3120\t Accuracy 0.9118\n",
      "Epoch [16][20]\t Batch [4500][5500]\t Training Loss 0.3116\t Accuracy 0.9119\n",
      "Epoch [16][20]\t Batch [4550][5500]\t Training Loss 0.3120\t Accuracy 0.9117\n",
      "Epoch [16][20]\t Batch [4600][5500]\t Training Loss 0.3124\t Accuracy 0.9116\n",
      "Epoch [16][20]\t Batch [4650][5500]\t Training Loss 0.3133\t Accuracy 0.9113\n",
      "Epoch [16][20]\t Batch [4700][5500]\t Training Loss 0.3123\t Accuracy 0.9116\n",
      "Epoch [16][20]\t Batch [4750][5500]\t Training Loss 0.3124\t Accuracy 0.9115\n",
      "Epoch [16][20]\t Batch [4800][5500]\t Training Loss 0.3126\t Accuracy 0.9114\n",
      "Epoch [16][20]\t Batch [4850][5500]\t Training Loss 0.3118\t Accuracy 0.9117\n",
      "Epoch [16][20]\t Batch [4900][5500]\t Training Loss 0.3116\t Accuracy 0.9116\n",
      "Epoch [16][20]\t Batch [4950][5500]\t Training Loss 0.3120\t Accuracy 0.9117\n",
      "Epoch [16][20]\t Batch [5000][5500]\t Training Loss 0.3129\t Accuracy 0.9114\n",
      "Epoch [16][20]\t Batch [5050][5500]\t Training Loss 0.3138\t Accuracy 0.9112\n",
      "Epoch [16][20]\t Batch [5100][5500]\t Training Loss 0.3137\t Accuracy 0.9112\n",
      "Epoch [16][20]\t Batch [5150][5500]\t Training Loss 0.3133\t Accuracy 0.9111\n",
      "Epoch [16][20]\t Batch [5200][5500]\t Training Loss 0.3128\t Accuracy 0.9114\n",
      "Epoch [16][20]\t Batch [5250][5500]\t Training Loss 0.3131\t Accuracy 0.9113\n",
      "Epoch [16][20]\t Batch [5300][5500]\t Training Loss 0.3139\t Accuracy 0.9110\n",
      "Epoch [16][20]\t Batch [5350][5500]\t Training Loss 0.3134\t Accuracy 0.9112\n",
      "Epoch [16][20]\t Batch [5400][5500]\t Training Loss 0.3137\t Accuracy 0.9110\n",
      "Epoch [16][20]\t Batch [5450][5500]\t Training Loss 0.3133\t Accuracy 0.9111\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3134\t Average training accuracy 0.9110\n",
      "Epoch [16]\t Average validation loss 0.2468\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [17][20]\t Batch [0][5500]\t Training Loss 0.0628\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][5500]\t Training Loss 0.2910\t Accuracy 0.9216\n",
      "Epoch [17][20]\t Batch [100][5500]\t Training Loss 0.3225\t Accuracy 0.9059\n",
      "Epoch [17][20]\t Batch [150][5500]\t Training Loss 0.3414\t Accuracy 0.9013\n",
      "Epoch [17][20]\t Batch [200][5500]\t Training Loss 0.3150\t Accuracy 0.9109\n",
      "Epoch [17][20]\t Batch [250][5500]\t Training Loss 0.3002\t Accuracy 0.9151\n",
      "Epoch [17][20]\t Batch [300][5500]\t Training Loss 0.2928\t Accuracy 0.9189\n",
      "Epoch [17][20]\t Batch [350][5500]\t Training Loss 0.2876\t Accuracy 0.9205\n",
      "Epoch [17][20]\t Batch [400][5500]\t Training Loss 0.2855\t Accuracy 0.9222\n",
      "Epoch [17][20]\t Batch [450][5500]\t Training Loss 0.2854\t Accuracy 0.9228\n",
      "Epoch [17][20]\t Batch [500][5500]\t Training Loss 0.2829\t Accuracy 0.9228\n",
      "Epoch [17][20]\t Batch [550][5500]\t Training Loss 0.2848\t Accuracy 0.9227\n",
      "Epoch [17][20]\t Batch [600][5500]\t Training Loss 0.2841\t Accuracy 0.9231\n",
      "Epoch [17][20]\t Batch [650][5500]\t Training Loss 0.2806\t Accuracy 0.9246\n",
      "Epoch [17][20]\t Batch [700][5500]\t Training Loss 0.2804\t Accuracy 0.9237\n",
      "Epoch [17][20]\t Batch [750][5500]\t Training Loss 0.2834\t Accuracy 0.9232\n",
      "Epoch [17][20]\t Batch [800][5500]\t Training Loss 0.2860\t Accuracy 0.9220\n",
      "Epoch [17][20]\t Batch [850][5500]\t Training Loss 0.2895\t Accuracy 0.9209\n",
      "Epoch [17][20]\t Batch [900][5500]\t Training Loss 0.2969\t Accuracy 0.9192\n",
      "Epoch [17][20]\t Batch [950][5500]\t Training Loss 0.2976\t Accuracy 0.9188\n",
      "Epoch [17][20]\t Batch [1000][5500]\t Training Loss 0.2951\t Accuracy 0.9191\n",
      "Epoch [17][20]\t Batch [1050][5500]\t Training Loss 0.2938\t Accuracy 0.9191\n",
      "Epoch [17][20]\t Batch [1100][5500]\t Training Loss 0.2909\t Accuracy 0.9203\n",
      "Epoch [17][20]\t Batch [1150][5500]\t Training Loss 0.2888\t Accuracy 0.9209\n",
      "Epoch [17][20]\t Batch [1200][5500]\t Training Loss 0.2931\t Accuracy 0.9192\n",
      "Epoch [17][20]\t Batch [1250][5500]\t Training Loss 0.2939\t Accuracy 0.9186\n",
      "Epoch [17][20]\t Batch [1300][5500]\t Training Loss 0.2975\t Accuracy 0.9178\n",
      "Epoch [17][20]\t Batch [1350][5500]\t Training Loss 0.2991\t Accuracy 0.9172\n",
      "Epoch [17][20]\t Batch [1400][5500]\t Training Loss 0.3006\t Accuracy 0.9164\n",
      "Epoch [17][20]\t Batch [1450][5500]\t Training Loss 0.3037\t Accuracy 0.9154\n",
      "Epoch [17][20]\t Batch [1500][5500]\t Training Loss 0.3076\t Accuracy 0.9144\n",
      "Epoch [17][20]\t Batch [1550][5500]\t Training Loss 0.3067\t Accuracy 0.9150\n",
      "Epoch [17][20]\t Batch [1600][5500]\t Training Loss 0.3077\t Accuracy 0.9144\n",
      "Epoch [17][20]\t Batch [1650][5500]\t Training Loss 0.3066\t Accuracy 0.9146\n",
      "Epoch [17][20]\t Batch [1700][5500]\t Training Loss 0.3073\t Accuracy 0.9141\n",
      "Epoch [17][20]\t Batch [1750][5500]\t Training Loss 0.3076\t Accuracy 0.9138\n",
      "Epoch [17][20]\t Batch [1800][5500]\t Training Loss 0.3091\t Accuracy 0.9130\n",
      "Epoch [17][20]\t Batch [1850][5500]\t Training Loss 0.3071\t Accuracy 0.9139\n",
      "Epoch [17][20]\t Batch [1900][5500]\t Training Loss 0.3054\t Accuracy 0.9145\n",
      "Epoch [17][20]\t Batch [1950][5500]\t Training Loss 0.3050\t Accuracy 0.9146\n",
      "Epoch [17][20]\t Batch [2000][5500]\t Training Loss 0.3036\t Accuracy 0.9149\n",
      "Epoch [17][20]\t Batch [2050][5500]\t Training Loss 0.3028\t Accuracy 0.9155\n",
      "Epoch [17][20]\t Batch [2100][5500]\t Training Loss 0.3054\t Accuracy 0.9149\n",
      "Epoch [17][20]\t Batch [2150][5500]\t Training Loss 0.3043\t Accuracy 0.9157\n",
      "Epoch [17][20]\t Batch [2200][5500]\t Training Loss 0.3030\t Accuracy 0.9160\n",
      "Epoch [17][20]\t Batch [2250][5500]\t Training Loss 0.3027\t Accuracy 0.9159\n",
      "Epoch [17][20]\t Batch [2300][5500]\t Training Loss 0.3027\t Accuracy 0.9155\n",
      "Epoch [17][20]\t Batch [2350][5500]\t Training Loss 0.3023\t Accuracy 0.9155\n",
      "Epoch [17][20]\t Batch [2400][5500]\t Training Loss 0.3027\t Accuracy 0.9153\n",
      "Epoch [17][20]\t Batch [2450][5500]\t Training Loss 0.3023\t Accuracy 0.9152\n",
      "Epoch [17][20]\t Batch [2500][5500]\t Training Loss 0.3038\t Accuracy 0.9146\n",
      "Epoch [17][20]\t Batch [2550][5500]\t Training Loss 0.3028\t Accuracy 0.9148\n",
      "Epoch [17][20]\t Batch [2600][5500]\t Training Loss 0.3022\t Accuracy 0.9151\n",
      "Epoch [17][20]\t Batch [2650][5500]\t Training Loss 0.3022\t Accuracy 0.9155\n",
      "Epoch [17][20]\t Batch [2700][5500]\t Training Loss 0.3035\t Accuracy 0.9151\n",
      "Epoch [17][20]\t Batch [2750][5500]\t Training Loss 0.3038\t Accuracy 0.9150\n",
      "Epoch [17][20]\t Batch [2800][5500]\t Training Loss 0.3032\t Accuracy 0.9150\n",
      "Epoch [17][20]\t Batch [2850][5500]\t Training Loss 0.3027\t Accuracy 0.9153\n",
      "Epoch [17][20]\t Batch [2900][5500]\t Training Loss 0.3025\t Accuracy 0.9153\n",
      "Epoch [17][20]\t Batch [2950][5500]\t Training Loss 0.3028\t Accuracy 0.9153\n",
      "Epoch [17][20]\t Batch [3000][5500]\t Training Loss 0.3038\t Accuracy 0.9150\n",
      "Epoch [17][20]\t Batch [3050][5500]\t Training Loss 0.3040\t Accuracy 0.9147\n",
      "Epoch [17][20]\t Batch [3100][5500]\t Training Loss 0.3050\t Accuracy 0.9145\n",
      "Epoch [17][20]\t Batch [3150][5500]\t Training Loss 0.3067\t Accuracy 0.9142\n",
      "Epoch [17][20]\t Batch [3200][5500]\t Training Loss 0.3074\t Accuracy 0.9137\n",
      "Epoch [17][20]\t Batch [3250][5500]\t Training Loss 0.3087\t Accuracy 0.9129\n",
      "Epoch [17][20]\t Batch [3300][5500]\t Training Loss 0.3086\t Accuracy 0.9130\n",
      "Epoch [17][20]\t Batch [3350][5500]\t Training Loss 0.3087\t Accuracy 0.9129\n",
      "Epoch [17][20]\t Batch [3400][5500]\t Training Loss 0.3070\t Accuracy 0.9134\n",
      "Epoch [17][20]\t Batch [3450][5500]\t Training Loss 0.3064\t Accuracy 0.9137\n",
      "Epoch [17][20]\t Batch [3500][5500]\t Training Loss 0.3069\t Accuracy 0.9134\n",
      "Epoch [17][20]\t Batch [3550][5500]\t Training Loss 0.3065\t Accuracy 0.9135\n",
      "Epoch [17][20]\t Batch [3600][5500]\t Training Loss 0.3057\t Accuracy 0.9137\n",
      "Epoch [17][20]\t Batch [3650][5500]\t Training Loss 0.3056\t Accuracy 0.9137\n",
      "Epoch [17][20]\t Batch [3700][5500]\t Training Loss 0.3046\t Accuracy 0.9140\n",
      "Epoch [17][20]\t Batch [3750][5500]\t Training Loss 0.3068\t Accuracy 0.9133\n",
      "Epoch [17][20]\t Batch [3800][5500]\t Training Loss 0.3071\t Accuracy 0.9132\n",
      "Epoch [17][20]\t Batch [3850][5500]\t Training Loss 0.3065\t Accuracy 0.9134\n",
      "Epoch [17][20]\t Batch [3900][5500]\t Training Loss 0.3063\t Accuracy 0.9134\n",
      "Epoch [17][20]\t Batch [3950][5500]\t Training Loss 0.3068\t Accuracy 0.9132\n",
      "Epoch [17][20]\t Batch [4000][5500]\t Training Loss 0.3068\t Accuracy 0.9132\n",
      "Epoch [17][20]\t Batch [4050][5500]\t Training Loss 0.3063\t Accuracy 0.9133\n",
      "Epoch [17][20]\t Batch [4100][5500]\t Training Loss 0.3057\t Accuracy 0.9136\n",
      "Epoch [17][20]\t Batch [4150][5500]\t Training Loss 0.3064\t Accuracy 0.9132\n",
      "Epoch [17][20]\t Batch [4200][5500]\t Training Loss 0.3064\t Accuracy 0.9132\n",
      "Epoch [17][20]\t Batch [4250][5500]\t Training Loss 0.3077\t Accuracy 0.9128\n",
      "Epoch [17][20]\t Batch [4300][5500]\t Training Loss 0.3080\t Accuracy 0.9127\n",
      "Epoch [17][20]\t Batch [4350][5500]\t Training Loss 0.3073\t Accuracy 0.9130\n",
      "Epoch [17][20]\t Batch [4400][5500]\t Training Loss 0.3072\t Accuracy 0.9130\n",
      "Epoch [17][20]\t Batch [4450][5500]\t Training Loss 0.3075\t Accuracy 0.9129\n",
      "Epoch [17][20]\t Batch [4500][5500]\t Training Loss 0.3071\t Accuracy 0.9130\n",
      "Epoch [17][20]\t Batch [4550][5500]\t Training Loss 0.3075\t Accuracy 0.9129\n",
      "Epoch [17][20]\t Batch [4600][5500]\t Training Loss 0.3079\t Accuracy 0.9127\n",
      "Epoch [17][20]\t Batch [4650][5500]\t Training Loss 0.3088\t Accuracy 0.9124\n",
      "Epoch [17][20]\t Batch [4700][5500]\t Training Loss 0.3079\t Accuracy 0.9127\n",
      "Epoch [17][20]\t Batch [4750][5500]\t Training Loss 0.3080\t Accuracy 0.9125\n",
      "Epoch [17][20]\t Batch [4800][5500]\t Training Loss 0.3081\t Accuracy 0.9125\n",
      "Epoch [17][20]\t Batch [4850][5500]\t Training Loss 0.3074\t Accuracy 0.9128\n",
      "Epoch [17][20]\t Batch [4900][5500]\t Training Loss 0.3071\t Accuracy 0.9127\n",
      "Epoch [17][20]\t Batch [4950][5500]\t Training Loss 0.3075\t Accuracy 0.9127\n",
      "Epoch [17][20]\t Batch [5000][5500]\t Training Loss 0.3084\t Accuracy 0.9125\n",
      "Epoch [17][20]\t Batch [5050][5500]\t Training Loss 0.3093\t Accuracy 0.9122\n",
      "Epoch [17][20]\t Batch [5100][5500]\t Training Loss 0.3092\t Accuracy 0.9122\n",
      "Epoch [17][20]\t Batch [5150][5500]\t Training Loss 0.3088\t Accuracy 0.9122\n",
      "Epoch [17][20]\t Batch [5200][5500]\t Training Loss 0.3083\t Accuracy 0.9125\n",
      "Epoch [17][20]\t Batch [5250][5500]\t Training Loss 0.3087\t Accuracy 0.9124\n",
      "Epoch [17][20]\t Batch [5300][5500]\t Training Loss 0.3095\t Accuracy 0.9121\n",
      "Epoch [17][20]\t Batch [5350][5500]\t Training Loss 0.3090\t Accuracy 0.9123\n",
      "Epoch [17][20]\t Batch [5400][5500]\t Training Loss 0.3092\t Accuracy 0.9121\n",
      "Epoch [17][20]\t Batch [5450][5500]\t Training Loss 0.3089\t Accuracy 0.9122\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3089\t Average training accuracy 0.9121\n",
      "Epoch [17]\t Average validation loss 0.2435\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [18][20]\t Batch [0][5500]\t Training Loss 0.0599\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][5500]\t Training Loss 0.2870\t Accuracy 0.9196\n",
      "Epoch [18][20]\t Batch [100][5500]\t Training Loss 0.3180\t Accuracy 0.9059\n",
      "Epoch [18][20]\t Batch [150][5500]\t Training Loss 0.3375\t Accuracy 0.9013\n",
      "Epoch [18][20]\t Batch [200][5500]\t Training Loss 0.3110\t Accuracy 0.9119\n",
      "Epoch [18][20]\t Batch [250][5500]\t Training Loss 0.2961\t Accuracy 0.9159\n",
      "Epoch [18][20]\t Batch [300][5500]\t Training Loss 0.2888\t Accuracy 0.9193\n",
      "Epoch [18][20]\t Batch [350][5500]\t Training Loss 0.2835\t Accuracy 0.9211\n",
      "Epoch [18][20]\t Batch [400][5500]\t Training Loss 0.2814\t Accuracy 0.9227\n",
      "Epoch [18][20]\t Batch [450][5500]\t Training Loss 0.2813\t Accuracy 0.9235\n",
      "Epoch [18][20]\t Batch [500][5500]\t Training Loss 0.2788\t Accuracy 0.9234\n",
      "Epoch [18][20]\t Batch [550][5500]\t Training Loss 0.2807\t Accuracy 0.9232\n",
      "Epoch [18][20]\t Batch [600][5500]\t Training Loss 0.2800\t Accuracy 0.9235\n",
      "Epoch [18][20]\t Batch [650][5500]\t Training Loss 0.2766\t Accuracy 0.9249\n",
      "Epoch [18][20]\t Batch [700][5500]\t Training Loss 0.2764\t Accuracy 0.9243\n",
      "Epoch [18][20]\t Batch [750][5500]\t Training Loss 0.2793\t Accuracy 0.9236\n",
      "Epoch [18][20]\t Batch [800][5500]\t Training Loss 0.2819\t Accuracy 0.9223\n",
      "Epoch [18][20]\t Batch [850][5500]\t Training Loss 0.2854\t Accuracy 0.9214\n",
      "Epoch [18][20]\t Batch [900][5500]\t Training Loss 0.2928\t Accuracy 0.9198\n",
      "Epoch [18][20]\t Batch [950][5500]\t Training Loss 0.2934\t Accuracy 0.9193\n",
      "Epoch [18][20]\t Batch [1000][5500]\t Training Loss 0.2910\t Accuracy 0.9196\n",
      "Epoch [18][20]\t Batch [1050][5500]\t Training Loss 0.2897\t Accuracy 0.9196\n",
      "Epoch [18][20]\t Batch [1100][5500]\t Training Loss 0.2869\t Accuracy 0.9208\n",
      "Epoch [18][20]\t Batch [1150][5500]\t Training Loss 0.2847\t Accuracy 0.9214\n",
      "Epoch [18][20]\t Batch [1200][5500]\t Training Loss 0.2890\t Accuracy 0.9197\n",
      "Epoch [18][20]\t Batch [1250][5500]\t Training Loss 0.2899\t Accuracy 0.9193\n",
      "Epoch [18][20]\t Batch [1300][5500]\t Training Loss 0.2934\t Accuracy 0.9184\n",
      "Epoch [18][20]\t Batch [1350][5500]\t Training Loss 0.2950\t Accuracy 0.9178\n",
      "Epoch [18][20]\t Batch [1400][5500]\t Training Loss 0.2965\t Accuracy 0.9171\n",
      "Epoch [18][20]\t Batch [1450][5500]\t Training Loss 0.2996\t Accuracy 0.9161\n",
      "Epoch [18][20]\t Batch [1500][5500]\t Training Loss 0.3034\t Accuracy 0.9151\n",
      "Epoch [18][20]\t Batch [1550][5500]\t Training Loss 0.3026\t Accuracy 0.9157\n",
      "Epoch [18][20]\t Batch [1600][5500]\t Training Loss 0.3035\t Accuracy 0.9151\n",
      "Epoch [18][20]\t Batch [1650][5500]\t Training Loss 0.3025\t Accuracy 0.9152\n",
      "Epoch [18][20]\t Batch [1700][5500]\t Training Loss 0.3031\t Accuracy 0.9148\n",
      "Epoch [18][20]\t Batch [1750][5500]\t Training Loss 0.3034\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [1800][5500]\t Training Loss 0.3049\t Accuracy 0.9138\n",
      "Epoch [18][20]\t Batch [1850][5500]\t Training Loss 0.3029\t Accuracy 0.9147\n",
      "Epoch [18][20]\t Batch [1900][5500]\t Training Loss 0.3012\t Accuracy 0.9153\n",
      "Epoch [18][20]\t Batch [1950][5500]\t Training Loss 0.3008\t Accuracy 0.9154\n",
      "Epoch [18][20]\t Batch [2000][5500]\t Training Loss 0.2994\t Accuracy 0.9157\n",
      "Epoch [18][20]\t Batch [2050][5500]\t Training Loss 0.2985\t Accuracy 0.9162\n",
      "Epoch [18][20]\t Batch [2100][5500]\t Training Loss 0.3013\t Accuracy 0.9157\n",
      "Epoch [18][20]\t Batch [2150][5500]\t Training Loss 0.3001\t Accuracy 0.9165\n",
      "Epoch [18][20]\t Batch [2200][5500]\t Training Loss 0.2989\t Accuracy 0.9168\n",
      "Epoch [18][20]\t Batch [2250][5500]\t Training Loss 0.2986\t Accuracy 0.9167\n",
      "Epoch [18][20]\t Batch [2300][5500]\t Training Loss 0.2985\t Accuracy 0.9163\n",
      "Epoch [18][20]\t Batch [2350][5500]\t Training Loss 0.2982\t Accuracy 0.9164\n",
      "Epoch [18][20]\t Batch [2400][5500]\t Training Loss 0.2986\t Accuracy 0.9162\n",
      "Epoch [18][20]\t Batch [2450][5500]\t Training Loss 0.2982\t Accuracy 0.9161\n",
      "Epoch [18][20]\t Batch [2500][5500]\t Training Loss 0.2997\t Accuracy 0.9155\n",
      "Epoch [18][20]\t Batch [2550][5500]\t Training Loss 0.2987\t Accuracy 0.9156\n",
      "Epoch [18][20]\t Batch [2600][5500]\t Training Loss 0.2981\t Accuracy 0.9160\n",
      "Epoch [18][20]\t Batch [2650][5500]\t Training Loss 0.2981\t Accuracy 0.9163\n",
      "Epoch [18][20]\t Batch [2700][5500]\t Training Loss 0.2994\t Accuracy 0.9160\n",
      "Epoch [18][20]\t Batch [2750][5500]\t Training Loss 0.2997\t Accuracy 0.9158\n",
      "Epoch [18][20]\t Batch [2800][5500]\t Training Loss 0.2991\t Accuracy 0.9158\n",
      "Epoch [18][20]\t Batch [2850][5500]\t Training Loss 0.2986\t Accuracy 0.9161\n",
      "Epoch [18][20]\t Batch [2900][5500]\t Training Loss 0.2984\t Accuracy 0.9161\n",
      "Epoch [18][20]\t Batch [2950][5500]\t Training Loss 0.2987\t Accuracy 0.9162\n",
      "Epoch [18][20]\t Batch [3000][5500]\t Training Loss 0.2997\t Accuracy 0.9158\n",
      "Epoch [18][20]\t Batch [3050][5500]\t Training Loss 0.2999\t Accuracy 0.9157\n",
      "Epoch [18][20]\t Batch [3100][5500]\t Training Loss 0.3008\t Accuracy 0.9154\n",
      "Epoch [18][20]\t Batch [3150][5500]\t Training Loss 0.3025\t Accuracy 0.9150\n",
      "Epoch [18][20]\t Batch [3200][5500]\t Training Loss 0.3032\t Accuracy 0.9147\n",
      "Epoch [18][20]\t Batch [3250][5500]\t Training Loss 0.3045\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [3300][5500]\t Training Loss 0.3044\t Accuracy 0.9141\n",
      "Epoch [18][20]\t Batch [3350][5500]\t Training Loss 0.3045\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [3400][5500]\t Training Loss 0.3028\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [3450][5500]\t Training Loss 0.3022\t Accuracy 0.9149\n",
      "Epoch [18][20]\t Batch [3500][5500]\t Training Loss 0.3027\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [3550][5500]\t Training Loss 0.3023\t Accuracy 0.9146\n",
      "Epoch [18][20]\t Batch [3600][5500]\t Training Loss 0.3015\t Accuracy 0.9149\n",
      "Epoch [18][20]\t Batch [3650][5500]\t Training Loss 0.3014\t Accuracy 0.9149\n",
      "Epoch [18][20]\t Batch [3700][5500]\t Training Loss 0.3005\t Accuracy 0.9152\n",
      "Epoch [18][20]\t Batch [3750][5500]\t Training Loss 0.3027\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [3800][5500]\t Training Loss 0.3029\t Accuracy 0.9144\n",
      "Epoch [18][20]\t Batch [3850][5500]\t Training Loss 0.3023\t Accuracy 0.9146\n",
      "Epoch [18][20]\t Batch [3900][5500]\t Training Loss 0.3021\t Accuracy 0.9146\n",
      "Epoch [18][20]\t Batch [3950][5500]\t Training Loss 0.3027\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [4000][5500]\t Training Loss 0.3027\t Accuracy 0.9144\n",
      "Epoch [18][20]\t Batch [4050][5500]\t Training Loss 0.3022\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [4100][5500]\t Training Loss 0.3016\t Accuracy 0.9148\n",
      "Epoch [18][20]\t Batch [4150][5500]\t Training Loss 0.3023\t Accuracy 0.9145\n",
      "Epoch [18][20]\t Batch [4200][5500]\t Training Loss 0.3022\t Accuracy 0.9144\n",
      "Epoch [18][20]\t Batch [4250][5500]\t Training Loss 0.3036\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [4300][5500]\t Training Loss 0.3039\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [4350][5500]\t Training Loss 0.3031\t Accuracy 0.9142\n",
      "Epoch [18][20]\t Batch [4400][5500]\t Training Loss 0.3031\t Accuracy 0.9141\n",
      "Epoch [18][20]\t Batch [4450][5500]\t Training Loss 0.3033\t Accuracy 0.9141\n",
      "Epoch [18][20]\t Batch [4500][5500]\t Training Loss 0.3029\t Accuracy 0.9142\n",
      "Epoch [18][20]\t Batch [4550][5500]\t Training Loss 0.3034\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [4600][5500]\t Training Loss 0.3037\t Accuracy 0.9139\n",
      "Epoch [18][20]\t Batch [4650][5500]\t Training Loss 0.3047\t Accuracy 0.9136\n",
      "Epoch [18][20]\t Batch [4700][5500]\t Training Loss 0.3038\t Accuracy 0.9139\n",
      "Epoch [18][20]\t Batch [4750][5500]\t Training Loss 0.3038\t Accuracy 0.9138\n",
      "Epoch [18][20]\t Batch [4800][5500]\t Training Loss 0.3040\t Accuracy 0.9137\n",
      "Epoch [18][20]\t Batch [4850][5500]\t Training Loss 0.3032\t Accuracy 0.9140\n",
      "Epoch [18][20]\t Batch [4900][5500]\t Training Loss 0.3030\t Accuracy 0.9139\n",
      "Epoch [18][20]\t Batch [4950][5500]\t Training Loss 0.3034\t Accuracy 0.9139\n",
      "Epoch [18][20]\t Batch [5000][5500]\t Training Loss 0.3043\t Accuracy 0.9137\n",
      "Epoch [18][20]\t Batch [5050][5500]\t Training Loss 0.3052\t Accuracy 0.9135\n",
      "Epoch [18][20]\t Batch [5100][5500]\t Training Loss 0.3051\t Accuracy 0.9135\n",
      "Epoch [18][20]\t Batch [5150][5500]\t Training Loss 0.3047\t Accuracy 0.9135\n",
      "Epoch [18][20]\t Batch [5200][5500]\t Training Loss 0.3042\t Accuracy 0.9138\n",
      "Epoch [18][20]\t Batch [5250][5500]\t Training Loss 0.3045\t Accuracy 0.9136\n",
      "Epoch [18][20]\t Batch [5300][5500]\t Training Loss 0.3054\t Accuracy 0.9133\n",
      "Epoch [18][20]\t Batch [5350][5500]\t Training Loss 0.3049\t Accuracy 0.9135\n",
      "Epoch [18][20]\t Batch [5400][5500]\t Training Loss 0.3051\t Accuracy 0.9134\n",
      "Epoch [18][20]\t Batch [5450][5500]\t Training Loss 0.3047\t Accuracy 0.9134\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3048\t Average training accuracy 0.9134\n",
      "Epoch [18]\t Average validation loss 0.2404\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [19][20]\t Batch [0][5500]\t Training Loss 0.0573\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][5500]\t Training Loss 0.2833\t Accuracy 0.9196\n",
      "Epoch [19][20]\t Batch [100][5500]\t Training Loss 0.3139\t Accuracy 0.9069\n",
      "Epoch [19][20]\t Batch [150][5500]\t Training Loss 0.3339\t Accuracy 0.9020\n",
      "Epoch [19][20]\t Batch [200][5500]\t Training Loss 0.3072\t Accuracy 0.9134\n",
      "Epoch [19][20]\t Batch [250][5500]\t Training Loss 0.2924\t Accuracy 0.9171\n",
      "Epoch [19][20]\t Batch [300][5500]\t Training Loss 0.2850\t Accuracy 0.9203\n",
      "Epoch [19][20]\t Batch [350][5500]\t Training Loss 0.2797\t Accuracy 0.9222\n",
      "Epoch [19][20]\t Batch [400][5500]\t Training Loss 0.2775\t Accuracy 0.9239\n",
      "Epoch [19][20]\t Batch [450][5500]\t Training Loss 0.2775\t Accuracy 0.9248\n",
      "Epoch [19][20]\t Batch [500][5500]\t Training Loss 0.2750\t Accuracy 0.9246\n",
      "Epoch [19][20]\t Batch [550][5500]\t Training Loss 0.2769\t Accuracy 0.9243\n",
      "Epoch [19][20]\t Batch [600][5500]\t Training Loss 0.2762\t Accuracy 0.9245\n",
      "Epoch [19][20]\t Batch [650][5500]\t Training Loss 0.2728\t Accuracy 0.9260\n",
      "Epoch [19][20]\t Batch [700][5500]\t Training Loss 0.2727\t Accuracy 0.9252\n",
      "Epoch [19][20]\t Batch [750][5500]\t Training Loss 0.2756\t Accuracy 0.9245\n",
      "Epoch [19][20]\t Batch [800][5500]\t Training Loss 0.2781\t Accuracy 0.9232\n",
      "Epoch [19][20]\t Batch [850][5500]\t Training Loss 0.2815\t Accuracy 0.9222\n",
      "Epoch [19][20]\t Batch [900][5500]\t Training Loss 0.2889\t Accuracy 0.9206\n",
      "Epoch [19][20]\t Batch [950][5500]\t Training Loss 0.2896\t Accuracy 0.9204\n",
      "Epoch [19][20]\t Batch [1000][5500]\t Training Loss 0.2872\t Accuracy 0.9206\n",
      "Epoch [19][20]\t Batch [1050][5500]\t Training Loss 0.2859\t Accuracy 0.9206\n",
      "Epoch [19][20]\t Batch [1100][5500]\t Training Loss 0.2831\t Accuracy 0.9217\n",
      "Epoch [19][20]\t Batch [1150][5500]\t Training Loss 0.2809\t Accuracy 0.9223\n",
      "Epoch [19][20]\t Batch [1200][5500]\t Training Loss 0.2852\t Accuracy 0.9207\n",
      "Epoch [19][20]\t Batch [1250][5500]\t Training Loss 0.2860\t Accuracy 0.9202\n",
      "Epoch [19][20]\t Batch [1300][5500]\t Training Loss 0.2896\t Accuracy 0.9193\n",
      "Epoch [19][20]\t Batch [1350][5500]\t Training Loss 0.2912\t Accuracy 0.9187\n",
      "Epoch [19][20]\t Batch [1400][5500]\t Training Loss 0.2926\t Accuracy 0.9181\n",
      "Epoch [19][20]\t Batch [1450][5500]\t Training Loss 0.2957\t Accuracy 0.9170\n",
      "Epoch [19][20]\t Batch [1500][5500]\t Training Loss 0.2995\t Accuracy 0.9161\n",
      "Epoch [19][20]\t Batch [1550][5500]\t Training Loss 0.2987\t Accuracy 0.9167\n",
      "Epoch [19][20]\t Batch [1600][5500]\t Training Loss 0.2996\t Accuracy 0.9161\n",
      "Epoch [19][20]\t Batch [1650][5500]\t Training Loss 0.2986\t Accuracy 0.9163\n",
      "Epoch [19][20]\t Batch [1700][5500]\t Training Loss 0.2992\t Accuracy 0.9158\n",
      "Epoch [19][20]\t Batch [1750][5500]\t Training Loss 0.2995\t Accuracy 0.9155\n",
      "Epoch [19][20]\t Batch [1800][5500]\t Training Loss 0.3009\t Accuracy 0.9148\n",
      "Epoch [19][20]\t Batch [1850][5500]\t Training Loss 0.2990\t Accuracy 0.9157\n",
      "Epoch [19][20]\t Batch [1900][5500]\t Training Loss 0.2973\t Accuracy 0.9163\n",
      "Epoch [19][20]\t Batch [1950][5500]\t Training Loss 0.2968\t Accuracy 0.9164\n",
      "Epoch [19][20]\t Batch [2000][5500]\t Training Loss 0.2954\t Accuracy 0.9167\n",
      "Epoch [19][20]\t Batch [2050][5500]\t Training Loss 0.2946\t Accuracy 0.9173\n",
      "Epoch [19][20]\t Batch [2100][5500]\t Training Loss 0.2974\t Accuracy 0.9167\n",
      "Epoch [19][20]\t Batch [2150][5500]\t Training Loss 0.2962\t Accuracy 0.9175\n",
      "Epoch [19][20]\t Batch [2200][5500]\t Training Loss 0.2950\t Accuracy 0.9179\n",
      "Epoch [19][20]\t Batch [2250][5500]\t Training Loss 0.2947\t Accuracy 0.9179\n",
      "Epoch [19][20]\t Batch [2300][5500]\t Training Loss 0.2947\t Accuracy 0.9174\n",
      "Epoch [19][20]\t Batch [2350][5500]\t Training Loss 0.2943\t Accuracy 0.9175\n",
      "Epoch [19][20]\t Batch [2400][5500]\t Training Loss 0.2947\t Accuracy 0.9173\n",
      "Epoch [19][20]\t Batch [2450][5500]\t Training Loss 0.2944\t Accuracy 0.9172\n",
      "Epoch [19][20]\t Batch [2500][5500]\t Training Loss 0.2958\t Accuracy 0.9166\n",
      "Epoch [19][20]\t Batch [2550][5500]\t Training Loss 0.2948\t Accuracy 0.9167\n",
      "Epoch [19][20]\t Batch [2600][5500]\t Training Loss 0.2942\t Accuracy 0.9170\n",
      "Epoch [19][20]\t Batch [2650][5500]\t Training Loss 0.2942\t Accuracy 0.9174\n",
      "Epoch [19][20]\t Batch [2700][5500]\t Training Loss 0.2955\t Accuracy 0.9170\n",
      "Epoch [19][20]\t Batch [2750][5500]\t Training Loss 0.2959\t Accuracy 0.9169\n",
      "Epoch [19][20]\t Batch [2800][5500]\t Training Loss 0.2952\t Accuracy 0.9169\n",
      "Epoch [19][20]\t Batch [2850][5500]\t Training Loss 0.2948\t Accuracy 0.9172\n",
      "Epoch [19][20]\t Batch [2900][5500]\t Training Loss 0.2946\t Accuracy 0.9171\n",
      "Epoch [19][20]\t Batch [2950][5500]\t Training Loss 0.2948\t Accuracy 0.9172\n",
      "Epoch [19][20]\t Batch [3000][5500]\t Training Loss 0.2958\t Accuracy 0.9169\n",
      "Epoch [19][20]\t Batch [3050][5500]\t Training Loss 0.2960\t Accuracy 0.9167\n",
      "Epoch [19][20]\t Batch [3100][5500]\t Training Loss 0.2969\t Accuracy 0.9164\n",
      "Epoch [19][20]\t Batch [3150][5500]\t Training Loss 0.2986\t Accuracy 0.9160\n",
      "Epoch [19][20]\t Batch [3200][5500]\t Training Loss 0.2993\t Accuracy 0.9157\n",
      "Epoch [19][20]\t Batch [3250][5500]\t Training Loss 0.3005\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [3300][5500]\t Training Loss 0.3005\t Accuracy 0.9151\n",
      "Epoch [19][20]\t Batch [3350][5500]\t Training Loss 0.3005\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [3400][5500]\t Training Loss 0.2989\t Accuracy 0.9155\n",
      "Epoch [19][20]\t Batch [3450][5500]\t Training Loss 0.2983\t Accuracy 0.9158\n",
      "Epoch [19][20]\t Batch [3500][5500]\t Training Loss 0.2988\t Accuracy 0.9155\n",
      "Epoch [19][20]\t Batch [3550][5500]\t Training Loss 0.2984\t Accuracy 0.9156\n",
      "Epoch [19][20]\t Batch [3600][5500]\t Training Loss 0.2976\t Accuracy 0.9158\n",
      "Epoch [19][20]\t Batch [3650][5500]\t Training Loss 0.2975\t Accuracy 0.9158\n",
      "Epoch [19][20]\t Batch [3700][5500]\t Training Loss 0.2966\t Accuracy 0.9161\n",
      "Epoch [19][20]\t Batch [3750][5500]\t Training Loss 0.2988\t Accuracy 0.9154\n",
      "Epoch [19][20]\t Batch [3800][5500]\t Training Loss 0.2990\t Accuracy 0.9153\n",
      "Epoch [19][20]\t Batch [3850][5500]\t Training Loss 0.2984\t Accuracy 0.9155\n",
      "Epoch [19][20]\t Batch [3900][5500]\t Training Loss 0.2982\t Accuracy 0.9155\n",
      "Epoch [19][20]\t Batch [3950][5500]\t Training Loss 0.2988\t Accuracy 0.9154\n",
      "Epoch [19][20]\t Batch [4000][5500]\t Training Loss 0.2988\t Accuracy 0.9153\n",
      "Epoch [19][20]\t Batch [4050][5500]\t Training Loss 0.2983\t Accuracy 0.9154\n",
      "Epoch [19][20]\t Batch [4100][5500]\t Training Loss 0.2977\t Accuracy 0.9157\n",
      "Epoch [19][20]\t Batch [4150][5500]\t Training Loss 0.2984\t Accuracy 0.9154\n",
      "Epoch [19][20]\t Batch [4200][5500]\t Training Loss 0.2983\t Accuracy 0.9154\n",
      "Epoch [19][20]\t Batch [4250][5500]\t Training Loss 0.2997\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [4300][5500]\t Training Loss 0.3000\t Accuracy 0.9149\n",
      "Epoch [19][20]\t Batch [4350][5500]\t Training Loss 0.2992\t Accuracy 0.9151\n",
      "Epoch [19][20]\t Batch [4400][5500]\t Training Loss 0.2992\t Accuracy 0.9151\n",
      "Epoch [19][20]\t Batch [4450][5500]\t Training Loss 0.2994\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [4500][5500]\t Training Loss 0.2990\t Accuracy 0.9151\n",
      "Epoch [19][20]\t Batch [4550][5500]\t Training Loss 0.2995\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [4600][5500]\t Training Loss 0.2999\t Accuracy 0.9149\n",
      "Epoch [19][20]\t Batch [4650][5500]\t Training Loss 0.3008\t Accuracy 0.9146\n",
      "Epoch [19][20]\t Batch [4700][5500]\t Training Loss 0.2999\t Accuracy 0.9149\n",
      "Epoch [19][20]\t Batch [4750][5500]\t Training Loss 0.3000\t Accuracy 0.9148\n",
      "Epoch [19][20]\t Batch [4800][5500]\t Training Loss 0.3001\t Accuracy 0.9148\n",
      "Epoch [19][20]\t Batch [4850][5500]\t Training Loss 0.2994\t Accuracy 0.9151\n",
      "Epoch [19][20]\t Batch [4900][5500]\t Training Loss 0.2992\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [4950][5500]\t Training Loss 0.2995\t Accuracy 0.9150\n",
      "Epoch [19][20]\t Batch [5000][5500]\t Training Loss 0.3004\t Accuracy 0.9148\n",
      "Epoch [19][20]\t Batch [5050][5500]\t Training Loss 0.3013\t Accuracy 0.9145\n",
      "Epoch [19][20]\t Batch [5100][5500]\t Training Loss 0.3012\t Accuracy 0.9146\n",
      "Epoch [19][20]\t Batch [5150][5500]\t Training Loss 0.3008\t Accuracy 0.9146\n",
      "Epoch [19][20]\t Batch [5200][5500]\t Training Loss 0.3003\t Accuracy 0.9149\n",
      "Epoch [19][20]\t Batch [5250][5500]\t Training Loss 0.3007\t Accuracy 0.9148\n",
      "Epoch [19][20]\t Batch [5300][5500]\t Training Loss 0.3015\t Accuracy 0.9145\n",
      "Epoch [19][20]\t Batch [5350][5500]\t Training Loss 0.3010\t Accuracy 0.9146\n",
      "Epoch [19][20]\t Batch [5400][5500]\t Training Loss 0.3012\t Accuracy 0.9145\n",
      "Epoch [19][20]\t Batch [5450][5500]\t Training Loss 0.3009\t Accuracy 0.9146\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3009\t Average training accuracy 0.9145\n",
      "Epoch [19]\t Average validation loss 0.2375\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [0][20]\t Batch [0][5500]\t Training Loss 2.6839\t Accuracy 0.0000\n",
      "Epoch [0][20]\t Batch [50][5500]\t Training Loss 2.1061\t Accuracy 0.3000\n",
      "Epoch [0][20]\t Batch [100][5500]\t Training Loss 1.5905\t Accuracy 0.4980\n",
      "Epoch [0][20]\t Batch [150][5500]\t Training Loss 1.2885\t Accuracy 0.5914\n",
      "Epoch [0][20]\t Batch [200][5500]\t Training Loss 1.0912\t Accuracy 0.6532\n",
      "Epoch [0][20]\t Batch [250][5500]\t Training Loss 0.9651\t Accuracy 0.6972\n",
      "Epoch [0][20]\t Batch [300][5500]\t Training Loss 0.8792\t Accuracy 0.7233\n",
      "Epoch [0][20]\t Batch [350][5500]\t Training Loss 0.8188\t Accuracy 0.7422\n",
      "Epoch [0][20]\t Batch [400][5500]\t Training Loss 0.7687\t Accuracy 0.7599\n",
      "Epoch [0][20]\t Batch [450][5500]\t Training Loss 0.7321\t Accuracy 0.7727\n",
      "Epoch [0][20]\t Batch [500][5500]\t Training Loss 0.6974\t Accuracy 0.7830\n",
      "Epoch [0][20]\t Batch [550][5500]\t Training Loss 0.6712\t Accuracy 0.7915\n",
      "Epoch [0][20]\t Batch [600][5500]\t Training Loss 0.6453\t Accuracy 0.7997\n",
      "Epoch [0][20]\t Batch [650][5500]\t Training Loss 0.6177\t Accuracy 0.8084\n",
      "Epoch [0][20]\t Batch [700][5500]\t Training Loss 0.5980\t Accuracy 0.8137\n",
      "Epoch [0][20]\t Batch [750][5500]\t Training Loss 0.5820\t Accuracy 0.8190\n",
      "Epoch [0][20]\t Batch [800][5500]\t Training Loss 0.5644\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [850][5500]\t Training Loss 0.5518\t Accuracy 0.8290\n",
      "Epoch [0][20]\t Batch [900][5500]\t Training Loss 0.5441\t Accuracy 0.8315\n",
      "Epoch [0][20]\t Batch [950][5500]\t Training Loss 0.5326\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [1000][5500]\t Training Loss 0.5183\t Accuracy 0.8410\n",
      "Epoch [0][20]\t Batch [1050][5500]\t Training Loss 0.5086\t Accuracy 0.8448\n",
      "Epoch [0][20]\t Batch [1100][5500]\t Training Loss 0.4985\t Accuracy 0.8480\n",
      "Epoch [0][20]\t Batch [1150][5500]\t Training Loss 0.4861\t Accuracy 0.8514\n",
      "Epoch [0][20]\t Batch [1200][5500]\t Training Loss 0.4807\t Accuracy 0.8532\n",
      "Epoch [0][20]\t Batch [1250][5500]\t Training Loss 0.4743\t Accuracy 0.8552\n",
      "Epoch [0][20]\t Batch [1300][5500]\t Training Loss 0.4707\t Accuracy 0.8570\n",
      "Epoch [0][20]\t Batch [1350][5500]\t Training Loss 0.4646\t Accuracy 0.8588\n",
      "Epoch [0][20]\t Batch [1400][5500]\t Training Loss 0.4583\t Accuracy 0.8606\n",
      "Epoch [0][20]\t Batch [1450][5500]\t Training Loss 0.4516\t Accuracy 0.8626\n",
      "Epoch [0][20]\t Batch [1500][5500]\t Training Loss 0.4473\t Accuracy 0.8638\n",
      "Epoch [0][20]\t Batch [1550][5500]\t Training Loss 0.4413\t Accuracy 0.8658\n",
      "Epoch [0][20]\t Batch [1600][5500]\t Training Loss 0.4360\t Accuracy 0.8675\n",
      "Epoch [0][20]\t Batch [1650][5500]\t Training Loss 0.4286\t Accuracy 0.8698\n",
      "Epoch [0][20]\t Batch [1700][5500]\t Training Loss 0.4234\t Accuracy 0.8713\n",
      "Epoch [0][20]\t Batch [1750][5500]\t Training Loss 0.4172\t Accuracy 0.8732\n",
      "Epoch [0][20]\t Batch [1800][5500]\t Training Loss 0.4127\t Accuracy 0.8745\n",
      "Epoch [0][20]\t Batch [1850][5500]\t Training Loss 0.4064\t Accuracy 0.8766\n",
      "Epoch [0][20]\t Batch [1900][5500]\t Training Loss 0.3999\t Accuracy 0.8786\n",
      "Epoch [0][20]\t Batch [1950][5500]\t Training Loss 0.3944\t Accuracy 0.8803\n",
      "Epoch [0][20]\t Batch [2000][5500]\t Training Loss 0.3891\t Accuracy 0.8820\n",
      "Epoch [0][20]\t Batch [2050][5500]\t Training Loss 0.3847\t Accuracy 0.8836\n",
      "Epoch [0][20]\t Batch [2100][5500]\t Training Loss 0.3833\t Accuracy 0.8843\n",
      "Epoch [0][20]\t Batch [2150][5500]\t Training Loss 0.3785\t Accuracy 0.8858\n",
      "Epoch [0][20]\t Batch [2200][5500]\t Training Loss 0.3740\t Accuracy 0.8871\n",
      "Epoch [0][20]\t Batch [2250][5500]\t Training Loss 0.3706\t Accuracy 0.8881\n",
      "Epoch [0][20]\t Batch [2300][5500]\t Training Loss 0.3671\t Accuracy 0.8891\n",
      "Epoch [0][20]\t Batch [2350][5500]\t Training Loss 0.3640\t Accuracy 0.8900\n",
      "Epoch [0][20]\t Batch [2400][5500]\t Training Loss 0.3615\t Accuracy 0.8910\n",
      "Epoch [0][20]\t Batch [2450][5500]\t Training Loss 0.3579\t Accuracy 0.8921\n",
      "Epoch [0][20]\t Batch [2500][5500]\t Training Loss 0.3554\t Accuracy 0.8928\n",
      "Epoch [0][20]\t Batch [2550][5500]\t Training Loss 0.3508\t Accuracy 0.8943\n",
      "Epoch [0][20]\t Batch [2600][5500]\t Training Loss 0.3476\t Accuracy 0.8953\n",
      "Epoch [0][20]\t Batch [2650][5500]\t Training Loss 0.3447\t Accuracy 0.8962\n",
      "Epoch [0][20]\t Batch [2700][5500]\t Training Loss 0.3437\t Accuracy 0.8969\n",
      "Epoch [0][20]\t Batch [2750][5500]\t Training Loss 0.3413\t Accuracy 0.8976\n",
      "Epoch [0][20]\t Batch [2800][5500]\t Training Loss 0.3380\t Accuracy 0.8987\n",
      "Epoch [0][20]\t Batch [2850][5500]\t Training Loss 0.3355\t Accuracy 0.8994\n",
      "Epoch [0][20]\t Batch [2900][5500]\t Training Loss 0.3331\t Accuracy 0.9001\n",
      "Epoch [0][20]\t Batch [2950][5500]\t Training Loss 0.3304\t Accuracy 0.9008\n",
      "Epoch [0][20]\t Batch [3000][5500]\t Training Loss 0.3283\t Accuracy 0.9014\n",
      "Epoch [0][20]\t Batch [3050][5500]\t Training Loss 0.3257\t Accuracy 0.9022\n",
      "Epoch [0][20]\t Batch [3100][5500]\t Training Loss 0.3235\t Accuracy 0.9028\n",
      "Epoch [0][20]\t Batch [3150][5500]\t Training Loss 0.3222\t Accuracy 0.9033\n",
      "Epoch [0][20]\t Batch [3200][5500]\t Training Loss 0.3206\t Accuracy 0.9038\n",
      "Epoch [0][20]\t Batch [3250][5500]\t Training Loss 0.3189\t Accuracy 0.9040\n",
      "Epoch [0][20]\t Batch [3300][5500]\t Training Loss 0.3168\t Accuracy 0.9048\n",
      "Epoch [0][20]\t Batch [3350][5500]\t Training Loss 0.3142\t Accuracy 0.9055\n",
      "Epoch [0][20]\t Batch [3400][5500]\t Training Loss 0.3110\t Accuracy 0.9064\n",
      "Epoch [0][20]\t Batch [3450][5500]\t Training Loss 0.3087\t Accuracy 0.9072\n",
      "Epoch [0][20]\t Batch [3500][5500]\t Training Loss 0.3075\t Accuracy 0.9075\n",
      "Epoch [0][20]\t Batch [3550][5500]\t Training Loss 0.3057\t Accuracy 0.9081\n",
      "Epoch [0][20]\t Batch [3600][5500]\t Training Loss 0.3033\t Accuracy 0.9089\n",
      "Epoch [0][20]\t Batch [3650][5500]\t Training Loss 0.3017\t Accuracy 0.9094\n",
      "Epoch [0][20]\t Batch [3700][5500]\t Training Loss 0.2994\t Accuracy 0.9101\n",
      "Epoch [0][20]\t Batch [3750][5500]\t Training Loss 0.2998\t Accuracy 0.9102\n",
      "Epoch [0][20]\t Batch [3800][5500]\t Training Loss 0.2982\t Accuracy 0.9107\n",
      "Epoch [0][20]\t Batch [3850][5500]\t Training Loss 0.2964\t Accuracy 0.9111\n",
      "Epoch [0][20]\t Batch [3900][5500]\t Training Loss 0.2947\t Accuracy 0.9116\n",
      "Epoch [0][20]\t Batch [3950][5500]\t Training Loss 0.2938\t Accuracy 0.9119\n",
      "Epoch [0][20]\t Batch [4000][5500]\t Training Loss 0.2926\t Accuracy 0.9122\n",
      "Epoch [0][20]\t Batch [4050][5500]\t Training Loss 0.2907\t Accuracy 0.9129\n",
      "Epoch [0][20]\t Batch [4100][5500]\t Training Loss 0.2890\t Accuracy 0.9134\n",
      "Epoch [0][20]\t Batch [4150][5500]\t Training Loss 0.2878\t Accuracy 0.9138\n",
      "Epoch [0][20]\t Batch [4200][5500]\t Training Loss 0.2862\t Accuracy 0.9142\n",
      "Epoch [0][20]\t Batch [4250][5500]\t Training Loss 0.2862\t Accuracy 0.9143\n",
      "Epoch [0][20]\t Batch [4300][5500]\t Training Loss 0.2858\t Accuracy 0.9144\n",
      "Epoch [0][20]\t Batch [4350][5500]\t Training Loss 0.2843\t Accuracy 0.9147\n",
      "Epoch [0][20]\t Batch [4400][5500]\t Training Loss 0.2833\t Accuracy 0.9150\n",
      "Epoch [0][20]\t Batch [4450][5500]\t Training Loss 0.2823\t Accuracy 0.9153\n",
      "Epoch [0][20]\t Batch [4500][5500]\t Training Loss 0.2806\t Accuracy 0.9158\n",
      "Epoch [0][20]\t Batch [4550][5500]\t Training Loss 0.2796\t Accuracy 0.9161\n",
      "Epoch [0][20]\t Batch [4600][5500]\t Training Loss 0.2785\t Accuracy 0.9164\n",
      "Epoch [0][20]\t Batch [4650][5500]\t Training Loss 0.2780\t Accuracy 0.9165\n",
      "Epoch [0][20]\t Batch [4700][5500]\t Training Loss 0.2760\t Accuracy 0.9170\n",
      "Epoch [0][20]\t Batch [4750][5500]\t Training Loss 0.2754\t Accuracy 0.9171\n",
      "Epoch [0][20]\t Batch [4800][5500]\t Training Loss 0.2743\t Accuracy 0.9174\n",
      "Epoch [0][20]\t Batch [4850][5500]\t Training Loss 0.2729\t Accuracy 0.9177\n",
      "Epoch [0][20]\t Batch [4900][5500]\t Training Loss 0.2718\t Accuracy 0.9181\n",
      "Epoch [0][20]\t Batch [4950][5500]\t Training Loss 0.2710\t Accuracy 0.9182\n",
      "Epoch [0][20]\t Batch [5000][5500]\t Training Loss 0.2711\t Accuracy 0.9183\n",
      "Epoch [0][20]\t Batch [5050][5500]\t Training Loss 0.2704\t Accuracy 0.9184\n",
      "Epoch [0][20]\t Batch [5100][5500]\t Training Loss 0.2694\t Accuracy 0.9188\n",
      "Epoch [0][20]\t Batch [5150][5500]\t Training Loss 0.2678\t Accuracy 0.9193\n",
      "Epoch [0][20]\t Batch [5200][5500]\t Training Loss 0.2666\t Accuracy 0.9196\n",
      "Epoch [0][20]\t Batch [5250][5500]\t Training Loss 0.2656\t Accuracy 0.9199\n",
      "Epoch [0][20]\t Batch [5300][5500]\t Training Loss 0.2647\t Accuracy 0.9203\n",
      "Epoch [0][20]\t Batch [5350][5500]\t Training Loss 0.2631\t Accuracy 0.9207\n",
      "Epoch [0][20]\t Batch [5400][5500]\t Training Loss 0.2624\t Accuracy 0.9208\n",
      "Epoch [0][20]\t Batch [5450][5500]\t Training Loss 0.2611\t Accuracy 0.9213\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2602\t Average training accuracy 0.9215\n",
      "Epoch [0]\t Average validation loss 0.1227\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [1][20]\t Batch [0][5500]\t Training Loss 0.0395\t Accuracy 1.0000\n",
      "Epoch [1][20]\t Batch [50][5500]\t Training Loss 0.1517\t Accuracy 0.9529\n",
      "Epoch [1][20]\t Batch [100][5500]\t Training Loss 0.1395\t Accuracy 0.9624\n",
      "Epoch [1][20]\t Batch [150][5500]\t Training Loss 0.1556\t Accuracy 0.9556\n",
      "Epoch [1][20]\t Batch [200][5500]\t Training Loss 0.1464\t Accuracy 0.9557\n",
      "Epoch [1][20]\t Batch [250][5500]\t Training Loss 0.1335\t Accuracy 0.9606\n",
      "Epoch [1][20]\t Batch [300][5500]\t Training Loss 0.1294\t Accuracy 0.9618\n",
      "Epoch [1][20]\t Batch [350][5500]\t Training Loss 0.1218\t Accuracy 0.9638\n",
      "Epoch [1][20]\t Batch [400][5500]\t Training Loss 0.1186\t Accuracy 0.9643\n",
      "Epoch [1][20]\t Batch [450][5500]\t Training Loss 0.1193\t Accuracy 0.9632\n",
      "Epoch [1][20]\t Batch [500][5500]\t Training Loss 0.1188\t Accuracy 0.9635\n",
      "Epoch [1][20]\t Batch [550][5500]\t Training Loss 0.1169\t Accuracy 0.9642\n",
      "Epoch [1][20]\t Batch [600][5500]\t Training Loss 0.1169\t Accuracy 0.9637\n",
      "Epoch [1][20]\t Batch [650][5500]\t Training Loss 0.1154\t Accuracy 0.9641\n",
      "Epoch [1][20]\t Batch [700][5500]\t Training Loss 0.1162\t Accuracy 0.9635\n",
      "Epoch [1][20]\t Batch [750][5500]\t Training Loss 0.1171\t Accuracy 0.9634\n",
      "Epoch [1][20]\t Batch [800][5500]\t Training Loss 0.1181\t Accuracy 0.9633\n",
      "Epoch [1][20]\t Batch [850][5500]\t Training Loss 0.1196\t Accuracy 0.9624\n",
      "Epoch [1][20]\t Batch [900][5500]\t Training Loss 0.1232\t Accuracy 0.9616\n",
      "Epoch [1][20]\t Batch [950][5500]\t Training Loss 0.1227\t Accuracy 0.9623\n",
      "Epoch [1][20]\t Batch [1000][5500]\t Training Loss 0.1208\t Accuracy 0.9626\n",
      "Epoch [1][20]\t Batch [1050][5500]\t Training Loss 0.1207\t Accuracy 0.9627\n",
      "Epoch [1][20]\t Batch [1100][5500]\t Training Loss 0.1194\t Accuracy 0.9634\n",
      "Epoch [1][20]\t Batch [1150][5500]\t Training Loss 0.1177\t Accuracy 0.9640\n",
      "Epoch [1][20]\t Batch [1200][5500]\t Training Loss 0.1200\t Accuracy 0.9632\n",
      "Epoch [1][20]\t Batch [1250][5500]\t Training Loss 0.1186\t Accuracy 0.9635\n",
      "Epoch [1][20]\t Batch [1300][5500]\t Training Loss 0.1209\t Accuracy 0.9633\n",
      "Epoch [1][20]\t Batch [1350][5500]\t Training Loss 0.1228\t Accuracy 0.9627\n",
      "Epoch [1][20]\t Batch [1400][5500]\t Training Loss 0.1225\t Accuracy 0.9627\n",
      "Epoch [1][20]\t Batch [1450][5500]\t Training Loss 0.1232\t Accuracy 0.9622\n",
      "Epoch [1][20]\t Batch [1500][5500]\t Training Loss 0.1235\t Accuracy 0.9623\n",
      "Epoch [1][20]\t Batch [1550][5500]\t Training Loss 0.1231\t Accuracy 0.9622\n",
      "Epoch [1][20]\t Batch [1600][5500]\t Training Loss 0.1247\t Accuracy 0.9615\n",
      "Epoch [1][20]\t Batch [1650][5500]\t Training Loss 0.1237\t Accuracy 0.9617\n",
      "Epoch [1][20]\t Batch [1700][5500]\t Training Loss 0.1240\t Accuracy 0.9615\n",
      "Epoch [1][20]\t Batch [1750][5500]\t Training Loss 0.1240\t Accuracy 0.9615\n",
      "Epoch [1][20]\t Batch [1800][5500]\t Training Loss 0.1244\t Accuracy 0.9615\n",
      "Epoch [1][20]\t Batch [1850][5500]\t Training Loss 0.1237\t Accuracy 0.9616\n",
      "Epoch [1][20]\t Batch [1900][5500]\t Training Loss 0.1221\t Accuracy 0.9622\n",
      "Epoch [1][20]\t Batch [1950][5500]\t Training Loss 0.1220\t Accuracy 0.9622\n",
      "Epoch [1][20]\t Batch [2000][5500]\t Training Loss 0.1209\t Accuracy 0.9626\n",
      "Epoch [1][20]\t Batch [2050][5500]\t Training Loss 0.1205\t Accuracy 0.9629\n",
      "Epoch [1][20]\t Batch [2100][5500]\t Training Loss 0.1215\t Accuracy 0.9626\n",
      "Epoch [1][20]\t Batch [2150][5500]\t Training Loss 0.1204\t Accuracy 0.9629\n",
      "Epoch [1][20]\t Batch [2200][5500]\t Training Loss 0.1197\t Accuracy 0.9632\n",
      "Epoch [1][20]\t Batch [2250][5500]\t Training Loss 0.1192\t Accuracy 0.9634\n",
      "Epoch [1][20]\t Batch [2300][5500]\t Training Loss 0.1188\t Accuracy 0.9635\n",
      "Epoch [1][20]\t Batch [2350][5500]\t Training Loss 0.1185\t Accuracy 0.9634\n",
      "Epoch [1][20]\t Batch [2400][5500]\t Training Loss 0.1190\t Accuracy 0.9635\n",
      "Epoch [1][20]\t Batch [2450][5500]\t Training Loss 0.1181\t Accuracy 0.9637\n",
      "Epoch [1][20]\t Batch [2500][5500]\t Training Loss 0.1184\t Accuracy 0.9637\n",
      "Epoch [1][20]\t Batch [2550][5500]\t Training Loss 0.1175\t Accuracy 0.9641\n",
      "Epoch [1][20]\t Batch [2600][5500]\t Training Loss 0.1173\t Accuracy 0.9642\n",
      "Epoch [1][20]\t Batch [2650][5500]\t Training Loss 0.1173\t Accuracy 0.9643\n",
      "Epoch [1][20]\t Batch [2700][5500]\t Training Loss 0.1183\t Accuracy 0.9640\n",
      "Epoch [1][20]\t Batch [2750][5500]\t Training Loss 0.1183\t Accuracy 0.9639\n",
      "Epoch [1][20]\t Batch [2800][5500]\t Training Loss 0.1187\t Accuracy 0.9639\n",
      "Epoch [1][20]\t Batch [2850][5500]\t Training Loss 0.1182\t Accuracy 0.9639\n",
      "Epoch [1][20]\t Batch [2900][5500]\t Training Loss 0.1184\t Accuracy 0.9639\n",
      "Epoch [1][20]\t Batch [2950][5500]\t Training Loss 0.1181\t Accuracy 0.9640\n",
      "Epoch [1][20]\t Batch [3000][5500]\t Training Loss 0.1174\t Accuracy 0.9641\n",
      "Epoch [1][20]\t Batch [3050][5500]\t Training Loss 0.1170\t Accuracy 0.9642\n",
      "Epoch [1][20]\t Batch [3100][5500]\t Training Loss 0.1167\t Accuracy 0.9643\n",
      "Epoch [1][20]\t Batch [3150][5500]\t Training Loss 0.1167\t Accuracy 0.9646\n",
      "Epoch [1][20]\t Batch [3200][5500]\t Training Loss 0.1167\t Accuracy 0.9647\n",
      "Epoch [1][20]\t Batch [3250][5500]\t Training Loss 0.1167\t Accuracy 0.9646\n",
      "Epoch [1][20]\t Batch [3300][5500]\t Training Loss 0.1163\t Accuracy 0.9647\n",
      "Epoch [1][20]\t Batch [3350][5500]\t Training Loss 0.1159\t Accuracy 0.9648\n",
      "Epoch [1][20]\t Batch [3400][5500]\t Training Loss 0.1152\t Accuracy 0.9651\n",
      "Epoch [1][20]\t Batch [3450][5500]\t Training Loss 0.1147\t Accuracy 0.9653\n",
      "Epoch [1][20]\t Batch [3500][5500]\t Training Loss 0.1147\t Accuracy 0.9653\n",
      "Epoch [1][20]\t Batch [3550][5500]\t Training Loss 0.1149\t Accuracy 0.9654\n",
      "Epoch [1][20]\t Batch [3600][5500]\t Training Loss 0.1144\t Accuracy 0.9655\n",
      "Epoch [1][20]\t Batch [3650][5500]\t Training Loss 0.1144\t Accuracy 0.9655\n",
      "Epoch [1][20]\t Batch [3700][5500]\t Training Loss 0.1139\t Accuracy 0.9657\n",
      "Epoch [1][20]\t Batch [3750][5500]\t Training Loss 0.1147\t Accuracy 0.9654\n",
      "Epoch [1][20]\t Batch [3800][5500]\t Training Loss 0.1143\t Accuracy 0.9656\n",
      "Epoch [1][20]\t Batch [3850][5500]\t Training Loss 0.1137\t Accuracy 0.9657\n",
      "Epoch [1][20]\t Batch [3900][5500]\t Training Loss 0.1132\t Accuracy 0.9659\n",
      "Epoch [1][20]\t Batch [3950][5500]\t Training Loss 0.1135\t Accuracy 0.9658\n",
      "Epoch [1][20]\t Batch [4000][5500]\t Training Loss 0.1133\t Accuracy 0.9657\n",
      "Epoch [1][20]\t Batch [4050][5500]\t Training Loss 0.1129\t Accuracy 0.9660\n",
      "Epoch [1][20]\t Batch [4100][5500]\t Training Loss 0.1125\t Accuracy 0.9662\n",
      "Epoch [1][20]\t Batch [4150][5500]\t Training Loss 0.1126\t Accuracy 0.9661\n",
      "Epoch [1][20]\t Batch [4200][5500]\t Training Loss 0.1123\t Accuracy 0.9663\n",
      "Epoch [1][20]\t Batch [4250][5500]\t Training Loss 0.1126\t Accuracy 0.9661\n",
      "Epoch [1][20]\t Batch [4300][5500]\t Training Loss 0.1129\t Accuracy 0.9662\n",
      "Epoch [1][20]\t Batch [4350][5500]\t Training Loss 0.1123\t Accuracy 0.9664\n",
      "Epoch [1][20]\t Batch [4400][5500]\t Training Loss 0.1122\t Accuracy 0.9664\n",
      "Epoch [1][20]\t Batch [4450][5500]\t Training Loss 0.1121\t Accuracy 0.9664\n",
      "Epoch [1][20]\t Batch [4500][5500]\t Training Loss 0.1116\t Accuracy 0.9666\n",
      "Epoch [1][20]\t Batch [4550][5500]\t Training Loss 0.1113\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [4600][5500]\t Training Loss 0.1112\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [4650][5500]\t Training Loss 0.1116\t Accuracy 0.9667\n",
      "Epoch [1][20]\t Batch [4700][5500]\t Training Loss 0.1110\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [4750][5500]\t Training Loss 0.1113\t Accuracy 0.9667\n",
      "Epoch [1][20]\t Batch [4800][5500]\t Training Loss 0.1112\t Accuracy 0.9667\n",
      "Epoch [1][20]\t Batch [4850][5500]\t Training Loss 0.1109\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [4900][5500]\t Training Loss 0.1108\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [4950][5500]\t Training Loss 0.1107\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [5000][5500]\t Training Loss 0.1113\t Accuracy 0.9667\n",
      "Epoch [1][20]\t Batch [5050][5500]\t Training Loss 0.1113\t Accuracy 0.9667\n",
      "Epoch [1][20]\t Batch [5100][5500]\t Training Loss 0.1109\t Accuracy 0.9668\n",
      "Epoch [1][20]\t Batch [5150][5500]\t Training Loss 0.1106\t Accuracy 0.9669\n",
      "Epoch [1][20]\t Batch [5200][5500]\t Training Loss 0.1103\t Accuracy 0.9670\n",
      "Epoch [1][20]\t Batch [5250][5500]\t Training Loss 0.1102\t Accuracy 0.9670\n",
      "Epoch [1][20]\t Batch [5300][5500]\t Training Loss 0.1100\t Accuracy 0.9671\n",
      "Epoch [1][20]\t Batch [5350][5500]\t Training Loss 0.1095\t Accuracy 0.9672\n",
      "Epoch [1][20]\t Batch [5400][5500]\t Training Loss 0.1094\t Accuracy 0.9671\n",
      "Epoch [1][20]\t Batch [5450][5500]\t Training Loss 0.1090\t Accuracy 0.9672\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1088\t Average training accuracy 0.9673\n",
      "Epoch [1]\t Average validation loss 0.0922\t Average validation accuracy 0.9710\n",
      "\n",
      "Epoch [2][20]\t Batch [0][5500]\t Training Loss 0.0098\t Accuracy 1.0000\n",
      "Epoch [2][20]\t Batch [50][5500]\t Training Loss 0.0938\t Accuracy 0.9765\n",
      "Epoch [2][20]\t Batch [100][5500]\t Training Loss 0.0900\t Accuracy 0.9733\n",
      "Epoch [2][20]\t Batch [150][5500]\t Training Loss 0.1018\t Accuracy 0.9702\n",
      "Epoch [2][20]\t Batch [200][5500]\t Training Loss 0.0940\t Accuracy 0.9726\n",
      "Epoch [2][20]\t Batch [250][5500]\t Training Loss 0.0846\t Accuracy 0.9749\n",
      "Epoch [2][20]\t Batch [300][5500]\t Training Loss 0.0840\t Accuracy 0.9761\n",
      "Epoch [2][20]\t Batch [350][5500]\t Training Loss 0.0793\t Accuracy 0.9769\n",
      "Epoch [2][20]\t Batch [400][5500]\t Training Loss 0.0782\t Accuracy 0.9758\n",
      "Epoch [2][20]\t Batch [450][5500]\t Training Loss 0.0791\t Accuracy 0.9756\n",
      "Epoch [2][20]\t Batch [500][5500]\t Training Loss 0.0785\t Accuracy 0.9758\n",
      "Epoch [2][20]\t Batch [550][5500]\t Training Loss 0.0772\t Accuracy 0.9768\n",
      "Epoch [2][20]\t Batch [600][5500]\t Training Loss 0.0782\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [650][5500]\t Training Loss 0.0769\t Accuracy 0.9768\n",
      "Epoch [2][20]\t Batch [700][5500]\t Training Loss 0.0771\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [750][5500]\t Training Loss 0.0765\t Accuracy 0.9770\n",
      "Epoch [2][20]\t Batch [800][5500]\t Training Loss 0.0777\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [850][5500]\t Training Loss 0.0785\t Accuracy 0.9765\n",
      "Epoch [2][20]\t Batch [900][5500]\t Training Loss 0.0809\t Accuracy 0.9759\n",
      "Epoch [2][20]\t Batch [950][5500]\t Training Loss 0.0790\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [1000][5500]\t Training Loss 0.0776\t Accuracy 0.9768\n",
      "Epoch [2][20]\t Batch [1050][5500]\t Training Loss 0.0781\t Accuracy 0.9761\n",
      "Epoch [2][20]\t Batch [1100][5500]\t Training Loss 0.0768\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [1150][5500]\t Training Loss 0.0764\t Accuracy 0.9769\n",
      "Epoch [2][20]\t Batch [1200][5500]\t Training Loss 0.0774\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [1250][5500]\t Training Loss 0.0759\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [1300][5500]\t Training Loss 0.0775\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [1350][5500]\t Training Loss 0.0775\t Accuracy 0.9765\n",
      "Epoch [2][20]\t Batch [1400][5500]\t Training Loss 0.0777\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [1450][5500]\t Training Loss 0.0768\t Accuracy 0.9766\n",
      "Epoch [2][20]\t Batch [1500][5500]\t Training Loss 0.0771\t Accuracy 0.9765\n",
      "Epoch [2][20]\t Batch [1550][5500]\t Training Loss 0.0767\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [1600][5500]\t Training Loss 0.0775\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [1650][5500]\t Training Loss 0.0767\t Accuracy 0.9766\n",
      "Epoch [2][20]\t Batch [1700][5500]\t Training Loss 0.0769\t Accuracy 0.9762\n",
      "Epoch [2][20]\t Batch [1750][5500]\t Training Loss 0.0769\t Accuracy 0.9763\n",
      "Epoch [2][20]\t Batch [1800][5500]\t Training Loss 0.0778\t Accuracy 0.9761\n",
      "Epoch [2][20]\t Batch [1850][5500]\t Training Loss 0.0776\t Accuracy 0.9760\n",
      "Epoch [2][20]\t Batch [1900][5500]\t Training Loss 0.0764\t Accuracy 0.9764\n",
      "Epoch [2][20]\t Batch [1950][5500]\t Training Loss 0.0761\t Accuracy 0.9765\n",
      "Epoch [2][20]\t Batch [2000][5500]\t Training Loss 0.0753\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [2050][5500]\t Training Loss 0.0753\t Accuracy 0.9767\n",
      "Epoch [2][20]\t Batch [2100][5500]\t Training Loss 0.0762\t Accuracy 0.9766\n",
      "Epoch [2][20]\t Batch [2150][5500]\t Training Loss 0.0753\t Accuracy 0.9768\n",
      "Epoch [2][20]\t Batch [2200][5500]\t Training Loss 0.0750\t Accuracy 0.9771\n",
      "Epoch [2][20]\t Batch [2250][5500]\t Training Loss 0.0750\t Accuracy 0.9770\n",
      "Epoch [2][20]\t Batch [2300][5500]\t Training Loss 0.0745\t Accuracy 0.9771\n",
      "Epoch [2][20]\t Batch [2350][5500]\t Training Loss 0.0742\t Accuracy 0.9771\n",
      "Epoch [2][20]\t Batch [2400][5500]\t Training Loss 0.0750\t Accuracy 0.9770\n",
      "Epoch [2][20]\t Batch [2450][5500]\t Training Loss 0.0744\t Accuracy 0.9773\n",
      "Epoch [2][20]\t Batch [2500][5500]\t Training Loss 0.0745\t Accuracy 0.9773\n",
      "Epoch [2][20]\t Batch [2550][5500]\t Training Loss 0.0740\t Accuracy 0.9775\n",
      "Epoch [2][20]\t Batch [2600][5500]\t Training Loss 0.0741\t Accuracy 0.9774\n",
      "Epoch [2][20]\t Batch [2650][5500]\t Training Loss 0.0743\t Accuracy 0.9775\n",
      "Epoch [2][20]\t Batch [2700][5500]\t Training Loss 0.0753\t Accuracy 0.9773\n",
      "Epoch [2][20]\t Batch [2750][5500]\t Training Loss 0.0757\t Accuracy 0.9773\n",
      "Epoch [2][20]\t Batch [2800][5500]\t Training Loss 0.0761\t Accuracy 0.9772\n",
      "Epoch [2][20]\t Batch [2850][5500]\t Training Loss 0.0759\t Accuracy 0.9772\n",
      "Epoch [2][20]\t Batch [2900][5500]\t Training Loss 0.0760\t Accuracy 0.9771\n",
      "Epoch [2][20]\t Batch [2950][5500]\t Training Loss 0.0761\t Accuracy 0.9770\n",
      "Epoch [2][20]\t Batch [3000][5500]\t Training Loss 0.0755\t Accuracy 0.9772\n",
      "Epoch [2][20]\t Batch [3050][5500]\t Training Loss 0.0752\t Accuracy 0.9773\n",
      "Epoch [2][20]\t Batch [3100][5500]\t Training Loss 0.0750\t Accuracy 0.9774\n",
      "Epoch [2][20]\t Batch [3150][5500]\t Training Loss 0.0751\t Accuracy 0.9774\n",
      "Epoch [2][20]\t Batch [3200][5500]\t Training Loss 0.0751\t Accuracy 0.9775\n",
      "Epoch [2][20]\t Batch [3250][5500]\t Training Loss 0.0752\t Accuracy 0.9775\n",
      "Epoch [2][20]\t Batch [3300][5500]\t Training Loss 0.0751\t Accuracy 0.9774\n",
      "Epoch [2][20]\t Batch [3350][5500]\t Training Loss 0.0747\t Accuracy 0.9775\n",
      "Epoch [2][20]\t Batch [3400][5500]\t Training Loss 0.0743\t Accuracy 0.9777\n",
      "Epoch [2][20]\t Batch [3450][5500]\t Training Loss 0.0742\t Accuracy 0.9779\n",
      "Epoch [2][20]\t Batch [3500][5500]\t Training Loss 0.0743\t Accuracy 0.9779\n",
      "Epoch [2][20]\t Batch [3550][5500]\t Training Loss 0.0745\t Accuracy 0.9778\n",
      "Epoch [2][20]\t Batch [3600][5500]\t Training Loss 0.0742\t Accuracy 0.9779\n",
      "Epoch [2][20]\t Batch [3650][5500]\t Training Loss 0.0743\t Accuracy 0.9778\n",
      "Epoch [2][20]\t Batch [3700][5500]\t Training Loss 0.0739\t Accuracy 0.9780\n",
      "Epoch [2][20]\t Batch [3750][5500]\t Training Loss 0.0743\t Accuracy 0.9778\n",
      "Epoch [2][20]\t Batch [3800][5500]\t Training Loss 0.0742\t Accuracy 0.9779\n",
      "Epoch [2][20]\t Batch [3850][5500]\t Training Loss 0.0737\t Accuracy 0.9780\n",
      "Epoch [2][20]\t Batch [3900][5500]\t Training Loss 0.0736\t Accuracy 0.9780\n",
      "Epoch [2][20]\t Batch [3950][5500]\t Training Loss 0.0738\t Accuracy 0.9780\n",
      "Epoch [2][20]\t Batch [4000][5500]\t Training Loss 0.0735\t Accuracy 0.9780\n",
      "Epoch [2][20]\t Batch [4050][5500]\t Training Loss 0.0733\t Accuracy 0.9782\n",
      "Epoch [2][20]\t Batch [4100][5500]\t Training Loss 0.0731\t Accuracy 0.9782\n",
      "Epoch [2][20]\t Batch [4150][5500]\t Training Loss 0.0732\t Accuracy 0.9781\n",
      "Epoch [2][20]\t Batch [4200][5500]\t Training Loss 0.0731\t Accuracy 0.9782\n",
      "Epoch [2][20]\t Batch [4250][5500]\t Training Loss 0.0731\t Accuracy 0.9781\n",
      "Epoch [2][20]\t Batch [4300][5500]\t Training Loss 0.0733\t Accuracy 0.9781\n",
      "Epoch [2][20]\t Batch [4350][5500]\t Training Loss 0.0732\t Accuracy 0.9783\n",
      "Epoch [2][20]\t Batch [4400][5500]\t Training Loss 0.0730\t Accuracy 0.9783\n",
      "Epoch [2][20]\t Batch [4450][5500]\t Training Loss 0.0730\t Accuracy 0.9783\n",
      "Epoch [2][20]\t Batch [4500][5500]\t Training Loss 0.0727\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [4550][5500]\t Training Loss 0.0726\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [4600][5500]\t Training Loss 0.0726\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [4650][5500]\t Training Loss 0.0727\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [4700][5500]\t Training Loss 0.0724\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [4750][5500]\t Training Loss 0.0727\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [4800][5500]\t Training Loss 0.0727\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [4850][5500]\t Training Loss 0.0724\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [4900][5500]\t Training Loss 0.0725\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [4950][5500]\t Training Loss 0.0725\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [5000][5500]\t Training Loss 0.0729\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [5050][5500]\t Training Loss 0.0728\t Accuracy 0.9785\n",
      "Epoch [2][20]\t Batch [5100][5500]\t Training Loss 0.0725\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [5150][5500]\t Training Loss 0.0724\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [5200][5500]\t Training Loss 0.0723\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [5250][5500]\t Training Loss 0.0722\t Accuracy 0.9787\n",
      "Epoch [2][20]\t Batch [5300][5500]\t Training Loss 0.0720\t Accuracy 0.9786\n",
      "Epoch [2][20]\t Batch [5350][5500]\t Training Loss 0.0717\t Accuracy 0.9788\n",
      "Epoch [2][20]\t Batch [5400][5500]\t Training Loss 0.0715\t Accuracy 0.9788\n",
      "Epoch [2][20]\t Batch [5450][5500]\t Training Loss 0.0714\t Accuracy 0.9788\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0712\t Average training accuracy 0.9789\n",
      "Epoch [2]\t Average validation loss 0.0800\t Average validation accuracy 0.9770\n",
      "\n",
      "Epoch [3][20]\t Batch [0][5500]\t Training Loss 0.0093\t Accuracy 1.0000\n",
      "Epoch [3][20]\t Batch [50][5500]\t Training Loss 0.0739\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [100][5500]\t Training Loss 0.0678\t Accuracy 0.9812\n",
      "Epoch [3][20]\t Batch [150][5500]\t Training Loss 0.0738\t Accuracy 0.9775\n",
      "Epoch [3][20]\t Batch [200][5500]\t Training Loss 0.0680\t Accuracy 0.9781\n",
      "Epoch [3][20]\t Batch [250][5500]\t Training Loss 0.0593\t Accuracy 0.9813\n",
      "Epoch [3][20]\t Batch [300][5500]\t Training Loss 0.0605\t Accuracy 0.9817\n",
      "Epoch [3][20]\t Batch [350][5500]\t Training Loss 0.0561\t Accuracy 0.9835\n",
      "Epoch [3][20]\t Batch [400][5500]\t Training Loss 0.0540\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [450][5500]\t Training Loss 0.0537\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [500][5500]\t Training Loss 0.0545\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [550][5500]\t Training Loss 0.0544\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [600][5500]\t Training Loss 0.0547\t Accuracy 0.9837\n",
      "Epoch [3][20]\t Batch [650][5500]\t Training Loss 0.0529\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [700][5500]\t Training Loss 0.0539\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [750][5500]\t Training Loss 0.0536\t Accuracy 0.9847\n",
      "Epoch [3][20]\t Batch [800][5500]\t Training Loss 0.0547\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [850][5500]\t Training Loss 0.0569\t Accuracy 0.9834\n",
      "Epoch [3][20]\t Batch [900][5500]\t Training Loss 0.0587\t Accuracy 0.9831\n",
      "Epoch [3][20]\t Batch [950][5500]\t Training Loss 0.0576\t Accuracy 0.9835\n",
      "Epoch [3][20]\t Batch [1000][5500]\t Training Loss 0.0560\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1050][5500]\t Training Loss 0.0571\t Accuracy 0.9833\n",
      "Epoch [3][20]\t Batch [1100][5500]\t Training Loss 0.0561\t Accuracy 0.9838\n",
      "Epoch [3][20]\t Batch [1150][5500]\t Training Loss 0.0552\t Accuracy 0.9838\n",
      "Epoch [3][20]\t Batch [1200][5500]\t Training Loss 0.0557\t Accuracy 0.9839\n",
      "Epoch [3][20]\t Batch [1250][5500]\t Training Loss 0.0546\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [1300][5500]\t Training Loss 0.0556\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1350][5500]\t Training Loss 0.0552\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [1400][5500]\t Training Loss 0.0551\t Accuracy 0.9839\n",
      "Epoch [3][20]\t Batch [1450][5500]\t Training Loss 0.0546\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1500][5500]\t Training Loss 0.0539\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [1550][5500]\t Training Loss 0.0538\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1600][5500]\t Training Loss 0.0544\t Accuracy 0.9838\n",
      "Epoch [3][20]\t Batch [1650][5500]\t Training Loss 0.0539\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1700][5500]\t Training Loss 0.0538\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [1750][5500]\t Training Loss 0.0538\t Accuracy 0.9838\n",
      "Epoch [3][20]\t Batch [1800][5500]\t Training Loss 0.0547\t Accuracy 0.9837\n",
      "Epoch [3][20]\t Batch [1850][5500]\t Training Loss 0.0544\t Accuracy 0.9836\n",
      "Epoch [3][20]\t Batch [1900][5500]\t Training Loss 0.0534\t Accuracy 0.9839\n",
      "Epoch [3][20]\t Batch [1950][5500]\t Training Loss 0.0531\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [2000][5500]\t Training Loss 0.0524\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [2050][5500]\t Training Loss 0.0526\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [2100][5500]\t Training Loss 0.0534\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [2150][5500]\t Training Loss 0.0528\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [2200][5500]\t Training Loss 0.0528\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2250][5500]\t Training Loss 0.0526\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2300][5500]\t Training Loss 0.0522\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2350][5500]\t Training Loss 0.0520\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2400][5500]\t Training Loss 0.0526\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2450][5500]\t Training Loss 0.0521\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [2500][5500]\t Training Loss 0.0521\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [2550][5500]\t Training Loss 0.0518\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [2600][5500]\t Training Loss 0.0520\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [2650][5500]\t Training Loss 0.0522\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [2700][5500]\t Training Loss 0.0529\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [2750][5500]\t Training Loss 0.0532\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [2800][5500]\t Training Loss 0.0534\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [2850][5500]\t Training Loss 0.0533\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [2900][5500]\t Training Loss 0.0533\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [2950][5500]\t Training Loss 0.0534\t Accuracy 0.9839\n",
      "Epoch [3][20]\t Batch [3000][5500]\t Training Loss 0.0530\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [3050][5500]\t Training Loss 0.0527\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3100][5500]\t Training Loss 0.0525\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3150][5500]\t Training Loss 0.0526\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3200][5500]\t Training Loss 0.0527\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3250][5500]\t Training Loss 0.0530\t Accuracy 0.9840\n",
      "Epoch [3][20]\t Batch [3300][5500]\t Training Loss 0.0528\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3350][5500]\t Training Loss 0.0526\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3400][5500]\t Training Loss 0.0523\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3450][5500]\t Training Loss 0.0524\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [3500][5500]\t Training Loss 0.0526\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3550][5500]\t Training Loss 0.0529\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3600][5500]\t Training Loss 0.0528\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3650][5500]\t Training Loss 0.0530\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3700][5500]\t Training Loss 0.0526\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3750][5500]\t Training Loss 0.0528\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3800][5500]\t Training Loss 0.0528\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [3850][5500]\t Training Loss 0.0526\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3900][5500]\t Training Loss 0.0525\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [3950][5500]\t Training Loss 0.0526\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [4000][5500]\t Training Loss 0.0524\t Accuracy 0.9841\n",
      "Epoch [3][20]\t Batch [4050][5500]\t Training Loss 0.0523\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [4100][5500]\t Training Loss 0.0523\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [4150][5500]\t Training Loss 0.0524\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [4200][5500]\t Training Loss 0.0523\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4250][5500]\t Training Loss 0.0522\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [4300][5500]\t Training Loss 0.0523\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [4350][5500]\t Training Loss 0.0523\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4400][5500]\t Training Loss 0.0523\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4450][5500]\t Training Loss 0.0523\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4500][5500]\t Training Loss 0.0520\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [4550][5500]\t Training Loss 0.0520\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [4600][5500]\t Training Loss 0.0519\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [4650][5500]\t Training Loss 0.0521\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [4700][5500]\t Training Loss 0.0518\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [4750][5500]\t Training Loss 0.0522\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [4800][5500]\t Training Loss 0.0523\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4850][5500]\t Training Loss 0.0520\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [4900][5500]\t Training Loss 0.0522\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [4950][5500]\t Training Loss 0.0522\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [5000][5500]\t Training Loss 0.0525\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [5050][5500]\t Training Loss 0.0524\t Accuracy 0.9842\n",
      "Epoch [3][20]\t Batch [5100][5500]\t Training Loss 0.0521\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [5150][5500]\t Training Loss 0.0521\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [5200][5500]\t Training Loss 0.0519\t Accuracy 0.9843\n",
      "Epoch [3][20]\t Batch [5250][5500]\t Training Loss 0.0518\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [5300][5500]\t Training Loss 0.0517\t Accuracy 0.9844\n",
      "Epoch [3][20]\t Batch [5350][5500]\t Training Loss 0.0515\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [5400][5500]\t Training Loss 0.0514\t Accuracy 0.9845\n",
      "Epoch [3][20]\t Batch [5450][5500]\t Training Loss 0.0513\t Accuracy 0.9846\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0512\t Average training accuracy 0.9846\n",
      "Epoch [3]\t Average validation loss 0.0749\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [4][20]\t Batch [0][5500]\t Training Loss 0.0045\t Accuracy 1.0000\n",
      "Epoch [4][20]\t Batch [50][5500]\t Training Loss 0.0620\t Accuracy 0.9824\n",
      "Epoch [4][20]\t Batch [100][5500]\t Training Loss 0.0527\t Accuracy 0.9842\n",
      "Epoch [4][20]\t Batch [150][5500]\t Training Loss 0.0555\t Accuracy 0.9828\n",
      "Epoch [4][20]\t Batch [200][5500]\t Training Loss 0.0486\t Accuracy 0.9856\n",
      "Epoch [4][20]\t Batch [250][5500]\t Training Loss 0.0423\t Accuracy 0.9876\n",
      "Epoch [4][20]\t Batch [300][5500]\t Training Loss 0.0437\t Accuracy 0.9880\n",
      "Epoch [4][20]\t Batch [350][5500]\t Training Loss 0.0409\t Accuracy 0.9886\n",
      "Epoch [4][20]\t Batch [400][5500]\t Training Loss 0.0398\t Accuracy 0.9888\n",
      "Epoch [4][20]\t Batch [450][5500]\t Training Loss 0.0403\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [500][5500]\t Training Loss 0.0406\t Accuracy 0.9894\n",
      "Epoch [4][20]\t Batch [550][5500]\t Training Loss 0.0411\t Accuracy 0.9895\n",
      "Epoch [4][20]\t Batch [600][5500]\t Training Loss 0.0417\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [650][5500]\t Training Loss 0.0402\t Accuracy 0.9897\n",
      "Epoch [4][20]\t Batch [700][5500]\t Training Loss 0.0410\t Accuracy 0.9896\n",
      "Epoch [4][20]\t Batch [750][5500]\t Training Loss 0.0409\t Accuracy 0.9896\n",
      "Epoch [4][20]\t Batch [800][5500]\t Training Loss 0.0411\t Accuracy 0.9895\n",
      "Epoch [4][20]\t Batch [850][5500]\t Training Loss 0.0423\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [900][5500]\t Training Loss 0.0434\t Accuracy 0.9886\n",
      "Epoch [4][20]\t Batch [950][5500]\t Training Loss 0.0427\t Accuracy 0.9884\n",
      "Epoch [4][20]\t Batch [1000][5500]\t Training Loss 0.0414\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [1050][5500]\t Training Loss 0.0425\t Accuracy 0.9881\n",
      "Epoch [4][20]\t Batch [1100][5500]\t Training Loss 0.0422\t Accuracy 0.9882\n",
      "Epoch [4][20]\t Batch [1150][5500]\t Training Loss 0.0413\t Accuracy 0.9884\n",
      "Epoch [4][20]\t Batch [1200][5500]\t Training Loss 0.0418\t Accuracy 0.9883\n",
      "Epoch [4][20]\t Batch [1250][5500]\t Training Loss 0.0409\t Accuracy 0.9887\n",
      "Epoch [4][20]\t Batch [1300][5500]\t Training Loss 0.0407\t Accuracy 0.9888\n",
      "Epoch [4][20]\t Batch [1350][5500]\t Training Loss 0.0405\t Accuracy 0.9887\n",
      "Epoch [4][20]\t Batch [1400][5500]\t Training Loss 0.0401\t Accuracy 0.9886\n",
      "Epoch [4][20]\t Batch [1450][5500]\t Training Loss 0.0398\t Accuracy 0.9887\n",
      "Epoch [4][20]\t Batch [1500][5500]\t Training Loss 0.0392\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [1550][5500]\t Training Loss 0.0388\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1600][5500]\t Training Loss 0.0390\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1650][5500]\t Training Loss 0.0386\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1700][5500]\t Training Loss 0.0384\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1750][5500]\t Training Loss 0.0384\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1800][5500]\t Training Loss 0.0389\t Accuracy 0.9888\n",
      "Epoch [4][20]\t Batch [1850][5500]\t Training Loss 0.0388\t Accuracy 0.9887\n",
      "Epoch [4][20]\t Batch [1900][5500]\t Training Loss 0.0382\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [1950][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [2000][5500]\t Training Loss 0.0376\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2050][5500]\t Training Loss 0.0377\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [2100][5500]\t Training Loss 0.0383\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [2150][5500]\t Training Loss 0.0380\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [2200][5500]\t Training Loss 0.0381\t Accuracy 0.9889\n",
      "Epoch [4][20]\t Batch [2250][5500]\t Training Loss 0.0379\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [2300][5500]\t Training Loss 0.0376\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [2350][5500]\t Training Loss 0.0373\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2400][5500]\t Training Loss 0.0375\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2450][5500]\t Training Loss 0.0371\t Accuracy 0.9893\n",
      "Epoch [4][20]\t Batch [2500][5500]\t Training Loss 0.0371\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2550][5500]\t Training Loss 0.0368\t Accuracy 0.9893\n",
      "Epoch [4][20]\t Batch [2600][5500]\t Training Loss 0.0370\t Accuracy 0.9894\n",
      "Epoch [4][20]\t Batch [2650][5500]\t Training Loss 0.0373\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2700][5500]\t Training Loss 0.0378\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2750][5500]\t Training Loss 0.0381\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [2800][5500]\t Training Loss 0.0382\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [2850][5500]\t Training Loss 0.0380\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [2900][5500]\t Training Loss 0.0382\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [2950][5500]\t Training Loss 0.0383\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3000][5500]\t Training Loss 0.0381\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3050][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3100][5500]\t Training Loss 0.0377\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3150][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3200][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3250][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3300][5500]\t Training Loss 0.0380\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3350][5500]\t Training Loss 0.0379\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3400][5500]\t Training Loss 0.0376\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3450][5500]\t Training Loss 0.0378\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [3500][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3550][5500]\t Training Loss 0.0380\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3600][5500]\t Training Loss 0.0380\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3650][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [3700][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3750][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [3800][5500]\t Training Loss 0.0379\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [3850][5500]\t Training Loss 0.0378\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [3900][5500]\t Training Loss 0.0377\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [3950][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4000][5500]\t Training Loss 0.0377\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [4050][5500]\t Training Loss 0.0376\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [4100][5500]\t Training Loss 0.0376\t Accuracy 0.9892\n",
      "Epoch [4][20]\t Batch [4150][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4200][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4250][5500]\t Training Loss 0.0378\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [4300][5500]\t Training Loss 0.0379\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [4350][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4400][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4450][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [4500][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4550][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4600][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4650][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4700][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4750][5500]\t Training Loss 0.0380\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [4800][5500]\t Training Loss 0.0381\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [4850][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4900][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [4950][5500]\t Training Loss 0.0380\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5000][5500]\t Training Loss 0.0382\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [5050][5500]\t Training Loss 0.0380\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5100][5500]\t Training Loss 0.0379\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5150][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5200][5500]\t Training Loss 0.0378\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5250][5500]\t Training Loss 0.0378\t Accuracy 0.9890\n",
      "Epoch [4][20]\t Batch [5300][5500]\t Training Loss 0.0377\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5350][5500]\t Training Loss 0.0376\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5400][5500]\t Training Loss 0.0376\t Accuracy 0.9891\n",
      "Epoch [4][20]\t Batch [5450][5500]\t Training Loss 0.0375\t Accuracy 0.9891\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0375\t Average training accuracy 0.9892\n",
      "Epoch [4]\t Average validation loss 0.0704\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [5][20]\t Batch [0][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [5][20]\t Batch [50][5500]\t Training Loss 0.0432\t Accuracy 0.9882\n",
      "Epoch [5][20]\t Batch [100][5500]\t Training Loss 0.0397\t Accuracy 0.9881\n",
      "Epoch [5][20]\t Batch [150][5500]\t Training Loss 0.0416\t Accuracy 0.9868\n",
      "Epoch [5][20]\t Batch [200][5500]\t Training Loss 0.0371\t Accuracy 0.9881\n",
      "Epoch [5][20]\t Batch [250][5500]\t Training Loss 0.0321\t Accuracy 0.9896\n",
      "Epoch [5][20]\t Batch [300][5500]\t Training Loss 0.0335\t Accuracy 0.9897\n",
      "Epoch [5][20]\t Batch [350][5500]\t Training Loss 0.0310\t Accuracy 0.9903\n",
      "Epoch [5][20]\t Batch [400][5500]\t Training Loss 0.0304\t Accuracy 0.9908\n",
      "Epoch [5][20]\t Batch [450][5500]\t Training Loss 0.0312\t Accuracy 0.9911\n",
      "Epoch [5][20]\t Batch [500][5500]\t Training Loss 0.0316\t Accuracy 0.9908\n",
      "Epoch [5][20]\t Batch [550][5500]\t Training Loss 0.0315\t Accuracy 0.9907\n",
      "Epoch [5][20]\t Batch [600][5500]\t Training Loss 0.0319\t Accuracy 0.9907\n",
      "Epoch [5][20]\t Batch [650][5500]\t Training Loss 0.0314\t Accuracy 0.9909\n",
      "Epoch [5][20]\t Batch [700][5500]\t Training Loss 0.0320\t Accuracy 0.9907\n",
      "Epoch [5][20]\t Batch [750][5500]\t Training Loss 0.0323\t Accuracy 0.9905\n",
      "Epoch [5][20]\t Batch [800][5500]\t Training Loss 0.0323\t Accuracy 0.9904\n",
      "Epoch [5][20]\t Batch [850][5500]\t Training Loss 0.0328\t Accuracy 0.9902\n",
      "Epoch [5][20]\t Batch [900][5500]\t Training Loss 0.0336\t Accuracy 0.9901\n",
      "Epoch [5][20]\t Batch [950][5500]\t Training Loss 0.0331\t Accuracy 0.9902\n",
      "Epoch [5][20]\t Batch [1000][5500]\t Training Loss 0.0320\t Accuracy 0.9907\n",
      "Epoch [5][20]\t Batch [1050][5500]\t Training Loss 0.0326\t Accuracy 0.9904\n",
      "Epoch [5][20]\t Batch [1100][5500]\t Training Loss 0.0321\t Accuracy 0.9907\n",
      "Epoch [5][20]\t Batch [1150][5500]\t Training Loss 0.0313\t Accuracy 0.9910\n",
      "Epoch [5][20]\t Batch [1200][5500]\t Training Loss 0.0318\t Accuracy 0.9908\n",
      "Epoch [5][20]\t Batch [1250][5500]\t Training Loss 0.0315\t Accuracy 0.9909\n",
      "Epoch [5][20]\t Batch [1300][5500]\t Training Loss 0.0313\t Accuracy 0.9910\n",
      "Epoch [5][20]\t Batch [1350][5500]\t Training Loss 0.0306\t Accuracy 0.9912\n",
      "Epoch [5][20]\t Batch [1400][5500]\t Training Loss 0.0303\t Accuracy 0.9912\n",
      "Epoch [5][20]\t Batch [1450][5500]\t Training Loss 0.0301\t Accuracy 0.9912\n",
      "Epoch [5][20]\t Batch [1500][5500]\t Training Loss 0.0298\t Accuracy 0.9913\n",
      "Epoch [5][20]\t Batch [1550][5500]\t Training Loss 0.0294\t Accuracy 0.9915\n",
      "Epoch [5][20]\t Batch [1600][5500]\t Training Loss 0.0296\t Accuracy 0.9915\n",
      "Epoch [5][20]\t Batch [1650][5500]\t Training Loss 0.0293\t Accuracy 0.9915\n",
      "Epoch [5][20]\t Batch [1700][5500]\t Training Loss 0.0291\t Accuracy 0.9917\n",
      "Epoch [5][20]\t Batch [1750][5500]\t Training Loss 0.0291\t Accuracy 0.9915\n",
      "Epoch [5][20]\t Batch [1800][5500]\t Training Loss 0.0293\t Accuracy 0.9914\n",
      "Epoch [5][20]\t Batch [1850][5500]\t Training Loss 0.0289\t Accuracy 0.9916\n",
      "Epoch [5][20]\t Batch [1900][5500]\t Training Loss 0.0285\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [1950][5500]\t Training Loss 0.0284\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2000][5500]\t Training Loss 0.0281\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [2050][5500]\t Training Loss 0.0283\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2100][5500]\t Training Loss 0.0287\t Accuracy 0.9917\n",
      "Epoch [5][20]\t Batch [2150][5500]\t Training Loss 0.0284\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2200][5500]\t Training Loss 0.0286\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2250][5500]\t Training Loss 0.0287\t Accuracy 0.9917\n",
      "Epoch [5][20]\t Batch [2300][5500]\t Training Loss 0.0284\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2350][5500]\t Training Loss 0.0282\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2400][5500]\t Training Loss 0.0284\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2450][5500]\t Training Loss 0.0281\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2500][5500]\t Training Loss 0.0280\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2550][5500]\t Training Loss 0.0280\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [2600][5500]\t Training Loss 0.0280\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2650][5500]\t Training Loss 0.0280\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [2700][5500]\t Training Loss 0.0285\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [2750][5500]\t Training Loss 0.0287\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2800][5500]\t Training Loss 0.0289\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2850][5500]\t Training Loss 0.0287\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2900][5500]\t Training Loss 0.0287\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [2950][5500]\t Training Loss 0.0288\t Accuracy 0.9917\n",
      "Epoch [5][20]\t Batch [3000][5500]\t Training Loss 0.0286\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [3050][5500]\t Training Loss 0.0284\t Accuracy 0.9918\n",
      "Epoch [5][20]\t Batch [3100][5500]\t Training Loss 0.0283\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [3150][5500]\t Training Loss 0.0283\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [3200][5500]\t Training Loss 0.0284\t Accuracy 0.9919\n",
      "Epoch [5][20]\t Batch [3250][5500]\t Training Loss 0.0284\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [3300][5500]\t Training Loss 0.0283\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [3350][5500]\t Training Loss 0.0282\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3400][5500]\t Training Loss 0.0280\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3450][5500]\t Training Loss 0.0282\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3500][5500]\t Training Loss 0.0283\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3550][5500]\t Training Loss 0.0285\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3600][5500]\t Training Loss 0.0284\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3650][5500]\t Training Loss 0.0286\t Accuracy 0.9920\n",
      "Epoch [5][20]\t Batch [3700][5500]\t Training Loss 0.0285\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3750][5500]\t Training Loss 0.0284\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [3800][5500]\t Training Loss 0.0283\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [3850][5500]\t Training Loss 0.0283\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [3900][5500]\t Training Loss 0.0282\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [3950][5500]\t Training Loss 0.0283\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4000][5500]\t Training Loss 0.0282\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4050][5500]\t Training Loss 0.0281\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4100][5500]\t Training Loss 0.0280\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4150][5500]\t Training Loss 0.0281\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4200][5500]\t Training Loss 0.0282\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4250][5500]\t Training Loss 0.0281\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4300][5500]\t Training Loss 0.0281\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4350][5500]\t Training Loss 0.0282\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4400][5500]\t Training Loss 0.0282\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4450][5500]\t Training Loss 0.0282\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4500][5500]\t Training Loss 0.0280\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4550][5500]\t Training Loss 0.0280\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4600][5500]\t Training Loss 0.0280\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4650][5500]\t Training Loss 0.0280\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4700][5500]\t Training Loss 0.0279\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [4750][5500]\t Training Loss 0.0281\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4800][5500]\t Training Loss 0.0282\t Accuracy 0.9921\n",
      "Epoch [5][20]\t Batch [4850][5500]\t Training Loss 0.0281\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4900][5500]\t Training Loss 0.0280\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [4950][5500]\t Training Loss 0.0280\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [5000][5500]\t Training Loss 0.0282\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [5050][5500]\t Training Loss 0.0280\t Accuracy 0.9922\n",
      "Epoch [5][20]\t Batch [5100][5500]\t Training Loss 0.0279\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5150][5500]\t Training Loss 0.0278\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5200][5500]\t Training Loss 0.0278\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5250][5500]\t Training Loss 0.0278\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5300][5500]\t Training Loss 0.0278\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5350][5500]\t Training Loss 0.0277\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5400][5500]\t Training Loss 0.0277\t Accuracy 0.9923\n",
      "Epoch [5][20]\t Batch [5450][5500]\t Training Loss 0.0276\t Accuracy 0.9924\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0275\t Average training accuracy 0.9924\n",
      "Epoch [5]\t Average validation loss 0.0768\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [6][20]\t Batch [0][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [6][20]\t Batch [50][5500]\t Training Loss 0.0389\t Accuracy 0.9922\n",
      "Epoch [6][20]\t Batch [100][5500]\t Training Loss 0.0300\t Accuracy 0.9941\n",
      "Epoch [6][20]\t Batch [150][5500]\t Training Loss 0.0308\t Accuracy 0.9921\n",
      "Epoch [6][20]\t Batch [200][5500]\t Training Loss 0.0290\t Accuracy 0.9920\n",
      "Epoch [6][20]\t Batch [250][5500]\t Training Loss 0.0249\t Accuracy 0.9932\n",
      "Epoch [6][20]\t Batch [300][5500]\t Training Loss 0.0252\t Accuracy 0.9937\n",
      "Epoch [6][20]\t Batch [350][5500]\t Training Loss 0.0236\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [400][5500]\t Training Loss 0.0235\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [450][5500]\t Training Loss 0.0243\t Accuracy 0.9942\n",
      "Epoch [6][20]\t Batch [500][5500]\t Training Loss 0.0243\t Accuracy 0.9938\n",
      "Epoch [6][20]\t Batch [550][5500]\t Training Loss 0.0241\t Accuracy 0.9940\n",
      "Epoch [6][20]\t Batch [600][5500]\t Training Loss 0.0243\t Accuracy 0.9938\n",
      "Epoch [6][20]\t Batch [650][5500]\t Training Loss 0.0239\t Accuracy 0.9939\n",
      "Epoch [6][20]\t Batch [700][5500]\t Training Loss 0.0241\t Accuracy 0.9939\n",
      "Epoch [6][20]\t Batch [750][5500]\t Training Loss 0.0238\t Accuracy 0.9939\n",
      "Epoch [6][20]\t Batch [800][5500]\t Training Loss 0.0237\t Accuracy 0.9936\n",
      "Epoch [6][20]\t Batch [850][5500]\t Training Loss 0.0248\t Accuracy 0.9931\n",
      "Epoch [6][20]\t Batch [900][5500]\t Training Loss 0.0245\t Accuracy 0.9932\n",
      "Epoch [6][20]\t Batch [950][5500]\t Training Loss 0.0241\t Accuracy 0.9935\n",
      "Epoch [6][20]\t Batch [1000][5500]\t Training Loss 0.0233\t Accuracy 0.9938\n",
      "Epoch [6][20]\t Batch [1050][5500]\t Training Loss 0.0237\t Accuracy 0.9937\n",
      "Epoch [6][20]\t Batch [1100][5500]\t Training Loss 0.0234\t Accuracy 0.9938\n",
      "Epoch [6][20]\t Batch [1150][5500]\t Training Loss 0.0228\t Accuracy 0.9939\n",
      "Epoch [6][20]\t Batch [1200][5500]\t Training Loss 0.0230\t Accuracy 0.9940\n",
      "Epoch [6][20]\t Batch [1250][5500]\t Training Loss 0.0227\t Accuracy 0.9941\n",
      "Epoch [6][20]\t Batch [1300][5500]\t Training Loss 0.0225\t Accuracy 0.9942\n",
      "Epoch [6][20]\t Batch [1350][5500]\t Training Loss 0.0224\t Accuracy 0.9942\n",
      "Epoch [6][20]\t Batch [1400][5500]\t Training Loss 0.0219\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [1450][5500]\t Training Loss 0.0219\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [1500][5500]\t Training Loss 0.0216\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [1550][5500]\t Training Loss 0.0213\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [1600][5500]\t Training Loss 0.0215\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [1650][5500]\t Training Loss 0.0211\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [1700][5500]\t Training Loss 0.0212\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [1750][5500]\t Training Loss 0.0212\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [1800][5500]\t Training Loss 0.0214\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [1850][5500]\t Training Loss 0.0216\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [1900][5500]\t Training Loss 0.0213\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [1950][5500]\t Training Loss 0.0212\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2000][5500]\t Training Loss 0.0209\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2050][5500]\t Training Loss 0.0209\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2100][5500]\t Training Loss 0.0213\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [2150][5500]\t Training Loss 0.0210\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2200][5500]\t Training Loss 0.0212\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [2250][5500]\t Training Loss 0.0211\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [2300][5500]\t Training Loss 0.0209\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2350][5500]\t Training Loss 0.0208\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2400][5500]\t Training Loss 0.0209\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [2450][5500]\t Training Loss 0.0207\t Accuracy 0.9943\n",
      "Epoch [6][20]\t Batch [2500][5500]\t Training Loss 0.0207\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2550][5500]\t Training Loss 0.0207\t Accuracy 0.9944\n",
      "Epoch [6][20]\t Batch [2600][5500]\t Training Loss 0.0208\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2650][5500]\t Training Loss 0.0208\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2700][5500]\t Training Loss 0.0212\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2750][5500]\t Training Loss 0.0211\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2800][5500]\t Training Loss 0.0213\t Accuracy 0.9945\n",
      "Epoch [6][20]\t Batch [2850][5500]\t Training Loss 0.0211\t Accuracy 0.9946\n",
      "Epoch [6][20]\t Batch [2900][5500]\t Training Loss 0.0209\t Accuracy 0.9947\n",
      "Epoch [6][20]\t Batch [2950][5500]\t Training Loss 0.0210\t Accuracy 0.9947\n",
      "Epoch [6][20]\t Batch [3000][5500]\t Training Loss 0.0208\t Accuracy 0.9947\n",
      "Epoch [6][20]\t Batch [3050][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3100][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3150][5500]\t Training Loss 0.0205\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3200][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3250][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3300][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3350][5500]\t Training Loss 0.0205\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3400][5500]\t Training Loss 0.0203\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3450][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3500][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3550][5500]\t Training Loss 0.0208\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3600][5500]\t Training Loss 0.0207\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3650][5500]\t Training Loss 0.0208\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3700][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3750][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3800][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3850][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [3900][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [3950][5500]\t Training Loss 0.0208\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4000][5500]\t Training Loss 0.0208\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4050][5500]\t Training Loss 0.0206\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4100][5500]\t Training Loss 0.0205\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4150][5500]\t Training Loss 0.0205\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4200][5500]\t Training Loss 0.0207\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4250][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4300][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4350][5500]\t Training Loss 0.0208\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4400][5500]\t Training Loss 0.0208\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4450][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][20]\t Batch [4500][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4550][5500]\t Training Loss 0.0206\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4600][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [4650][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [4700][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [4750][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [4800][5500]\t Training Loss 0.0207\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4850][5500]\t Training Loss 0.0205\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [4900][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [4950][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5000][5500]\t Training Loss 0.0205\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5050][5500]\t Training Loss 0.0204\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5100][5500]\t Training Loss 0.0203\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5150][5500]\t Training Loss 0.0202\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5200][5500]\t Training Loss 0.0202\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5250][5500]\t Training Loss 0.0202\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5300][5500]\t Training Loss 0.0202\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5350][5500]\t Training Loss 0.0202\t Accuracy 0.9950\n",
      "Epoch [6][20]\t Batch [5400][5500]\t Training Loss 0.0202\t Accuracy 0.9949\n",
      "Epoch [6][20]\t Batch [5450][5500]\t Training Loss 0.0201\t Accuracy 0.9950\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0201\t Average training accuracy 0.9950\n",
      "Epoch [6]\t Average validation loss 0.0800\t Average validation accuracy 0.9784\n",
      "\n",
      "Epoch [7][20]\t Batch [0][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [7][20]\t Batch [50][5500]\t Training Loss 0.0231\t Accuracy 0.9961\n",
      "Epoch [7][20]\t Batch [100][5500]\t Training Loss 0.0180\t Accuracy 0.9960\n",
      "Epoch [7][20]\t Batch [150][5500]\t Training Loss 0.0207\t Accuracy 0.9947\n",
      "Epoch [7][20]\t Batch [200][5500]\t Training Loss 0.0186\t Accuracy 0.9955\n",
      "Epoch [7][20]\t Batch [250][5500]\t Training Loss 0.0174\t Accuracy 0.9956\n",
      "Epoch [7][20]\t Batch [300][5500]\t Training Loss 0.0184\t Accuracy 0.9957\n",
      "Epoch [7][20]\t Batch [350][5500]\t Training Loss 0.0174\t Accuracy 0.9960\n",
      "Epoch [7][20]\t Batch [400][5500]\t Training Loss 0.0166\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [450][5500]\t Training Loss 0.0174\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [500][5500]\t Training Loss 0.0174\t Accuracy 0.9964\n",
      "Epoch [7][20]\t Batch [550][5500]\t Training Loss 0.0173\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [600][5500]\t Training Loss 0.0175\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [650][5500]\t Training Loss 0.0171\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [700][5500]\t Training Loss 0.0170\t Accuracy 0.9964\n",
      "Epoch [7][20]\t Batch [750][5500]\t Training Loss 0.0169\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [800][5500]\t Training Loss 0.0168\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [850][5500]\t Training Loss 0.0175\t Accuracy 0.9962\n",
      "Epoch [7][20]\t Batch [900][5500]\t Training Loss 0.0174\t Accuracy 0.9962\n",
      "Epoch [7][20]\t Batch [950][5500]\t Training Loss 0.0171\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [1000][5500]\t Training Loss 0.0166\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [1050][5500]\t Training Loss 0.0167\t Accuracy 0.9964\n",
      "Epoch [7][20]\t Batch [1100][5500]\t Training Loss 0.0166\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [1150][5500]\t Training Loss 0.0164\t Accuracy 0.9964\n",
      "Epoch [7][20]\t Batch [1200][5500]\t Training Loss 0.0165\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [1250][5500]\t Training Loss 0.0164\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [1300][5500]\t Training Loss 0.0165\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [1350][5500]\t Training Loss 0.0163\t Accuracy 0.9963\n",
      "Epoch [7][20]\t Batch [1400][5500]\t Training Loss 0.0159\t Accuracy 0.9964\n",
      "Epoch [7][20]\t Batch [1450][5500]\t Training Loss 0.0157\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [1500][5500]\t Training Loss 0.0155\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [1550][5500]\t Training Loss 0.0154\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [1600][5500]\t Training Loss 0.0153\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [1650][5500]\t Training Loss 0.0150\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [1700][5500]\t Training Loss 0.0150\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [1750][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [1800][5500]\t Training Loss 0.0148\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [1850][5500]\t Training Loss 0.0149\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [1900][5500]\t Training Loss 0.0147\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [1950][5500]\t Training Loss 0.0146\t Accuracy 0.9969\n",
      "Epoch [7][20]\t Batch [2000][5500]\t Training Loss 0.0144\t Accuracy 0.9970\n",
      "Epoch [7][20]\t Batch [2050][5500]\t Training Loss 0.0144\t Accuracy 0.9969\n",
      "Epoch [7][20]\t Batch [2100][5500]\t Training Loss 0.0148\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2150][5500]\t Training Loss 0.0147\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2200][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2250][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2300][5500]\t Training Loss 0.0146\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2350][5500]\t Training Loss 0.0144\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2400][5500]\t Training Loss 0.0146\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2450][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2500][5500]\t Training Loss 0.0144\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2550][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [2600][5500]\t Training Loss 0.0146\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2650][5500]\t Training Loss 0.0146\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [2700][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [2750][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [2800][5500]\t Training Loss 0.0151\t Accuracy 0.9965\n",
      "Epoch [7][20]\t Batch [2850][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [2900][5500]\t Training Loss 0.0148\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [2950][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3000][5500]\t Training Loss 0.0148\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [3050][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [3100][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3150][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3200][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3250][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [3300][5500]\t Training Loss 0.0147\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3350][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3400][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3450][5500]\t Training Loss 0.0148\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3500][5500]\t Training Loss 0.0148\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [3550][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3600][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3650][5500]\t Training Loss 0.0151\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3700][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3750][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3800][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3850][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3900][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [3950][5500]\t Training Loss 0.0151\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4000][5500]\t Training Loss 0.0151\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4050][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4100][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4150][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4200][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4250][5500]\t Training Loss 0.0149\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4300][5500]\t Training Loss 0.0150\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4350][5500]\t Training Loss 0.0151\t Accuracy 0.9966\n",
      "Epoch [7][20]\t Batch [4400][5500]\t Training Loss 0.0151\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4450][5500]\t Training Loss 0.0150\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4500][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4550][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4600][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4650][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4700][5500]\t Training Loss 0.0148\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4750][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4800][5500]\t Training Loss 0.0149\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4850][5500]\t Training Loss 0.0148\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4900][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [4950][5500]\t Training Loss 0.0147\t Accuracy 0.9967\n",
      "Epoch [7][20]\t Batch [5000][5500]\t Training Loss 0.0148\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5050][5500]\t Training Loss 0.0147\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5100][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5150][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5200][5500]\t Training Loss 0.0146\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5250][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5300][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5350][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5400][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "Epoch [7][20]\t Batch [5450][5500]\t Training Loss 0.0145\t Accuracy 0.9968\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0145\t Average training accuracy 0.9968\n",
      "Epoch [7]\t Average validation loss 0.0786\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [8][20]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [8][20]\t Batch [50][5500]\t Training Loss 0.0147\t Accuracy 0.9961\n",
      "Epoch [8][20]\t Batch [100][5500]\t Training Loss 0.0120\t Accuracy 0.9970\n",
      "Epoch [8][20]\t Batch [150][5500]\t Training Loss 0.0154\t Accuracy 0.9960\n",
      "Epoch [8][20]\t Batch [200][5500]\t Training Loss 0.0135\t Accuracy 0.9965\n",
      "Epoch [8][20]\t Batch [250][5500]\t Training Loss 0.0131\t Accuracy 0.9968\n",
      "Epoch [8][20]\t Batch [300][5500]\t Training Loss 0.0130\t Accuracy 0.9967\n",
      "Epoch [8][20]\t Batch [350][5500]\t Training Loss 0.0123\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [400][5500]\t Training Loss 0.0115\t Accuracy 0.9975\n",
      "Epoch [8][20]\t Batch [450][5500]\t Training Loss 0.0122\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [500][5500]\t Training Loss 0.0120\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [550][5500]\t Training Loss 0.0118\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [600][5500]\t Training Loss 0.0118\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [650][5500]\t Training Loss 0.0116\t Accuracy 0.9975\n",
      "Epoch [8][20]\t Batch [700][5500]\t Training Loss 0.0117\t Accuracy 0.9976\n",
      "Epoch [8][20]\t Batch [750][5500]\t Training Loss 0.0119\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [800][5500]\t Training Loss 0.0118\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [850][5500]\t Training Loss 0.0118\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [900][5500]\t Training Loss 0.0118\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [950][5500]\t Training Loss 0.0116\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [1000][5500]\t Training Loss 0.0114\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [1050][5500]\t Training Loss 0.0117\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [1100][5500]\t Training Loss 0.0119\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [1150][5500]\t Training Loss 0.0116\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [1200][5500]\t Training Loss 0.0120\t Accuracy 0.9972\n",
      "Epoch [8][20]\t Batch [1250][5500]\t Training Loss 0.0118\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [1300][5500]\t Training Loss 0.0118\t Accuracy 0.9974\n",
      "Epoch [8][20]\t Batch [1350][5500]\t Training Loss 0.0117\t Accuracy 0.9973\n",
      "Epoch [8][20]\t Batch [1400][5500]\t Training Loss 0.0115\t Accuracy 0.9974\n",
      "Epoch [8][20]\t Batch [1450][5500]\t Training Loss 0.0113\t Accuracy 0.9975\n",
      "Epoch [8][20]\t Batch [1500][5500]\t Training Loss 0.0111\t Accuracy 0.9976\n",
      "Epoch [8][20]\t Batch [1550][5500]\t Training Loss 0.0110\t Accuracy 0.9976\n",
      "Epoch [8][20]\t Batch [1600][5500]\t Training Loss 0.0110\t Accuracy 0.9976\n",
      "Epoch [8][20]\t Batch [1650][5500]\t Training Loss 0.0108\t Accuracy 0.9976\n",
      "Epoch [8][20]\t Batch [1700][5500]\t Training Loss 0.0108\t Accuracy 0.9977\n",
      "Epoch [8][20]\t Batch [1750][5500]\t Training Loss 0.0108\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [1800][5500]\t Training Loss 0.0107\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [1850][5500]\t Training Loss 0.0107\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [1900][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [1950][5500]\t Training Loss 0.0106\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2000][5500]\t Training Loss 0.0105\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [2050][5500]\t Training Loss 0.0104\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2100][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2150][5500]\t Training Loss 0.0104\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2200][5500]\t Training Loss 0.0104\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2250][5500]\t Training Loss 0.0104\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2300][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2350][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2400][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2450][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2500][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2550][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2600][5500]\t Training Loss 0.0104\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [2650][5500]\t Training Loss 0.0105\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [2700][5500]\t Training Loss 0.0107\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [2750][5500]\t Training Loss 0.0106\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [2800][5500]\t Training Loss 0.0107\t Accuracy 0.9978\n",
      "Epoch [8][20]\t Batch [2850][5500]\t Training Loss 0.0106\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2900][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [2950][5500]\t Training Loss 0.0106\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [3000][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [3050][5500]\t Training Loss 0.0104\t Accuracy 0.9979\n",
      "Epoch [8][20]\t Batch [3100][5500]\t Training Loss 0.0104\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3150][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3200][5500]\t Training Loss 0.0104\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3250][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3300][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3350][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3400][5500]\t Training Loss 0.0102\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3450][5500]\t Training Loss 0.0104\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3500][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3550][5500]\t Training Loss 0.0105\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3600][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3650][5500]\t Training Loss 0.0105\t Accuracy 0.9980\n",
      "Epoch [8][20]\t Batch [3700][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3750][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3800][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3850][5500]\t Training Loss 0.0103\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3900][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [3950][5500]\t Training Loss 0.0105\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4000][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4050][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4100][5500]\t Training Loss 0.0103\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4150][5500]\t Training Loss 0.0103\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4200][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4250][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4300][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4350][5500]\t Training Loss 0.0105\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4400][5500]\t Training Loss 0.0105\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4450][5500]\t Training Loss 0.0105\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4500][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4550][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4600][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4650][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [4700][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4750][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4800][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4850][5500]\t Training Loss 0.0104\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4900][5500]\t Training Loss 0.0103\t Accuracy 0.9981\n",
      "Epoch [8][20]\t Batch [4950][5500]\t Training Loss 0.0103\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5000][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5050][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5100][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5150][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5200][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5250][5500]\t Training Loss 0.0104\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5300][5500]\t Training Loss 0.0103\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5350][5500]\t Training Loss 0.0103\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5400][5500]\t Training Loss 0.0103\t Accuracy 0.9982\n",
      "Epoch [8][20]\t Batch [5450][5500]\t Training Loss 0.0103\t Accuracy 0.9981\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0103\t Average training accuracy 0.9981\n",
      "Epoch [8]\t Average validation loss 0.0771\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [9][20]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [9][20]\t Batch [50][5500]\t Training Loss 0.0099\t Accuracy 0.9980\n",
      "Epoch [9][20]\t Batch [100][5500]\t Training Loss 0.0082\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [150][5500]\t Training Loss 0.0118\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [200][5500]\t Training Loss 0.0103\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [250][5500]\t Training Loss 0.0100\t Accuracy 0.9984\n",
      "Epoch [9][20]\t Batch [300][5500]\t Training Loss 0.0098\t Accuracy 0.9983\n",
      "Epoch [9][20]\t Batch [350][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [400][5500]\t Training Loss 0.0087\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [450][5500]\t Training Loss 0.0091\t Accuracy 0.9984\n",
      "Epoch [9][20]\t Batch [500][5500]\t Training Loss 0.0087\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [550][5500]\t Training Loss 0.0088\t Accuracy 0.9985\n",
      "Epoch [9][20]\t Batch [600][5500]\t Training Loss 0.0087\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [650][5500]\t Training Loss 0.0084\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [700][5500]\t Training Loss 0.0085\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [750][5500]\t Training Loss 0.0085\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [800][5500]\t Training Loss 0.0084\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [850][5500]\t Training Loss 0.0085\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [900][5500]\t Training Loss 0.0085\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [950][5500]\t Training Loss 0.0084\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1000][5500]\t Training Loss 0.0082\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1050][5500]\t Training Loss 0.0083\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1100][5500]\t Training Loss 0.0083\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [1150][5500]\t Training Loss 0.0081\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1200][5500]\t Training Loss 0.0082\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1250][5500]\t Training Loss 0.0081\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [1300][5500]\t Training Loss 0.0082\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [1350][5500]\t Training Loss 0.0082\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [1400][5500]\t Training Loss 0.0081\t Accuracy 0.9986\n",
      "Epoch [9][20]\t Batch [1450][5500]\t Training Loss 0.0080\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1500][5500]\t Training Loss 0.0079\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1550][5500]\t Training Loss 0.0078\t Accuracy 0.9987\n",
      "Epoch [9][20]\t Batch [1600][5500]\t Training Loss 0.0079\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [1650][5500]\t Training Loss 0.0078\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [1700][5500]\t Training Loss 0.0077\t Accuracy 0.9988\n",
      "Epoch [9][20]\t Batch [1750][5500]\t Training Loss 0.0077\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [1800][5500]\t Training Loss 0.0077\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [1850][5500]\t Training Loss 0.0077\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [1900][5500]\t Training Loss 0.0076\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [1950][5500]\t Training Loss 0.0076\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [2000][5500]\t Training Loss 0.0075\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [2050][5500]\t Training Loss 0.0075\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [2100][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2150][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2200][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2250][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2300][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2350][5500]\t Training Loss 0.0073\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2400][5500]\t Training Loss 0.0073\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2450][5500]\t Training Loss 0.0073\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2500][5500]\t Training Loss 0.0072\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2550][5500]\t Training Loss 0.0072\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2600][5500]\t Training Loss 0.0073\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2650][5500]\t Training Loss 0.0073\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2700][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2750][5500]\t Training Loss 0.0075\t Accuracy 0.9991\n",
      "Epoch [9][20]\t Batch [2800][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2850][5500]\t Training Loss 0.0075\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [2900][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [2950][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3000][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3050][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3100][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3150][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3200][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3250][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3300][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3350][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3400][5500]\t Training Loss 0.0073\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3450][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3500][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3550][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3600][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3650][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3700][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3750][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3800][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3850][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3900][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [3950][5500]\t Training Loss 0.0076\t Accuracy 0.9989\n",
      "Epoch [9][20]\t Batch [4000][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4050][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4100][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4150][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4200][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4250][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4300][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4350][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4400][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4450][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4500][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4550][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4600][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4650][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4700][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4750][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4800][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4850][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4900][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [4950][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5000][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5050][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5100][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5150][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5200][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5250][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5300][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5350][5500]\t Training Loss 0.0076\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5400][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [9][20]\t Batch [5450][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0075\t Average training accuracy 0.9990\n",
      "Epoch [9]\t Average validation loss 0.0707\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [10][20]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [50][5500]\t Training Loss 0.0072\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [100][5500]\t Training Loss 0.0063\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [150][5500]\t Training Loss 0.0093\t Accuracy 0.9987\n",
      "Epoch [10][20]\t Batch [200][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [10][20]\t Batch [250][5500]\t Training Loss 0.0086\t Accuracy 0.9984\n",
      "Epoch [10][20]\t Batch [300][5500]\t Training Loss 0.0082\t Accuracy 0.9987\n",
      "Epoch [10][20]\t Batch [350][5500]\t Training Loss 0.0076\t Accuracy 0.9989\n",
      "Epoch [10][20]\t Batch [400][5500]\t Training Loss 0.0071\t Accuracy 0.9990\n",
      "Epoch [10][20]\t Batch [450][5500]\t Training Loss 0.0073\t Accuracy 0.9989\n",
      "Epoch [10][20]\t Batch [500][5500]\t Training Loss 0.0069\t Accuracy 0.9990\n",
      "Epoch [10][20]\t Batch [550][5500]\t Training Loss 0.0068\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [600][5500]\t Training Loss 0.0069\t Accuracy 0.9990\n",
      "Epoch [10][20]\t Batch [650][5500]\t Training Loss 0.0068\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [700][5500]\t Training Loss 0.0068\t Accuracy 0.9990\n",
      "Epoch [10][20]\t Batch [750][5500]\t Training Loss 0.0068\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [800][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [850][5500]\t Training Loss 0.0068\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [900][5500]\t Training Loss 0.0069\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [950][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1000][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1050][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1100][5500]\t Training Loss 0.0066\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1150][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1200][5500]\t Training Loss 0.0064\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1250][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1300][5500]\t Training Loss 0.0063\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1350][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1400][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [10][20]\t Batch [1450][5500]\t Training Loss 0.0063\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1500][5500]\t Training Loss 0.0062\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1550][5500]\t Training Loss 0.0062\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1600][5500]\t Training Loss 0.0062\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [1650][5500]\t Training Loss 0.0061\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [1700][5500]\t Training Loss 0.0061\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1750][5500]\t Training Loss 0.0061\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1800][5500]\t Training Loss 0.0060\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1850][5500]\t Training Loss 0.0060\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [1900][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [1950][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2000][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2050][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2100][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2150][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2200][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2250][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2300][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2350][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2400][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2450][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2500][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2550][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2600][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2650][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [10][20]\t Batch [2700][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2750][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2800][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2850][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2900][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [2950][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3000][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3050][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3100][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3150][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3200][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3250][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3300][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3350][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3400][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3450][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3500][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3550][5500]\t Training Loss 0.0060\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [3600][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3650][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3700][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3750][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3800][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3850][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [3900][5500]\t Training Loss 0.0059\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [3950][5500]\t Training Loss 0.0059\t Accuracy 0.9992\n",
      "Epoch [10][20]\t Batch [4000][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4050][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4100][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4150][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4200][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4250][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4300][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4350][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4400][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4450][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4500][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4550][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4600][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4650][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4700][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4750][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4800][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4850][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4900][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [4950][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5000][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5050][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5100][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5150][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5200][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5250][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5300][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5350][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5400][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [10][20]\t Batch [5450][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0059\t Average training accuracy 0.9993\n",
      "Epoch [10]\t Average validation loss 0.0693\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [11][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [50][5500]\t Training Loss 0.0058\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [100][5500]\t Training Loss 0.0053\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [150][5500]\t Training Loss 0.0074\t Accuracy 0.9993\n",
      "Epoch [11][20]\t Batch [200][5500]\t Training Loss 0.0067\t Accuracy 0.9990\n",
      "Epoch [11][20]\t Batch [250][5500]\t Training Loss 0.0066\t Accuracy 0.9984\n",
      "Epoch [11][20]\t Batch [300][5500]\t Training Loss 0.0064\t Accuracy 0.9983\n",
      "Epoch [11][20]\t Batch [350][5500]\t Training Loss 0.0060\t Accuracy 0.9986\n",
      "Epoch [11][20]\t Batch [400][5500]\t Training Loss 0.0058\t Accuracy 0.9988\n",
      "Epoch [11][20]\t Batch [450][5500]\t Training Loss 0.0061\t Accuracy 0.9987\n",
      "Epoch [11][20]\t Batch [500][5500]\t Training Loss 0.0058\t Accuracy 0.9988\n",
      "Epoch [11][20]\t Batch [550][5500]\t Training Loss 0.0056\t Accuracy 0.9989\n",
      "Epoch [11][20]\t Batch [600][5500]\t Training Loss 0.0056\t Accuracy 0.9990\n",
      "Epoch [11][20]\t Batch [650][5500]\t Training Loss 0.0056\t Accuracy 0.9991\n",
      "Epoch [11][20]\t Batch [700][5500]\t Training Loss 0.0056\t Accuracy 0.9991\n",
      "Epoch [11][20]\t Batch [750][5500]\t Training Loss 0.0056\t Accuracy 0.9992\n",
      "Epoch [11][20]\t Batch [800][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [11][20]\t Batch [850][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [11][20]\t Batch [900][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [11][20]\t Batch [950][5500]\t Training Loss 0.0054\t Accuracy 0.9994\n",
      "Epoch [11][20]\t Batch [1000][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][20]\t Batch [1050][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][20]\t Batch [1100][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1150][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1200][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1250][5500]\t Training Loss 0.0050\t Accuracy 0.9994\n",
      "Epoch [11][20]\t Batch [1300][5500]\t Training Loss 0.0050\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1350][5500]\t Training Loss 0.0049\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1400][5500]\t Training Loss 0.0049\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1450][5500]\t Training Loss 0.0048\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1500][5500]\t Training Loss 0.0048\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1550][5500]\t Training Loss 0.0048\t Accuracy 0.9995\n",
      "Epoch [11][20]\t Batch [1600][5500]\t Training Loss 0.0047\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1650][5500]\t Training Loss 0.0047\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1700][5500]\t Training Loss 0.0046\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1750][5500]\t Training Loss 0.0046\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1800][5500]\t Training Loss 0.0046\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1850][5500]\t Training Loss 0.0046\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1900][5500]\t Training Loss 0.0045\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [1950][5500]\t Training Loss 0.0045\t Accuracy 0.9996\n",
      "Epoch [11][20]\t Batch [2000][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2050][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2100][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2150][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2200][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2250][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2300][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2350][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2400][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2450][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2500][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2550][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2600][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2650][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2700][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2750][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2800][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2850][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2900][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [2950][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3000][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3050][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3100][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3150][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3200][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3250][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3300][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3350][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3400][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3450][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3500][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3550][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3600][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3650][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3700][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3750][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3800][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3850][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3900][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [3950][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4000][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4050][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4100][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4150][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4200][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4250][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4300][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4350][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4400][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4450][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4500][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4550][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4600][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4650][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4700][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4750][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4800][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4850][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4900][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [4950][5500]\t Training Loss 0.0044\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5000][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5050][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5100][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5150][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5200][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5250][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5300][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5350][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5400][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "Epoch [11][20]\t Batch [5450][5500]\t Training Loss 0.0045\t Accuracy 0.9997\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0045\t Average training accuracy 0.9997\n",
      "Epoch [11]\t Average validation loss 0.0662\t Average validation accuracy 0.9854\n",
      "\n",
      "Epoch [12][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][5500]\t Training Loss 0.0044\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [100][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [150][5500]\t Training Loss 0.0050\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [200][5500]\t Training Loss 0.0047\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [250][5500]\t Training Loss 0.0045\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [300][5500]\t Training Loss 0.0045\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [350][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [400][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [450][5500]\t Training Loss 0.0042\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [500][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [550][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [600][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [650][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [700][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [750][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [800][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [850][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [900][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [950][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1000][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1050][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1100][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1150][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1200][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1250][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1300][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1350][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1400][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1450][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1500][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1550][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1600][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1650][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1700][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1750][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1800][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1850][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1900][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [1950][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2000][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2050][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2100][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2150][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2200][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2250][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2300][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2350][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2400][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2450][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2500][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2550][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2600][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2650][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [2700][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2750][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2800][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2850][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2900][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [2950][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3000][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3050][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3100][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3150][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3200][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3250][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3300][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3400][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3450][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3500][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3550][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3600][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3650][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3700][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3750][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3800][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3850][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [3900][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [3950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [4000][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4050][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4100][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4150][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4200][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4250][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4300][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4350][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4400][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4450][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4500][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4550][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4600][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4650][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [12][20]\t Batch [4700][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4750][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4800][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4850][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4900][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [4950][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5000][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5050][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5100][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5150][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5200][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5250][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5300][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5350][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5400][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [12][20]\t Batch [5450][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0034\t Average training accuracy 0.9999\n",
      "Epoch [12]\t Average validation loss 0.0660\t Average validation accuracy 0.9848\n",
      "\n",
      "Epoch [13][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [100][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [150][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [200][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [250][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [300][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [350][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [400][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [450][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [500][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [550][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [600][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [650][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [700][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [750][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [800][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [850][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [900][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [950][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1000][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1050][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1100][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1150][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1200][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1250][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1300][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1350][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1500][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1550][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1600][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1650][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1700][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1750][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1800][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1850][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1900][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [1950][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2000][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2050][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2150][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2200][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2250][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2300][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2350][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2450][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2500][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2550][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2650][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2700][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2750][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2800][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2850][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2900][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [2950][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3000][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3050][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3150][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3200][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3250][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3300][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3350][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [3450][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3500][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3550][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3600][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3650][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3700][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3750][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3800][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3850][5500]\t Training Loss 0.0026\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3900][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [3950][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4000][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4050][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4100][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4150][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4200][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4250][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4300][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4350][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4400][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4450][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4500][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4550][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4600][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4650][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4700][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4750][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4800][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4850][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4900][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [4950][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5000][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5050][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5100][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5150][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5200][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5250][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5300][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5350][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5400][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [13][20]\t Batch [5450][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0027\t Average training accuracy 0.9999\n",
      "Epoch [13]\t Average validation loss 0.0647\t Average validation accuracy 0.9848\n",
      "\n",
      "Epoch [14][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [100][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [150][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [250][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [300][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [350][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [450][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [550][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [650][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [750][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [800][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [850][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [900][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [950][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1000][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1050][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1100][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1150][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1200][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1250][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1300][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [1950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [2950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [3950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [4950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [5450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0023\t Average training accuracy 1.0000\n",
      "Epoch [14]\t Average validation loss 0.0638\t Average validation accuracy 0.9848\n",
      "\n",
      "Epoch [15][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [150][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [800][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [1950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [2950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [3950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [4950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [5450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0020\t Average training accuracy 1.0000\n",
      "Epoch [15]\t Average validation loss 0.0633\t Average validation accuracy 0.9856\n",
      "\n",
      "Epoch [16][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [450][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [550][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [600][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [650][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [700][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [750][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [800][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [850][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [900][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [950][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1000][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1050][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1150][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1350][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [1950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [2950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [3950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [4950][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5000][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5050][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5150][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5350][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5400][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [5450][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0018\t Average training accuracy 1.0000\n",
      "Epoch [16]\t Average validation loss 0.0630\t Average validation accuracy 0.9854\n",
      "\n",
      "Epoch [17][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [850][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [900][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1050][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1850][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [1950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [2950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [3950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4050][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4850][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4900][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [4950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5050][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [5450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0016\t Average training accuracy 1.0000\n",
      "Epoch [17]\t Average validation loss 0.0631\t Average validation accuracy 0.9854\n",
      "\n",
      "Epoch [18][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [150][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [1950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [2950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [3950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [4950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [5450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0015\t Average training accuracy 1.0000\n",
      "Epoch [18]\t Average validation loss 0.0630\t Average validation accuracy 0.9854\n",
      "\n",
      "Epoch [19][20]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [1950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [2950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [3950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [4950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [5450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0014\t Average training accuracy 1.0000\n",
      "Epoch [19]\t Average validation loss 0.0629\t Average validation accuracy 0.9852\n",
      "\n",
      "Epoch [0][20]\t Batch [0][1100]\t Training Loss 6.5605\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][1100]\t Training Loss 1.3940\t Accuracy 0.1647\n",
      "Epoch [0][20]\t Batch [100][1100]\t Training Loss 0.9810\t Accuracy 0.2053\n",
      "Epoch [0][20]\t Batch [150][1100]\t Training Loss 0.8346\t Accuracy 0.2264\n",
      "Epoch [0][20]\t Batch [200][1100]\t Training Loss 0.7543\t Accuracy 0.2507\n",
      "Epoch [0][20]\t Batch [250][1100]\t Training Loss 0.7018\t Accuracy 0.2700\n",
      "Epoch [0][20]\t Batch [300][1100]\t Training Loss 0.6654\t Accuracy 0.2847\n",
      "Epoch [0][20]\t Batch [350][1100]\t Training Loss 0.6362\t Accuracy 0.2999\n",
      "Epoch [0][20]\t Batch [400][1100]\t Training Loss 0.6132\t Accuracy 0.3121\n",
      "Epoch [0][20]\t Batch [450][1100]\t Training Loss 0.5937\t Accuracy 0.3253\n",
      "Epoch [0][20]\t Batch [500][1100]\t Training Loss 0.5765\t Accuracy 0.3392\n",
      "Epoch [0][20]\t Batch [550][1100]\t Training Loss 0.5616\t Accuracy 0.3515\n",
      "Epoch [0][20]\t Batch [600][1100]\t Training Loss 0.5483\t Accuracy 0.3649\n",
      "Epoch [0][20]\t Batch [650][1100]\t Training Loss 0.5377\t Accuracy 0.3733\n",
      "Epoch [0][20]\t Batch [700][1100]\t Training Loss 0.5269\t Accuracy 0.3848\n",
      "Epoch [0][20]\t Batch [750][1100]\t Training Loss 0.5180\t Accuracy 0.3932\n",
      "Epoch [0][20]\t Batch [800][1100]\t Training Loss 0.5091\t Accuracy 0.4035\n",
      "Epoch [0][20]\t Batch [850][1100]\t Training Loss 0.5017\t Accuracy 0.4112\n",
      "Epoch [0][20]\t Batch [900][1100]\t Training Loss 0.4943\t Accuracy 0.4196\n",
      "Epoch [0][20]\t Batch [950][1100]\t Training Loss 0.4876\t Accuracy 0.4277\n",
      "Epoch [0][20]\t Batch [1000][1100]\t Training Loss 0.4812\t Accuracy 0.4353\n",
      "Epoch [0][20]\t Batch [1050][1100]\t Training Loss 0.4750\t Accuracy 0.4431\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4691\t Average training accuracy 0.4501\n",
      "Epoch [0]\t Average validation loss 0.3333\t Average validation accuracy 0.6496\n",
      "\n",
      "Epoch [1][20]\t Batch [0][1100]\t Training Loss 0.3293\t Accuracy 0.6600\n",
      "Epoch [1][20]\t Batch [50][1100]\t Training Loss 0.3397\t Accuracy 0.6216\n",
      "Epoch [1][20]\t Batch [100][1100]\t Training Loss 0.3369\t Accuracy 0.6283\n",
      "Epoch [1][20]\t Batch [150][1100]\t Training Loss 0.3351\t Accuracy 0.6311\n",
      "Epoch [1][20]\t Batch [200][1100]\t Training Loss 0.3340\t Accuracy 0.6368\n",
      "Epoch [1][20]\t Batch [250][1100]\t Training Loss 0.3320\t Accuracy 0.6414\n",
      "Epoch [1][20]\t Batch [300][1100]\t Training Loss 0.3336\t Accuracy 0.6388\n",
      "Epoch [1][20]\t Batch [350][1100]\t Training Loss 0.3328\t Accuracy 0.6401\n",
      "Epoch [1][20]\t Batch [400][1100]\t Training Loss 0.3316\t Accuracy 0.6432\n",
      "Epoch [1][20]\t Batch [450][1100]\t Training Loss 0.3302\t Accuracy 0.6463\n",
      "Epoch [1][20]\t Batch [500][1100]\t Training Loss 0.3287\t Accuracy 0.6498\n",
      "Epoch [1][20]\t Batch [550][1100]\t Training Loss 0.3275\t Accuracy 0.6536\n",
      "Epoch [1][20]\t Batch [600][1100]\t Training Loss 0.3262\t Accuracy 0.6561\n",
      "Epoch [1][20]\t Batch [650][1100]\t Training Loss 0.3263\t Accuracy 0.6551\n",
      "Epoch [1][20]\t Batch [700][1100]\t Training Loss 0.3247\t Accuracy 0.6588\n",
      "Epoch [1][20]\t Batch [750][1100]\t Training Loss 0.3241\t Accuracy 0.6599\n",
      "Epoch [1][20]\t Batch [800][1100]\t Training Loss 0.3228\t Accuracy 0.6631\n",
      "Epoch [1][20]\t Batch [850][1100]\t Training Loss 0.3224\t Accuracy 0.6635\n",
      "Epoch [1][20]\t Batch [900][1100]\t Training Loss 0.3213\t Accuracy 0.6659\n",
      "Epoch [1][20]\t Batch [950][1100]\t Training Loss 0.3206\t Accuracy 0.6675\n",
      "Epoch [1][20]\t Batch [1000][1100]\t Training Loss 0.3198\t Accuracy 0.6694\n",
      "Epoch [1][20]\t Batch [1050][1100]\t Training Loss 0.3186\t Accuracy 0.6718\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3174\t Average training accuracy 0.6741\n",
      "Epoch [1]\t Average validation loss 0.2801\t Average validation accuracy 0.7664\n",
      "\n",
      "Epoch [2][20]\t Batch [0][1100]\t Training Loss 0.2848\t Accuracy 0.7600\n",
      "Epoch [2][20]\t Batch [50][1100]\t Training Loss 0.2899\t Accuracy 0.7275\n",
      "Epoch [2][20]\t Batch [100][1100]\t Training Loss 0.2889\t Accuracy 0.7303\n",
      "Epoch [2][20]\t Batch [150][1100]\t Training Loss 0.2880\t Accuracy 0.7335\n",
      "Epoch [2][20]\t Batch [200][1100]\t Training Loss 0.2881\t Accuracy 0.7347\n",
      "Epoch [2][20]\t Batch [250][1100]\t Training Loss 0.2873\t Accuracy 0.7371\n",
      "Epoch [2][20]\t Batch [300][1100]\t Training Loss 0.2902\t Accuracy 0.7310\n",
      "Epoch [2][20]\t Batch [350][1100]\t Training Loss 0.2904\t Accuracy 0.7298\n",
      "Epoch [2][20]\t Batch [400][1100]\t Training Loss 0.2898\t Accuracy 0.7307\n",
      "Epoch [2][20]\t Batch [450][1100]\t Training Loss 0.2891\t Accuracy 0.7319\n",
      "Epoch [2][20]\t Batch [500][1100]\t Training Loss 0.2884\t Accuracy 0.7332\n",
      "Epoch [2][20]\t Batch [550][1100]\t Training Loss 0.2878\t Accuracy 0.7345\n",
      "Epoch [2][20]\t Batch [600][1100]\t Training Loss 0.2873\t Accuracy 0.7353\n",
      "Epoch [2][20]\t Batch [650][1100]\t Training Loss 0.2882\t Accuracy 0.7333\n",
      "Epoch [2][20]\t Batch [700][1100]\t Training Loss 0.2872\t Accuracy 0.7354\n",
      "Epoch [2][20]\t Batch [750][1100]\t Training Loss 0.2870\t Accuracy 0.7359\n",
      "Epoch [2][20]\t Batch [800][1100]\t Training Loss 0.2865\t Accuracy 0.7373\n",
      "Epoch [2][20]\t Batch [850][1100]\t Training Loss 0.2866\t Accuracy 0.7364\n",
      "Epoch [2][20]\t Batch [900][1100]\t Training Loss 0.2860\t Accuracy 0.7377\n",
      "Epoch [2][20]\t Batch [950][1100]\t Training Loss 0.2858\t Accuracy 0.7383\n",
      "Epoch [2][20]\t Batch [1000][1100]\t Training Loss 0.2855\t Accuracy 0.7389\n",
      "Epoch [2][20]\t Batch [1050][1100]\t Training Loss 0.2848\t Accuracy 0.7404\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2841\t Average training accuracy 0.7414\n",
      "Epoch [2]\t Average validation loss 0.2566\t Average validation accuracy 0.8048\n",
      "\n",
      "Epoch [3][20]\t Batch [0][1100]\t Training Loss 0.2658\t Accuracy 0.8200\n",
      "Epoch [3][20]\t Batch [50][1100]\t Training Loss 0.2677\t Accuracy 0.7686\n",
      "Epoch [3][20]\t Batch [100][1100]\t Training Loss 0.2671\t Accuracy 0.7739\n",
      "Epoch [3][20]\t Batch [150][1100]\t Training Loss 0.2664\t Accuracy 0.7747\n",
      "Epoch [3][20]\t Batch [200][1100]\t Training Loss 0.2670\t Accuracy 0.7748\n",
      "Epoch [3][20]\t Batch [250][1100]\t Training Loss 0.2667\t Accuracy 0.7752\n",
      "Epoch [3][20]\t Batch [300][1100]\t Training Loss 0.2700\t Accuracy 0.7686\n",
      "Epoch [3][20]\t Batch [350][1100]\t Training Loss 0.2704\t Accuracy 0.7667\n",
      "Epoch [3][20]\t Batch [400][1100]\t Training Loss 0.2700\t Accuracy 0.7680\n",
      "Epoch [3][20]\t Batch [450][1100]\t Training Loss 0.2694\t Accuracy 0.7686\n",
      "Epoch [3][20]\t Batch [500][1100]\t Training Loss 0.2690\t Accuracy 0.7695\n",
      "Epoch [3][20]\t Batch [550][1100]\t Training Loss 0.2687\t Accuracy 0.7701\n",
      "Epoch [3][20]\t Batch [600][1100]\t Training Loss 0.2685\t Accuracy 0.7703\n",
      "Epoch [3][20]\t Batch [650][1100]\t Training Loss 0.2695\t Accuracy 0.7678\n",
      "Epoch [3][20]\t Batch [700][1100]\t Training Loss 0.2687\t Accuracy 0.7693\n",
      "Epoch [3][20]\t Batch [750][1100]\t Training Loss 0.2688\t Accuracy 0.7698\n",
      "Epoch [3][20]\t Batch [800][1100]\t Training Loss 0.2684\t Accuracy 0.7706\n",
      "Epoch [3][20]\t Batch [850][1100]\t Training Loss 0.2687\t Accuracy 0.7699\n",
      "Epoch [3][20]\t Batch [900][1100]\t Training Loss 0.2682\t Accuracy 0.7705\n",
      "Epoch [3][20]\t Batch [950][1100]\t Training Loss 0.2683\t Accuracy 0.7709\n",
      "Epoch [3][20]\t Batch [1000][1100]\t Training Loss 0.2682\t Accuracy 0.7709\n",
      "Epoch [3][20]\t Batch [1050][1100]\t Training Loss 0.2676\t Accuracy 0.7719\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2671\t Average training accuracy 0.7729\n",
      "Epoch [3]\t Average validation loss 0.2429\t Average validation accuracy 0.8264\n",
      "\n",
      "Epoch [4][20]\t Batch [0][1100]\t Training Loss 0.2545\t Accuracy 0.8200\n",
      "Epoch [4][20]\t Batch [50][1100]\t Training Loss 0.2546\t Accuracy 0.7922\n",
      "Epoch [4][20]\t Batch [100][1100]\t Training Loss 0.2540\t Accuracy 0.7972\n",
      "Epoch [4][20]\t Batch [150][1100]\t Training Loss 0.2536\t Accuracy 0.7968\n",
      "Epoch [4][20]\t Batch [200][1100]\t Training Loss 0.2544\t Accuracy 0.7968\n",
      "Epoch [4][20]\t Batch [250][1100]\t Training Loss 0.2543\t Accuracy 0.7955\n",
      "Epoch [4][20]\t Batch [300][1100]\t Training Loss 0.2578\t Accuracy 0.7882\n",
      "Epoch [4][20]\t Batch [350][1100]\t Training Loss 0.2583\t Accuracy 0.7866\n",
      "Epoch [4][20]\t Batch [400][1100]\t Training Loss 0.2580\t Accuracy 0.7875\n",
      "Epoch [4][20]\t Batch [450][1100]\t Training Loss 0.2575\t Accuracy 0.7887\n",
      "Epoch [4][20]\t Batch [500][1100]\t Training Loss 0.2572\t Accuracy 0.7887\n",
      "Epoch [4][20]\t Batch [550][1100]\t Training Loss 0.2571\t Accuracy 0.7888\n",
      "Epoch [4][20]\t Batch [600][1100]\t Training Loss 0.2569\t Accuracy 0.7889\n",
      "Epoch [4][20]\t Batch [650][1100]\t Training Loss 0.2580\t Accuracy 0.7862\n",
      "Epoch [4][20]\t Batch [700][1100]\t Training Loss 0.2574\t Accuracy 0.7876\n",
      "Epoch [4][20]\t Batch [750][1100]\t Training Loss 0.2575\t Accuracy 0.7879\n",
      "Epoch [4][20]\t Batch [800][1100]\t Training Loss 0.2572\t Accuracy 0.7884\n",
      "Epoch [4][20]\t Batch [850][1100]\t Training Loss 0.2575\t Accuracy 0.7876\n",
      "Epoch [4][20]\t Batch [900][1100]\t Training Loss 0.2572\t Accuracy 0.7879\n",
      "Epoch [4][20]\t Batch [950][1100]\t Training Loss 0.2573\t Accuracy 0.7879\n",
      "Epoch [4][20]\t Batch [1000][1100]\t Training Loss 0.2574\t Accuracy 0.7878\n",
      "Epoch [4][20]\t Batch [1050][1100]\t Training Loss 0.2569\t Accuracy 0.7886\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2564\t Average training accuracy 0.7895\n",
      "Epoch [4]\t Average validation loss 0.2338\t Average validation accuracy 0.8376\n",
      "\n",
      "Epoch [5][20]\t Batch [0][1100]\t Training Loss 0.2469\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][1100]\t Training Loss 0.2458\t Accuracy 0.8106\n",
      "Epoch [5][20]\t Batch [100][1100]\t Training Loss 0.2452\t Accuracy 0.8139\n",
      "Epoch [5][20]\t Batch [150][1100]\t Training Loss 0.2448\t Accuracy 0.8111\n",
      "Epoch [5][20]\t Batch [200][1100]\t Training Loss 0.2459\t Accuracy 0.8107\n",
      "Epoch [5][20]\t Batch [250][1100]\t Training Loss 0.2459\t Accuracy 0.8093\n",
      "Epoch [5][20]\t Batch [300][1100]\t Training Loss 0.2495\t Accuracy 0.8015\n",
      "Epoch [5][20]\t Batch [350][1100]\t Training Loss 0.2501\t Accuracy 0.8004\n",
      "Epoch [5][20]\t Batch [400][1100]\t Training Loss 0.2497\t Accuracy 0.8009\n",
      "Epoch [5][20]\t Batch [450][1100]\t Training Loss 0.2494\t Accuracy 0.8019\n",
      "Epoch [5][20]\t Batch [500][1100]\t Training Loss 0.2491\t Accuracy 0.8018\n",
      "Epoch [5][20]\t Batch [550][1100]\t Training Loss 0.2490\t Accuracy 0.8018\n",
      "Epoch [5][20]\t Batch [600][1100]\t Training Loss 0.2489\t Accuracy 0.8019\n",
      "Epoch [5][20]\t Batch [650][1100]\t Training Loss 0.2501\t Accuracy 0.7990\n",
      "Epoch [5][20]\t Batch [700][1100]\t Training Loss 0.2495\t Accuracy 0.8003\n",
      "Epoch [5][20]\t Batch [750][1100]\t Training Loss 0.2496\t Accuracy 0.8007\n",
      "Epoch [5][20]\t Batch [800][1100]\t Training Loss 0.2494\t Accuracy 0.8011\n",
      "Epoch [5][20]\t Batch [850][1100]\t Training Loss 0.2498\t Accuracy 0.8004\n",
      "Epoch [5][20]\t Batch [900][1100]\t Training Loss 0.2495\t Accuracy 0.8004\n",
      "Epoch [5][20]\t Batch [950][1100]\t Training Loss 0.2496\t Accuracy 0.8003\n",
      "Epoch [5][20]\t Batch [1000][1100]\t Training Loss 0.2497\t Accuracy 0.8002\n",
      "Epoch [5][20]\t Batch [1050][1100]\t Training Loss 0.2493\t Accuracy 0.8010\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2489\t Average training accuracy 0.8018\n",
      "Epoch [5]\t Average validation loss 0.2271\t Average validation accuracy 0.8446\n",
      "\n",
      "Epoch [6][20]\t Batch [0][1100]\t Training Loss 0.2413\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][1100]\t Training Loss 0.2394\t Accuracy 0.8176\n",
      "Epoch [6][20]\t Batch [100][1100]\t Training Loss 0.2386\t Accuracy 0.8228\n",
      "Epoch [6][20]\t Batch [150][1100]\t Training Loss 0.2384\t Accuracy 0.8203\n",
      "Epoch [6][20]\t Batch [200][1100]\t Training Loss 0.2395\t Accuracy 0.8202\n",
      "Epoch [6][20]\t Batch [250][1100]\t Training Loss 0.2397\t Accuracy 0.8182\n",
      "Epoch [6][20]\t Batch [300][1100]\t Training Loss 0.2433\t Accuracy 0.8105\n",
      "Epoch [6][20]\t Batch [350][1100]\t Training Loss 0.2439\t Accuracy 0.8096\n",
      "Epoch [6][20]\t Batch [400][1100]\t Training Loss 0.2436\t Accuracy 0.8102\n",
      "Epoch [6][20]\t Batch [450][1100]\t Training Loss 0.2433\t Accuracy 0.8107\n",
      "Epoch [6][20]\t Batch [500][1100]\t Training Loss 0.2431\t Accuracy 0.8103\n",
      "Epoch [6][20]\t Batch [550][1100]\t Training Loss 0.2430\t Accuracy 0.8104\n",
      "Epoch [6][20]\t Batch [600][1100]\t Training Loss 0.2430\t Accuracy 0.8104\n",
      "Epoch [6][20]\t Batch [650][1100]\t Training Loss 0.2442\t Accuracy 0.8076\n",
      "Epoch [6][20]\t Batch [700][1100]\t Training Loss 0.2436\t Accuracy 0.8088\n",
      "Epoch [6][20]\t Batch [750][1100]\t Training Loss 0.2437\t Accuracy 0.8089\n",
      "Epoch [6][20]\t Batch [800][1100]\t Training Loss 0.2436\t Accuracy 0.8093\n",
      "Epoch [6][20]\t Batch [850][1100]\t Training Loss 0.2440\t Accuracy 0.8087\n",
      "Epoch [6][20]\t Batch [900][1100]\t Training Loss 0.2437\t Accuracy 0.8089\n",
      "Epoch [6][20]\t Batch [950][1100]\t Training Loss 0.2439\t Accuracy 0.8087\n",
      "Epoch [6][20]\t Batch [1000][1100]\t Training Loss 0.2440\t Accuracy 0.8085\n",
      "Epoch [6][20]\t Batch [1050][1100]\t Training Loss 0.2436\t Accuracy 0.8092\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2432\t Average training accuracy 0.8100\n",
      "Epoch [6]\t Average validation loss 0.2220\t Average validation accuracy 0.8498\n",
      "\n",
      "Epoch [7][20]\t Batch [0][1100]\t Training Loss 0.2371\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][1100]\t Training Loss 0.2344\t Accuracy 0.8267\n",
      "Epoch [7][20]\t Batch [100][1100]\t Training Loss 0.2335\t Accuracy 0.8329\n",
      "Epoch [7][20]\t Batch [150][1100]\t Training Loss 0.2333\t Accuracy 0.8297\n",
      "Epoch [7][20]\t Batch [200][1100]\t Training Loss 0.2346\t Accuracy 0.8288\n",
      "Epoch [7][20]\t Batch [250][1100]\t Training Loss 0.2348\t Accuracy 0.8269\n",
      "Epoch [7][20]\t Batch [300][1100]\t Training Loss 0.2385\t Accuracy 0.8189\n",
      "Epoch [7][20]\t Batch [350][1100]\t Training Loss 0.2391\t Accuracy 0.8170\n",
      "Epoch [7][20]\t Batch [400][1100]\t Training Loss 0.2388\t Accuracy 0.8177\n",
      "Epoch [7][20]\t Batch [450][1100]\t Training Loss 0.2385\t Accuracy 0.8180\n",
      "Epoch [7][20]\t Batch [500][1100]\t Training Loss 0.2384\t Accuracy 0.8178\n",
      "Epoch [7][20]\t Batch [550][1100]\t Training Loss 0.2383\t Accuracy 0.8176\n",
      "Epoch [7][20]\t Batch [600][1100]\t Training Loss 0.2383\t Accuracy 0.8176\n",
      "Epoch [7][20]\t Batch [650][1100]\t Training Loss 0.2395\t Accuracy 0.8148\n",
      "Epoch [7][20]\t Batch [700][1100]\t Training Loss 0.2389\t Accuracy 0.8159\n",
      "Epoch [7][20]\t Batch [750][1100]\t Training Loss 0.2391\t Accuracy 0.8162\n",
      "Epoch [7][20]\t Batch [800][1100]\t Training Loss 0.2390\t Accuracy 0.8163\n",
      "Epoch [7][20]\t Batch [850][1100]\t Training Loss 0.2394\t Accuracy 0.8157\n",
      "Epoch [7][20]\t Batch [900][1100]\t Training Loss 0.2391\t Accuracy 0.8160\n",
      "Epoch [7][20]\t Batch [950][1100]\t Training Loss 0.2393\t Accuracy 0.8159\n",
      "Epoch [7][20]\t Batch [1000][1100]\t Training Loss 0.2395\t Accuracy 0.8155\n",
      "Epoch [7][20]\t Batch [1050][1100]\t Training Loss 0.2391\t Accuracy 0.8159\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2388\t Average training accuracy 0.8166\n",
      "Epoch [7]\t Average validation loss 0.2179\t Average validation accuracy 0.8548\n",
      "\n",
      "Epoch [8][20]\t Batch [0][1100]\t Training Loss 0.2337\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][1100]\t Training Loss 0.2303\t Accuracy 0.8310\n",
      "Epoch [8][20]\t Batch [100][1100]\t Training Loss 0.2294\t Accuracy 0.8378\n",
      "Epoch [8][20]\t Batch [150][1100]\t Training Loss 0.2293\t Accuracy 0.8342\n",
      "Epoch [8][20]\t Batch [200][1100]\t Training Loss 0.2306\t Accuracy 0.8337\n",
      "Epoch [8][20]\t Batch [250][1100]\t Training Loss 0.2309\t Accuracy 0.8316\n",
      "Epoch [8][20]\t Batch [300][1100]\t Training Loss 0.2346\t Accuracy 0.8235\n",
      "Epoch [8][20]\t Batch [350][1100]\t Training Loss 0.2352\t Accuracy 0.8214\n",
      "Epoch [8][20]\t Batch [400][1100]\t Training Loss 0.2349\t Accuracy 0.8224\n",
      "Epoch [8][20]\t Batch [450][1100]\t Training Loss 0.2346\t Accuracy 0.8227\n",
      "Epoch [8][20]\t Batch [500][1100]\t Training Loss 0.2345\t Accuracy 0.8224\n",
      "Epoch [8][20]\t Batch [550][1100]\t Training Loss 0.2345\t Accuracy 0.8222\n",
      "Epoch [8][20]\t Batch [600][1100]\t Training Loss 0.2344\t Accuracy 0.8222\n",
      "Epoch [8][20]\t Batch [650][1100]\t Training Loss 0.2357\t Accuracy 0.8194\n",
      "Epoch [8][20]\t Batch [700][1100]\t Training Loss 0.2351\t Accuracy 0.8205\n",
      "Epoch [8][20]\t Batch [750][1100]\t Training Loss 0.2353\t Accuracy 0.8207\n",
      "Epoch [8][20]\t Batch [800][1100]\t Training Loss 0.2352\t Accuracy 0.8208\n",
      "Epoch [8][20]\t Batch [850][1100]\t Training Loss 0.2356\t Accuracy 0.8202\n",
      "Epoch [8][20]\t Batch [900][1100]\t Training Loss 0.2354\t Accuracy 0.8205\n",
      "Epoch [8][20]\t Batch [950][1100]\t Training Loss 0.2356\t Accuracy 0.8204\n",
      "Epoch [8][20]\t Batch [1000][1100]\t Training Loss 0.2358\t Accuracy 0.8200\n",
      "Epoch [8][20]\t Batch [1050][1100]\t Training Loss 0.2354\t Accuracy 0.8203\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2351\t Average training accuracy 0.8209\n",
      "Epoch [8]\t Average validation loss 0.2145\t Average validation accuracy 0.8588\n",
      "\n",
      "Epoch [9][20]\t Batch [0][1100]\t Training Loss 0.2309\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][1100]\t Training Loss 0.2270\t Accuracy 0.8345\n",
      "Epoch [9][20]\t Batch [100][1100]\t Training Loss 0.2260\t Accuracy 0.8410\n",
      "Epoch [9][20]\t Batch [150][1100]\t Training Loss 0.2259\t Accuracy 0.8375\n",
      "Epoch [9][20]\t Batch [200][1100]\t Training Loss 0.2273\t Accuracy 0.8371\n",
      "Epoch [9][20]\t Batch [250][1100]\t Training Loss 0.2276\t Accuracy 0.8359\n",
      "Epoch [9][20]\t Batch [300][1100]\t Training Loss 0.2313\t Accuracy 0.8280\n",
      "Epoch [9][20]\t Batch [350][1100]\t Training Loss 0.2319\t Accuracy 0.8259\n",
      "Epoch [9][20]\t Batch [400][1100]\t Training Loss 0.2316\t Accuracy 0.8268\n",
      "Epoch [9][20]\t Batch [450][1100]\t Training Loss 0.2314\t Accuracy 0.8269\n",
      "Epoch [9][20]\t Batch [500][1100]\t Training Loss 0.2313\t Accuracy 0.8267\n",
      "Epoch [9][20]\t Batch [550][1100]\t Training Loss 0.2313\t Accuracy 0.8268\n",
      "Epoch [9][20]\t Batch [600][1100]\t Training Loss 0.2312\t Accuracy 0.8267\n",
      "Epoch [9][20]\t Batch [650][1100]\t Training Loss 0.2325\t Accuracy 0.8240\n",
      "Epoch [9][20]\t Batch [700][1100]\t Training Loss 0.2319\t Accuracy 0.8250\n",
      "Epoch [9][20]\t Batch [750][1100]\t Training Loss 0.2322\t Accuracy 0.8251\n",
      "Epoch [9][20]\t Batch [800][1100]\t Training Loss 0.2321\t Accuracy 0.8251\n",
      "Epoch [9][20]\t Batch [850][1100]\t Training Loss 0.2325\t Accuracy 0.8247\n",
      "Epoch [9][20]\t Batch [900][1100]\t Training Loss 0.2322\t Accuracy 0.8250\n",
      "Epoch [9][20]\t Batch [950][1100]\t Training Loss 0.2325\t Accuracy 0.8248\n",
      "Epoch [9][20]\t Batch [1000][1100]\t Training Loss 0.2327\t Accuracy 0.8243\n",
      "Epoch [9][20]\t Batch [1050][1100]\t Training Loss 0.2323\t Accuracy 0.8246\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2320\t Average training accuracy 0.8252\n",
      "Epoch [9]\t Average validation loss 0.2116\t Average validation accuracy 0.8618\n",
      "\n",
      "Epoch [10][20]\t Batch [0][1100]\t Training Loss 0.2285\t Accuracy 0.8600\n",
      "Epoch [10][20]\t Batch [50][1100]\t Training Loss 0.2242\t Accuracy 0.8345\n",
      "Epoch [10][20]\t Batch [100][1100]\t Training Loss 0.2231\t Accuracy 0.8420\n",
      "Epoch [10][20]\t Batch [150][1100]\t Training Loss 0.2230\t Accuracy 0.8396\n",
      "Epoch [10][20]\t Batch [200][1100]\t Training Loss 0.2244\t Accuracy 0.8394\n",
      "Epoch [10][20]\t Batch [250][1100]\t Training Loss 0.2248\t Accuracy 0.8383\n",
      "Epoch [10][20]\t Batch [300][1100]\t Training Loss 0.2285\t Accuracy 0.8303\n",
      "Epoch [10][20]\t Batch [350][1100]\t Training Loss 0.2292\t Accuracy 0.8285\n",
      "Epoch [10][20]\t Batch [400][1100]\t Training Loss 0.2288\t Accuracy 0.8295\n",
      "Epoch [10][20]\t Batch [450][1100]\t Training Loss 0.2286\t Accuracy 0.8297\n",
      "Epoch [10][20]\t Batch [500][1100]\t Training Loss 0.2285\t Accuracy 0.8297\n",
      "Epoch [10][20]\t Batch [550][1100]\t Training Loss 0.2285\t Accuracy 0.8297\n",
      "Epoch [10][20]\t Batch [600][1100]\t Training Loss 0.2285\t Accuracy 0.8299\n",
      "Epoch [10][20]\t Batch [650][1100]\t Training Loss 0.2297\t Accuracy 0.8272\n",
      "Epoch [10][20]\t Batch [700][1100]\t Training Loss 0.2292\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [750][1100]\t Training Loss 0.2295\t Accuracy 0.8283\n",
      "Epoch [10][20]\t Batch [800][1100]\t Training Loss 0.2294\t Accuracy 0.8285\n",
      "Epoch [10][20]\t Batch [850][1100]\t Training Loss 0.2298\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [900][1100]\t Training Loss 0.2295\t Accuracy 0.8285\n",
      "Epoch [10][20]\t Batch [950][1100]\t Training Loss 0.2298\t Accuracy 0.8283\n",
      "Epoch [10][20]\t Batch [1000][1100]\t Training Loss 0.2300\t Accuracy 0.8278\n",
      "Epoch [10][20]\t Batch [1050][1100]\t Training Loss 0.2296\t Accuracy 0.8280\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2293\t Average training accuracy 0.8285\n",
      "Epoch [10]\t Average validation loss 0.2092\t Average validation accuracy 0.8650\n",
      "\n",
      "Epoch [11][20]\t Batch [0][1100]\t Training Loss 0.2265\t Accuracy 0.8600\n",
      "Epoch [11][20]\t Batch [50][1100]\t Training Loss 0.2217\t Accuracy 0.8380\n",
      "Epoch [11][20]\t Batch [100][1100]\t Training Loss 0.2205\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [150][1100]\t Training Loss 0.2205\t Accuracy 0.8429\n",
      "Epoch [11][20]\t Batch [200][1100]\t Training Loss 0.2220\t Accuracy 0.8425\n",
      "Epoch [11][20]\t Batch [250][1100]\t Training Loss 0.2223\t Accuracy 0.8410\n",
      "Epoch [11][20]\t Batch [300][1100]\t Training Loss 0.2261\t Accuracy 0.8329\n",
      "Epoch [11][20]\t Batch [350][1100]\t Training Loss 0.2267\t Accuracy 0.8311\n",
      "Epoch [11][20]\t Batch [400][1100]\t Training Loss 0.2264\t Accuracy 0.8320\n",
      "Epoch [11][20]\t Batch [450][1100]\t Training Loss 0.2262\t Accuracy 0.8322\n",
      "Epoch [11][20]\t Batch [500][1100]\t Training Loss 0.2261\t Accuracy 0.8321\n",
      "Epoch [11][20]\t Batch [550][1100]\t Training Loss 0.2261\t Accuracy 0.8322\n",
      "Epoch [11][20]\t Batch [600][1100]\t Training Loss 0.2261\t Accuracy 0.8324\n",
      "Epoch [11][20]\t Batch [650][1100]\t Training Loss 0.2274\t Accuracy 0.8298\n",
      "Epoch [11][20]\t Batch [700][1100]\t Training Loss 0.2268\t Accuracy 0.8308\n",
      "Epoch [11][20]\t Batch [750][1100]\t Training Loss 0.2271\t Accuracy 0.8309\n",
      "Epoch [11][20]\t Batch [800][1100]\t Training Loss 0.2270\t Accuracy 0.8310\n",
      "Epoch [11][20]\t Batch [850][1100]\t Training Loss 0.2274\t Accuracy 0.8306\n",
      "Epoch [11][20]\t Batch [900][1100]\t Training Loss 0.2272\t Accuracy 0.8309\n",
      "Epoch [11][20]\t Batch [950][1100]\t Training Loss 0.2274\t Accuracy 0.8307\n",
      "Epoch [11][20]\t Batch [1000][1100]\t Training Loss 0.2277\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [1050][1100]\t Training Loss 0.2273\t Accuracy 0.8301\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2270\t Average training accuracy 0.8307\n",
      "Epoch [11]\t Average validation loss 0.2070\t Average validation accuracy 0.8682\n",
      "\n",
      "Epoch [12][20]\t Batch [0][1100]\t Training Loss 0.2247\t Accuracy 0.8600\n",
      "Epoch [12][20]\t Batch [50][1100]\t Training Loss 0.2195\t Accuracy 0.8388\n",
      "Epoch [12][20]\t Batch [100][1100]\t Training Loss 0.2183\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [150][1100]\t Training Loss 0.2183\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [200][1100]\t Training Loss 0.2198\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [250][1100]\t Training Loss 0.2202\t Accuracy 0.8429\n",
      "Epoch [12][20]\t Batch [300][1100]\t Training Loss 0.2240\t Accuracy 0.8349\n",
      "Epoch [12][20]\t Batch [350][1100]\t Training Loss 0.2246\t Accuracy 0.8331\n",
      "Epoch [12][20]\t Batch [400][1100]\t Training Loss 0.2243\t Accuracy 0.8340\n",
      "Epoch [12][20]\t Batch [450][1100]\t Training Loss 0.2240\t Accuracy 0.8344\n",
      "Epoch [12][20]\t Batch [500][1100]\t Training Loss 0.2240\t Accuracy 0.8343\n",
      "Epoch [12][20]\t Batch [550][1100]\t Training Loss 0.2240\t Accuracy 0.8343\n",
      "Epoch [12][20]\t Batch [600][1100]\t Training Loss 0.2240\t Accuracy 0.8345\n",
      "Epoch [12][20]\t Batch [650][1100]\t Training Loss 0.2252\t Accuracy 0.8320\n",
      "Epoch [12][20]\t Batch [700][1100]\t Training Loss 0.2247\t Accuracy 0.8329\n",
      "Epoch [12][20]\t Batch [750][1100]\t Training Loss 0.2250\t Accuracy 0.8330\n",
      "Epoch [12][20]\t Batch [800][1100]\t Training Loss 0.2249\t Accuracy 0.8330\n",
      "Epoch [12][20]\t Batch [850][1100]\t Training Loss 0.2253\t Accuracy 0.8326\n",
      "Epoch [12][20]\t Batch [900][1100]\t Training Loss 0.2251\t Accuracy 0.8330\n",
      "Epoch [12][20]\t Batch [950][1100]\t Training Loss 0.2254\t Accuracy 0.8327\n",
      "Epoch [12][20]\t Batch [1000][1100]\t Training Loss 0.2256\t Accuracy 0.8321\n",
      "Epoch [12][20]\t Batch [1050][1100]\t Training Loss 0.2252\t Accuracy 0.8322\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2250\t Average training accuracy 0.8329\n",
      "Epoch [12]\t Average validation loss 0.2051\t Average validation accuracy 0.8702\n",
      "\n",
      "Epoch [13][20]\t Batch [0][1100]\t Training Loss 0.2231\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][1100]\t Training Loss 0.2176\t Accuracy 0.8420\n",
      "Epoch [13][20]\t Batch [100][1100]\t Training Loss 0.2163\t Accuracy 0.8497\n",
      "Epoch [13][20]\t Batch [150][1100]\t Training Loss 0.2164\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [200][1100]\t Training Loss 0.2178\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [250][1100]\t Training Loss 0.2183\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [300][1100]\t Training Loss 0.2221\t Accuracy 0.8381\n",
      "Epoch [13][20]\t Batch [350][1100]\t Training Loss 0.2227\t Accuracy 0.8360\n",
      "Epoch [13][20]\t Batch [400][1100]\t Training Loss 0.2223\t Accuracy 0.8368\n",
      "Epoch [13][20]\t Batch [450][1100]\t Training Loss 0.2221\t Accuracy 0.8371\n",
      "Epoch [13][20]\t Batch [500][1100]\t Training Loss 0.2221\t Accuracy 0.8371\n",
      "Epoch [13][20]\t Batch [550][1100]\t Training Loss 0.2221\t Accuracy 0.8369\n",
      "Epoch [13][20]\t Batch [600][1100]\t Training Loss 0.2221\t Accuracy 0.8372\n",
      "Epoch [13][20]\t Batch [650][1100]\t Training Loss 0.2233\t Accuracy 0.8343\n",
      "Epoch [13][20]\t Batch [700][1100]\t Training Loss 0.2229\t Accuracy 0.8352\n",
      "Epoch [13][20]\t Batch [750][1100]\t Training Loss 0.2231\t Accuracy 0.8354\n",
      "Epoch [13][20]\t Batch [800][1100]\t Training Loss 0.2231\t Accuracy 0.8353\n",
      "Epoch [13][20]\t Batch [850][1100]\t Training Loss 0.2235\t Accuracy 0.8349\n",
      "Epoch [13][20]\t Batch [900][1100]\t Training Loss 0.2233\t Accuracy 0.8352\n",
      "Epoch [13][20]\t Batch [950][1100]\t Training Loss 0.2235\t Accuracy 0.8350\n",
      "Epoch [13][20]\t Batch [1000][1100]\t Training Loss 0.2237\t Accuracy 0.8343\n",
      "Epoch [13][20]\t Batch [1050][1100]\t Training Loss 0.2234\t Accuracy 0.8344\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2231\t Average training accuracy 0.8351\n",
      "Epoch [13]\t Average validation loss 0.2033\t Average validation accuracy 0.8726\n",
      "\n",
      "Epoch [14][20]\t Batch [0][1100]\t Training Loss 0.2216\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][1100]\t Training Loss 0.2159\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [100][1100]\t Training Loss 0.2145\t Accuracy 0.8523\n",
      "Epoch [14][20]\t Batch [150][1100]\t Training Loss 0.2146\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [200][1100]\t Training Loss 0.2161\t Accuracy 0.8484\n",
      "Epoch [14][20]\t Batch [250][1100]\t Training Loss 0.2165\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [300][1100]\t Training Loss 0.2203\t Accuracy 0.8395\n",
      "Epoch [14][20]\t Batch [350][1100]\t Training Loss 0.2209\t Accuracy 0.8376\n",
      "Epoch [14][20]\t Batch [400][1100]\t Training Loss 0.2206\t Accuracy 0.8388\n",
      "Epoch [14][20]\t Batch [450][1100]\t Training Loss 0.2204\t Accuracy 0.8393\n",
      "Epoch [14][20]\t Batch [500][1100]\t Training Loss 0.2204\t Accuracy 0.8391\n",
      "Epoch [14][20]\t Batch [550][1100]\t Training Loss 0.2204\t Accuracy 0.8387\n",
      "Epoch [14][20]\t Batch [600][1100]\t Training Loss 0.2204\t Accuracy 0.8387\n",
      "Epoch [14][20]\t Batch [650][1100]\t Training Loss 0.2216\t Accuracy 0.8359\n",
      "Epoch [14][20]\t Batch [700][1100]\t Training Loss 0.2211\t Accuracy 0.8367\n",
      "Epoch [14][20]\t Batch [750][1100]\t Training Loss 0.2214\t Accuracy 0.8369\n",
      "Epoch [14][20]\t Batch [800][1100]\t Training Loss 0.2214\t Accuracy 0.8368\n",
      "Epoch [14][20]\t Batch [850][1100]\t Training Loss 0.2218\t Accuracy 0.8364\n",
      "Epoch [14][20]\t Batch [900][1100]\t Training Loss 0.2216\t Accuracy 0.8370\n",
      "Epoch [14][20]\t Batch [950][1100]\t Training Loss 0.2218\t Accuracy 0.8367\n",
      "Epoch [14][20]\t Batch [1000][1100]\t Training Loss 0.2221\t Accuracy 0.8360\n",
      "Epoch [14][20]\t Batch [1050][1100]\t Training Loss 0.2217\t Accuracy 0.8361\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2214\t Average training accuracy 0.8367\n",
      "Epoch [14]\t Average validation loss 0.2018\t Average validation accuracy 0.8734\n",
      "\n",
      "Epoch [15][20]\t Batch [0][1100]\t Training Loss 0.2203\t Accuracy 0.8800\n",
      "Epoch [15][20]\t Batch [50][1100]\t Training Loss 0.2143\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [100][1100]\t Training Loss 0.2129\t Accuracy 0.8543\n",
      "Epoch [15][20]\t Batch [150][1100]\t Training Loss 0.2130\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [200][1100]\t Training Loss 0.2145\t Accuracy 0.8497\n",
      "Epoch [15][20]\t Batch [250][1100]\t Training Loss 0.2149\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [300][1100]\t Training Loss 0.2187\t Accuracy 0.8404\n",
      "Epoch [15][20]\t Batch [350][1100]\t Training Loss 0.2194\t Accuracy 0.8387\n",
      "Epoch [15][20]\t Batch [400][1100]\t Training Loss 0.2190\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [450][1100]\t Training Loss 0.2188\t Accuracy 0.8406\n",
      "Epoch [15][20]\t Batch [500][1100]\t Training Loss 0.2188\t Accuracy 0.8403\n",
      "Epoch [15][20]\t Batch [550][1100]\t Training Loss 0.2188\t Accuracy 0.8399\n",
      "Epoch [15][20]\t Batch [600][1100]\t Training Loss 0.2188\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [650][1100]\t Training Loss 0.2201\t Accuracy 0.8371\n",
      "Epoch [15][20]\t Batch [700][1100]\t Training Loss 0.2196\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [750][1100]\t Training Loss 0.2199\t Accuracy 0.8382\n",
      "Epoch [15][20]\t Batch [800][1100]\t Training Loss 0.2198\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [850][1100]\t Training Loss 0.2202\t Accuracy 0.8377\n",
      "Epoch [15][20]\t Batch [900][1100]\t Training Loss 0.2200\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [950][1100]\t Training Loss 0.2203\t Accuracy 0.8381\n",
      "Epoch [15][20]\t Batch [1000][1100]\t Training Loss 0.2205\t Accuracy 0.8374\n",
      "Epoch [15][20]\t Batch [1050][1100]\t Training Loss 0.2202\t Accuracy 0.8375\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2199\t Average training accuracy 0.8381\n",
      "Epoch [15]\t Average validation loss 0.2003\t Average validation accuracy 0.8756\n",
      "\n",
      "Epoch [16][20]\t Batch [0][1100]\t Training Loss 0.2191\t Accuracy 0.8800\n",
      "Epoch [16][20]\t Batch [50][1100]\t Training Loss 0.2129\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [100][1100]\t Training Loss 0.2114\t Accuracy 0.8560\n",
      "Epoch [16][20]\t Batch [150][1100]\t Training Loss 0.2115\t Accuracy 0.8521\n",
      "Epoch [16][20]\t Batch [200][1100]\t Training Loss 0.2130\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [250][1100]\t Training Loss 0.2135\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [300][1100]\t Training Loss 0.2173\t Accuracy 0.8416\n",
      "Epoch [16][20]\t Batch [350][1100]\t Training Loss 0.2179\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [400][1100]\t Training Loss 0.2176\t Accuracy 0.8413\n",
      "Epoch [16][20]\t Batch [450][1100]\t Training Loss 0.2173\t Accuracy 0.8417\n",
      "Epoch [16][20]\t Batch [500][1100]\t Training Loss 0.2174\t Accuracy 0.8416\n",
      "Epoch [16][20]\t Batch [550][1100]\t Training Loss 0.2174\t Accuracy 0.8413\n",
      "Epoch [16][20]\t Batch [600][1100]\t Training Loss 0.2174\t Accuracy 0.8416\n",
      "Epoch [16][20]\t Batch [650][1100]\t Training Loss 0.2186\t Accuracy 0.8386\n",
      "Epoch [16][20]\t Batch [700][1100]\t Training Loss 0.2182\t Accuracy 0.8395\n",
      "Epoch [16][20]\t Batch [750][1100]\t Training Loss 0.2184\t Accuracy 0.8398\n",
      "Epoch [16][20]\t Batch [800][1100]\t Training Loss 0.2184\t Accuracy 0.8396\n",
      "Epoch [16][20]\t Batch [850][1100]\t Training Loss 0.2188\t Accuracy 0.8392\n",
      "Epoch [16][20]\t Batch [900][1100]\t Training Loss 0.2186\t Accuracy 0.8398\n",
      "Epoch [16][20]\t Batch [950][1100]\t Training Loss 0.2189\t Accuracy 0.8395\n",
      "Epoch [16][20]\t Batch [1000][1100]\t Training Loss 0.2191\t Accuracy 0.8389\n",
      "Epoch [16][20]\t Batch [1050][1100]\t Training Loss 0.2188\t Accuracy 0.8389\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2185\t Average training accuracy 0.8396\n",
      "Epoch [16]\t Average validation loss 0.1990\t Average validation accuracy 0.8766\n",
      "\n",
      "Epoch [17][20]\t Batch [0][1100]\t Training Loss 0.2180\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [50][1100]\t Training Loss 0.2115\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [100][1100]\t Training Loss 0.2100\t Accuracy 0.8580\n",
      "Epoch [17][20]\t Batch [150][1100]\t Training Loss 0.2102\t Accuracy 0.8543\n",
      "Epoch [17][20]\t Batch [200][1100]\t Training Loss 0.2117\t Accuracy 0.8531\n",
      "Epoch [17][20]\t Batch [250][1100]\t Training Loss 0.2121\t Accuracy 0.8507\n",
      "Epoch [17][20]\t Batch [300][1100]\t Training Loss 0.2159\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [350][1100]\t Training Loss 0.2166\t Accuracy 0.8414\n",
      "Epoch [17][20]\t Batch [400][1100]\t Training Loss 0.2162\t Accuracy 0.8428\n",
      "Epoch [17][20]\t Batch [450][1100]\t Training Loss 0.2160\t Accuracy 0.8431\n",
      "Epoch [17][20]\t Batch [500][1100]\t Training Loss 0.2160\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [550][1100]\t Training Loss 0.2161\t Accuracy 0.8429\n",
      "Epoch [17][20]\t Batch [600][1100]\t Training Loss 0.2160\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [650][1100]\t Training Loss 0.2173\t Accuracy 0.8402\n",
      "Epoch [17][20]\t Batch [700][1100]\t Training Loss 0.2168\t Accuracy 0.8410\n",
      "Epoch [17][20]\t Batch [750][1100]\t Training Loss 0.2171\t Accuracy 0.8412\n",
      "Epoch [17][20]\t Batch [800][1100]\t Training Loss 0.2171\t Accuracy 0.8411\n",
      "Epoch [17][20]\t Batch [850][1100]\t Training Loss 0.2175\t Accuracy 0.8408\n",
      "Epoch [17][20]\t Batch [900][1100]\t Training Loss 0.2173\t Accuracy 0.8413\n",
      "Epoch [17][20]\t Batch [950][1100]\t Training Loss 0.2175\t Accuracy 0.8411\n",
      "Epoch [17][20]\t Batch [1000][1100]\t Training Loss 0.2178\t Accuracy 0.8405\n",
      "Epoch [17][20]\t Batch [1050][1100]\t Training Loss 0.2174\t Accuracy 0.8405\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2172\t Average training accuracy 0.8411\n",
      "Epoch [17]\t Average validation loss 0.1978\t Average validation accuracy 0.8774\n",
      "\n",
      "Epoch [18][20]\t Batch [0][1100]\t Training Loss 0.2170\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][1100]\t Training Loss 0.2103\t Accuracy 0.8529\n",
      "Epoch [18][20]\t Batch [100][1100]\t Training Loss 0.2087\t Accuracy 0.8592\n",
      "Epoch [18][20]\t Batch [150][1100]\t Training Loss 0.2089\t Accuracy 0.8558\n",
      "Epoch [18][20]\t Batch [200][1100]\t Training Loss 0.2104\t Accuracy 0.8544\n",
      "Epoch [18][20]\t Batch [250][1100]\t Training Loss 0.2109\t Accuracy 0.8521\n",
      "Epoch [18][20]\t Batch [300][1100]\t Training Loss 0.2147\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [350][1100]\t Training Loss 0.2153\t Accuracy 0.8430\n",
      "Epoch [18][20]\t Batch [400][1100]\t Training Loss 0.2149\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [450][1100]\t Training Loss 0.2147\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [500][1100]\t Training Loss 0.2148\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [550][1100]\t Training Loss 0.2148\t Accuracy 0.8445\n",
      "Epoch [18][20]\t Batch [600][1100]\t Training Loss 0.2148\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [650][1100]\t Training Loss 0.2160\t Accuracy 0.8416\n",
      "Epoch [18][20]\t Batch [700][1100]\t Training Loss 0.2156\t Accuracy 0.8425\n",
      "Epoch [18][20]\t Batch [750][1100]\t Training Loss 0.2159\t Accuracy 0.8427\n",
      "Epoch [18][20]\t Batch [800][1100]\t Training Loss 0.2158\t Accuracy 0.8426\n",
      "Epoch [18][20]\t Batch [850][1100]\t Training Loss 0.2162\t Accuracy 0.8424\n",
      "Epoch [18][20]\t Batch [900][1100]\t Training Loss 0.2161\t Accuracy 0.8429\n",
      "Epoch [18][20]\t Batch [950][1100]\t Training Loss 0.2163\t Accuracy 0.8426\n",
      "Epoch [18][20]\t Batch [1000][1100]\t Training Loss 0.2165\t Accuracy 0.8420\n",
      "Epoch [18][20]\t Batch [1050][1100]\t Training Loss 0.2162\t Accuracy 0.8420\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2160\t Average training accuracy 0.8425\n",
      "Epoch [18]\t Average validation loss 0.1966\t Average validation accuracy 0.8788\n",
      "\n",
      "Epoch [19][20]\t Batch [0][1100]\t Training Loss 0.2160\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][1100]\t Training Loss 0.2091\t Accuracy 0.8537\n",
      "Epoch [19][20]\t Batch [100][1100]\t Training Loss 0.2075\t Accuracy 0.8604\n",
      "Epoch [19][20]\t Batch [150][1100]\t Training Loss 0.2077\t Accuracy 0.8575\n",
      "Epoch [19][20]\t Batch [200][1100]\t Training Loss 0.2092\t Accuracy 0.8560\n",
      "Epoch [19][20]\t Batch [250][1100]\t Training Loss 0.2097\t Accuracy 0.8537\n",
      "Epoch [19][20]\t Batch [300][1100]\t Training Loss 0.2135\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [350][1100]\t Training Loss 0.2141\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [400][1100]\t Training Loss 0.2138\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [450][1100]\t Training Loss 0.2135\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [500][1100]\t Training Loss 0.2136\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [550][1100]\t Training Loss 0.2136\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [600][1100]\t Training Loss 0.2136\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [650][1100]\t Training Loss 0.2149\t Accuracy 0.8431\n",
      "Epoch [19][20]\t Batch [700][1100]\t Training Loss 0.2144\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [750][1100]\t Training Loss 0.2147\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [800][1100]\t Training Loss 0.2146\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [850][1100]\t Training Loss 0.2151\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [900][1100]\t Training Loss 0.2149\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [950][1100]\t Training Loss 0.2151\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [1000][1100]\t Training Loss 0.2154\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [1050][1100]\t Training Loss 0.2151\t Accuracy 0.8435\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2148\t Average training accuracy 0.8440\n",
      "Epoch [19]\t Average validation loss 0.1955\t Average validation accuracy 0.8800\n",
      "\n",
      "Epoch [0][20]\t Batch [0][1100]\t Training Loss 3.2611\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][1100]\t Training Loss 1.2742\t Accuracy 0.2573\n",
      "Epoch [0][20]\t Batch [100][1100]\t Training Loss 0.8532\t Accuracy 0.3432\n",
      "Epoch [0][20]\t Batch [150][1100]\t Training Loss 0.6893\t Accuracy 0.4114\n",
      "Epoch [0][20]\t Batch [200][1100]\t Training Loss 0.5933\t Accuracy 0.4758\n",
      "Epoch [0][20]\t Batch [250][1100]\t Training Loss 0.5268\t Accuracy 0.5299\n",
      "Epoch [0][20]\t Batch [300][1100]\t Training Loss 0.4796\t Accuracy 0.5634\n",
      "Epoch [0][20]\t Batch [350][1100]\t Training Loss 0.4414\t Accuracy 0.5937\n",
      "Epoch [0][20]\t Batch [400][1100]\t Training Loss 0.4098\t Accuracy 0.6211\n",
      "Epoch [0][20]\t Batch [450][1100]\t Training Loss 0.3837\t Accuracy 0.6441\n",
      "Epoch [0][20]\t Batch [500][1100]\t Training Loss 0.3617\t Accuracy 0.6632\n",
      "Epoch [0][20]\t Batch [550][1100]\t Training Loss 0.3425\t Accuracy 0.6810\n",
      "Epoch [0][20]\t Batch [600][1100]\t Training Loss 0.3259\t Accuracy 0.6959\n",
      "Epoch [0][20]\t Batch [650][1100]\t Training Loss 0.3127\t Accuracy 0.7074\n",
      "Epoch [0][20]\t Batch [700][1100]\t Training Loss 0.2994\t Accuracy 0.7202\n",
      "Epoch [0][20]\t Batch [750][1100]\t Training Loss 0.2881\t Accuracy 0.7309\n",
      "Epoch [0][20]\t Batch [800][1100]\t Training Loss 0.2779\t Accuracy 0.7410\n",
      "Epoch [0][20]\t Batch [850][1100]\t Training Loss 0.2689\t Accuracy 0.7495\n",
      "Epoch [0][20]\t Batch [900][1100]\t Training Loss 0.2603\t Accuracy 0.7574\n",
      "Epoch [0][20]\t Batch [950][1100]\t Training Loss 0.2529\t Accuracy 0.7644\n",
      "Epoch [0][20]\t Batch [1000][1100]\t Training Loss 0.2463\t Accuracy 0.7705\n",
      "Epoch [0][20]\t Batch [1050][1100]\t Training Loss 0.2398\t Accuracy 0.7763\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2341\t Average training accuracy 0.7817\n",
      "Epoch [0]\t Average validation loss 0.0923\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [1][20]\t Batch [0][1100]\t Training Loss 0.1033\t Accuracy 0.9000\n",
      "Epoch [1][20]\t Batch [50][1100]\t Training Loss 0.1035\t Accuracy 0.9098\n",
      "Epoch [1][20]\t Batch [100][1100]\t Training Loss 0.1015\t Accuracy 0.9101\n",
      "Epoch [1][20]\t Batch [150][1100]\t Training Loss 0.1022\t Accuracy 0.9103\n",
      "Epoch [1][20]\t Batch [200][1100]\t Training Loss 0.1030\t Accuracy 0.9087\n",
      "Epoch [1][20]\t Batch [250][1100]\t Training Loss 0.1024\t Accuracy 0.9097\n",
      "Epoch [1][20]\t Batch [300][1100]\t Training Loss 0.1048\t Accuracy 0.9056\n",
      "Epoch [1][20]\t Batch [350][1100]\t Training Loss 0.1046\t Accuracy 0.9054\n",
      "Epoch [1][20]\t Batch [400][1100]\t Training Loss 0.1034\t Accuracy 0.9074\n",
      "Epoch [1][20]\t Batch [450][1100]\t Training Loss 0.1033\t Accuracy 0.9078\n",
      "Epoch [1][20]\t Batch [500][1100]\t Training Loss 0.1029\t Accuracy 0.9085\n",
      "Epoch [1][20]\t Batch [550][1100]\t Training Loss 0.1024\t Accuracy 0.9095\n",
      "Epoch [1][20]\t Batch [600][1100]\t Training Loss 0.1023\t Accuracy 0.9093\n",
      "Epoch [1][20]\t Batch [650][1100]\t Training Loss 0.1028\t Accuracy 0.9086\n",
      "Epoch [1][20]\t Batch [700][1100]\t Training Loss 0.1018\t Accuracy 0.9098\n",
      "Epoch [1][20]\t Batch [750][1100]\t Training Loss 0.1016\t Accuracy 0.9099\n",
      "Epoch [1][20]\t Batch [800][1100]\t Training Loss 0.1014\t Accuracy 0.9105\n",
      "Epoch [1][20]\t Batch [850][1100]\t Training Loss 0.1012\t Accuracy 0.9105\n",
      "Epoch [1][20]\t Batch [900][1100]\t Training Loss 0.1007\t Accuracy 0.9108\n",
      "Epoch [1][20]\t Batch [950][1100]\t Training Loss 0.1005\t Accuracy 0.9109\n",
      "Epoch [1][20]\t Batch [1000][1100]\t Training Loss 0.1005\t Accuracy 0.9109\n",
      "Epoch [1][20]\t Batch [1050][1100]\t Training Loss 0.1000\t Accuracy 0.9115\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0996\t Average training accuracy 0.9118\n",
      "Epoch [1]\t Average validation loss 0.0774\t Average validation accuracy 0.9450\n",
      "\n",
      "Epoch [2][20]\t Batch [0][1100]\t Training Loss 0.0928\t Accuracy 0.9200\n",
      "Epoch [2][20]\t Batch [50][1100]\t Training Loss 0.0858\t Accuracy 0.9314\n",
      "Epoch [2][20]\t Batch [100][1100]\t Training Loss 0.0844\t Accuracy 0.9349\n",
      "Epoch [2][20]\t Batch [150][1100]\t Training Loss 0.0861\t Accuracy 0.9321\n",
      "Epoch [2][20]\t Batch [200][1100]\t Training Loss 0.0876\t Accuracy 0.9280\n",
      "Epoch [2][20]\t Batch [250][1100]\t Training Loss 0.0876\t Accuracy 0.9281\n",
      "Epoch [2][20]\t Batch [300][1100]\t Training Loss 0.0902\t Accuracy 0.9250\n",
      "Epoch [2][20]\t Batch [350][1100]\t Training Loss 0.0903\t Accuracy 0.9244\n",
      "Epoch [2][20]\t Batch [400][1100]\t Training Loss 0.0894\t Accuracy 0.9258\n",
      "Epoch [2][20]\t Batch [450][1100]\t Training Loss 0.0894\t Accuracy 0.9261\n",
      "Epoch [2][20]\t Batch [500][1100]\t Training Loss 0.0892\t Accuracy 0.9263\n",
      "Epoch [2][20]\t Batch [550][1100]\t Training Loss 0.0891\t Accuracy 0.9261\n",
      "Epoch [2][20]\t Batch [600][1100]\t Training Loss 0.0892\t Accuracy 0.9256\n",
      "Epoch [2][20]\t Batch [650][1100]\t Training Loss 0.0898\t Accuracy 0.9243\n",
      "Epoch [2][20]\t Batch [700][1100]\t Training Loss 0.0891\t Accuracy 0.9252\n",
      "Epoch [2][20]\t Batch [750][1100]\t Training Loss 0.0891\t Accuracy 0.9248\n",
      "Epoch [2][20]\t Batch [800][1100]\t Training Loss 0.0891\t Accuracy 0.9248\n",
      "Epoch [2][20]\t Batch [850][1100]\t Training Loss 0.0891\t Accuracy 0.9247\n",
      "Epoch [2][20]\t Batch [900][1100]\t Training Loss 0.0887\t Accuracy 0.9251\n",
      "Epoch [2][20]\t Batch [950][1100]\t Training Loss 0.0887\t Accuracy 0.9250\n",
      "Epoch [2][20]\t Batch [1000][1100]\t Training Loss 0.0888\t Accuracy 0.9247\n",
      "Epoch [2][20]\t Batch [1050][1100]\t Training Loss 0.0885\t Accuracy 0.9252\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0883\t Average training accuracy 0.9254\n",
      "Epoch [2]\t Average validation loss 0.0715\t Average validation accuracy 0.9480\n",
      "\n",
      "Epoch [3][20]\t Batch [0][1100]\t Training Loss 0.0880\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [50][1100]\t Training Loss 0.0782\t Accuracy 0.9384\n",
      "Epoch [3][20]\t Batch [100][1100]\t Training Loss 0.0769\t Accuracy 0.9424\n",
      "Epoch [3][20]\t Batch [150][1100]\t Training Loss 0.0790\t Accuracy 0.9397\n",
      "Epoch [3][20]\t Batch [200][1100]\t Training Loss 0.0804\t Accuracy 0.9357\n",
      "Epoch [3][20]\t Batch [250][1100]\t Training Loss 0.0806\t Accuracy 0.9352\n",
      "Epoch [3][20]\t Batch [300][1100]\t Training Loss 0.0829\t Accuracy 0.9326\n",
      "Epoch [3][20]\t Batch [350][1100]\t Training Loss 0.0831\t Accuracy 0.9330\n",
      "Epoch [3][20]\t Batch [400][1100]\t Training Loss 0.0822\t Accuracy 0.9346\n",
      "Epoch [3][20]\t Batch [450][1100]\t Training Loss 0.0822\t Accuracy 0.9345\n",
      "Epoch [3][20]\t Batch [500][1100]\t Training Loss 0.0820\t Accuracy 0.9348\n",
      "Epoch [3][20]\t Batch [550][1100]\t Training Loss 0.0821\t Accuracy 0.9344\n",
      "Epoch [3][20]\t Batch [600][1100]\t Training Loss 0.0823\t Accuracy 0.9337\n",
      "Epoch [3][20]\t Batch [650][1100]\t Training Loss 0.0829\t Accuracy 0.9325\n",
      "Epoch [3][20]\t Batch [700][1100]\t Training Loss 0.0822\t Accuracy 0.9330\n",
      "Epoch [3][20]\t Batch [750][1100]\t Training Loss 0.0823\t Accuracy 0.9329\n",
      "Epoch [3][20]\t Batch [800][1100]\t Training Loss 0.0823\t Accuracy 0.9329\n",
      "Epoch [3][20]\t Batch [850][1100]\t Training Loss 0.0823\t Accuracy 0.9325\n",
      "Epoch [3][20]\t Batch [900][1100]\t Training Loss 0.0820\t Accuracy 0.9329\n",
      "Epoch [3][20]\t Batch [950][1100]\t Training Loss 0.0820\t Accuracy 0.9328\n",
      "Epoch [3][20]\t Batch [1000][1100]\t Training Loss 0.0822\t Accuracy 0.9324\n",
      "Epoch [3][20]\t Batch [1050][1100]\t Training Loss 0.0819\t Accuracy 0.9328\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0817\t Average training accuracy 0.9330\n",
      "Epoch [3]\t Average validation loss 0.0679\t Average validation accuracy 0.9508\n",
      "\n",
      "Epoch [4][20]\t Batch [0][1100]\t Training Loss 0.0832\t Accuracy 0.9400\n",
      "Epoch [4][20]\t Batch [50][1100]\t Training Loss 0.0732\t Accuracy 0.9435\n",
      "Epoch [4][20]\t Batch [100][1100]\t Training Loss 0.0719\t Accuracy 0.9475\n",
      "Epoch [4][20]\t Batch [150][1100]\t Training Loss 0.0739\t Accuracy 0.9441\n",
      "Epoch [4][20]\t Batch [200][1100]\t Training Loss 0.0755\t Accuracy 0.9412\n",
      "Epoch [4][20]\t Batch [250][1100]\t Training Loss 0.0756\t Accuracy 0.9406\n",
      "Epoch [4][20]\t Batch [300][1100]\t Training Loss 0.0777\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [350][1100]\t Training Loss 0.0780\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [400][1100]\t Training Loss 0.0771\t Accuracy 0.9398\n",
      "Epoch [4][20]\t Batch [450][1100]\t Training Loss 0.0771\t Accuracy 0.9399\n",
      "Epoch [4][20]\t Batch [500][1100]\t Training Loss 0.0769\t Accuracy 0.9405\n",
      "Epoch [4][20]\t Batch [550][1100]\t Training Loss 0.0770\t Accuracy 0.9399\n",
      "Epoch [4][20]\t Batch [600][1100]\t Training Loss 0.0772\t Accuracy 0.9389\n",
      "Epoch [4][20]\t Batch [650][1100]\t Training Loss 0.0777\t Accuracy 0.9378\n",
      "Epoch [4][20]\t Batch [700][1100]\t Training Loss 0.0771\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [750][1100]\t Training Loss 0.0772\t Accuracy 0.9382\n",
      "Epoch [4][20]\t Batch [800][1100]\t Training Loss 0.0772\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [850][1100]\t Training Loss 0.0773\t Accuracy 0.9381\n",
      "Epoch [4][20]\t Batch [900][1100]\t Training Loss 0.0769\t Accuracy 0.9386\n",
      "Epoch [4][20]\t Batch [950][1100]\t Training Loss 0.0770\t Accuracy 0.9384\n",
      "Epoch [4][20]\t Batch [1000][1100]\t Training Loss 0.0772\t Accuracy 0.9381\n",
      "Epoch [4][20]\t Batch [1050][1100]\t Training Loss 0.0770\t Accuracy 0.9384\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0768\t Average training accuracy 0.9386\n",
      "Epoch [4]\t Average validation loss 0.0650\t Average validation accuracy 0.9538\n",
      "\n",
      "Epoch [5][20]\t Batch [0][1100]\t Training Loss 0.0797\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][1100]\t Training Loss 0.0696\t Accuracy 0.9463\n",
      "Epoch [5][20]\t Batch [100][1100]\t Training Loss 0.0682\t Accuracy 0.9509\n",
      "Epoch [5][20]\t Batch [150][1100]\t Training Loss 0.0700\t Accuracy 0.9483\n",
      "Epoch [5][20]\t Batch [200][1100]\t Training Loss 0.0716\t Accuracy 0.9453\n",
      "Epoch [5][20]\t Batch [250][1100]\t Training Loss 0.0716\t Accuracy 0.9442\n",
      "Epoch [5][20]\t Batch [300][1100]\t Training Loss 0.0737\t Accuracy 0.9420\n",
      "Epoch [5][20]\t Batch [350][1100]\t Training Loss 0.0739\t Accuracy 0.9420\n",
      "Epoch [5][20]\t Batch [400][1100]\t Training Loss 0.0731\t Accuracy 0.9435\n",
      "Epoch [5][20]\t Batch [450][1100]\t Training Loss 0.0730\t Accuracy 0.9435\n",
      "Epoch [5][20]\t Batch [500][1100]\t Training Loss 0.0728\t Accuracy 0.9440\n",
      "Epoch [5][20]\t Batch [550][1100]\t Training Loss 0.0729\t Accuracy 0.9436\n",
      "Epoch [5][20]\t Batch [600][1100]\t Training Loss 0.0732\t Accuracy 0.9427\n",
      "Epoch [5][20]\t Batch [650][1100]\t Training Loss 0.0737\t Accuracy 0.9418\n",
      "Epoch [5][20]\t Batch [700][1100]\t Training Loss 0.0731\t Accuracy 0.9423\n",
      "Epoch [5][20]\t Batch [750][1100]\t Training Loss 0.0732\t Accuracy 0.9423\n",
      "Epoch [5][20]\t Batch [800][1100]\t Training Loss 0.0732\t Accuracy 0.9424\n",
      "Epoch [5][20]\t Batch [850][1100]\t Training Loss 0.0733\t Accuracy 0.9423\n",
      "Epoch [5][20]\t Batch [900][1100]\t Training Loss 0.0729\t Accuracy 0.9429\n",
      "Epoch [5][20]\t Batch [950][1100]\t Training Loss 0.0730\t Accuracy 0.9427\n",
      "Epoch [5][20]\t Batch [1000][1100]\t Training Loss 0.0732\t Accuracy 0.9424\n",
      "Epoch [5][20]\t Batch [1050][1100]\t Training Loss 0.0730\t Accuracy 0.9428\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0729\t Average training accuracy 0.9428\n",
      "Epoch [5]\t Average validation loss 0.0626\t Average validation accuracy 0.9556\n",
      "\n",
      "Epoch [6][20]\t Batch [0][1100]\t Training Loss 0.0772\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][1100]\t Training Loss 0.0667\t Accuracy 0.9467\n",
      "Epoch [6][20]\t Batch [100][1100]\t Training Loss 0.0654\t Accuracy 0.9527\n",
      "Epoch [6][20]\t Batch [150][1100]\t Training Loss 0.0669\t Accuracy 0.9513\n",
      "Epoch [6][20]\t Batch [200][1100]\t Training Loss 0.0684\t Accuracy 0.9486\n",
      "Epoch [6][20]\t Batch [250][1100]\t Training Loss 0.0685\t Accuracy 0.9475\n",
      "Epoch [6][20]\t Batch [300][1100]\t Training Loss 0.0704\t Accuracy 0.9453\n",
      "Epoch [6][20]\t Batch [350][1100]\t Training Loss 0.0706\t Accuracy 0.9455\n",
      "Epoch [6][20]\t Batch [400][1100]\t Training Loss 0.0698\t Accuracy 0.9466\n",
      "Epoch [6][20]\t Batch [450][1100]\t Training Loss 0.0697\t Accuracy 0.9468\n",
      "Epoch [6][20]\t Batch [500][1100]\t Training Loss 0.0694\t Accuracy 0.9471\n",
      "Epoch [6][20]\t Batch [550][1100]\t Training Loss 0.0696\t Accuracy 0.9468\n",
      "Epoch [6][20]\t Batch [600][1100]\t Training Loss 0.0699\t Accuracy 0.9461\n",
      "Epoch [6][20]\t Batch [650][1100]\t Training Loss 0.0704\t Accuracy 0.9454\n",
      "Epoch [6][20]\t Batch [700][1100]\t Training Loss 0.0698\t Accuracy 0.9459\n",
      "Epoch [6][20]\t Batch [750][1100]\t Training Loss 0.0698\t Accuracy 0.9458\n",
      "Epoch [6][20]\t Batch [800][1100]\t Training Loss 0.0698\t Accuracy 0.9460\n",
      "Epoch [6][20]\t Batch [850][1100]\t Training Loss 0.0700\t Accuracy 0.9459\n",
      "Epoch [6][20]\t Batch [900][1100]\t Training Loss 0.0696\t Accuracy 0.9464\n",
      "Epoch [6][20]\t Batch [950][1100]\t Training Loss 0.0697\t Accuracy 0.9463\n",
      "Epoch [6][20]\t Batch [1000][1100]\t Training Loss 0.0700\t Accuracy 0.9460\n",
      "Epoch [6][20]\t Batch [1050][1100]\t Training Loss 0.0698\t Accuracy 0.9462\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0697\t Average training accuracy 0.9464\n",
      "Epoch [6]\t Average validation loss 0.0606\t Average validation accuracy 0.9576\n",
      "\n",
      "Epoch [7][20]\t Batch [0][1100]\t Training Loss 0.0752\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][1100]\t Training Loss 0.0645\t Accuracy 0.9514\n",
      "Epoch [7][20]\t Batch [100][1100]\t Training Loss 0.0631\t Accuracy 0.9552\n",
      "Epoch [7][20]\t Batch [150][1100]\t Training Loss 0.0643\t Accuracy 0.9539\n",
      "Epoch [7][20]\t Batch [200][1100]\t Training Loss 0.0657\t Accuracy 0.9515\n",
      "Epoch [7][20]\t Batch [250][1100]\t Training Loss 0.0658\t Accuracy 0.9510\n",
      "Epoch [7][20]\t Batch [300][1100]\t Training Loss 0.0677\t Accuracy 0.9488\n",
      "Epoch [7][20]\t Batch [350][1100]\t Training Loss 0.0679\t Accuracy 0.9489\n",
      "Epoch [7][20]\t Batch [400][1100]\t Training Loss 0.0671\t Accuracy 0.9499\n",
      "Epoch [7][20]\t Batch [450][1100]\t Training Loss 0.0670\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [500][1100]\t Training Loss 0.0667\t Accuracy 0.9503\n",
      "Epoch [7][20]\t Batch [550][1100]\t Training Loss 0.0669\t Accuracy 0.9499\n",
      "Epoch [7][20]\t Batch [600][1100]\t Training Loss 0.0672\t Accuracy 0.9493\n",
      "Epoch [7][20]\t Batch [650][1100]\t Training Loss 0.0677\t Accuracy 0.9487\n",
      "Epoch [7][20]\t Batch [700][1100]\t Training Loss 0.0670\t Accuracy 0.9492\n",
      "Epoch [7][20]\t Batch [750][1100]\t Training Loss 0.0671\t Accuracy 0.9491\n",
      "Epoch [7][20]\t Batch [800][1100]\t Training Loss 0.0671\t Accuracy 0.9492\n",
      "Epoch [7][20]\t Batch [850][1100]\t Training Loss 0.0673\t Accuracy 0.9490\n",
      "Epoch [7][20]\t Batch [900][1100]\t Training Loss 0.0669\t Accuracy 0.9496\n",
      "Epoch [7][20]\t Batch [950][1100]\t Training Loss 0.0671\t Accuracy 0.9495\n",
      "Epoch [7][20]\t Batch [1000][1100]\t Training Loss 0.0673\t Accuracy 0.9492\n",
      "Epoch [7][20]\t Batch [1050][1100]\t Training Loss 0.0672\t Accuracy 0.9493\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0670\t Average training accuracy 0.9495\n",
      "Epoch [7]\t Average validation loss 0.0591\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [8][20]\t Batch [0][1100]\t Training Loss 0.0734\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][1100]\t Training Loss 0.0626\t Accuracy 0.9529\n",
      "Epoch [8][20]\t Batch [100][1100]\t Training Loss 0.0611\t Accuracy 0.9568\n",
      "Epoch [8][20]\t Batch [150][1100]\t Training Loss 0.0622\t Accuracy 0.9559\n",
      "Epoch [8][20]\t Batch [200][1100]\t Training Loss 0.0635\t Accuracy 0.9536\n",
      "Epoch [8][20]\t Batch [250][1100]\t Training Loss 0.0636\t Accuracy 0.9531\n",
      "Epoch [8][20]\t Batch [300][1100]\t Training Loss 0.0654\t Accuracy 0.9511\n",
      "Epoch [8][20]\t Batch [350][1100]\t Training Loss 0.0655\t Accuracy 0.9514\n",
      "Epoch [8][20]\t Batch [400][1100]\t Training Loss 0.0649\t Accuracy 0.9523\n",
      "Epoch [8][20]\t Batch [450][1100]\t Training Loss 0.0647\t Accuracy 0.9524\n",
      "Epoch [8][20]\t Batch [500][1100]\t Training Loss 0.0645\t Accuracy 0.9525\n",
      "Epoch [8][20]\t Batch [550][1100]\t Training Loss 0.0648\t Accuracy 0.9522\n",
      "Epoch [8][20]\t Batch [600][1100]\t Training Loss 0.0650\t Accuracy 0.9518\n",
      "Epoch [8][20]\t Batch [650][1100]\t Training Loss 0.0655\t Accuracy 0.9511\n",
      "Epoch [8][20]\t Batch [700][1100]\t Training Loss 0.0649\t Accuracy 0.9518\n",
      "Epoch [8][20]\t Batch [750][1100]\t Training Loss 0.0649\t Accuracy 0.9517\n",
      "Epoch [8][20]\t Batch [800][1100]\t Training Loss 0.0649\t Accuracy 0.9515\n",
      "Epoch [8][20]\t Batch [850][1100]\t Training Loss 0.0651\t Accuracy 0.9513\n",
      "Epoch [8][20]\t Batch [900][1100]\t Training Loss 0.0648\t Accuracy 0.9518\n",
      "Epoch [8][20]\t Batch [950][1100]\t Training Loss 0.0649\t Accuracy 0.9515\n",
      "Epoch [8][20]\t Batch [1000][1100]\t Training Loss 0.0651\t Accuracy 0.9511\n",
      "Epoch [8][20]\t Batch [1050][1100]\t Training Loss 0.0650\t Accuracy 0.9512\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0649\t Average training accuracy 0.9513\n",
      "Epoch [8]\t Average validation loss 0.0579\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [9][20]\t Batch [0][1100]\t Training Loss 0.0715\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][1100]\t Training Loss 0.0610\t Accuracy 0.9553\n",
      "Epoch [9][20]\t Batch [100][1100]\t Training Loss 0.0594\t Accuracy 0.9586\n",
      "Epoch [9][20]\t Batch [150][1100]\t Training Loss 0.0604\t Accuracy 0.9577\n",
      "Epoch [9][20]\t Batch [200][1100]\t Training Loss 0.0617\t Accuracy 0.9555\n",
      "Epoch [9][20]\t Batch [250][1100]\t Training Loss 0.0617\t Accuracy 0.9551\n",
      "Epoch [9][20]\t Batch [300][1100]\t Training Loss 0.0636\t Accuracy 0.9538\n",
      "Epoch [9][20]\t Batch [350][1100]\t Training Loss 0.0637\t Accuracy 0.9540\n",
      "Epoch [9][20]\t Batch [400][1100]\t Training Loss 0.0631\t Accuracy 0.9546\n",
      "Epoch [9][20]\t Batch [450][1100]\t Training Loss 0.0629\t Accuracy 0.9545\n",
      "Epoch [9][20]\t Batch [500][1100]\t Training Loss 0.0627\t Accuracy 0.9548\n",
      "Epoch [9][20]\t Batch [550][1100]\t Training Loss 0.0630\t Accuracy 0.9543\n",
      "Epoch [9][20]\t Batch [600][1100]\t Training Loss 0.0632\t Accuracy 0.9538\n",
      "Epoch [9][20]\t Batch [650][1100]\t Training Loss 0.0637\t Accuracy 0.9531\n",
      "Epoch [9][20]\t Batch [700][1100]\t Training Loss 0.0631\t Accuracy 0.9537\n",
      "Epoch [9][20]\t Batch [750][1100]\t Training Loss 0.0631\t Accuracy 0.9537\n",
      "Epoch [9][20]\t Batch [800][1100]\t Training Loss 0.0632\t Accuracy 0.9535\n",
      "Epoch [9][20]\t Batch [850][1100]\t Training Loss 0.0633\t Accuracy 0.9532\n",
      "Epoch [9][20]\t Batch [900][1100]\t Training Loss 0.0630\t Accuracy 0.9538\n",
      "Epoch [9][20]\t Batch [950][1100]\t Training Loss 0.0632\t Accuracy 0.9536\n",
      "Epoch [9][20]\t Batch [1000][1100]\t Training Loss 0.0634\t Accuracy 0.9532\n",
      "Epoch [9][20]\t Batch [1050][1100]\t Training Loss 0.0633\t Accuracy 0.9533\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0632\t Average training accuracy 0.9533\n",
      "Epoch [9]\t Average validation loss 0.0569\t Average validation accuracy 0.9610\n",
      "\n",
      "Epoch [10][20]\t Batch [0][1100]\t Training Loss 0.0695\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][1100]\t Training Loss 0.0596\t Accuracy 0.9588\n",
      "Epoch [10][20]\t Batch [100][1100]\t Training Loss 0.0581\t Accuracy 0.9610\n",
      "Epoch [10][20]\t Batch [150][1100]\t Training Loss 0.0590\t Accuracy 0.9596\n",
      "Epoch [10][20]\t Batch [200][1100]\t Training Loss 0.0603\t Accuracy 0.9576\n",
      "Epoch [10][20]\t Batch [250][1100]\t Training Loss 0.0603\t Accuracy 0.9572\n",
      "Epoch [10][20]\t Batch [300][1100]\t Training Loss 0.0621\t Accuracy 0.9558\n",
      "Epoch [10][20]\t Batch [350][1100]\t Training Loss 0.0622\t Accuracy 0.9560\n",
      "Epoch [10][20]\t Batch [400][1100]\t Training Loss 0.0616\t Accuracy 0.9564\n",
      "Epoch [10][20]\t Batch [450][1100]\t Training Loss 0.0615\t Accuracy 0.9562\n",
      "Epoch [10][20]\t Batch [500][1100]\t Training Loss 0.0612\t Accuracy 0.9564\n",
      "Epoch [10][20]\t Batch [550][1100]\t Training Loss 0.0615\t Accuracy 0.9559\n",
      "Epoch [10][20]\t Batch [600][1100]\t Training Loss 0.0618\t Accuracy 0.9552\n",
      "Epoch [10][20]\t Batch [650][1100]\t Training Loss 0.0622\t Accuracy 0.9545\n",
      "Epoch [10][20]\t Batch [700][1100]\t Training Loss 0.0616\t Accuracy 0.9551\n",
      "Epoch [10][20]\t Batch [750][1100]\t Training Loss 0.0617\t Accuracy 0.9551\n",
      "Epoch [10][20]\t Batch [800][1100]\t Training Loss 0.0617\t Accuracy 0.9550\n",
      "Epoch [10][20]\t Batch [850][1100]\t Training Loss 0.0619\t Accuracy 0.9547\n",
      "Epoch [10][20]\t Batch [900][1100]\t Training Loss 0.0616\t Accuracy 0.9552\n",
      "Epoch [10][20]\t Batch [950][1100]\t Training Loss 0.0617\t Accuracy 0.9551\n",
      "Epoch [10][20]\t Batch [1000][1100]\t Training Loss 0.0620\t Accuracy 0.9546\n",
      "Epoch [10][20]\t Batch [1050][1100]\t Training Loss 0.0619\t Accuracy 0.9546\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0618\t Average training accuracy 0.9545\n",
      "Epoch [10]\t Average validation loss 0.0560\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [11][20]\t Batch [0][1100]\t Training Loss 0.0686\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][1100]\t Training Loss 0.0584\t Accuracy 0.9592\n",
      "Epoch [11][20]\t Batch [100][1100]\t Training Loss 0.0569\t Accuracy 0.9614\n",
      "Epoch [11][20]\t Batch [150][1100]\t Training Loss 0.0577\t Accuracy 0.9604\n",
      "Epoch [11][20]\t Batch [200][1100]\t Training Loss 0.0590\t Accuracy 0.9586\n",
      "Epoch [11][20]\t Batch [250][1100]\t Training Loss 0.0590\t Accuracy 0.9582\n",
      "Epoch [11][20]\t Batch [300][1100]\t Training Loss 0.0608\t Accuracy 0.9568\n",
      "Epoch [11][20]\t Batch [350][1100]\t Training Loss 0.0609\t Accuracy 0.9570\n",
      "Epoch [11][20]\t Batch [400][1100]\t Training Loss 0.0603\t Accuracy 0.9572\n",
      "Epoch [11][20]\t Batch [450][1100]\t Training Loss 0.0602\t Accuracy 0.9569\n",
      "Epoch [11][20]\t Batch [500][1100]\t Training Loss 0.0600\t Accuracy 0.9572\n",
      "Epoch [11][20]\t Batch [550][1100]\t Training Loss 0.0602\t Accuracy 0.9568\n",
      "Epoch [11][20]\t Batch [600][1100]\t Training Loss 0.0605\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [650][1100]\t Training Loss 0.0609\t Accuracy 0.9555\n",
      "Epoch [11][20]\t Batch [700][1100]\t Training Loss 0.0604\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [750][1100]\t Training Loss 0.0604\t Accuracy 0.9561\n",
      "Epoch [11][20]\t Batch [800][1100]\t Training Loss 0.0604\t Accuracy 0.9561\n",
      "Epoch [11][20]\t Batch [850][1100]\t Training Loss 0.0606\t Accuracy 0.9558\n",
      "Epoch [11][20]\t Batch [900][1100]\t Training Loss 0.0603\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [950][1100]\t Training Loss 0.0604\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [1000][1100]\t Training Loss 0.0607\t Accuracy 0.9557\n",
      "Epoch [11][20]\t Batch [1050][1100]\t Training Loss 0.0606\t Accuracy 0.9557\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0606\t Average training accuracy 0.9557\n",
      "Epoch [11]\t Average validation loss 0.0552\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [12][20]\t Batch [0][1100]\t Training Loss 0.0673\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][1100]\t Training Loss 0.0573\t Accuracy 0.9596\n",
      "Epoch [12][20]\t Batch [100][1100]\t Training Loss 0.0558\t Accuracy 0.9618\n",
      "Epoch [12][20]\t Batch [150][1100]\t Training Loss 0.0566\t Accuracy 0.9609\n",
      "Epoch [12][20]\t Batch [200][1100]\t Training Loss 0.0578\t Accuracy 0.9599\n",
      "Epoch [12][20]\t Batch [250][1100]\t Training Loss 0.0578\t Accuracy 0.9594\n",
      "Epoch [12][20]\t Batch [300][1100]\t Training Loss 0.0596\t Accuracy 0.9577\n",
      "Epoch [12][20]\t Batch [350][1100]\t Training Loss 0.0597\t Accuracy 0.9575\n",
      "Epoch [12][20]\t Batch [400][1100]\t Training Loss 0.0591\t Accuracy 0.9579\n",
      "Epoch [12][20]\t Batch [450][1100]\t Training Loss 0.0590\t Accuracy 0.9579\n",
      "Epoch [12][20]\t Batch [500][1100]\t Training Loss 0.0588\t Accuracy 0.9582\n",
      "Epoch [12][20]\t Batch [550][1100]\t Training Loss 0.0590\t Accuracy 0.9577\n",
      "Epoch [12][20]\t Batch [600][1100]\t Training Loss 0.0593\t Accuracy 0.9571\n",
      "Epoch [12][20]\t Batch [650][1100]\t Training Loss 0.0597\t Accuracy 0.9566\n",
      "Epoch [12][20]\t Batch [700][1100]\t Training Loss 0.0592\t Accuracy 0.9572\n",
      "Epoch [12][20]\t Batch [750][1100]\t Training Loss 0.0592\t Accuracy 0.9572\n",
      "Epoch [12][20]\t Batch [800][1100]\t Training Loss 0.0593\t Accuracy 0.9571\n",
      "Epoch [12][20]\t Batch [850][1100]\t Training Loss 0.0594\t Accuracy 0.9568\n",
      "Epoch [12][20]\t Batch [900][1100]\t Training Loss 0.0592\t Accuracy 0.9573\n",
      "Epoch [12][20]\t Batch [950][1100]\t Training Loss 0.0593\t Accuracy 0.9572\n",
      "Epoch [12][20]\t Batch [1000][1100]\t Training Loss 0.0596\t Accuracy 0.9568\n",
      "Epoch [12][20]\t Batch [1050][1100]\t Training Loss 0.0595\t Accuracy 0.9568\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0594\t Average training accuracy 0.9568\n",
      "Epoch [12]\t Average validation loss 0.0546\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [13][20]\t Batch [0][1100]\t Training Loss 0.0668\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][1100]\t Training Loss 0.0563\t Accuracy 0.9608\n",
      "Epoch [13][20]\t Batch [100][1100]\t Training Loss 0.0548\t Accuracy 0.9628\n",
      "Epoch [13][20]\t Batch [150][1100]\t Training Loss 0.0556\t Accuracy 0.9619\n",
      "Epoch [13][20]\t Batch [200][1100]\t Training Loss 0.0568\t Accuracy 0.9607\n",
      "Epoch [13][20]\t Batch [250][1100]\t Training Loss 0.0567\t Accuracy 0.9602\n",
      "Epoch [13][20]\t Batch [300][1100]\t Training Loss 0.0585\t Accuracy 0.9584\n",
      "Epoch [13][20]\t Batch [350][1100]\t Training Loss 0.0586\t Accuracy 0.9584\n",
      "Epoch [13][20]\t Batch [400][1100]\t Training Loss 0.0581\t Accuracy 0.9587\n",
      "Epoch [13][20]\t Batch [450][1100]\t Training Loss 0.0580\t Accuracy 0.9587\n",
      "Epoch [13][20]\t Batch [500][1100]\t Training Loss 0.0577\t Accuracy 0.9589\n",
      "Epoch [13][20]\t Batch [550][1100]\t Training Loss 0.0580\t Accuracy 0.9586\n",
      "Epoch [13][20]\t Batch [600][1100]\t Training Loss 0.0583\t Accuracy 0.9581\n",
      "Epoch [13][20]\t Batch [650][1100]\t Training Loss 0.0587\t Accuracy 0.9578\n",
      "Epoch [13][20]\t Batch [700][1100]\t Training Loss 0.0582\t Accuracy 0.9583\n",
      "Epoch [13][20]\t Batch [750][1100]\t Training Loss 0.0582\t Accuracy 0.9583\n",
      "Epoch [13][20]\t Batch [800][1100]\t Training Loss 0.0583\t Accuracy 0.9583\n",
      "Epoch [13][20]\t Batch [850][1100]\t Training Loss 0.0585\t Accuracy 0.9579\n",
      "Epoch [13][20]\t Batch [900][1100]\t Training Loss 0.0582\t Accuracy 0.9584\n",
      "Epoch [13][20]\t Batch [950][1100]\t Training Loss 0.0583\t Accuracy 0.9583\n",
      "Epoch [13][20]\t Batch [1000][1100]\t Training Loss 0.0586\t Accuracy 0.9577\n",
      "Epoch [13][20]\t Batch [1050][1100]\t Training Loss 0.0585\t Accuracy 0.9577\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0585\t Average training accuracy 0.9578\n",
      "Epoch [13]\t Average validation loss 0.0540\t Average validation accuracy 0.9646\n",
      "\n",
      "Epoch [14][20]\t Batch [0][1100]\t Training Loss 0.0662\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][1100]\t Training Loss 0.0555\t Accuracy 0.9612\n",
      "Epoch [14][20]\t Batch [100][1100]\t Training Loss 0.0540\t Accuracy 0.9636\n",
      "Epoch [14][20]\t Batch [150][1100]\t Training Loss 0.0548\t Accuracy 0.9629\n",
      "Epoch [14][20]\t Batch [200][1100]\t Training Loss 0.0559\t Accuracy 0.9618\n",
      "Epoch [14][20]\t Batch [250][1100]\t Training Loss 0.0559\t Accuracy 0.9611\n",
      "Epoch [14][20]\t Batch [300][1100]\t Training Loss 0.0576\t Accuracy 0.9595\n",
      "Epoch [14][20]\t Batch [350][1100]\t Training Loss 0.0577\t Accuracy 0.9597\n",
      "Epoch [14][20]\t Batch [400][1100]\t Training Loss 0.0571\t Accuracy 0.9599\n",
      "Epoch [14][20]\t Batch [450][1100]\t Training Loss 0.0570\t Accuracy 0.9597\n",
      "Epoch [14][20]\t Batch [500][1100]\t Training Loss 0.0568\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [550][1100]\t Training Loss 0.0571\t Accuracy 0.9597\n",
      "Epoch [14][20]\t Batch [600][1100]\t Training Loss 0.0574\t Accuracy 0.9593\n",
      "Epoch [14][20]\t Batch [650][1100]\t Training Loss 0.0578\t Accuracy 0.9589\n",
      "Epoch [14][20]\t Batch [700][1100]\t Training Loss 0.0573\t Accuracy 0.9594\n",
      "Epoch [14][20]\t Batch [750][1100]\t Training Loss 0.0574\t Accuracy 0.9594\n",
      "Epoch [14][20]\t Batch [800][1100]\t Training Loss 0.0574\t Accuracy 0.9593\n",
      "Epoch [14][20]\t Batch [850][1100]\t Training Loss 0.0576\t Accuracy 0.9589\n",
      "Epoch [14][20]\t Batch [900][1100]\t Training Loss 0.0573\t Accuracy 0.9594\n",
      "Epoch [14][20]\t Batch [950][1100]\t Training Loss 0.0574\t Accuracy 0.9592\n",
      "Epoch [14][20]\t Batch [1000][1100]\t Training Loss 0.0577\t Accuracy 0.9587\n",
      "Epoch [14][20]\t Batch [1050][1100]\t Training Loss 0.0577\t Accuracy 0.9587\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0576\t Average training accuracy 0.9587\n",
      "Epoch [14]\t Average validation loss 0.0534\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [15][20]\t Batch [0][1100]\t Training Loss 0.0650\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][1100]\t Training Loss 0.0547\t Accuracy 0.9635\n",
      "Epoch [15][20]\t Batch [100][1100]\t Training Loss 0.0532\t Accuracy 0.9648\n",
      "Epoch [15][20]\t Batch [150][1100]\t Training Loss 0.0540\t Accuracy 0.9640\n",
      "Epoch [15][20]\t Batch [200][1100]\t Training Loss 0.0552\t Accuracy 0.9628\n",
      "Epoch [15][20]\t Batch [250][1100]\t Training Loss 0.0551\t Accuracy 0.9622\n",
      "Epoch [15][20]\t Batch [300][1100]\t Training Loss 0.0567\t Accuracy 0.9605\n",
      "Epoch [15][20]\t Batch [350][1100]\t Training Loss 0.0568\t Accuracy 0.9607\n",
      "Epoch [15][20]\t Batch [400][1100]\t Training Loss 0.0563\t Accuracy 0.9608\n",
      "Epoch [15][20]\t Batch [450][1100]\t Training Loss 0.0562\t Accuracy 0.9605\n",
      "Epoch [15][20]\t Batch [500][1100]\t Training Loss 0.0560\t Accuracy 0.9607\n",
      "Epoch [15][20]\t Batch [550][1100]\t Training Loss 0.0563\t Accuracy 0.9604\n",
      "Epoch [15][20]\t Batch [600][1100]\t Training Loss 0.0565\t Accuracy 0.9598\n",
      "Epoch [15][20]\t Batch [650][1100]\t Training Loss 0.0570\t Accuracy 0.9594\n",
      "Epoch [15][20]\t Batch [700][1100]\t Training Loss 0.0565\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [750][1100]\t Training Loss 0.0566\t Accuracy 0.9599\n",
      "Epoch [15][20]\t Batch [800][1100]\t Training Loss 0.0566\t Accuracy 0.9599\n",
      "Epoch [15][20]\t Batch [850][1100]\t Training Loss 0.0568\t Accuracy 0.9595\n",
      "Epoch [15][20]\t Batch [900][1100]\t Training Loss 0.0565\t Accuracy 0.9598\n",
      "Epoch [15][20]\t Batch [950][1100]\t Training Loss 0.0566\t Accuracy 0.9597\n",
      "Epoch [15][20]\t Batch [1000][1100]\t Training Loss 0.0569\t Accuracy 0.9591\n",
      "Epoch [15][20]\t Batch [1050][1100]\t Training Loss 0.0569\t Accuracy 0.9591\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0568\t Average training accuracy 0.9592\n",
      "Epoch [15]\t Average validation loss 0.0529\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [16][20]\t Batch [0][1100]\t Training Loss 0.0643\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][1100]\t Training Loss 0.0540\t Accuracy 0.9655\n",
      "Epoch [16][20]\t Batch [100][1100]\t Training Loss 0.0525\t Accuracy 0.9661\n",
      "Epoch [16][20]\t Batch [150][1100]\t Training Loss 0.0533\t Accuracy 0.9652\n",
      "Epoch [16][20]\t Batch [200][1100]\t Training Loss 0.0544\t Accuracy 0.9637\n",
      "Epoch [16][20]\t Batch [250][1100]\t Training Loss 0.0543\t Accuracy 0.9629\n",
      "Epoch [16][20]\t Batch [300][1100]\t Training Loss 0.0559\t Accuracy 0.9613\n",
      "Epoch [16][20]\t Batch [350][1100]\t Training Loss 0.0560\t Accuracy 0.9615\n",
      "Epoch [16][20]\t Batch [400][1100]\t Training Loss 0.0555\t Accuracy 0.9615\n",
      "Epoch [16][20]\t Batch [450][1100]\t Training Loss 0.0555\t Accuracy 0.9613\n",
      "Epoch [16][20]\t Batch [500][1100]\t Training Loss 0.0553\t Accuracy 0.9615\n",
      "Epoch [16][20]\t Batch [550][1100]\t Training Loss 0.0555\t Accuracy 0.9613\n",
      "Epoch [16][20]\t Batch [600][1100]\t Training Loss 0.0558\t Accuracy 0.9607\n",
      "Epoch [16][20]\t Batch [650][1100]\t Training Loss 0.0562\t Accuracy 0.9603\n",
      "Epoch [16][20]\t Batch [700][1100]\t Training Loss 0.0557\t Accuracy 0.9609\n",
      "Epoch [16][20]\t Batch [750][1100]\t Training Loss 0.0558\t Accuracy 0.9607\n",
      "Epoch [16][20]\t Batch [800][1100]\t Training Loss 0.0559\t Accuracy 0.9607\n",
      "Epoch [16][20]\t Batch [850][1100]\t Training Loss 0.0560\t Accuracy 0.9604\n",
      "Epoch [16][20]\t Batch [900][1100]\t Training Loss 0.0558\t Accuracy 0.9607\n",
      "Epoch [16][20]\t Batch [950][1100]\t Training Loss 0.0559\t Accuracy 0.9605\n",
      "Epoch [16][20]\t Batch [1000][1100]\t Training Loss 0.0562\t Accuracy 0.9601\n",
      "Epoch [16][20]\t Batch [1050][1100]\t Training Loss 0.0562\t Accuracy 0.9601\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0561\t Average training accuracy 0.9602\n",
      "Epoch [16]\t Average validation loss 0.0525\t Average validation accuracy 0.9668\n",
      "\n",
      "Epoch [17][20]\t Batch [0][1100]\t Training Loss 0.0640\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][1100]\t Training Loss 0.0534\t Accuracy 0.9659\n",
      "Epoch [17][20]\t Batch [100][1100]\t Training Loss 0.0519\t Accuracy 0.9669\n",
      "Epoch [17][20]\t Batch [150][1100]\t Training Loss 0.0527\t Accuracy 0.9656\n",
      "Epoch [17][20]\t Batch [200][1100]\t Training Loss 0.0538\t Accuracy 0.9642\n",
      "Epoch [17][20]\t Batch [250][1100]\t Training Loss 0.0537\t Accuracy 0.9635\n",
      "Epoch [17][20]\t Batch [300][1100]\t Training Loss 0.0553\t Accuracy 0.9618\n",
      "Epoch [17][20]\t Batch [350][1100]\t Training Loss 0.0554\t Accuracy 0.9619\n",
      "Epoch [17][20]\t Batch [400][1100]\t Training Loss 0.0549\t Accuracy 0.9619\n",
      "Epoch [17][20]\t Batch [450][1100]\t Training Loss 0.0548\t Accuracy 0.9617\n",
      "Epoch [17][20]\t Batch [500][1100]\t Training Loss 0.0546\t Accuracy 0.9619\n",
      "Epoch [17][20]\t Batch [550][1100]\t Training Loss 0.0548\t Accuracy 0.9616\n",
      "Epoch [17][20]\t Batch [600][1100]\t Training Loss 0.0551\t Accuracy 0.9611\n",
      "Epoch [17][20]\t Batch [650][1100]\t Training Loss 0.0556\t Accuracy 0.9606\n",
      "Epoch [17][20]\t Batch [700][1100]\t Training Loss 0.0551\t Accuracy 0.9612\n",
      "Epoch [17][20]\t Batch [750][1100]\t Training Loss 0.0552\t Accuracy 0.9610\n",
      "Epoch [17][20]\t Batch [800][1100]\t Training Loss 0.0552\t Accuracy 0.9610\n",
      "Epoch [17][20]\t Batch [850][1100]\t Training Loss 0.0554\t Accuracy 0.9607\n",
      "Epoch [17][20]\t Batch [900][1100]\t Training Loss 0.0551\t Accuracy 0.9610\n",
      "Epoch [17][20]\t Batch [950][1100]\t Training Loss 0.0553\t Accuracy 0.9609\n",
      "Epoch [17][20]\t Batch [1000][1100]\t Training Loss 0.0555\t Accuracy 0.9605\n",
      "Epoch [17][20]\t Batch [1050][1100]\t Training Loss 0.0555\t Accuracy 0.9606\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0555\t Average training accuracy 0.9607\n",
      "Epoch [17]\t Average validation loss 0.0521\t Average validation accuracy 0.9668\n",
      "\n",
      "Epoch [18][20]\t Batch [0][1100]\t Training Loss 0.0635\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][1100]\t Training Loss 0.0528\t Accuracy 0.9659\n",
      "Epoch [18][20]\t Batch [100][1100]\t Training Loss 0.0513\t Accuracy 0.9669\n",
      "Epoch [18][20]\t Batch [150][1100]\t Training Loss 0.0521\t Accuracy 0.9657\n",
      "Epoch [18][20]\t Batch [200][1100]\t Training Loss 0.0533\t Accuracy 0.9647\n",
      "Epoch [18][20]\t Batch [250][1100]\t Training Loss 0.0531\t Accuracy 0.9639\n",
      "Epoch [18][20]\t Batch [300][1100]\t Training Loss 0.0547\t Accuracy 0.9625\n",
      "Epoch [18][20]\t Batch [350][1100]\t Training Loss 0.0548\t Accuracy 0.9625\n",
      "Epoch [18][20]\t Batch [400][1100]\t Training Loss 0.0543\t Accuracy 0.9625\n",
      "Epoch [18][20]\t Batch [450][1100]\t Training Loss 0.0542\t Accuracy 0.9623\n",
      "Epoch [18][20]\t Batch [500][1100]\t Training Loss 0.0540\t Accuracy 0.9625\n",
      "Epoch [18][20]\t Batch [550][1100]\t Training Loss 0.0542\t Accuracy 0.9624\n",
      "Epoch [18][20]\t Batch [600][1100]\t Training Loss 0.0545\t Accuracy 0.9619\n",
      "Epoch [18][20]\t Batch [650][1100]\t Training Loss 0.0550\t Accuracy 0.9613\n",
      "Epoch [18][20]\t Batch [700][1100]\t Training Loss 0.0545\t Accuracy 0.9619\n",
      "Epoch [18][20]\t Batch [750][1100]\t Training Loss 0.0546\t Accuracy 0.9617\n",
      "Epoch [18][20]\t Batch [800][1100]\t Training Loss 0.0546\t Accuracy 0.9617\n",
      "Epoch [18][20]\t Batch [850][1100]\t Training Loss 0.0548\t Accuracy 0.9614\n",
      "Epoch [18][20]\t Batch [900][1100]\t Training Loss 0.0546\t Accuracy 0.9616\n",
      "Epoch [18][20]\t Batch [950][1100]\t Training Loss 0.0547\t Accuracy 0.9614\n",
      "Epoch [18][20]\t Batch [1000][1100]\t Training Loss 0.0550\t Accuracy 0.9611\n",
      "Epoch [18][20]\t Batch [1050][1100]\t Training Loss 0.0549\t Accuracy 0.9612\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0549\t Average training accuracy 0.9613\n",
      "Epoch [18]\t Average validation loss 0.0518\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [19][20]\t Batch [0][1100]\t Training Loss 0.0631\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][1100]\t Training Loss 0.0522\t Accuracy 0.9667\n",
      "Epoch [19][20]\t Batch [100][1100]\t Training Loss 0.0508\t Accuracy 0.9677\n",
      "Epoch [19][20]\t Batch [150][1100]\t Training Loss 0.0516\t Accuracy 0.9661\n",
      "Epoch [19][20]\t Batch [200][1100]\t Training Loss 0.0527\t Accuracy 0.9649\n",
      "Epoch [19][20]\t Batch [250][1100]\t Training Loss 0.0526\t Accuracy 0.9643\n",
      "Epoch [19][20]\t Batch [300][1100]\t Training Loss 0.0541\t Accuracy 0.9631\n",
      "Epoch [19][20]\t Batch [350][1100]\t Training Loss 0.0542\t Accuracy 0.9631\n",
      "Epoch [19][20]\t Batch [400][1100]\t Training Loss 0.0537\t Accuracy 0.9632\n",
      "Epoch [19][20]\t Batch [450][1100]\t Training Loss 0.0536\t Accuracy 0.9630\n",
      "Epoch [19][20]\t Batch [500][1100]\t Training Loss 0.0534\t Accuracy 0.9632\n",
      "Epoch [19][20]\t Batch [550][1100]\t Training Loss 0.0537\t Accuracy 0.9630\n",
      "Epoch [19][20]\t Batch [600][1100]\t Training Loss 0.0540\t Accuracy 0.9625\n",
      "Epoch [19][20]\t Batch [650][1100]\t Training Loss 0.0544\t Accuracy 0.9619\n",
      "Epoch [19][20]\t Batch [700][1100]\t Training Loss 0.0539\t Accuracy 0.9625\n",
      "Epoch [19][20]\t Batch [750][1100]\t Training Loss 0.0540\t Accuracy 0.9623\n",
      "Epoch [19][20]\t Batch [800][1100]\t Training Loss 0.0541\t Accuracy 0.9622\n",
      "Epoch [19][20]\t Batch [850][1100]\t Training Loss 0.0542\t Accuracy 0.9620\n",
      "Epoch [19][20]\t Batch [900][1100]\t Training Loss 0.0540\t Accuracy 0.9622\n",
      "Epoch [19][20]\t Batch [950][1100]\t Training Loss 0.0541\t Accuracy 0.9621\n",
      "Epoch [19][20]\t Batch [1000][1100]\t Training Loss 0.0544\t Accuracy 0.9617\n",
      "Epoch [19][20]\t Batch [1050][1100]\t Training Loss 0.0544\t Accuracy 0.9618\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0543\t Average training accuracy 0.9619\n",
      "Epoch [19]\t Average validation loss 0.0514\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [0][20]\t Batch [0][1100]\t Training Loss 2.8660\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][1100]\t Training Loss 2.7539\t Accuracy 0.1275\n",
      "Epoch [0][20]\t Batch [100][1100]\t Training Loss 2.6614\t Accuracy 0.1382\n",
      "Epoch [0][20]\t Batch [150][1100]\t Training Loss 2.5820\t Accuracy 0.1444\n",
      "Epoch [0][20]\t Batch [200][1100]\t Training Loss 2.5270\t Accuracy 0.1494\n",
      "Epoch [0][20]\t Batch [250][1100]\t Training Loss 2.4765\t Accuracy 0.1578\n",
      "Epoch [0][20]\t Batch [300][1100]\t Training Loss 2.4372\t Accuracy 0.1667\n",
      "Epoch [0][20]\t Batch [350][1100]\t Training Loss 2.4035\t Accuracy 0.1789\n",
      "Epoch [0][20]\t Batch [400][1100]\t Training Loss 2.3757\t Accuracy 0.1903\n",
      "Epoch [0][20]\t Batch [450][1100]\t Training Loss 2.3490\t Accuracy 0.2049\n",
      "Epoch [0][20]\t Batch [500][1100]\t Training Loss 2.3264\t Accuracy 0.2173\n",
      "Epoch [0][20]\t Batch [550][1100]\t Training Loss 2.3056\t Accuracy 0.2309\n",
      "Epoch [0][20]\t Batch [600][1100]\t Training Loss 2.2860\t Accuracy 0.2451\n",
      "Epoch [0][20]\t Batch [650][1100]\t Training Loss 2.2687\t Accuracy 0.2577\n",
      "Epoch [0][20]\t Batch [700][1100]\t Training Loss 2.2512\t Accuracy 0.2721\n",
      "Epoch [0][20]\t Batch [750][1100]\t Training Loss 2.2349\t Accuracy 0.2854\n",
      "Epoch [0][20]\t Batch [800][1100]\t Training Loss 2.2193\t Accuracy 0.2986\n",
      "Epoch [0][20]\t Batch [850][1100]\t Training Loss 2.2049\t Accuracy 0.3104\n",
      "Epoch [0][20]\t Batch [900][1100]\t Training Loss 2.1907\t Accuracy 0.3227\n",
      "Epoch [0][20]\t Batch [950][1100]\t Training Loss 2.1773\t Accuracy 0.3333\n",
      "Epoch [0][20]\t Batch [1000][1100]\t Training Loss 2.1635\t Accuracy 0.3446\n",
      "Epoch [0][20]\t Batch [1050][1100]\t Training Loss 2.1503\t Accuracy 0.3549\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1374\t Average training accuracy 0.3652\n",
      "Epoch [0]\t Average validation loss 1.8323\t Average validation accuracy 0.6298\n",
      "\n",
      "Epoch [1][20]\t Batch [0][1100]\t Training Loss 1.8669\t Accuracy 0.6000\n",
      "Epoch [1][20]\t Batch [50][1100]\t Training Loss 1.8402\t Accuracy 0.6004\n",
      "Epoch [1][20]\t Batch [100][1100]\t Training Loss 1.8295\t Accuracy 0.6083\n",
      "Epoch [1][20]\t Batch [150][1100]\t Training Loss 1.8180\t Accuracy 0.6132\n",
      "Epoch [1][20]\t Batch [200][1100]\t Training Loss 1.8102\t Accuracy 0.6137\n",
      "Epoch [1][20]\t Batch [250][1100]\t Training Loss 1.7976\t Accuracy 0.6184\n",
      "Epoch [1][20]\t Batch [300][1100]\t Training Loss 1.7934\t Accuracy 0.6169\n",
      "Epoch [1][20]\t Batch [350][1100]\t Training Loss 1.7866\t Accuracy 0.6201\n",
      "Epoch [1][20]\t Batch [400][1100]\t Training Loss 1.7790\t Accuracy 0.6207\n",
      "Epoch [1][20]\t Batch [450][1100]\t Training Loss 1.7690\t Accuracy 0.6247\n",
      "Epoch [1][20]\t Batch [500][1100]\t Training Loss 1.7610\t Accuracy 0.6271\n",
      "Epoch [1][20]\t Batch [550][1100]\t Training Loss 1.7524\t Accuracy 0.6296\n",
      "Epoch [1][20]\t Batch [600][1100]\t Training Loss 1.7435\t Accuracy 0.6331\n",
      "Epoch [1][20]\t Batch [650][1100]\t Training Loss 1.7390\t Accuracy 0.6336\n",
      "Epoch [1][20]\t Batch [700][1100]\t Training Loss 1.7306\t Accuracy 0.6372\n",
      "Epoch [1][20]\t Batch [750][1100]\t Training Loss 1.7224\t Accuracy 0.6415\n",
      "Epoch [1][20]\t Batch [800][1100]\t Training Loss 1.7144\t Accuracy 0.6447\n",
      "Epoch [1][20]\t Batch [850][1100]\t Training Loss 1.7078\t Accuracy 0.6467\n",
      "Epoch [1][20]\t Batch [900][1100]\t Training Loss 1.7002\t Accuracy 0.6493\n",
      "Epoch [1][20]\t Batch [950][1100]\t Training Loss 1.6932\t Accuracy 0.6517\n",
      "Epoch [1][20]\t Batch [1000][1100]\t Training Loss 1.6854\t Accuracy 0.6546\n",
      "Epoch [1][20]\t Batch [1050][1100]\t Training Loss 1.6777\t Accuracy 0.6571\n",
      "\n",
      "Epoch [1]\t Average training loss 1.6700\t Average training accuracy 0.6604\n",
      "Epoch [1]\t Average validation loss 1.4618\t Average validation accuracy 0.7616\n",
      "\n",
      "Epoch [2][20]\t Batch [0][1100]\t Training Loss 1.5375\t Accuracy 0.7000\n",
      "Epoch [2][20]\t Batch [50][1100]\t Training Loss 1.4894\t Accuracy 0.7227\n",
      "Epoch [2][20]\t Batch [100][1100]\t Training Loss 1.4789\t Accuracy 0.7277\n",
      "Epoch [2][20]\t Batch [150][1100]\t Training Loss 1.4712\t Accuracy 0.7294\n",
      "Epoch [2][20]\t Batch [200][1100]\t Training Loss 1.4667\t Accuracy 0.7266\n",
      "Epoch [2][20]\t Batch [250][1100]\t Training Loss 1.4566\t Accuracy 0.7300\n",
      "Epoch [2][20]\t Batch [300][1100]\t Training Loss 1.4581\t Accuracy 0.7240\n",
      "Epoch [2][20]\t Batch [350][1100]\t Training Loss 1.4550\t Accuracy 0.7239\n",
      "Epoch [2][20]\t Batch [400][1100]\t Training Loss 1.4499\t Accuracy 0.7261\n",
      "Epoch [2][20]\t Batch [450][1100]\t Training Loss 1.4422\t Accuracy 0.7283\n",
      "Epoch [2][20]\t Batch [500][1100]\t Training Loss 1.4369\t Accuracy 0.7289\n",
      "Epoch [2][20]\t Batch [550][1100]\t Training Loss 1.4307\t Accuracy 0.7305\n",
      "Epoch [2][20]\t Batch [600][1100]\t Training Loss 1.4243\t Accuracy 0.7316\n",
      "Epoch [2][20]\t Batch [650][1100]\t Training Loss 1.4240\t Accuracy 0.7308\n",
      "Epoch [2][20]\t Batch [700][1100]\t Training Loss 1.4179\t Accuracy 0.7331\n",
      "Epoch [2][20]\t Batch [750][1100]\t Training Loss 1.4121\t Accuracy 0.7354\n",
      "Epoch [2][20]\t Batch [800][1100]\t Training Loss 1.4065\t Accuracy 0.7366\n",
      "Epoch [2][20]\t Batch [850][1100]\t Training Loss 1.4026\t Accuracy 0.7370\n",
      "Epoch [2][20]\t Batch [900][1100]\t Training Loss 1.3974\t Accuracy 0.7379\n",
      "Epoch [2][20]\t Batch [950][1100]\t Training Loss 1.3927\t Accuracy 0.7392\n",
      "Epoch [2][20]\t Batch [1000][1100]\t Training Loss 1.3872\t Accuracy 0.7403\n",
      "Epoch [2][20]\t Batch [1050][1100]\t Training Loss 1.3818\t Accuracy 0.7415\n",
      "\n",
      "Epoch [2]\t Average training loss 1.3763\t Average training accuracy 0.7431\n",
      "Epoch [2]\t Average validation loss 1.2039\t Average validation accuracy 0.8164\n",
      "\n",
      "Epoch [3][20]\t Batch [0][1100]\t Training Loss 1.3023\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][1100]\t Training Loss 1.2458\t Accuracy 0.7714\n",
      "Epoch [3][20]\t Batch [100][1100]\t Training Loss 1.2360\t Accuracy 0.7768\n",
      "Epoch [3][20]\t Batch [150][1100]\t Training Loss 1.2307\t Accuracy 0.7768\n",
      "Epoch [3][20]\t Batch [200][1100]\t Training Loss 1.2290\t Accuracy 0.7733\n",
      "Epoch [3][20]\t Batch [250][1100]\t Training Loss 1.2207\t Accuracy 0.7770\n",
      "Epoch [3][20]\t Batch [300][1100]\t Training Loss 1.2258\t Accuracy 0.7705\n",
      "Epoch [3][20]\t Batch [350][1100]\t Training Loss 1.2252\t Accuracy 0.7694\n",
      "Epoch [3][20]\t Batch [400][1100]\t Training Loss 1.2215\t Accuracy 0.7706\n",
      "Epoch [3][20]\t Batch [450][1100]\t Training Loss 1.2155\t Accuracy 0.7721\n",
      "Epoch [3][20]\t Batch [500][1100]\t Training Loss 1.2119\t Accuracy 0.7725\n",
      "Epoch [3][20]\t Batch [550][1100]\t Training Loss 1.2073\t Accuracy 0.7739\n",
      "Epoch [3][20]\t Batch [600][1100]\t Training Loss 1.2027\t Accuracy 0.7751\n",
      "Epoch [3][20]\t Batch [650][1100]\t Training Loss 1.2051\t Accuracy 0.7735\n",
      "Epoch [3][20]\t Batch [700][1100]\t Training Loss 1.2006\t Accuracy 0.7753\n",
      "Epoch [3][20]\t Batch [750][1100]\t Training Loss 1.1962\t Accuracy 0.7775\n",
      "Epoch [3][20]\t Batch [800][1100]\t Training Loss 1.1923\t Accuracy 0.7781\n",
      "Epoch [3][20]\t Batch [850][1100]\t Training Loss 1.1902\t Accuracy 0.7779\n",
      "Epoch [3][20]\t Batch [900][1100]\t Training Loss 1.1864\t Accuracy 0.7786\n",
      "Epoch [3][20]\t Batch [950][1100]\t Training Loss 1.1833\t Accuracy 0.7792\n",
      "Epoch [3][20]\t Batch [1000][1100]\t Training Loss 1.1794\t Accuracy 0.7799\n",
      "Epoch [3][20]\t Batch [1050][1100]\t Training Loss 1.1755\t Accuracy 0.7805\n",
      "\n",
      "Epoch [3]\t Average training loss 1.1714\t Average training accuracy 0.7816\n",
      "Epoch [3]\t Average validation loss 1.0228\t Average validation accuracy 0.8456\n",
      "\n",
      "Epoch [4][20]\t Batch [0][1100]\t Training Loss 1.1335\t Accuracy 0.8000\n",
      "Epoch [4][20]\t Batch [50][1100]\t Training Loss 1.0747\t Accuracy 0.7957\n",
      "Epoch [4][20]\t Batch [100][1100]\t Training Loss 1.0655\t Accuracy 0.8042\n",
      "Epoch [4][20]\t Batch [150][1100]\t Training Loss 1.0617\t Accuracy 0.8048\n",
      "Epoch [4][20]\t Batch [200][1100]\t Training Loss 1.0621\t Accuracy 0.8008\n",
      "Epoch [4][20]\t Batch [250][1100]\t Training Loss 1.0550\t Accuracy 0.8045\n",
      "Epoch [4][20]\t Batch [300][1100]\t Training Loss 1.0626\t Accuracy 0.7991\n",
      "Epoch [4][20]\t Batch [350][1100]\t Training Loss 1.0635\t Accuracy 0.7975\n",
      "Epoch [4][20]\t Batch [400][1100]\t Training Loss 1.0605\t Accuracy 0.7996\n",
      "Epoch [4][20]\t Batch [450][1100]\t Training Loss 1.0556\t Accuracy 0.8014\n",
      "Epoch [4][20]\t Batch [500][1100]\t Training Loss 1.0532\t Accuracy 0.8010\n",
      "Epoch [4][20]\t Batch [550][1100]\t Training Loss 1.0497\t Accuracy 0.8021\n",
      "Epoch [4][20]\t Batch [600][1100]\t Training Loss 1.0463\t Accuracy 0.8029\n",
      "Epoch [4][20]\t Batch [650][1100]\t Training Loss 1.0504\t Accuracy 0.8007\n",
      "Epoch [4][20]\t Batch [700][1100]\t Training Loss 1.0468\t Accuracy 0.8021\n",
      "Epoch [4][20]\t Batch [750][1100]\t Training Loss 1.0434\t Accuracy 0.8034\n",
      "Epoch [4][20]\t Batch [800][1100]\t Training Loss 1.0407\t Accuracy 0.8036\n",
      "Epoch [4][20]\t Batch [850][1100]\t Training Loss 1.0397\t Accuracy 0.8032\n",
      "Epoch [4][20]\t Batch [900][1100]\t Training Loss 1.0368\t Accuracy 0.8035\n",
      "Epoch [4][20]\t Batch [950][1100]\t Training Loss 1.0347\t Accuracy 0.8037\n",
      "Epoch [4][20]\t Batch [1000][1100]\t Training Loss 1.0318\t Accuracy 0.8038\n",
      "Epoch [4][20]\t Batch [1050][1100]\t Training Loss 1.0289\t Accuracy 0.8043\n",
      "\n",
      "Epoch [4]\t Average training loss 1.0259\t Average training accuracy 0.8049\n",
      "Epoch [4]\t Average validation loss 0.8926\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [5][20]\t Batch [0][1100]\t Training Loss 1.0100\t Accuracy 0.8000\n",
      "Epoch [5][20]\t Batch [50][1100]\t Training Loss 0.9512\t Accuracy 0.8114\n",
      "Epoch [5][20]\t Batch [100][1100]\t Training Loss 0.9425\t Accuracy 0.8190\n",
      "Epoch [5][20]\t Batch [150][1100]\t Training Loss 0.9396\t Accuracy 0.8193\n",
      "Epoch [5][20]\t Batch [200][1100]\t Training Loss 0.9416\t Accuracy 0.8166\n",
      "Epoch [5][20]\t Batch [250][1100]\t Training Loss 0.9354\t Accuracy 0.8202\n",
      "Epoch [5][20]\t Batch [300][1100]\t Training Loss 0.9446\t Accuracy 0.8150\n",
      "Epoch [5][20]\t Batch [350][1100]\t Training Loss 0.9464\t Accuracy 0.8137\n",
      "Epoch [5][20]\t Batch [400][1100]\t Training Loss 0.9438\t Accuracy 0.8158\n",
      "Epoch [5][20]\t Batch [450][1100]\t Training Loss 0.9397\t Accuracy 0.8174\n",
      "Epoch [5][20]\t Batch [500][1100]\t Training Loss 0.9380\t Accuracy 0.8172\n",
      "Epoch [5][20]\t Batch [550][1100]\t Training Loss 0.9353\t Accuracy 0.8180\n",
      "Epoch [5][20]\t Batch [600][1100]\t Training Loss 0.9326\t Accuracy 0.8187\n",
      "Epoch [5][20]\t Batch [650][1100]\t Training Loss 0.9378\t Accuracy 0.8165\n",
      "Epoch [5][20]\t Batch [700][1100]\t Training Loss 0.9348\t Accuracy 0.8177\n",
      "Epoch [5][20]\t Batch [750][1100]\t Training Loss 0.9321\t Accuracy 0.8186\n",
      "Epoch [5][20]\t Batch [800][1100]\t Training Loss 0.9301\t Accuracy 0.8188\n",
      "Epoch [5][20]\t Batch [850][1100]\t Training Loss 0.9298\t Accuracy 0.8180\n",
      "Epoch [5][20]\t Batch [900][1100]\t Training Loss 0.9275\t Accuracy 0.8184\n",
      "Epoch [5][20]\t Batch [950][1100]\t Training Loss 0.9261\t Accuracy 0.8187\n",
      "Epoch [5][20]\t Batch [1000][1100]\t Training Loss 0.9240\t Accuracy 0.8187\n",
      "Epoch [5][20]\t Batch [1050][1100]\t Training Loss 0.9217\t Accuracy 0.8188\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9194\t Average training accuracy 0.8193\n",
      "Epoch [5]\t Average validation loss 0.7960\t Average validation accuracy 0.8726\n",
      "\n",
      "Epoch [6][20]\t Batch [0][1100]\t Training Loss 0.9171\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][1100]\t Training Loss 0.8593\t Accuracy 0.8275\n",
      "Epoch [6][20]\t Batch [100][1100]\t Training Loss 0.8509\t Accuracy 0.8333\n",
      "Epoch [6][20]\t Batch [150][1100]\t Training Loss 0.8485\t Accuracy 0.8331\n",
      "Epoch [6][20]\t Batch [200][1100]\t Training Loss 0.8518\t Accuracy 0.8308\n",
      "Epoch [6][20]\t Batch [250][1100]\t Training Loss 0.8462\t Accuracy 0.8325\n",
      "Epoch [6][20]\t Batch [300][1100]\t Training Loss 0.8565\t Accuracy 0.8267\n",
      "Epoch [6][20]\t Batch [350][1100]\t Training Loss 0.8590\t Accuracy 0.8254\n",
      "Epoch [6][20]\t Batch [400][1100]\t Training Loss 0.8565\t Accuracy 0.8278\n",
      "Epoch [6][20]\t Batch [450][1100]\t Training Loss 0.8529\t Accuracy 0.8294\n",
      "Epoch [6][20]\t Batch [500][1100]\t Training Loss 0.8517\t Accuracy 0.8293\n",
      "Epoch [6][20]\t Batch [550][1100]\t Training Loss 0.8495\t Accuracy 0.8301\n",
      "Epoch [6][20]\t Batch [600][1100]\t Training Loss 0.8475\t Accuracy 0.8307\n",
      "Epoch [6][20]\t Batch [650][1100]\t Training Loss 0.8533\t Accuracy 0.8283\n",
      "Epoch [6][20]\t Batch [700][1100]\t Training Loss 0.8507\t Accuracy 0.8294\n",
      "Epoch [6][20]\t Batch [750][1100]\t Training Loss 0.8484\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [800][1100]\t Training Loss 0.8468\t Accuracy 0.8301\n",
      "Epoch [6][20]\t Batch [850][1100]\t Training Loss 0.8470\t Accuracy 0.8294\n",
      "Epoch [6][20]\t Batch [900][1100]\t Training Loss 0.8452\t Accuracy 0.8296\n",
      "Epoch [6][20]\t Batch [950][1100]\t Training Loss 0.8442\t Accuracy 0.8298\n",
      "Epoch [6][20]\t Batch [1000][1100]\t Training Loss 0.8426\t Accuracy 0.8297\n",
      "Epoch [6][20]\t Batch [1050][1100]\t Training Loss 0.8408\t Accuracy 0.8300\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8389\t Average training accuracy 0.8302\n",
      "Epoch [6]\t Average validation loss 0.7222\t Average validation accuracy 0.8802\n",
      "\n",
      "Epoch [7][20]\t Batch [0][1100]\t Training Loss 0.8453\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][1100]\t Training Loss 0.7886\t Accuracy 0.8357\n",
      "Epoch [7][20]\t Batch [100][1100]\t Training Loss 0.7804\t Accuracy 0.8412\n",
      "Epoch [7][20]\t Batch [150][1100]\t Training Loss 0.7784\t Accuracy 0.8411\n",
      "Epoch [7][20]\t Batch [200][1100]\t Training Loss 0.7828\t Accuracy 0.8386\n",
      "Epoch [7][20]\t Batch [250][1100]\t Training Loss 0.7775\t Accuracy 0.8403\n",
      "Epoch [7][20]\t Batch [300][1100]\t Training Loss 0.7887\t Accuracy 0.8347\n",
      "Epoch [7][20]\t Batch [350][1100]\t Training Loss 0.7915\t Accuracy 0.8338\n",
      "Epoch [7][20]\t Batch [400][1100]\t Training Loss 0.7891\t Accuracy 0.8359\n",
      "Epoch [7][20]\t Batch [450][1100]\t Training Loss 0.7859\t Accuracy 0.8371\n",
      "Epoch [7][20]\t Batch [500][1100]\t Training Loss 0.7851\t Accuracy 0.8370\n",
      "Epoch [7][20]\t Batch [550][1100]\t Training Loss 0.7832\t Accuracy 0.8377\n",
      "Epoch [7][20]\t Batch [600][1100]\t Training Loss 0.7816\t Accuracy 0.8379\n",
      "Epoch [7][20]\t Batch [650][1100]\t Training Loss 0.7879\t Accuracy 0.8356\n",
      "Epoch [7][20]\t Batch [700][1100]\t Training Loss 0.7855\t Accuracy 0.8369\n",
      "Epoch [7][20]\t Batch [750][1100]\t Training Loss 0.7835\t Accuracy 0.8375\n",
      "Epoch [7][20]\t Batch [800][1100]\t Training Loss 0.7823\t Accuracy 0.8378\n",
      "Epoch [7][20]\t Batch [850][1100]\t Training Loss 0.7828\t Accuracy 0.8371\n",
      "Epoch [7][20]\t Batch [900][1100]\t Training Loss 0.7813\t Accuracy 0.8374\n",
      "Epoch [7][20]\t Batch [950][1100]\t Training Loss 0.7806\t Accuracy 0.8374\n",
      "Epoch [7][20]\t Batch [1000][1100]\t Training Loss 0.7794\t Accuracy 0.8371\n",
      "Epoch [7][20]\t Batch [1050][1100]\t Training Loss 0.7780\t Accuracy 0.8372\n",
      "\n",
      "Epoch [7]\t Average training loss 0.7764\t Average training accuracy 0.8374\n",
      "Epoch [7]\t Average validation loss 0.6643\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [8][20]\t Batch [0][1100]\t Training Loss 0.7885\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [50][1100]\t Training Loss 0.7328\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [100][1100]\t Training Loss 0.7247\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [150][1100]\t Training Loss 0.7229\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [200][1100]\t Training Loss 0.7281\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [250][1100]\t Training Loss 0.7232\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [300][1100]\t Training Loss 0.7350\t Accuracy 0.8416\n",
      "Epoch [8][20]\t Batch [350][1100]\t Training Loss 0.7381\t Accuracy 0.8405\n",
      "Epoch [8][20]\t Batch [400][1100]\t Training Loss 0.7356\t Accuracy 0.8424\n",
      "Epoch [8][20]\t Batch [450][1100]\t Training Loss 0.7328\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [500][1100]\t Training Loss 0.7323\t Accuracy 0.8433\n",
      "Epoch [8][20]\t Batch [550][1100]\t Training Loss 0.7307\t Accuracy 0.8442\n",
      "Epoch [8][20]\t Batch [600][1100]\t Training Loss 0.7293\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [650][1100]\t Training Loss 0.7359\t Accuracy 0.8421\n",
      "Epoch [8][20]\t Batch [700][1100]\t Training Loss 0.7336\t Accuracy 0.8432\n",
      "Epoch [8][20]\t Batch [750][1100]\t Training Loss 0.7319\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [800][1100]\t Training Loss 0.7310\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [850][1100]\t Training Loss 0.7316\t Accuracy 0.8436\n",
      "Epoch [8][20]\t Batch [900][1100]\t Training Loss 0.7303\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [950][1100]\t Training Loss 0.7299\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [1000][1100]\t Training Loss 0.7290\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [1050][1100]\t Training Loss 0.7278\t Accuracy 0.8436\n",
      "\n",
      "Epoch [8]\t Average training loss 0.7265\t Average training accuracy 0.8437\n",
      "Epoch [8]\t Average validation loss 0.6177\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [9][20]\t Batch [0][1100]\t Training Loss 0.7424\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][1100]\t Training Loss 0.6876\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [100][1100]\t Training Loss 0.6796\t Accuracy 0.8562\n",
      "Epoch [9][20]\t Batch [150][1100]\t Training Loss 0.6780\t Accuracy 0.8547\n",
      "Epoch [9][20]\t Batch [200][1100]\t Training Loss 0.6839\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [250][1100]\t Training Loss 0.6792\t Accuracy 0.8527\n",
      "Epoch [9][20]\t Batch [300][1100]\t Training Loss 0.6915\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [350][1100]\t Training Loss 0.6947\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [400][1100]\t Training Loss 0.6923\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [450][1100]\t Training Loss 0.6897\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [500][1100]\t Training Loss 0.6893\t Accuracy 0.8483\n",
      "Epoch [9][20]\t Batch [550][1100]\t Training Loss 0.6879\t Accuracy 0.8494\n",
      "Epoch [9][20]\t Batch [600][1100]\t Training Loss 0.6869\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [650][1100]\t Training Loss 0.6936\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [700][1100]\t Training Loss 0.6914\t Accuracy 0.8488\n",
      "Epoch [9][20]\t Batch [750][1100]\t Training Loss 0.6899\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [800][1100]\t Training Loss 0.6891\t Accuracy 0.8499\n",
      "Epoch [9][20]\t Batch [850][1100]\t Training Loss 0.6899\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [900][1100]\t Training Loss 0.6888\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [950][1100]\t Training Loss 0.6885\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [1000][1100]\t Training Loss 0.6879\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [1050][1100]\t Training Loss 0.6869\t Accuracy 0.8489\n",
      "\n",
      "Epoch [9]\t Average training loss 0.6858\t Average training accuracy 0.8490\n",
      "Epoch [9]\t Average validation loss 0.5796\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [10][20]\t Batch [0][1100]\t Training Loss 0.7042\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][1100]\t Training Loss 0.6502\t Accuracy 0.8576\n",
      "Epoch [10][20]\t Batch [100][1100]\t Training Loss 0.6423\t Accuracy 0.8630\n",
      "Epoch [10][20]\t Batch [150][1100]\t Training Loss 0.6408\t Accuracy 0.8616\n",
      "Epoch [10][20]\t Batch [200][1100]\t Training Loss 0.6473\t Accuracy 0.8581\n",
      "Epoch [10][20]\t Batch [250][1100]\t Training Loss 0.6429\t Accuracy 0.8584\n",
      "Epoch [10][20]\t Batch [300][1100]\t Training Loss 0.6555\t Accuracy 0.8530\n",
      "Epoch [10][20]\t Batch [350][1100]\t Training Loss 0.6588\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [400][1100]\t Training Loss 0.6564\t Accuracy 0.8538\n",
      "Epoch [10][20]\t Batch [450][1100]\t Training Loss 0.6539\t Accuracy 0.8543\n",
      "Epoch [10][20]\t Batch [500][1100]\t Training Loss 0.6537\t Accuracy 0.8541\n",
      "Epoch [10][20]\t Batch [550][1100]\t Training Loss 0.6525\t Accuracy 0.8550\n",
      "Epoch [10][20]\t Batch [600][1100]\t Training Loss 0.6516\t Accuracy 0.8549\n",
      "Epoch [10][20]\t Batch [650][1100]\t Training Loss 0.6585\t Accuracy 0.8526\n",
      "Epoch [10][20]\t Batch [700][1100]\t Training Loss 0.6564\t Accuracy 0.8538\n",
      "Epoch [10][20]\t Batch [750][1100]\t Training Loss 0.6550\t Accuracy 0.8542\n",
      "Epoch [10][20]\t Batch [800][1100]\t Training Loss 0.6543\t Accuracy 0.8548\n",
      "Epoch [10][20]\t Batch [850][1100]\t Training Loss 0.6553\t Accuracy 0.8542\n",
      "Epoch [10][20]\t Batch [900][1100]\t Training Loss 0.6542\t Accuracy 0.8543\n",
      "Epoch [10][20]\t Batch [950][1100]\t Training Loss 0.6541\t Accuracy 0.8539\n",
      "Epoch [10][20]\t Batch [1000][1100]\t Training Loss 0.6536\t Accuracy 0.8535\n",
      "Epoch [10][20]\t Batch [1050][1100]\t Training Loss 0.6528\t Accuracy 0.8535\n",
      "\n",
      "Epoch [10]\t Average training loss 0.6519\t Average training accuracy 0.8534\n",
      "Epoch [10]\t Average validation loss 0.5478\t Average validation accuracy 0.8942\n",
      "\n",
      "Epoch [11][20]\t Batch [0][1100]\t Training Loss 0.6721\t Accuracy 0.8400\n",
      "Epoch [11][20]\t Batch [50][1100]\t Training Loss 0.6189\t Accuracy 0.8627\n",
      "Epoch [11][20]\t Batch [100][1100]\t Training Loss 0.6109\t Accuracy 0.8667\n",
      "Epoch [11][20]\t Batch [150][1100]\t Training Loss 0.6095\t Accuracy 0.8669\n",
      "Epoch [11][20]\t Batch [200][1100]\t Training Loss 0.6165\t Accuracy 0.8632\n",
      "Epoch [11][20]\t Batch [250][1100]\t Training Loss 0.6123\t Accuracy 0.8632\n",
      "Epoch [11][20]\t Batch [300][1100]\t Training Loss 0.6252\t Accuracy 0.8577\n",
      "Epoch [11][20]\t Batch [350][1100]\t Training Loss 0.6286\t Accuracy 0.8562\n",
      "Epoch [11][20]\t Batch [400][1100]\t Training Loss 0.6261\t Accuracy 0.8577\n",
      "Epoch [11][20]\t Batch [450][1100]\t Training Loss 0.6238\t Accuracy 0.8580\n",
      "Epoch [11][20]\t Batch [500][1100]\t Training Loss 0.6238\t Accuracy 0.8578\n",
      "Epoch [11][20]\t Batch [550][1100]\t Training Loss 0.6227\t Accuracy 0.8586\n",
      "Epoch [11][20]\t Batch [600][1100]\t Training Loss 0.6220\t Accuracy 0.8588\n",
      "Epoch [11][20]\t Batch [650][1100]\t Training Loss 0.6289\t Accuracy 0.8563\n",
      "Epoch [11][20]\t Batch [700][1100]\t Training Loss 0.6268\t Accuracy 0.8576\n",
      "Epoch [11][20]\t Batch [750][1100]\t Training Loss 0.6255\t Accuracy 0.8580\n",
      "Epoch [11][20]\t Batch [800][1100]\t Training Loss 0.6250\t Accuracy 0.8585\n",
      "Epoch [11][20]\t Batch [850][1100]\t Training Loss 0.6260\t Accuracy 0.8579\n",
      "Epoch [11][20]\t Batch [900][1100]\t Training Loss 0.6250\t Accuracy 0.8580\n",
      "Epoch [11][20]\t Batch [950][1100]\t Training Loss 0.6250\t Accuracy 0.8577\n",
      "Epoch [11][20]\t Batch [1000][1100]\t Training Loss 0.6247\t Accuracy 0.8572\n",
      "Epoch [11][20]\t Batch [1050][1100]\t Training Loss 0.6240\t Accuracy 0.8572\n",
      "\n",
      "Epoch [11]\t Average training loss 0.6232\t Average training accuracy 0.8571\n",
      "Epoch [11]\t Average validation loss 0.5208\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [12][20]\t Batch [0][1100]\t Training Loss 0.6448\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][1100]\t Training Loss 0.5921\t Accuracy 0.8651\n",
      "Epoch [12][20]\t Batch [100][1100]\t Training Loss 0.5841\t Accuracy 0.8695\n",
      "Epoch [12][20]\t Batch [150][1100]\t Training Loss 0.5828\t Accuracy 0.8706\n",
      "Epoch [12][20]\t Batch [200][1100]\t Training Loss 0.5903\t Accuracy 0.8669\n",
      "Epoch [12][20]\t Batch [250][1100]\t Training Loss 0.5862\t Accuracy 0.8670\n",
      "Epoch [12][20]\t Batch [300][1100]\t Training Loss 0.5994\t Accuracy 0.8618\n",
      "Epoch [12][20]\t Batch [350][1100]\t Training Loss 0.6028\t Accuracy 0.8604\n",
      "Epoch [12][20]\t Batch [400][1100]\t Training Loss 0.6003\t Accuracy 0.8616\n",
      "Epoch [12][20]\t Batch [450][1100]\t Training Loss 0.5981\t Accuracy 0.8620\n",
      "Epoch [12][20]\t Batch [500][1100]\t Training Loss 0.5981\t Accuracy 0.8617\n",
      "Epoch [12][20]\t Batch [550][1100]\t Training Loss 0.5971\t Accuracy 0.8625\n",
      "Epoch [12][20]\t Batch [600][1100]\t Training Loss 0.5966\t Accuracy 0.8626\n",
      "Epoch [12][20]\t Batch [650][1100]\t Training Loss 0.6036\t Accuracy 0.8600\n",
      "Epoch [12][20]\t Batch [700][1100]\t Training Loss 0.6015\t Accuracy 0.8613\n",
      "Epoch [12][20]\t Batch [750][1100]\t Training Loss 0.6003\t Accuracy 0.8616\n",
      "Epoch [12][20]\t Batch [800][1100]\t Training Loss 0.5998\t Accuracy 0.8621\n",
      "Epoch [12][20]\t Batch [850][1100]\t Training Loss 0.6009\t Accuracy 0.8614\n",
      "Epoch [12][20]\t Batch [900][1100]\t Training Loss 0.6000\t Accuracy 0.8614\n",
      "Epoch [12][20]\t Batch [950][1100]\t Training Loss 0.6001\t Accuracy 0.8611\n",
      "Epoch [12][20]\t Batch [1000][1100]\t Training Loss 0.5999\t Accuracy 0.8606\n",
      "Epoch [12][20]\t Batch [1050][1100]\t Training Loss 0.5993\t Accuracy 0.8606\n",
      "\n",
      "Epoch [12]\t Average training loss 0.5986\t Average training accuracy 0.8605\n",
      "Epoch [12]\t Average validation loss 0.4978\t Average validation accuracy 0.8988\n",
      "\n",
      "Epoch [13][20]\t Batch [0][1100]\t Training Loss 0.6211\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][1100]\t Training Loss 0.5690\t Accuracy 0.8690\n",
      "Epoch [13][20]\t Batch [100][1100]\t Training Loss 0.5609\t Accuracy 0.8721\n",
      "Epoch [13][20]\t Batch [150][1100]\t Training Loss 0.5598\t Accuracy 0.8734\n",
      "Epoch [13][20]\t Batch [200][1100]\t Training Loss 0.5676\t Accuracy 0.8702\n",
      "Epoch [13][20]\t Batch [250][1100]\t Training Loss 0.5637\t Accuracy 0.8706\n",
      "Epoch [13][20]\t Batch [300][1100]\t Training Loss 0.5770\t Accuracy 0.8654\n",
      "Epoch [13][20]\t Batch [350][1100]\t Training Loss 0.5805\t Accuracy 0.8640\n",
      "Epoch [13][20]\t Batch [400][1100]\t Training Loss 0.5779\t Accuracy 0.8654\n",
      "Epoch [13][20]\t Batch [450][1100]\t Training Loss 0.5759\t Accuracy 0.8659\n",
      "Epoch [13][20]\t Batch [500][1100]\t Training Loss 0.5760\t Accuracy 0.8655\n",
      "Epoch [13][20]\t Batch [550][1100]\t Training Loss 0.5751\t Accuracy 0.8662\n",
      "Epoch [13][20]\t Batch [600][1100]\t Training Loss 0.5746\t Accuracy 0.8661\n",
      "Epoch [13][20]\t Batch [650][1100]\t Training Loss 0.5816\t Accuracy 0.8636\n",
      "Epoch [13][20]\t Batch [700][1100]\t Training Loss 0.5796\t Accuracy 0.8647\n",
      "Epoch [13][20]\t Batch [750][1100]\t Training Loss 0.5784\t Accuracy 0.8649\n",
      "Epoch [13][20]\t Batch [800][1100]\t Training Loss 0.5781\t Accuracy 0.8653\n",
      "Epoch [13][20]\t Batch [850][1100]\t Training Loss 0.5792\t Accuracy 0.8646\n",
      "Epoch [13][20]\t Batch [900][1100]\t Training Loss 0.5784\t Accuracy 0.8647\n",
      "Epoch [13][20]\t Batch [950][1100]\t Training Loss 0.5784\t Accuracy 0.8643\n",
      "Epoch [13][20]\t Batch [1000][1100]\t Training Loss 0.5784\t Accuracy 0.8638\n",
      "Epoch [13][20]\t Batch [1050][1100]\t Training Loss 0.5780\t Accuracy 0.8637\n",
      "\n",
      "Epoch [13]\t Average training loss 0.5773\t Average training accuracy 0.8635\n",
      "Epoch [13]\t Average validation loss 0.4778\t Average validation accuracy 0.9010\n",
      "\n",
      "Epoch [14][20]\t Batch [0][1100]\t Training Loss 0.6003\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][1100]\t Training Loss 0.5489\t Accuracy 0.8722\n",
      "Epoch [14][20]\t Batch [100][1100]\t Training Loss 0.5407\t Accuracy 0.8760\n",
      "Epoch [14][20]\t Batch [150][1100]\t Training Loss 0.5396\t Accuracy 0.8764\n",
      "Epoch [14][20]\t Batch [200][1100]\t Training Loss 0.5478\t Accuracy 0.8729\n",
      "Epoch [14][20]\t Batch [250][1100]\t Training Loss 0.5440\t Accuracy 0.8735\n",
      "Epoch [14][20]\t Batch [300][1100]\t Training Loss 0.5575\t Accuracy 0.8679\n",
      "Epoch [14][20]\t Batch [350][1100]\t Training Loss 0.5610\t Accuracy 0.8667\n",
      "Epoch [14][20]\t Batch [400][1100]\t Training Loss 0.5584\t Accuracy 0.8680\n",
      "Epoch [14][20]\t Batch [450][1100]\t Training Loss 0.5564\t Accuracy 0.8687\n",
      "Epoch [14][20]\t Batch [500][1100]\t Training Loss 0.5566\t Accuracy 0.8681\n",
      "Epoch [14][20]\t Batch [550][1100]\t Training Loss 0.5558\t Accuracy 0.8687\n",
      "Epoch [14][20]\t Batch [600][1100]\t Training Loss 0.5554\t Accuracy 0.8686\n",
      "Epoch [14][20]\t Batch [650][1100]\t Training Loss 0.5624\t Accuracy 0.8660\n",
      "Epoch [14][20]\t Batch [700][1100]\t Training Loss 0.5604\t Accuracy 0.8672\n",
      "Epoch [14][20]\t Batch [750][1100]\t Training Loss 0.5593\t Accuracy 0.8673\n",
      "Epoch [14][20]\t Batch [800][1100]\t Training Loss 0.5590\t Accuracy 0.8677\n",
      "Epoch [14][20]\t Batch [850][1100]\t Training Loss 0.5602\t Accuracy 0.8669\n",
      "Epoch [14][20]\t Batch [900][1100]\t Training Loss 0.5594\t Accuracy 0.8671\n",
      "Epoch [14][20]\t Batch [950][1100]\t Training Loss 0.5595\t Accuracy 0.8667\n",
      "Epoch [14][20]\t Batch [1000][1100]\t Training Loss 0.5596\t Accuracy 0.8662\n",
      "Epoch [14][20]\t Batch [1050][1100]\t Training Loss 0.5592\t Accuracy 0.8661\n",
      "\n",
      "Epoch [14]\t Average training loss 0.5587\t Average training accuracy 0.8660\n",
      "Epoch [14]\t Average validation loss 0.4603\t Average validation accuracy 0.9022\n",
      "\n",
      "Epoch [15][20]\t Batch [0][1100]\t Training Loss 0.5820\t Accuracy 0.8800\n",
      "Epoch [15][20]\t Batch [50][1100]\t Training Loss 0.5312\t Accuracy 0.8757\n",
      "Epoch [15][20]\t Batch [100][1100]\t Training Loss 0.5229\t Accuracy 0.8794\n",
      "Epoch [15][20]\t Batch [150][1100]\t Training Loss 0.5219\t Accuracy 0.8799\n",
      "Epoch [15][20]\t Batch [200][1100]\t Training Loss 0.5304\t Accuracy 0.8763\n",
      "Epoch [15][20]\t Batch [250][1100]\t Training Loss 0.5267\t Accuracy 0.8767\n",
      "Epoch [15][20]\t Batch [300][1100]\t Training Loss 0.5404\t Accuracy 0.8714\n",
      "Epoch [15][20]\t Batch [350][1100]\t Training Loss 0.5438\t Accuracy 0.8699\n",
      "Epoch [15][20]\t Batch [400][1100]\t Training Loss 0.5411\t Accuracy 0.8712\n",
      "Epoch [15][20]\t Batch [450][1100]\t Training Loss 0.5393\t Accuracy 0.8720\n",
      "Epoch [15][20]\t Batch [500][1100]\t Training Loss 0.5395\t Accuracy 0.8717\n",
      "Epoch [15][20]\t Batch [550][1100]\t Training Loss 0.5387\t Accuracy 0.8722\n",
      "Epoch [15][20]\t Batch [600][1100]\t Training Loss 0.5385\t Accuracy 0.8719\n",
      "Epoch [15][20]\t Batch [650][1100]\t Training Loss 0.5455\t Accuracy 0.8692\n",
      "Epoch [15][20]\t Batch [700][1100]\t Training Loss 0.5434\t Accuracy 0.8703\n",
      "Epoch [15][20]\t Batch [750][1100]\t Training Loss 0.5424\t Accuracy 0.8705\n",
      "Epoch [15][20]\t Batch [800][1100]\t Training Loss 0.5422\t Accuracy 0.8708\n",
      "Epoch [15][20]\t Batch [850][1100]\t Training Loss 0.5433\t Accuracy 0.8702\n",
      "Epoch [15][20]\t Batch [900][1100]\t Training Loss 0.5426\t Accuracy 0.8703\n",
      "Epoch [15][20]\t Batch [950][1100]\t Training Loss 0.5428\t Accuracy 0.8699\n",
      "Epoch [15][20]\t Batch [1000][1100]\t Training Loss 0.5430\t Accuracy 0.8694\n",
      "Epoch [15][20]\t Batch [1050][1100]\t Training Loss 0.5426\t Accuracy 0.8691\n",
      "\n",
      "Epoch [15]\t Average training loss 0.5422\t Average training accuracy 0.8689\n",
      "Epoch [15]\t Average validation loss 0.4449\t Average validation accuracy 0.9044\n",
      "\n",
      "Epoch [16][20]\t Batch [0][1100]\t Training Loss 0.5656\t Accuracy 0.8800\n",
      "Epoch [16][20]\t Batch [50][1100]\t Training Loss 0.5155\t Accuracy 0.8773\n",
      "Epoch [16][20]\t Batch [100][1100]\t Training Loss 0.5070\t Accuracy 0.8812\n",
      "Epoch [16][20]\t Batch [150][1100]\t Training Loss 0.5062\t Accuracy 0.8819\n",
      "Epoch [16][20]\t Batch [200][1100]\t Training Loss 0.5149\t Accuracy 0.8787\n",
      "Epoch [16][20]\t Batch [250][1100]\t Training Loss 0.5113\t Accuracy 0.8792\n",
      "Epoch [16][20]\t Batch [300][1100]\t Training Loss 0.5251\t Accuracy 0.8738\n",
      "Epoch [16][20]\t Batch [350][1100]\t Training Loss 0.5285\t Accuracy 0.8725\n",
      "Epoch [16][20]\t Batch [400][1100]\t Training Loss 0.5258\t Accuracy 0.8739\n",
      "Epoch [16][20]\t Batch [450][1100]\t Training Loss 0.5241\t Accuracy 0.8747\n",
      "Epoch [16][20]\t Batch [500][1100]\t Training Loss 0.5243\t Accuracy 0.8741\n",
      "Epoch [16][20]\t Batch [550][1100]\t Training Loss 0.5236\t Accuracy 0.8746\n",
      "Epoch [16][20]\t Batch [600][1100]\t Training Loss 0.5234\t Accuracy 0.8742\n",
      "Epoch [16][20]\t Batch [650][1100]\t Training Loss 0.5304\t Accuracy 0.8715\n",
      "Epoch [16][20]\t Batch [700][1100]\t Training Loss 0.5283\t Accuracy 0.8724\n",
      "Epoch [16][20]\t Batch [750][1100]\t Training Loss 0.5274\t Accuracy 0.8728\n",
      "Epoch [16][20]\t Batch [800][1100]\t Training Loss 0.5272\t Accuracy 0.8730\n",
      "Epoch [16][20]\t Batch [850][1100]\t Training Loss 0.5284\t Accuracy 0.8724\n",
      "Epoch [16][20]\t Batch [900][1100]\t Training Loss 0.5277\t Accuracy 0.8726\n",
      "Epoch [16][20]\t Batch [950][1100]\t Training Loss 0.5279\t Accuracy 0.8722\n",
      "Epoch [16][20]\t Batch [1000][1100]\t Training Loss 0.5281\t Accuracy 0.8717\n",
      "Epoch [16][20]\t Batch [1050][1100]\t Training Loss 0.5279\t Accuracy 0.8714\n",
      "\n",
      "Epoch [16]\t Average training loss 0.5275\t Average training accuracy 0.8712\n",
      "Epoch [16]\t Average validation loss 0.4313\t Average validation accuracy 0.9056\n",
      "\n",
      "Epoch [17][20]\t Batch [0][1100]\t Training Loss 0.5509\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [50][1100]\t Training Loss 0.5014\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [100][1100]\t Training Loss 0.4928\t Accuracy 0.8834\n",
      "Epoch [17][20]\t Batch [150][1100]\t Training Loss 0.4921\t Accuracy 0.8844\n",
      "Epoch [17][20]\t Batch [200][1100]\t Training Loss 0.5011\t Accuracy 0.8813\n",
      "Epoch [17][20]\t Batch [250][1100]\t Training Loss 0.4976\t Accuracy 0.8819\n",
      "Epoch [17][20]\t Batch [300][1100]\t Training Loss 0.5115\t Accuracy 0.8763\n",
      "Epoch [17][20]\t Batch [350][1100]\t Training Loss 0.5148\t Accuracy 0.8750\n",
      "Epoch [17][20]\t Batch [400][1100]\t Training Loss 0.5121\t Accuracy 0.8764\n",
      "Epoch [17][20]\t Batch [450][1100]\t Training Loss 0.5104\t Accuracy 0.8772\n",
      "Epoch [17][20]\t Batch [500][1100]\t Training Loss 0.5107\t Accuracy 0.8766\n",
      "Epoch [17][20]\t Batch [550][1100]\t Training Loss 0.5101\t Accuracy 0.8770\n",
      "Epoch [17][20]\t Batch [600][1100]\t Training Loss 0.5099\t Accuracy 0.8764\n",
      "Epoch [17][20]\t Batch [650][1100]\t Training Loss 0.5169\t Accuracy 0.8735\n",
      "Epoch [17][20]\t Batch [700][1100]\t Training Loss 0.5148\t Accuracy 0.8744\n",
      "Epoch [17][20]\t Batch [750][1100]\t Training Loss 0.5139\t Accuracy 0.8748\n",
      "Epoch [17][20]\t Batch [800][1100]\t Training Loss 0.5137\t Accuracy 0.8749\n",
      "Epoch [17][20]\t Batch [850][1100]\t Training Loss 0.5149\t Accuracy 0.8743\n",
      "Epoch [17][20]\t Batch [900][1100]\t Training Loss 0.5143\t Accuracy 0.8744\n",
      "Epoch [17][20]\t Batch [950][1100]\t Training Loss 0.5146\t Accuracy 0.8740\n",
      "Epoch [17][20]\t Batch [1000][1100]\t Training Loss 0.5149\t Accuracy 0.8735\n",
      "Epoch [17][20]\t Batch [1050][1100]\t Training Loss 0.5147\t Accuracy 0.8732\n",
      "\n",
      "Epoch [17]\t Average training loss 0.5143\t Average training accuracy 0.8730\n",
      "Epoch [17]\t Average validation loss 0.4191\t Average validation accuracy 0.9072\n",
      "\n",
      "Epoch [18][20]\t Batch [0][1100]\t Training Loss 0.5376\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][1100]\t Training Loss 0.4888\t Accuracy 0.8827\n",
      "Epoch [18][20]\t Batch [100][1100]\t Training Loss 0.4801\t Accuracy 0.8848\n",
      "Epoch [18][20]\t Batch [150][1100]\t Training Loss 0.4795\t Accuracy 0.8854\n",
      "Epoch [18][20]\t Batch [200][1100]\t Training Loss 0.4886\t Accuracy 0.8824\n",
      "Epoch [18][20]\t Batch [250][1100]\t Training Loss 0.4852\t Accuracy 0.8832\n",
      "Epoch [18][20]\t Batch [300][1100]\t Training Loss 0.4992\t Accuracy 0.8779\n",
      "Epoch [18][20]\t Batch [350][1100]\t Training Loss 0.5025\t Accuracy 0.8769\n",
      "Epoch [18][20]\t Batch [400][1100]\t Training Loss 0.4998\t Accuracy 0.8785\n",
      "Epoch [18][20]\t Batch [450][1100]\t Training Loss 0.4981\t Accuracy 0.8793\n",
      "Epoch [18][20]\t Batch [500][1100]\t Training Loss 0.4984\t Accuracy 0.8786\n",
      "Epoch [18][20]\t Batch [550][1100]\t Training Loss 0.4978\t Accuracy 0.8791\n",
      "Epoch [18][20]\t Batch [600][1100]\t Training Loss 0.4978\t Accuracy 0.8786\n",
      "Epoch [18][20]\t Batch [650][1100]\t Training Loss 0.5047\t Accuracy 0.8756\n",
      "Epoch [18][20]\t Batch [700][1100]\t Training Loss 0.5026\t Accuracy 0.8764\n",
      "Epoch [18][20]\t Batch [750][1100]\t Training Loss 0.5018\t Accuracy 0.8767\n",
      "Epoch [18][20]\t Batch [800][1100]\t Training Loss 0.5016\t Accuracy 0.8769\n",
      "Epoch [18][20]\t Batch [850][1100]\t Training Loss 0.5029\t Accuracy 0.8762\n",
      "Epoch [18][20]\t Batch [900][1100]\t Training Loss 0.5022\t Accuracy 0.8763\n",
      "Epoch [18][20]\t Batch [950][1100]\t Training Loss 0.5025\t Accuracy 0.8758\n",
      "Epoch [18][20]\t Batch [1000][1100]\t Training Loss 0.5029\t Accuracy 0.8752\n",
      "Epoch [18][20]\t Batch [1050][1100]\t Training Loss 0.5027\t Accuracy 0.8749\n",
      "\n",
      "Epoch [18]\t Average training loss 0.5024\t Average training accuracy 0.8748\n",
      "Epoch [18]\t Average validation loss 0.4081\t Average validation accuracy 0.9084\n",
      "\n",
      "Epoch [19][20]\t Batch [0][1100]\t Training Loss 0.5255\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][1100]\t Training Loss 0.4774\t Accuracy 0.8843\n",
      "Epoch [19][20]\t Batch [100][1100]\t Training Loss 0.4685\t Accuracy 0.8863\n",
      "Epoch [19][20]\t Batch [150][1100]\t Training Loss 0.4680\t Accuracy 0.8868\n",
      "Epoch [19][20]\t Batch [200][1100]\t Training Loss 0.4774\t Accuracy 0.8836\n",
      "Epoch [19][20]\t Batch [250][1100]\t Training Loss 0.4741\t Accuracy 0.8842\n",
      "Epoch [19][20]\t Batch [300][1100]\t Training Loss 0.4881\t Accuracy 0.8791\n",
      "Epoch [19][20]\t Batch [350][1100]\t Training Loss 0.4913\t Accuracy 0.8786\n",
      "Epoch [19][20]\t Batch [400][1100]\t Training Loss 0.4886\t Accuracy 0.8802\n",
      "Epoch [19][20]\t Batch [450][1100]\t Training Loss 0.4870\t Accuracy 0.8811\n",
      "Epoch [19][20]\t Batch [500][1100]\t Training Loss 0.4873\t Accuracy 0.8805\n",
      "Epoch [19][20]\t Batch [550][1100]\t Training Loss 0.4868\t Accuracy 0.8812\n",
      "Epoch [19][20]\t Batch [600][1100]\t Training Loss 0.4868\t Accuracy 0.8809\n",
      "Epoch [19][20]\t Batch [650][1100]\t Training Loss 0.4937\t Accuracy 0.8778\n",
      "Epoch [19][20]\t Batch [700][1100]\t Training Loss 0.4916\t Accuracy 0.8786\n",
      "Epoch [19][20]\t Batch [750][1100]\t Training Loss 0.4908\t Accuracy 0.8788\n",
      "Epoch [19][20]\t Batch [800][1100]\t Training Loss 0.4907\t Accuracy 0.8789\n",
      "Epoch [19][20]\t Batch [850][1100]\t Training Loss 0.4919\t Accuracy 0.8782\n",
      "Epoch [19][20]\t Batch [900][1100]\t Training Loss 0.4913\t Accuracy 0.8782\n",
      "Epoch [19][20]\t Batch [950][1100]\t Training Loss 0.4916\t Accuracy 0.8777\n",
      "Epoch [19][20]\t Batch [1000][1100]\t Training Loss 0.4920\t Accuracy 0.8770\n",
      "Epoch [19][20]\t Batch [1050][1100]\t Training Loss 0.4919\t Accuracy 0.8768\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4916\t Average training accuracy 0.8767\n",
      "Epoch [19]\t Average validation loss 0.3982\t Average validation accuracy 0.9098\n",
      "\n",
      "Epoch [0][20]\t Batch [0][1100]\t Training Loss 2.6158\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][1100]\t Training Loss 2.0259\t Accuracy 0.3141\n",
      "Epoch [0][20]\t Batch [100][1100]\t Training Loss 1.4961\t Accuracy 0.5152\n",
      "Epoch [0][20]\t Batch [150][1100]\t Training Loss 1.1773\t Accuracy 0.6216\n",
      "Epoch [0][20]\t Batch [200][1100]\t Training Loss 1.0125\t Accuracy 0.6782\n",
      "Epoch [0][20]\t Batch [250][1100]\t Training Loss 0.8913\t Accuracy 0.7185\n",
      "Epoch [0][20]\t Batch [300][1100]\t Training Loss 0.8313\t Accuracy 0.7393\n",
      "Epoch [0][20]\t Batch [350][1100]\t Training Loss 0.7697\t Accuracy 0.7593\n",
      "Epoch [0][20]\t Batch [400][1100]\t Training Loss 0.7148\t Accuracy 0.7763\n",
      "Epoch [0][20]\t Batch [450][1100]\t Training Loss 0.6721\t Accuracy 0.7910\n",
      "Epoch [0][20]\t Batch [500][1100]\t Training Loss 0.6373\t Accuracy 0.8017\n",
      "Epoch [0][20]\t Batch [550][1100]\t Training Loss 0.6088\t Accuracy 0.8117\n",
      "Epoch [0][20]\t Batch [600][1100]\t Training Loss 0.5841\t Accuracy 0.8196\n",
      "Epoch [0][20]\t Batch [650][1100]\t Training Loss 0.5654\t Accuracy 0.8251\n",
      "Epoch [0][20]\t Batch [700][1100]\t Training Loss 0.5427\t Accuracy 0.8325\n",
      "Epoch [0][20]\t Batch [750][1100]\t Training Loss 0.5250\t Accuracy 0.8382\n",
      "Epoch [0][20]\t Batch [800][1100]\t Training Loss 0.5085\t Accuracy 0.8433\n",
      "Epoch [0][20]\t Batch [850][1100]\t Training Loss 0.4947\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [900][1100]\t Training Loss 0.4806\t Accuracy 0.8521\n",
      "Epoch [0][20]\t Batch [950][1100]\t Training Loss 0.4693\t Accuracy 0.8556\n",
      "Epoch [0][20]\t Batch [1000][1100]\t Training Loss 0.4584\t Accuracy 0.8592\n",
      "Epoch [0][20]\t Batch [1050][1100]\t Training Loss 0.4479\t Accuracy 0.8627\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4388\t Average training accuracy 0.8655\n",
      "Epoch [0]\t Average validation loss 0.1711\t Average validation accuracy 0.9540\n",
      "\n",
      "Epoch [1][20]\t Batch [0][1100]\t Training Loss 0.1958\t Accuracy 0.9200\n",
      "Epoch [1][20]\t Batch [50][1100]\t Training Loss 0.2048\t Accuracy 0.9424\n",
      "Epoch [1][20]\t Batch [100][1100]\t Training Loss 0.1917\t Accuracy 0.9483\n",
      "Epoch [1][20]\t Batch [150][1100]\t Training Loss 0.1933\t Accuracy 0.9481\n",
      "Epoch [1][20]\t Batch [200][1100]\t Training Loss 0.1994\t Accuracy 0.9442\n",
      "Epoch [1][20]\t Batch [250][1100]\t Training Loss 0.1981\t Accuracy 0.9441\n",
      "Epoch [1][20]\t Batch [300][1100]\t Training Loss 0.2069\t Accuracy 0.9414\n",
      "Epoch [1][20]\t Batch [350][1100]\t Training Loss 0.2070\t Accuracy 0.9414\n",
      "Epoch [1][20]\t Batch [400][1100]\t Training Loss 0.2029\t Accuracy 0.9423\n",
      "Epoch [1][20]\t Batch [450][1100]\t Training Loss 0.2016\t Accuracy 0.9434\n",
      "Epoch [1][20]\t Batch [500][1100]\t Training Loss 0.1994\t Accuracy 0.9438\n",
      "Epoch [1][20]\t Batch [550][1100]\t Training Loss 0.1989\t Accuracy 0.9442\n",
      "Epoch [1][20]\t Batch [600][1100]\t Training Loss 0.1973\t Accuracy 0.9444\n",
      "Epoch [1][20]\t Batch [650][1100]\t Training Loss 0.1978\t Accuracy 0.9440\n",
      "Epoch [1][20]\t Batch [700][1100]\t Training Loss 0.1951\t Accuracy 0.9449\n",
      "Epoch [1][20]\t Batch [750][1100]\t Training Loss 0.1935\t Accuracy 0.9452\n",
      "Epoch [1][20]\t Batch [800][1100]\t Training Loss 0.1918\t Accuracy 0.9457\n",
      "Epoch [1][20]\t Batch [850][1100]\t Training Loss 0.1913\t Accuracy 0.9459\n",
      "Epoch [1][20]\t Batch [900][1100]\t Training Loss 0.1896\t Accuracy 0.9461\n",
      "Epoch [1][20]\t Batch [950][1100]\t Training Loss 0.1896\t Accuracy 0.9461\n",
      "Epoch [1][20]\t Batch [1000][1100]\t Training Loss 0.1892\t Accuracy 0.9461\n",
      "Epoch [1][20]\t Batch [1050][1100]\t Training Loss 0.1881\t Accuracy 0.9465\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1870\t Average training accuracy 0.9466\n",
      "Epoch [1]\t Average validation loss 0.1283\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [2][20]\t Batch [0][1100]\t Training Loss 0.1423\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][1100]\t Training Loss 0.1496\t Accuracy 0.9588\n",
      "Epoch [2][20]\t Batch [100][1100]\t Training Loss 0.1368\t Accuracy 0.9644\n",
      "Epoch [2][20]\t Batch [150][1100]\t Training Loss 0.1397\t Accuracy 0.9630\n",
      "Epoch [2][20]\t Batch [200][1100]\t Training Loss 0.1443\t Accuracy 0.9609\n",
      "Epoch [2][20]\t Batch [250][1100]\t Training Loss 0.1432\t Accuracy 0.9606\n",
      "Epoch [2][20]\t Batch [300][1100]\t Training Loss 0.1493\t Accuracy 0.9589\n",
      "Epoch [2][20]\t Batch [350][1100]\t Training Loss 0.1500\t Accuracy 0.9585\n",
      "Epoch [2][20]\t Batch [400][1100]\t Training Loss 0.1471\t Accuracy 0.9594\n",
      "Epoch [2][20]\t Batch [450][1100]\t Training Loss 0.1466\t Accuracy 0.9596\n",
      "Epoch [2][20]\t Batch [500][1100]\t Training Loss 0.1445\t Accuracy 0.9602\n",
      "Epoch [2][20]\t Batch [550][1100]\t Training Loss 0.1448\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [600][1100]\t Training Loss 0.1439\t Accuracy 0.9599\n",
      "Epoch [2][20]\t Batch [650][1100]\t Training Loss 0.1446\t Accuracy 0.9595\n",
      "Epoch [2][20]\t Batch [700][1100]\t Training Loss 0.1433\t Accuracy 0.9599\n",
      "Epoch [2][20]\t Batch [750][1100]\t Training Loss 0.1423\t Accuracy 0.9602\n",
      "Epoch [2][20]\t Batch [800][1100]\t Training Loss 0.1414\t Accuracy 0.9606\n",
      "Epoch [2][20]\t Batch [850][1100]\t Training Loss 0.1414\t Accuracy 0.9607\n",
      "Epoch [2][20]\t Batch [900][1100]\t Training Loss 0.1405\t Accuracy 0.9609\n",
      "Epoch [2][20]\t Batch [950][1100]\t Training Loss 0.1412\t Accuracy 0.9607\n",
      "Epoch [2][20]\t Batch [1000][1100]\t Training Loss 0.1414\t Accuracy 0.9607\n",
      "Epoch [2][20]\t Batch [1050][1100]\t Training Loss 0.1408\t Accuracy 0.9608\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1402\t Average training accuracy 0.9610\n",
      "Epoch [2]\t Average validation loss 0.1084\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [3][20]\t Batch [0][1100]\t Training Loss 0.1012\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][1100]\t Training Loss 0.1185\t Accuracy 0.9651\n",
      "Epoch [3][20]\t Batch [100][1100]\t Training Loss 0.1086\t Accuracy 0.9709\n",
      "Epoch [3][20]\t Batch [150][1100]\t Training Loss 0.1118\t Accuracy 0.9695\n",
      "Epoch [3][20]\t Batch [200][1100]\t Training Loss 0.1153\t Accuracy 0.9674\n",
      "Epoch [3][20]\t Batch [250][1100]\t Training Loss 0.1143\t Accuracy 0.9673\n",
      "Epoch [3][20]\t Batch [300][1100]\t Training Loss 0.1184\t Accuracy 0.9657\n",
      "Epoch [3][20]\t Batch [350][1100]\t Training Loss 0.1187\t Accuracy 0.9655\n",
      "Epoch [3][20]\t Batch [400][1100]\t Training Loss 0.1164\t Accuracy 0.9665\n",
      "Epoch [3][20]\t Batch [450][1100]\t Training Loss 0.1161\t Accuracy 0.9666\n",
      "Epoch [3][20]\t Batch [500][1100]\t Training Loss 0.1140\t Accuracy 0.9674\n",
      "Epoch [3][20]\t Batch [550][1100]\t Training Loss 0.1149\t Accuracy 0.9670\n",
      "Epoch [3][20]\t Batch [600][1100]\t Training Loss 0.1143\t Accuracy 0.9668\n",
      "Epoch [3][20]\t Batch [650][1100]\t Training Loss 0.1151\t Accuracy 0.9666\n",
      "Epoch [3][20]\t Batch [700][1100]\t Training Loss 0.1143\t Accuracy 0.9670\n",
      "Epoch [3][20]\t Batch [750][1100]\t Training Loss 0.1138\t Accuracy 0.9672\n",
      "Epoch [3][20]\t Batch [800][1100]\t Training Loss 0.1132\t Accuracy 0.9676\n",
      "Epoch [3][20]\t Batch [850][1100]\t Training Loss 0.1134\t Accuracy 0.9676\n",
      "Epoch [3][20]\t Batch [900][1100]\t Training Loss 0.1128\t Accuracy 0.9678\n",
      "Epoch [3][20]\t Batch [950][1100]\t Training Loss 0.1137\t Accuracy 0.9676\n",
      "Epoch [3][20]\t Batch [1000][1100]\t Training Loss 0.1141\t Accuracy 0.9675\n",
      "Epoch [3][20]\t Batch [1050][1100]\t Training Loss 0.1137\t Accuracy 0.9677\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1134\t Average training accuracy 0.9678\n",
      "Epoch [3]\t Average validation loss 0.0975\t Average validation accuracy 0.9724\n",
      "\n",
      "Epoch [4][20]\t Batch [0][1100]\t Training Loss 0.0749\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][1100]\t Training Loss 0.0970\t Accuracy 0.9737\n",
      "Epoch [4][20]\t Batch [100][1100]\t Training Loss 0.0890\t Accuracy 0.9764\n",
      "Epoch [4][20]\t Batch [150][1100]\t Training Loss 0.0928\t Accuracy 0.9752\n",
      "Epoch [4][20]\t Batch [200][1100]\t Training Loss 0.0950\t Accuracy 0.9733\n",
      "Epoch [4][20]\t Batch [250][1100]\t Training Loss 0.0939\t Accuracy 0.9734\n",
      "Epoch [4][20]\t Batch [300][1100]\t Training Loss 0.0965\t Accuracy 0.9726\n",
      "Epoch [4][20]\t Batch [350][1100]\t Training Loss 0.0968\t Accuracy 0.9727\n",
      "Epoch [4][20]\t Batch [400][1100]\t Training Loss 0.0952\t Accuracy 0.9732\n",
      "Epoch [4][20]\t Batch [450][1100]\t Training Loss 0.0950\t Accuracy 0.9731\n",
      "Epoch [4][20]\t Batch [500][1100]\t Training Loss 0.0933\t Accuracy 0.9735\n",
      "Epoch [4][20]\t Batch [550][1100]\t Training Loss 0.0945\t Accuracy 0.9730\n",
      "Epoch [4][20]\t Batch [600][1100]\t Training Loss 0.0943\t Accuracy 0.9727\n",
      "Epoch [4][20]\t Batch [650][1100]\t Training Loss 0.0950\t Accuracy 0.9726\n",
      "Epoch [4][20]\t Batch [700][1100]\t Training Loss 0.0946\t Accuracy 0.9729\n",
      "Epoch [4][20]\t Batch [750][1100]\t Training Loss 0.0944\t Accuracy 0.9730\n",
      "Epoch [4][20]\t Batch [800][1100]\t Training Loss 0.0941\t Accuracy 0.9734\n",
      "Epoch [4][20]\t Batch [850][1100]\t Training Loss 0.0943\t Accuracy 0.9732\n",
      "Epoch [4][20]\t Batch [900][1100]\t Training Loss 0.0939\t Accuracy 0.9735\n",
      "Epoch [4][20]\t Batch [950][1100]\t Training Loss 0.0949\t Accuracy 0.9731\n",
      "Epoch [4][20]\t Batch [1000][1100]\t Training Loss 0.0954\t Accuracy 0.9730\n",
      "Epoch [4][20]\t Batch [1050][1100]\t Training Loss 0.0951\t Accuracy 0.9731\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0948\t Average training accuracy 0.9732\n",
      "Epoch [4]\t Average validation loss 0.0903\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [5][20]\t Batch [0][1100]\t Training Loss 0.0604\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [50][1100]\t Training Loss 0.0822\t Accuracy 0.9773\n",
      "Epoch [5][20]\t Batch [100][1100]\t Training Loss 0.0754\t Accuracy 0.9792\n",
      "Epoch [5][20]\t Batch [150][1100]\t Training Loss 0.0792\t Accuracy 0.9785\n",
      "Epoch [5][20]\t Batch [200][1100]\t Training Loss 0.0805\t Accuracy 0.9773\n",
      "Epoch [5][20]\t Batch [250][1100]\t Training Loss 0.0793\t Accuracy 0.9775\n",
      "Epoch [5][20]\t Batch [300][1100]\t Training Loss 0.0806\t Accuracy 0.9769\n",
      "Epoch [5][20]\t Batch [350][1100]\t Training Loss 0.0812\t Accuracy 0.9767\n",
      "Epoch [5][20]\t Batch [400][1100]\t Training Loss 0.0800\t Accuracy 0.9772\n",
      "Epoch [5][20]\t Batch [450][1100]\t Training Loss 0.0799\t Accuracy 0.9773\n",
      "Epoch [5][20]\t Batch [500][1100]\t Training Loss 0.0784\t Accuracy 0.9777\n",
      "Epoch [5][20]\t Batch [550][1100]\t Training Loss 0.0798\t Accuracy 0.9773\n",
      "Epoch [5][20]\t Batch [600][1100]\t Training Loss 0.0797\t Accuracy 0.9771\n",
      "Epoch [5][20]\t Batch [650][1100]\t Training Loss 0.0804\t Accuracy 0.9769\n",
      "Epoch [5][20]\t Batch [700][1100]\t Training Loss 0.0802\t Accuracy 0.9771\n",
      "Epoch [5][20]\t Batch [750][1100]\t Training Loss 0.0802\t Accuracy 0.9772\n",
      "Epoch [5][20]\t Batch [800][1100]\t Training Loss 0.0801\t Accuracy 0.9773\n",
      "Epoch [5][20]\t Batch [850][1100]\t Training Loss 0.0803\t Accuracy 0.9772\n",
      "Epoch [5][20]\t Batch [900][1100]\t Training Loss 0.0801\t Accuracy 0.9774\n",
      "Epoch [5][20]\t Batch [950][1100]\t Training Loss 0.0809\t Accuracy 0.9772\n",
      "Epoch [5][20]\t Batch [1000][1100]\t Training Loss 0.0815\t Accuracy 0.9770\n",
      "Epoch [5][20]\t Batch [1050][1100]\t Training Loss 0.0812\t Accuracy 0.9770\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0810\t Average training accuracy 0.9771\n",
      "Epoch [5]\t Average validation loss 0.0854\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [6][20]\t Batch [0][1100]\t Training Loss 0.0524\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [50][1100]\t Training Loss 0.0718\t Accuracy 0.9792\n",
      "Epoch [6][20]\t Batch [100][1100]\t Training Loss 0.0654\t Accuracy 0.9820\n",
      "Epoch [6][20]\t Batch [150][1100]\t Training Loss 0.0689\t Accuracy 0.9813\n",
      "Epoch [6][20]\t Batch [200][1100]\t Training Loss 0.0697\t Accuracy 0.9805\n",
      "Epoch [6][20]\t Batch [250][1100]\t Training Loss 0.0683\t Accuracy 0.9806\n",
      "Epoch [6][20]\t Batch [300][1100]\t Training Loss 0.0689\t Accuracy 0.9802\n",
      "Epoch [6][20]\t Batch [350][1100]\t Training Loss 0.0695\t Accuracy 0.9801\n",
      "Epoch [6][20]\t Batch [400][1100]\t Training Loss 0.0686\t Accuracy 0.9804\n",
      "Epoch [6][20]\t Batch [450][1100]\t Training Loss 0.0687\t Accuracy 0.9804\n",
      "Epoch [6][20]\t Batch [500][1100]\t Training Loss 0.0675\t Accuracy 0.9806\n",
      "Epoch [6][20]\t Batch [550][1100]\t Training Loss 0.0690\t Accuracy 0.9803\n",
      "Epoch [6][20]\t Batch [600][1100]\t Training Loss 0.0689\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [650][1100]\t Training Loss 0.0695\t Accuracy 0.9801\n",
      "Epoch [6][20]\t Batch [700][1100]\t Training Loss 0.0694\t Accuracy 0.9802\n",
      "Epoch [6][20]\t Batch [750][1100]\t Training Loss 0.0695\t Accuracy 0.9802\n",
      "Epoch [6][20]\t Batch [800][1100]\t Training Loss 0.0695\t Accuracy 0.9803\n",
      "Epoch [6][20]\t Batch [850][1100]\t Training Loss 0.0696\t Accuracy 0.9802\n",
      "Epoch [6][20]\t Batch [900][1100]\t Training Loss 0.0696\t Accuracy 0.9804\n",
      "Epoch [6][20]\t Batch [950][1100]\t Training Loss 0.0703\t Accuracy 0.9801\n",
      "Epoch [6][20]\t Batch [1000][1100]\t Training Loss 0.0709\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [1050][1100]\t Training Loss 0.0706\t Accuracy 0.9800\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0705\t Average training accuracy 0.9800\n",
      "Epoch [6]\t Average validation loss 0.0817\t Average validation accuracy 0.9754\n",
      "\n",
      "Epoch [7][20]\t Batch [0][1100]\t Training Loss 0.0465\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][1100]\t Training Loss 0.0630\t Accuracy 0.9824\n",
      "Epoch [7][20]\t Batch [100][1100]\t Training Loss 0.0575\t Accuracy 0.9848\n",
      "Epoch [7][20]\t Batch [150][1100]\t Training Loss 0.0606\t Accuracy 0.9841\n",
      "Epoch [7][20]\t Batch [200][1100]\t Training Loss 0.0612\t Accuracy 0.9835\n",
      "Epoch [7][20]\t Batch [250][1100]\t Training Loss 0.0597\t Accuracy 0.9834\n",
      "Epoch [7][20]\t Batch [300][1100]\t Training Loss 0.0598\t Accuracy 0.9830\n",
      "Epoch [7][20]\t Batch [350][1100]\t Training Loss 0.0604\t Accuracy 0.9828\n",
      "Epoch [7][20]\t Batch [400][1100]\t Training Loss 0.0598\t Accuracy 0.9831\n",
      "Epoch [7][20]\t Batch [450][1100]\t Training Loss 0.0600\t Accuracy 0.9830\n",
      "Epoch [7][20]\t Batch [500][1100]\t Training Loss 0.0589\t Accuracy 0.9834\n",
      "Epoch [7][20]\t Batch [550][1100]\t Training Loss 0.0604\t Accuracy 0.9830\n",
      "Epoch [7][20]\t Batch [600][1100]\t Training Loss 0.0603\t Accuracy 0.9829\n",
      "Epoch [7][20]\t Batch [650][1100]\t Training Loss 0.0609\t Accuracy 0.9829\n",
      "Epoch [7][20]\t Batch [700][1100]\t Training Loss 0.0608\t Accuracy 0.9830\n",
      "Epoch [7][20]\t Batch [750][1100]\t Training Loss 0.0610\t Accuracy 0.9829\n",
      "Epoch [7][20]\t Batch [800][1100]\t Training Loss 0.0610\t Accuracy 0.9830\n",
      "Epoch [7][20]\t Batch [850][1100]\t Training Loss 0.0612\t Accuracy 0.9828\n",
      "Epoch [7][20]\t Batch [900][1100]\t Training Loss 0.0612\t Accuracy 0.9829\n",
      "Epoch [7][20]\t Batch [950][1100]\t Training Loss 0.0619\t Accuracy 0.9827\n",
      "Epoch [7][20]\t Batch [1000][1100]\t Training Loss 0.0624\t Accuracy 0.9826\n",
      "Epoch [7][20]\t Batch [1050][1100]\t Training Loss 0.0622\t Accuracy 0.9825\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0621\t Average training accuracy 0.9826\n",
      "Epoch [7]\t Average validation loss 0.0792\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [8][20]\t Batch [0][1100]\t Training Loss 0.0407\t Accuracy 1.0000\n",
      "Epoch [8][20]\t Batch [50][1100]\t Training Loss 0.0559\t Accuracy 0.9835\n",
      "Epoch [8][20]\t Batch [100][1100]\t Training Loss 0.0514\t Accuracy 0.9861\n",
      "Epoch [8][20]\t Batch [150][1100]\t Training Loss 0.0540\t Accuracy 0.9856\n",
      "Epoch [8][20]\t Batch [200][1100]\t Training Loss 0.0545\t Accuracy 0.9853\n",
      "Epoch [8][20]\t Batch [250][1100]\t Training Loss 0.0530\t Accuracy 0.9856\n",
      "Epoch [8][20]\t Batch [300][1100]\t Training Loss 0.0528\t Accuracy 0.9856\n",
      "Epoch [8][20]\t Batch [350][1100]\t Training Loss 0.0533\t Accuracy 0.9852\n",
      "Epoch [8][20]\t Batch [400][1100]\t Training Loss 0.0529\t Accuracy 0.9854\n",
      "Epoch [8][20]\t Batch [450][1100]\t Training Loss 0.0532\t Accuracy 0.9854\n",
      "Epoch [8][20]\t Batch [500][1100]\t Training Loss 0.0521\t Accuracy 0.9857\n",
      "Epoch [8][20]\t Batch [550][1100]\t Training Loss 0.0536\t Accuracy 0.9854\n",
      "Epoch [8][20]\t Batch [600][1100]\t Training Loss 0.0535\t Accuracy 0.9852\n",
      "Epoch [8][20]\t Batch [650][1100]\t Training Loss 0.0539\t Accuracy 0.9852\n",
      "Epoch [8][20]\t Batch [700][1100]\t Training Loss 0.0539\t Accuracy 0.9851\n",
      "Epoch [8][20]\t Batch [750][1100]\t Training Loss 0.0541\t Accuracy 0.9850\n",
      "Epoch [8][20]\t Batch [800][1100]\t Training Loss 0.0542\t Accuracy 0.9851\n",
      "Epoch [8][20]\t Batch [850][1100]\t Training Loss 0.0543\t Accuracy 0.9850\n",
      "Epoch [8][20]\t Batch [900][1100]\t Training Loss 0.0545\t Accuracy 0.9850\n",
      "Epoch [8][20]\t Batch [950][1100]\t Training Loss 0.0551\t Accuracy 0.9849\n",
      "Epoch [8][20]\t Batch [1000][1100]\t Training Loss 0.0556\t Accuracy 0.9847\n",
      "Epoch [8][20]\t Batch [1050][1100]\t Training Loss 0.0554\t Accuracy 0.9847\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0554\t Average training accuracy 0.9848\n",
      "Epoch [8]\t Average validation loss 0.0777\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [9][20]\t Batch [0][1100]\t Training Loss 0.0380\t Accuracy 1.0000\n",
      "Epoch [9][20]\t Batch [50][1100]\t Training Loss 0.0497\t Accuracy 0.9843\n",
      "Epoch [9][20]\t Batch [100][1100]\t Training Loss 0.0463\t Accuracy 0.9875\n",
      "Epoch [9][20]\t Batch [150][1100]\t Training Loss 0.0484\t Accuracy 0.9869\n",
      "Epoch [9][20]\t Batch [200][1100]\t Training Loss 0.0490\t Accuracy 0.9864\n",
      "Epoch [9][20]\t Batch [250][1100]\t Training Loss 0.0476\t Accuracy 0.9870\n",
      "Epoch [9][20]\t Batch [300][1100]\t Training Loss 0.0472\t Accuracy 0.9869\n",
      "Epoch [9][20]\t Batch [350][1100]\t Training Loss 0.0476\t Accuracy 0.9867\n",
      "Epoch [9][20]\t Batch [400][1100]\t Training Loss 0.0473\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [450][1100]\t Training Loss 0.0476\t Accuracy 0.9869\n",
      "Epoch [9][20]\t Batch [500][1100]\t Training Loss 0.0464\t Accuracy 0.9871\n",
      "Epoch [9][20]\t Batch [550][1100]\t Training Loss 0.0479\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [600][1100]\t Training Loss 0.0478\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [650][1100]\t Training Loss 0.0482\t Accuracy 0.9869\n",
      "Epoch [9][20]\t Batch [700][1100]\t Training Loss 0.0481\t Accuracy 0.9869\n",
      "Epoch [9][20]\t Batch [750][1100]\t Training Loss 0.0484\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [800][1100]\t Training Loss 0.0484\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [850][1100]\t Training Loss 0.0485\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [900][1100]\t Training Loss 0.0487\t Accuracy 0.9868\n",
      "Epoch [9][20]\t Batch [950][1100]\t Training Loss 0.0493\t Accuracy 0.9866\n",
      "Epoch [9][20]\t Batch [1000][1100]\t Training Loss 0.0498\t Accuracy 0.9864\n",
      "Epoch [9][20]\t Batch [1050][1100]\t Training Loss 0.0496\t Accuracy 0.9865\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0497\t Average training accuracy 0.9865\n",
      "Epoch [9]\t Average validation loss 0.0769\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [10][20]\t Batch [0][1100]\t Training Loss 0.0349\t Accuracy 1.0000\n",
      "Epoch [10][20]\t Batch [50][1100]\t Training Loss 0.0447\t Accuracy 0.9867\n",
      "Epoch [10][20]\t Batch [100][1100]\t Training Loss 0.0422\t Accuracy 0.9891\n",
      "Epoch [10][20]\t Batch [150][1100]\t Training Loss 0.0440\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [200][1100]\t Training Loss 0.0443\t Accuracy 0.9879\n",
      "Epoch [10][20]\t Batch [250][1100]\t Training Loss 0.0429\t Accuracy 0.9886\n",
      "Epoch [10][20]\t Batch [300][1100]\t Training Loss 0.0424\t Accuracy 0.9886\n",
      "Epoch [10][20]\t Batch [350][1100]\t Training Loss 0.0426\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [400][1100]\t Training Loss 0.0424\t Accuracy 0.9884\n",
      "Epoch [10][20]\t Batch [450][1100]\t Training Loss 0.0426\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [500][1100]\t Training Loss 0.0415\t Accuracy 0.9885\n",
      "Epoch [10][20]\t Batch [550][1100]\t Training Loss 0.0429\t Accuracy 0.9882\n",
      "Epoch [10][20]\t Batch [600][1100]\t Training Loss 0.0428\t Accuracy 0.9882\n",
      "Epoch [10][20]\t Batch [650][1100]\t Training Loss 0.0431\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [700][1100]\t Training Loss 0.0430\t Accuracy 0.9884\n",
      "Epoch [10][20]\t Batch [750][1100]\t Training Loss 0.0433\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [800][1100]\t Training Loss 0.0434\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [850][1100]\t Training Loss 0.0435\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [900][1100]\t Training Loss 0.0437\t Accuracy 0.9883\n",
      "Epoch [10][20]\t Batch [950][1100]\t Training Loss 0.0443\t Accuracy 0.9881\n",
      "Epoch [10][20]\t Batch [1000][1100]\t Training Loss 0.0447\t Accuracy 0.9880\n",
      "Epoch [10][20]\t Batch [1050][1100]\t Training Loss 0.0446\t Accuracy 0.9881\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0447\t Average training accuracy 0.9881\n",
      "Epoch [10]\t Average validation loss 0.0766\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [11][20]\t Batch [0][1100]\t Training Loss 0.0320\t Accuracy 1.0000\n",
      "Epoch [11][20]\t Batch [50][1100]\t Training Loss 0.0405\t Accuracy 0.9871\n",
      "Epoch [11][20]\t Batch [100][1100]\t Training Loss 0.0386\t Accuracy 0.9889\n",
      "Epoch [11][20]\t Batch [150][1100]\t Training Loss 0.0399\t Accuracy 0.9887\n",
      "Epoch [11][20]\t Batch [200][1100]\t Training Loss 0.0401\t Accuracy 0.9883\n",
      "Epoch [11][20]\t Batch [250][1100]\t Training Loss 0.0388\t Accuracy 0.9889\n",
      "Epoch [11][20]\t Batch [300][1100]\t Training Loss 0.0382\t Accuracy 0.9894\n",
      "Epoch [11][20]\t Batch [350][1100]\t Training Loss 0.0382\t Accuracy 0.9893\n",
      "Epoch [11][20]\t Batch [400][1100]\t Training Loss 0.0380\t Accuracy 0.9894\n",
      "Epoch [11][20]\t Batch [450][1100]\t Training Loss 0.0383\t Accuracy 0.9892\n",
      "Epoch [11][20]\t Batch [500][1100]\t Training Loss 0.0372\t Accuracy 0.9896\n",
      "Epoch [11][20]\t Batch [550][1100]\t Training Loss 0.0385\t Accuracy 0.9893\n",
      "Epoch [11][20]\t Batch [600][1100]\t Training Loss 0.0384\t Accuracy 0.9894\n",
      "Epoch [11][20]\t Batch [650][1100]\t Training Loss 0.0387\t Accuracy 0.9895\n",
      "Epoch [11][20]\t Batch [700][1100]\t Training Loss 0.0386\t Accuracy 0.9896\n",
      "Epoch [11][20]\t Batch [750][1100]\t Training Loss 0.0389\t Accuracy 0.9896\n",
      "Epoch [11][20]\t Batch [800][1100]\t Training Loss 0.0390\t Accuracy 0.9896\n",
      "Epoch [11][20]\t Batch [850][1100]\t Training Loss 0.0391\t Accuracy 0.9895\n",
      "Epoch [11][20]\t Batch [900][1100]\t Training Loss 0.0393\t Accuracy 0.9895\n",
      "Epoch [11][20]\t Batch [950][1100]\t Training Loss 0.0399\t Accuracy 0.9892\n",
      "Epoch [11][20]\t Batch [1000][1100]\t Training Loss 0.0402\t Accuracy 0.9892\n",
      "Epoch [11][20]\t Batch [1050][1100]\t Training Loss 0.0402\t Accuracy 0.9892\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0403\t Average training accuracy 0.9892\n",
      "Epoch [11]\t Average validation loss 0.0767\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [12][20]\t Batch [0][1100]\t Training Loss 0.0296\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][1100]\t Training Loss 0.0368\t Accuracy 0.9882\n",
      "Epoch [12][20]\t Batch [100][1100]\t Training Loss 0.0351\t Accuracy 0.9911\n",
      "Epoch [12][20]\t Batch [150][1100]\t Training Loss 0.0363\t Accuracy 0.9905\n",
      "Epoch [12][20]\t Batch [200][1100]\t Training Loss 0.0362\t Accuracy 0.9900\n",
      "Epoch [12][20]\t Batch [250][1100]\t Training Loss 0.0351\t Accuracy 0.9904\n",
      "Epoch [12][20]\t Batch [300][1100]\t Training Loss 0.0344\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [350][1100]\t Training Loss 0.0343\t Accuracy 0.9908\n",
      "Epoch [12][20]\t Batch [400][1100]\t Training Loss 0.0341\t Accuracy 0.9909\n",
      "Epoch [12][20]\t Batch [450][1100]\t Training Loss 0.0343\t Accuracy 0.9907\n",
      "Epoch [12][20]\t Batch [500][1100]\t Training Loss 0.0333\t Accuracy 0.9911\n",
      "Epoch [12][20]\t Batch [550][1100]\t Training Loss 0.0346\t Accuracy 0.9908\n",
      "Epoch [12][20]\t Batch [600][1100]\t Training Loss 0.0345\t Accuracy 0.9908\n",
      "Epoch [12][20]\t Batch [650][1100]\t Training Loss 0.0348\t Accuracy 0.9910\n",
      "Epoch [12][20]\t Batch [700][1100]\t Training Loss 0.0347\t Accuracy 0.9911\n",
      "Epoch [12][20]\t Batch [750][1100]\t Training Loss 0.0350\t Accuracy 0.9910\n",
      "Epoch [12][20]\t Batch [800][1100]\t Training Loss 0.0352\t Accuracy 0.9910\n",
      "Epoch [12][20]\t Batch [850][1100]\t Training Loss 0.0352\t Accuracy 0.9909\n",
      "Epoch [12][20]\t Batch [900][1100]\t Training Loss 0.0355\t Accuracy 0.9909\n",
      "Epoch [12][20]\t Batch [950][1100]\t Training Loss 0.0360\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [1000][1100]\t Training Loss 0.0362\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [1050][1100]\t Training Loss 0.0363\t Accuracy 0.9907\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0364\t Average training accuracy 0.9907\n",
      "Epoch [12]\t Average validation loss 0.0763\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [13][20]\t Batch [0][1100]\t Training Loss 0.0267\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][1100]\t Training Loss 0.0336\t Accuracy 0.9918\n",
      "Epoch [13][20]\t Batch [100][1100]\t Training Loss 0.0319\t Accuracy 0.9933\n",
      "Epoch [13][20]\t Batch [150][1100]\t Training Loss 0.0331\t Accuracy 0.9926\n",
      "Epoch [13][20]\t Batch [200][1100]\t Training Loss 0.0327\t Accuracy 0.9919\n",
      "Epoch [13][20]\t Batch [250][1100]\t Training Loss 0.0318\t Accuracy 0.9923\n",
      "Epoch [13][20]\t Batch [300][1100]\t Training Loss 0.0310\t Accuracy 0.9925\n",
      "Epoch [13][20]\t Batch [350][1100]\t Training Loss 0.0307\t Accuracy 0.9927\n",
      "Epoch [13][20]\t Batch [400][1100]\t Training Loss 0.0305\t Accuracy 0.9927\n",
      "Epoch [13][20]\t Batch [450][1100]\t Training Loss 0.0307\t Accuracy 0.9924\n",
      "Epoch [13][20]\t Batch [500][1100]\t Training Loss 0.0299\t Accuracy 0.9927\n",
      "Epoch [13][20]\t Batch [550][1100]\t Training Loss 0.0311\t Accuracy 0.9923\n",
      "Epoch [13][20]\t Batch [600][1100]\t Training Loss 0.0310\t Accuracy 0.9924\n",
      "Epoch [13][20]\t Batch [650][1100]\t Training Loss 0.0313\t Accuracy 0.9925\n",
      "Epoch [13][20]\t Batch [700][1100]\t Training Loss 0.0313\t Accuracy 0.9926\n",
      "Epoch [13][20]\t Batch [750][1100]\t Training Loss 0.0316\t Accuracy 0.9926\n",
      "Epoch [13][20]\t Batch [800][1100]\t Training Loss 0.0318\t Accuracy 0.9925\n",
      "Epoch [13][20]\t Batch [850][1100]\t Training Loss 0.0318\t Accuracy 0.9924\n",
      "Epoch [13][20]\t Batch [900][1100]\t Training Loss 0.0321\t Accuracy 0.9924\n",
      "Epoch [13][20]\t Batch [950][1100]\t Training Loss 0.0325\t Accuracy 0.9921\n",
      "Epoch [13][20]\t Batch [1000][1100]\t Training Loss 0.0328\t Accuracy 0.9921\n",
      "Epoch [13][20]\t Batch [1050][1100]\t Training Loss 0.0328\t Accuracy 0.9921\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0329\t Average training accuracy 0.9921\n",
      "Epoch [13]\t Average validation loss 0.0755\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [14][20]\t Batch [0][1100]\t Training Loss 0.0251\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][1100]\t Training Loss 0.0309\t Accuracy 0.9925\n",
      "Epoch [14][20]\t Batch [100][1100]\t Training Loss 0.0292\t Accuracy 0.9939\n",
      "Epoch [14][20]\t Batch [150][1100]\t Training Loss 0.0302\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [200][1100]\t Training Loss 0.0297\t Accuracy 0.9932\n",
      "Epoch [14][20]\t Batch [250][1100]\t Training Loss 0.0289\t Accuracy 0.9934\n",
      "Epoch [14][20]\t Batch [300][1100]\t Training Loss 0.0281\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [350][1100]\t Training Loss 0.0278\t Accuracy 0.9937\n",
      "Epoch [14][20]\t Batch [400][1100]\t Training Loss 0.0276\t Accuracy 0.9939\n",
      "Epoch [14][20]\t Batch [450][1100]\t Training Loss 0.0278\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [500][1100]\t Training Loss 0.0270\t Accuracy 0.9939\n",
      "Epoch [14][20]\t Batch [550][1100]\t Training Loss 0.0282\t Accuracy 0.9935\n",
      "Epoch [14][20]\t Batch [600][1100]\t Training Loss 0.0281\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [650][1100]\t Training Loss 0.0284\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [700][1100]\t Training Loss 0.0284\t Accuracy 0.9937\n",
      "Epoch [14][20]\t Batch [750][1100]\t Training Loss 0.0287\t Accuracy 0.9937\n",
      "Epoch [14][20]\t Batch [800][1100]\t Training Loss 0.0289\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [850][1100]\t Training Loss 0.0289\t Accuracy 0.9936\n",
      "Epoch [14][20]\t Batch [900][1100]\t Training Loss 0.0292\t Accuracy 0.9935\n",
      "Epoch [14][20]\t Batch [950][1100]\t Training Loss 0.0296\t Accuracy 0.9933\n",
      "Epoch [14][20]\t Batch [1000][1100]\t Training Loss 0.0298\t Accuracy 0.9933\n",
      "Epoch [14][20]\t Batch [1050][1100]\t Training Loss 0.0298\t Accuracy 0.9933\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0298\t Average training accuracy 0.9932\n",
      "Epoch [14]\t Average validation loss 0.0746\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [15][20]\t Batch [0][1100]\t Training Loss 0.0225\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][1100]\t Training Loss 0.0281\t Accuracy 0.9929\n",
      "Epoch [15][20]\t Batch [100][1100]\t Training Loss 0.0266\t Accuracy 0.9945\n",
      "Epoch [15][20]\t Batch [150][1100]\t Training Loss 0.0276\t Accuracy 0.9944\n",
      "Epoch [15][20]\t Batch [200][1100]\t Training Loss 0.0270\t Accuracy 0.9941\n",
      "Epoch [15][20]\t Batch [250][1100]\t Training Loss 0.0264\t Accuracy 0.9941\n",
      "Epoch [15][20]\t Batch [300][1100]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [15][20]\t Batch [350][1100]\t Training Loss 0.0253\t Accuracy 0.9947\n",
      "Epoch [15][20]\t Batch [400][1100]\t Training Loss 0.0250\t Accuracy 0.9947\n",
      "Epoch [15][20]\t Batch [450][1100]\t Training Loss 0.0252\t Accuracy 0.9945\n",
      "Epoch [15][20]\t Batch [500][1100]\t Training Loss 0.0245\t Accuracy 0.9947\n",
      "Epoch [15][20]\t Batch [550][1100]\t Training Loss 0.0256\t Accuracy 0.9944\n",
      "Epoch [15][20]\t Batch [600][1100]\t Training Loss 0.0255\t Accuracy 0.9946\n",
      "Epoch [15][20]\t Batch [650][1100]\t Training Loss 0.0258\t Accuracy 0.9947\n",
      "Epoch [15][20]\t Batch [700][1100]\t Training Loss 0.0258\t Accuracy 0.9948\n",
      "Epoch [15][20]\t Batch [750][1100]\t Training Loss 0.0261\t Accuracy 0.9948\n",
      "Epoch [15][20]\t Batch [800][1100]\t Training Loss 0.0263\t Accuracy 0.9947\n",
      "Epoch [15][20]\t Batch [850][1100]\t Training Loss 0.0263\t Accuracy 0.9946\n",
      "Epoch [15][20]\t Batch [900][1100]\t Training Loss 0.0265\t Accuracy 0.9945\n",
      "Epoch [15][20]\t Batch [950][1100]\t Training Loss 0.0269\t Accuracy 0.9943\n",
      "Epoch [15][20]\t Batch [1000][1100]\t Training Loss 0.0271\t Accuracy 0.9943\n",
      "Epoch [15][20]\t Batch [1050][1100]\t Training Loss 0.0271\t Accuracy 0.9942\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0272\t Average training accuracy 0.9942\n",
      "Epoch [15]\t Average validation loss 0.0741\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [16][20]\t Batch [0][1100]\t Training Loss 0.0214\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][1100]\t Training Loss 0.0255\t Accuracy 0.9941\n",
      "Epoch [16][20]\t Batch [100][1100]\t Training Loss 0.0242\t Accuracy 0.9954\n",
      "Epoch [16][20]\t Batch [150][1100]\t Training Loss 0.0253\t Accuracy 0.9954\n",
      "Epoch [16][20]\t Batch [200][1100]\t Training Loss 0.0247\t Accuracy 0.9953\n",
      "Epoch [16][20]\t Batch [250][1100]\t Training Loss 0.0242\t Accuracy 0.9953\n",
      "Epoch [16][20]\t Batch [300][1100]\t Training Loss 0.0234\t Accuracy 0.9955\n",
      "Epoch [16][20]\t Batch [350][1100]\t Training Loss 0.0231\t Accuracy 0.9957\n",
      "Epoch [16][20]\t Batch [400][1100]\t Training Loss 0.0228\t Accuracy 0.9958\n",
      "Epoch [16][20]\t Batch [450][1100]\t Training Loss 0.0230\t Accuracy 0.9955\n",
      "Epoch [16][20]\t Batch [500][1100]\t Training Loss 0.0223\t Accuracy 0.9957\n",
      "Epoch [16][20]\t Batch [550][1100]\t Training Loss 0.0234\t Accuracy 0.9954\n",
      "Epoch [16][20]\t Batch [600][1100]\t Training Loss 0.0233\t Accuracy 0.9955\n",
      "Epoch [16][20]\t Batch [650][1100]\t Training Loss 0.0235\t Accuracy 0.9956\n",
      "Epoch [16][20]\t Batch [700][1100]\t Training Loss 0.0236\t Accuracy 0.9956\n",
      "Epoch [16][20]\t Batch [750][1100]\t Training Loss 0.0239\t Accuracy 0.9955\n",
      "Epoch [16][20]\t Batch [800][1100]\t Training Loss 0.0241\t Accuracy 0.9954\n",
      "Epoch [16][20]\t Batch [850][1100]\t Training Loss 0.0240\t Accuracy 0.9953\n",
      "Epoch [16][20]\t Batch [900][1100]\t Training Loss 0.0242\t Accuracy 0.9952\n",
      "Epoch [16][20]\t Batch [950][1100]\t Training Loss 0.0246\t Accuracy 0.9950\n",
      "Epoch [16][20]\t Batch [1000][1100]\t Training Loss 0.0247\t Accuracy 0.9950\n",
      "Epoch [16][20]\t Batch [1050][1100]\t Training Loss 0.0248\t Accuracy 0.9950\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0248\t Average training accuracy 0.9949\n",
      "Epoch [16]\t Average validation loss 0.0734\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [17][20]\t Batch [0][1100]\t Training Loss 0.0200\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][1100]\t Training Loss 0.0235\t Accuracy 0.9941\n",
      "Epoch [17][20]\t Batch [100][1100]\t Training Loss 0.0224\t Accuracy 0.9954\n",
      "Epoch [17][20]\t Batch [150][1100]\t Training Loss 0.0235\t Accuracy 0.9955\n",
      "Epoch [17][20]\t Batch [200][1100]\t Training Loss 0.0229\t Accuracy 0.9958\n",
      "Epoch [17][20]\t Batch [250][1100]\t Training Loss 0.0224\t Accuracy 0.9960\n",
      "Epoch [17][20]\t Batch [300][1100]\t Training Loss 0.0217\t Accuracy 0.9962\n",
      "Epoch [17][20]\t Batch [350][1100]\t Training Loss 0.0213\t Accuracy 0.9963\n",
      "Epoch [17][20]\t Batch [400][1100]\t Training Loss 0.0210\t Accuracy 0.9964\n",
      "Epoch [17][20]\t Batch [450][1100]\t Training Loss 0.0211\t Accuracy 0.9961\n",
      "Epoch [17][20]\t Batch [500][1100]\t Training Loss 0.0206\t Accuracy 0.9963\n",
      "Epoch [17][20]\t Batch [550][1100]\t Training Loss 0.0215\t Accuracy 0.9960\n",
      "Epoch [17][20]\t Batch [600][1100]\t Training Loss 0.0214\t Accuracy 0.9961\n",
      "Epoch [17][20]\t Batch [650][1100]\t Training Loss 0.0216\t Accuracy 0.9961\n",
      "Epoch [17][20]\t Batch [700][1100]\t Training Loss 0.0217\t Accuracy 0.9961\n",
      "Epoch [17][20]\t Batch [750][1100]\t Training Loss 0.0220\t Accuracy 0.9961\n",
      "Epoch [17][20]\t Batch [800][1100]\t Training Loss 0.0221\t Accuracy 0.9960\n",
      "Epoch [17][20]\t Batch [850][1100]\t Training Loss 0.0221\t Accuracy 0.9959\n",
      "Epoch [17][20]\t Batch [900][1100]\t Training Loss 0.0223\t Accuracy 0.9958\n",
      "Epoch [17][20]\t Batch [950][1100]\t Training Loss 0.0226\t Accuracy 0.9956\n",
      "Epoch [17][20]\t Batch [1000][1100]\t Training Loss 0.0227\t Accuracy 0.9957\n",
      "Epoch [17][20]\t Batch [1050][1100]\t Training Loss 0.0227\t Accuracy 0.9956\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0227\t Average training accuracy 0.9955\n",
      "Epoch [17]\t Average validation loss 0.0726\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [18][20]\t Batch [0][1100]\t Training Loss 0.0192\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][1100]\t Training Loss 0.0213\t Accuracy 0.9949\n",
      "Epoch [18][20]\t Batch [100][1100]\t Training Loss 0.0205\t Accuracy 0.9960\n",
      "Epoch [18][20]\t Batch [150][1100]\t Training Loss 0.0217\t Accuracy 0.9958\n",
      "Epoch [18][20]\t Batch [200][1100]\t Training Loss 0.0212\t Accuracy 0.9961\n",
      "Epoch [18][20]\t Batch [250][1100]\t Training Loss 0.0207\t Accuracy 0.9962\n",
      "Epoch [18][20]\t Batch [300][1100]\t Training Loss 0.0201\t Accuracy 0.9964\n",
      "Epoch [18][20]\t Batch [350][1100]\t Training Loss 0.0197\t Accuracy 0.9966\n",
      "Epoch [18][20]\t Batch [400][1100]\t Training Loss 0.0194\t Accuracy 0.9968\n",
      "Epoch [18][20]\t Batch [450][1100]\t Training Loss 0.0196\t Accuracy 0.9966\n",
      "Epoch [18][20]\t Batch [500][1100]\t Training Loss 0.0190\t Accuracy 0.9968\n",
      "Epoch [18][20]\t Batch [550][1100]\t Training Loss 0.0199\t Accuracy 0.9964\n",
      "Epoch [18][20]\t Batch [600][1100]\t Training Loss 0.0197\t Accuracy 0.9965\n",
      "Epoch [18][20]\t Batch [650][1100]\t Training Loss 0.0200\t Accuracy 0.9966\n",
      "Epoch [18][20]\t Batch [700][1100]\t Training Loss 0.0200\t Accuracy 0.9967\n",
      "Epoch [18][20]\t Batch [750][1100]\t Training Loss 0.0202\t Accuracy 0.9966\n",
      "Epoch [18][20]\t Batch [800][1100]\t Training Loss 0.0204\t Accuracy 0.9965\n",
      "Epoch [18][20]\t Batch [850][1100]\t Training Loss 0.0203\t Accuracy 0.9965\n",
      "Epoch [18][20]\t Batch [900][1100]\t Training Loss 0.0205\t Accuracy 0.9965\n",
      "Epoch [18][20]\t Batch [950][1100]\t Training Loss 0.0208\t Accuracy 0.9964\n",
      "Epoch [18][20]\t Batch [1000][1100]\t Training Loss 0.0209\t Accuracy 0.9964\n",
      "Epoch [18][20]\t Batch [1050][1100]\t Training Loss 0.0209\t Accuracy 0.9963\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0209\t Average training accuracy 0.9962\n",
      "Epoch [18]\t Average validation loss 0.0719\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [19][20]\t Batch [0][1100]\t Training Loss 0.0184\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][1100]\t Training Loss 0.0194\t Accuracy 0.9957\n",
      "Epoch [19][20]\t Batch [100][1100]\t Training Loss 0.0190\t Accuracy 0.9964\n",
      "Epoch [19][20]\t Batch [150][1100]\t Training Loss 0.0202\t Accuracy 0.9962\n",
      "Epoch [19][20]\t Batch [200][1100]\t Training Loss 0.0197\t Accuracy 0.9965\n",
      "Epoch [19][20]\t Batch [250][1100]\t Training Loss 0.0193\t Accuracy 0.9965\n",
      "Epoch [19][20]\t Batch [300][1100]\t Training Loss 0.0188\t Accuracy 0.9967\n",
      "Epoch [19][20]\t Batch [350][1100]\t Training Loss 0.0184\t Accuracy 0.9969\n",
      "Epoch [19][20]\t Batch [400][1100]\t Training Loss 0.0181\t Accuracy 0.9971\n",
      "Epoch [19][20]\t Batch [450][1100]\t Training Loss 0.0182\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [500][1100]\t Training Loss 0.0177\t Accuracy 0.9972\n",
      "Epoch [19][20]\t Batch [550][1100]\t Training Loss 0.0185\t Accuracy 0.9969\n",
      "Epoch [19][20]\t Batch [600][1100]\t Training Loss 0.0183\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [650][1100]\t Training Loss 0.0185\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [700][1100]\t Training Loss 0.0185\t Accuracy 0.9971\n",
      "Epoch [19][20]\t Batch [750][1100]\t Training Loss 0.0187\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [800][1100]\t Training Loss 0.0189\t Accuracy 0.9969\n",
      "Epoch [19][20]\t Batch [850][1100]\t Training Loss 0.0188\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [900][1100]\t Training Loss 0.0190\t Accuracy 0.9970\n",
      "Epoch [19][20]\t Batch [950][1100]\t Training Loss 0.0192\t Accuracy 0.9969\n",
      "Epoch [19][20]\t Batch [1000][1100]\t Training Loss 0.0193\t Accuracy 0.9969\n",
      "Epoch [19][20]\t Batch [1050][1100]\t Training Loss 0.0193\t Accuracy 0.9969\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0192\t Average training accuracy 0.9969\n",
      "Epoch [19]\t Average validation loss 0.0711\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.6095\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.1527\t Accuracy 0.0759\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8814\t Accuracy 0.0869\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7769\t Accuracy 0.1008\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7195\t Accuracy 0.1144\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.6790\t Accuracy 0.1329\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6482\t Accuracy 0.1504\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6245\t Accuracy 0.1676\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6039\t Accuracy 0.1873\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.5868\t Accuracy 0.2057\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5719\t Accuracy 0.2228\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5580\t Average training accuracy 0.2420\n",
      "Epoch [0]\t Average validation loss 0.4049\t Average validation accuracy 0.4696\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4142\t Accuracy 0.4600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4041\t Accuracy 0.4622\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4003\t Accuracy 0.4757\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3973\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3952\t Accuracy 0.4857\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3916\t Accuracy 0.4948\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3880\t Accuracy 0.5020\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3860\t Accuracy 0.5077\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3829\t Accuracy 0.5154\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3805\t Accuracy 0.5217\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3780\t Accuracy 0.5279\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3748\t Average training accuracy 0.5359\n",
      "Epoch [1]\t Average validation loss 0.3308\t Average validation accuracy 0.6522\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3377\t Accuracy 0.6500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3369\t Accuracy 0.6304\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3353\t Accuracy 0.6336\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3356\t Accuracy 0.6323\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3354\t Accuracy 0.6351\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3339\t Accuracy 0.6402\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3323\t Accuracy 0.6432\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3322\t Accuracy 0.6430\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3308\t Accuracy 0.6455\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3300\t Accuracy 0.6473\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3291\t Accuracy 0.6496\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3272\t Average training accuracy 0.6537\n",
      "Epoch [2]\t Average validation loss 0.2968\t Average validation accuracy 0.7300\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3030\t Accuracy 0.7400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3058\t Accuracy 0.6965\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3048\t Accuracy 0.7014\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3064\t Accuracy 0.6982\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3067\t Accuracy 0.6991\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3058\t Accuracy 0.7014\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3049\t Accuracy 0.7041\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3054\t Accuracy 0.7033\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3046\t Accuracy 0.7054\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3044\t Accuracy 0.7054\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3040\t Accuracy 0.7067\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3027\t Average training accuracy 0.7091\n",
      "Epoch [3]\t Average validation loss 0.2770\t Average validation accuracy 0.7714\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2826\t Accuracy 0.7500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2874\t Accuracy 0.7339\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2867\t Accuracy 0.7377\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2890\t Accuracy 0.7325\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2893\t Accuracy 0.7338\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2887\t Accuracy 0.7349\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2882\t Accuracy 0.7366\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2889\t Accuracy 0.7364\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2884\t Accuracy 0.7379\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2885\t Accuracy 0.7371\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2883\t Accuracy 0.7378\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2872\t Average training accuracy 0.7397\n",
      "Epoch [4]\t Average validation loss 0.2638\t Average validation accuracy 0.7946\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2691\t Accuracy 0.7700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2749\t Accuracy 0.7608\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2744\t Accuracy 0.7623\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2772\t Accuracy 0.7568\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2775\t Accuracy 0.7563\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2770\t Accuracy 0.7580\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2767\t Accuracy 0.7588\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2775\t Accuracy 0.7580\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2772\t Accuracy 0.7587\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2774\t Accuracy 0.7578\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2773\t Accuracy 0.7582\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2764\t Average training accuracy 0.7594\n",
      "Epoch [5]\t Average validation loss 0.2542\t Average validation accuracy 0.8150\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2594\t Accuracy 0.7700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2657\t Accuracy 0.7769\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2654\t Accuracy 0.7795\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2685\t Accuracy 0.7727\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2687\t Accuracy 0.7722\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2684\t Accuracy 0.7731\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2682\t Accuracy 0.7735\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2691\t Accuracy 0.7727\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2688\t Accuracy 0.7734\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2691\t Accuracy 0.7725\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2691\t Accuracy 0.7726\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2683\t Average training accuracy 0.7738\n",
      "Epoch [6]\t Average validation loss 0.2468\t Average validation accuracy 0.8236\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2519\t Accuracy 0.7700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2586\t Accuracy 0.7912\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2584\t Accuracy 0.7927\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2617\t Accuracy 0.7855\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2619\t Accuracy 0.7850\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2617\t Accuracy 0.7854\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2615\t Accuracy 0.7855\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2624\t Accuracy 0.7845\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2622\t Accuracy 0.7847\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2625\t Accuracy 0.7835\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2627\t Accuracy 0.7836\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2619\t Average training accuracy 0.7847\n",
      "Epoch [7]\t Average validation loss 0.2409\t Average validation accuracy 0.8326\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2461\t Accuracy 0.7900\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2529\t Accuracy 0.8008\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2528\t Accuracy 0.8014\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2562\t Accuracy 0.7940\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2564\t Accuracy 0.7937\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2562\t Accuracy 0.7941\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2561\t Accuracy 0.7941\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2570\t Accuracy 0.7926\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2569\t Accuracy 0.7927\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2573\t Accuracy 0.7916\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2574\t Accuracy 0.7916\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2567\t Average training accuracy 0.7926\n",
      "Epoch [8]\t Average validation loss 0.2361\t Average validation accuracy 0.8416\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2413\t Accuracy 0.7900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2481\t Accuracy 0.8086\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2482\t Accuracy 0.8088\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2517\t Accuracy 0.8011\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2519\t Accuracy 0.8001\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2517\t Accuracy 0.8007\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2517\t Accuracy 0.8006\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2526\t Accuracy 0.7989\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2525\t Accuracy 0.7994\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2529\t Accuracy 0.7982\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2531\t Accuracy 0.7984\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2524\t Average training accuracy 0.7993\n",
      "Epoch [9]\t Average validation loss 0.2320\t Average validation accuracy 0.8486\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2373\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2441\t Accuracy 0.8139\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2443\t Accuracy 0.8145\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2479\t Accuracy 0.8072\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2481\t Accuracy 0.8059\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2479\t Accuracy 0.8059\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2479\t Accuracy 0.8055\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2488\t Accuracy 0.8039\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2488\t Accuracy 0.8043\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2492\t Accuracy 0.8032\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2494\t Accuracy 0.8033\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2487\t Average training accuracy 0.8043\n",
      "Epoch [10]\t Average validation loss 0.2285\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2339\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2407\t Accuracy 0.8180\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2409\t Accuracy 0.8183\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2446\t Accuracy 0.8114\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2448\t Accuracy 0.8098\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2447\t Accuracy 0.8096\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2447\t Accuracy 0.8092\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2456\t Accuracy 0.8076\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2456\t Accuracy 0.8082\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2460\t Accuracy 0.8073\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2462\t Accuracy 0.8074\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2456\t Average training accuracy 0.8084\n",
      "Epoch [11]\t Average validation loss 0.2255\t Average validation accuracy 0.8556\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2310\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2377\t Accuracy 0.8225\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2380\t Accuracy 0.8222\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2418\t Accuracy 0.8150\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2419\t Accuracy 0.8134\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2419\t Accuracy 0.8133\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2418\t Accuracy 0.8133\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2428\t Accuracy 0.8118\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2428\t Accuracy 0.8124\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2432\t Accuracy 0.8115\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2434\t Accuracy 0.8116\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2428\t Average training accuracy 0.8125\n",
      "Epoch [12]\t Average validation loss 0.2229\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2284\t Accuracy 0.8700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2351\t Accuracy 0.8269\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2355\t Accuracy 0.8257\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2393\t Accuracy 0.8178\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2394\t Accuracy 0.8166\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2394\t Accuracy 0.8163\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2394\t Accuracy 0.8162\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2403\t Accuracy 0.8147\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2403\t Accuracy 0.8153\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2407\t Accuracy 0.8143\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2410\t Accuracy 0.8145\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2404\t Average training accuracy 0.8153\n",
      "Epoch [13]\t Average validation loss 0.2206\t Average validation accuracy 0.8594\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2262\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2327\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2332\t Accuracy 0.8281\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2370\t Accuracy 0.8201\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2372\t Accuracy 0.8190\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2371\t Accuracy 0.8191\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2372\t Accuracy 0.8189\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2381\t Accuracy 0.8175\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2381\t Accuracy 0.8183\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2385\t Accuracy 0.8173\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2388\t Accuracy 0.8174\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2382\t Average training accuracy 0.8182\n",
      "Epoch [14]\t Average validation loss 0.2185\t Average validation accuracy 0.8630\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2242\t Accuracy 0.8700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2306\t Accuracy 0.8324\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2312\t Accuracy 0.8305\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2350\t Accuracy 0.8226\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2352\t Accuracy 0.8216\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2352\t Accuracy 0.8215\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2352\t Accuracy 0.8210\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2361\t Accuracy 0.8199\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2361\t Accuracy 0.8206\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2365\t Accuracy 0.8196\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2369\t Accuracy 0.8197\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2363\t Average training accuracy 0.8205\n",
      "Epoch [15]\t Average validation loss 0.2167\t Average validation accuracy 0.8648\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2223\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2287\t Accuracy 0.8337\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2293\t Accuracy 0.8319\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2332\t Accuracy 0.8244\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2333\t Accuracy 0.8237\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2334\t Accuracy 0.8240\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2334\t Accuracy 0.8235\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2343\t Accuracy 0.8222\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2344\t Accuracy 0.8228\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2348\t Accuracy 0.8218\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2351\t Accuracy 0.8217\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2345\t Average training accuracy 0.8226\n",
      "Epoch [16]\t Average validation loss 0.2150\t Average validation accuracy 0.8662\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2207\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2270\t Accuracy 0.8351\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2276\t Accuracy 0.8336\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2316\t Accuracy 0.8264\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2317\t Accuracy 0.8255\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2317\t Accuracy 0.8257\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2317\t Accuracy 0.8252\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2326\t Accuracy 0.8238\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2327\t Accuracy 0.8246\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2331\t Accuracy 0.8238\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2335\t Accuracy 0.8238\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2329\t Average training accuracy 0.8246\n",
      "Epoch [17]\t Average validation loss 0.2135\t Average validation accuracy 0.8672\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2191\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2254\t Accuracy 0.8373\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2261\t Accuracy 0.8350\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2301\t Accuracy 0.8278\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2302\t Accuracy 0.8272\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2302\t Accuracy 0.8275\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2303\t Accuracy 0.8271\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2311\t Accuracy 0.8257\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2312\t Accuracy 0.8263\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2317\t Accuracy 0.8255\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2320\t Accuracy 0.8254\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2314\t Average training accuracy 0.8262\n",
      "Epoch [18]\t Average validation loss 0.2121\t Average validation accuracy 0.8678\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2177\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2239\t Accuracy 0.8404\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2247\t Accuracy 0.8372\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2287\t Accuracy 0.8297\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2288\t Accuracy 0.8292\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2288\t Accuracy 0.8295\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2289\t Accuracy 0.8291\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2298\t Accuracy 0.8276\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2299\t Accuracy 0.8281\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2303\t Accuracy 0.8272\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2306\t Accuracy 0.8270\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2301\t Average training accuracy 0.8278\n",
      "Epoch [19]\t Average validation loss 0.2108\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.8768\t Accuracy 0.0500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.4105\t Accuracy 0.1929\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.9388\t Accuracy 0.2333\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7610\t Accuracy 0.2929\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6585\t Accuracy 0.3569\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5873\t Accuracy 0.4084\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.5330\t Accuracy 0.4516\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4915\t Accuracy 0.4897\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4578\t Accuracy 0.5248\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4289\t Accuracy 0.5577\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.4046\t Accuracy 0.5845\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3833\t Average training accuracy 0.6080\n",
      "Epoch [0]\t Average validation loss 0.1452\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1637\t Accuracy 0.8300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1517\t Accuracy 0.8606\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1496\t Accuracy 0.8614\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1498\t Accuracy 0.8613\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1467\t Accuracy 0.8646\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1437\t Accuracy 0.8681\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1412\t Accuracy 0.8713\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1398\t Accuracy 0.8716\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1386\t Accuracy 0.8721\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1371\t Accuracy 0.8731\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1362\t Accuracy 0.8740\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1346\t Average training accuracy 0.8751\n",
      "Epoch [1]\t Average validation loss 0.1032\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1194\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1131\t Accuracy 0.8994\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1156\t Accuracy 0.8945\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1179\t Accuracy 0.8915\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1171\t Accuracy 0.8923\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1160\t Accuracy 0.8941\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1154\t Accuracy 0.8952\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1153\t Accuracy 0.8950\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1153\t Accuracy 0.8947\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1150\t Accuracy 0.8954\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1150\t Accuracy 0.8952\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1143\t Average training accuracy 0.8959\n",
      "Epoch [2]\t Average validation loss 0.0931\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1035\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1013\t Accuracy 0.9163\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1044\t Accuracy 0.9119\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1069\t Accuracy 0.9087\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1062\t Accuracy 0.9093\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1054\t Accuracy 0.9105\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1051\t Accuracy 0.9105\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1051\t Accuracy 0.9103\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1052\t Accuracy 0.9096\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1050\t Accuracy 0.9099\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1052\t Accuracy 0.9093\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1047\t Average training accuracy 0.9095\n",
      "Epoch [3]\t Average validation loss 0.0867\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0929\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0938\t Accuracy 0.9261\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0967\t Accuracy 0.9203\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0993\t Accuracy 0.9163\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0987\t Accuracy 0.9169\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0981\t Accuracy 0.9178\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0980\t Accuracy 0.9177\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0981\t Accuracy 0.9173\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0983\t Accuracy 0.9165\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0981\t Accuracy 0.9167\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0985\t Accuracy 0.9159\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0981\t Average training accuracy 0.9161\n",
      "Epoch [4]\t Average validation loss 0.0826\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0874\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0889\t Accuracy 0.9300\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0919\t Accuracy 0.9251\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0944\t Accuracy 0.9211\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0938\t Accuracy 0.9223\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0934\t Accuracy 0.9230\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0934\t Accuracy 0.9225\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0935\t Accuracy 0.9222\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0937\t Accuracy 0.9215\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0936\t Accuracy 0.9218\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0939\t Accuracy 0.9211\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0936\t Average training accuracy 0.9211\n",
      "Epoch [5]\t Average validation loss 0.0796\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0840\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0852\t Accuracy 0.9329\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0881\t Accuracy 0.9288\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0906\t Accuracy 0.9252\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0902\t Accuracy 0.9261\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0897\t Accuracy 0.9269\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0897\t Accuracy 0.9263\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0898\t Accuracy 0.9261\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0901\t Accuracy 0.9255\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0899\t Accuracy 0.9256\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0903\t Accuracy 0.9250\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0900\t Average training accuracy 0.9249\n",
      "Epoch [6]\t Average validation loss 0.0773\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0811\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0821\t Accuracy 0.9357\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0849\t Accuracy 0.9316\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0874\t Accuracy 0.9278\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0870\t Accuracy 0.9289\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0865\t Accuracy 0.9299\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0866\t Accuracy 0.9290\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0867\t Accuracy 0.9289\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0869\t Accuracy 0.9283\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0868\t Accuracy 0.9283\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0872\t Accuracy 0.9277\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0869\t Average training accuracy 0.9277\n",
      "Epoch [7]\t Average validation loss 0.0751\t Average validation accuracy 0.9442\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0781\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0793\t Accuracy 0.9371\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0820\t Accuracy 0.9332\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0845\t Accuracy 0.9294\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0841\t Accuracy 0.9308\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0837\t Accuracy 0.9318\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0838\t Accuracy 0.9310\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0839\t Accuracy 0.9310\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0841\t Accuracy 0.9305\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0841\t Accuracy 0.9307\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0844\t Accuracy 0.9301\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0842\t Average training accuracy 0.9304\n",
      "Epoch [8]\t Average validation loss 0.0731\t Average validation accuracy 0.9456\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0755\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0770\t Accuracy 0.9384\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0796\t Accuracy 0.9356\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0821\t Accuracy 0.9317\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0816\t Accuracy 0.9331\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0812\t Accuracy 0.9340\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0814\t Accuracy 0.9332\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0815\t Accuracy 0.9334\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0818\t Accuracy 0.9329\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0817\t Accuracy 0.9331\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0821\t Accuracy 0.9326\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0818\t Average training accuracy 0.9328\n",
      "Epoch [9]\t Average validation loss 0.0714\t Average validation accuracy 0.9460\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0734\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0750\t Accuracy 0.9416\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0775\t Accuracy 0.9380\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0800\t Accuracy 0.9344\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0796\t Accuracy 0.9355\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0792\t Accuracy 0.9362\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0794\t Accuracy 0.9354\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0795\t Accuracy 0.9355\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0798\t Accuracy 0.9351\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0797\t Accuracy 0.9353\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0801\t Accuracy 0.9348\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0799\t Average training accuracy 0.9348\n",
      "Epoch [10]\t Average validation loss 0.0699\t Average validation accuracy 0.9460\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0718\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0733\t Accuracy 0.9424\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0758\t Accuracy 0.9393\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0782\t Accuracy 0.9358\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0778\t Accuracy 0.9366\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0774\t Accuracy 0.9373\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0777\t Accuracy 0.9366\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0777\t Accuracy 0.9366\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0780\t Accuracy 0.9362\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0780\t Accuracy 0.9366\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0784\t Accuracy 0.9361\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0781\t Average training accuracy 0.9362\n",
      "Epoch [11]\t Average validation loss 0.0685\t Average validation accuracy 0.9484\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0702\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0719\t Accuracy 0.9435\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0743\t Accuracy 0.9405\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0767\t Accuracy 0.9371\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0762\t Accuracy 0.9380\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0759\t Accuracy 0.9385\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0761\t Accuracy 0.9379\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0762\t Accuracy 0.9381\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0765\t Accuracy 0.9378\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0764\t Accuracy 0.9381\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0768\t Accuracy 0.9376\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0766\t Average training accuracy 0.9377\n",
      "Epoch [12]\t Average validation loss 0.0673\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0687\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0705\t Accuracy 0.9461\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0729\t Accuracy 0.9423\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0753\t Accuracy 0.9386\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0748\t Accuracy 0.9395\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0744\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0747\t Accuracy 0.9394\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0748\t Accuracy 0.9397\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0751\t Accuracy 0.9392\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0750\t Accuracy 0.9396\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0753\t Accuracy 0.9393\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0751\t Average training accuracy 0.9392\n",
      "Epoch [13]\t Average validation loss 0.0661\t Average validation accuracy 0.9506\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0672\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0693\t Accuracy 0.9482\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0717\t Accuracy 0.9440\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0739\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0734\t Accuracy 0.9411\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0731\t Accuracy 0.9412\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0734\t Accuracy 0.9406\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0735\t Accuracy 0.9408\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0738\t Accuracy 0.9403\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0737\t Accuracy 0.9408\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0740\t Accuracy 0.9405\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0738\t Average training accuracy 0.9404\n",
      "Epoch [14]\t Average validation loss 0.0649\t Average validation accuracy 0.9514\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0659\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0680\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0705\t Accuracy 0.9450\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0727\t Accuracy 0.9411\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0722\t Accuracy 0.9422\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0719\t Accuracy 0.9424\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0722\t Accuracy 0.9418\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0722\t Accuracy 0.9421\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0726\t Accuracy 0.9416\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0725\t Accuracy 0.9418\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0728\t Accuracy 0.9416\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0726\t Average training accuracy 0.9415\n",
      "Epoch [15]\t Average validation loss 0.0640\t Average validation accuracy 0.9534\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0648\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0669\t Accuracy 0.9498\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0694\t Accuracy 0.9455\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0716\t Accuracy 0.9418\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0711\t Accuracy 0.9430\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0707\t Accuracy 0.9433\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0710\t Accuracy 0.9427\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0711\t Accuracy 0.9429\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0715\t Accuracy 0.9424\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0714\t Accuracy 0.9426\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0717\t Accuracy 0.9423\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0715\t Average training accuracy 0.9423\n",
      "Epoch [16]\t Average validation loss 0.0631\t Average validation accuracy 0.9542\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0636\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0659\t Accuracy 0.9506\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0683\t Accuracy 0.9464\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0706\t Accuracy 0.9428\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0700\t Accuracy 0.9442\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0697\t Accuracy 0.9444\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0700\t Accuracy 0.9438\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0701\t Accuracy 0.9439\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0704\t Accuracy 0.9433\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0704\t Accuracy 0.9436\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0706\t Accuracy 0.9432\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0704\t Average training accuracy 0.9432\n",
      "Epoch [17]\t Average validation loss 0.0624\t Average validation accuracy 0.9546\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0626\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0649\t Accuracy 0.9514\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0674\t Accuracy 0.9472\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0697\t Accuracy 0.9433\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0691\t Accuracy 0.9447\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0688\t Accuracy 0.9451\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0691\t Accuracy 0.9447\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0692\t Accuracy 0.9447\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0695\t Accuracy 0.9440\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0695\t Accuracy 0.9442\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0697\t Accuracy 0.9439\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0695\t Average training accuracy 0.9439\n",
      "Epoch [18]\t Average validation loss 0.0618\t Average validation accuracy 0.9544\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0617\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0641\t Accuracy 0.9518\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0666\t Accuracy 0.9475\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0689\t Accuracy 0.9441\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0683\t Accuracy 0.9455\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0679\t Accuracy 0.9459\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0683\t Accuracy 0.9455\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0684\t Accuracy 0.9456\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0687\t Accuracy 0.9449\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0687\t Accuracy 0.9451\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0689\t Accuracy 0.9448\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0687\t Average training accuracy 0.9448\n",
      "Epoch [19]\t Average validation loss 0.0613\t Average validation accuracy 0.9562\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.4305\t Accuracy 0.0500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.9737\t Accuracy 0.0882\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.7745\t Accuracy 0.1068\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.6583\t Accuracy 0.1290\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5718\t Accuracy 0.1519\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.5067\t Accuracy 0.1701\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.4554\t Accuracy 0.1881\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.4145\t Accuracy 0.2031\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3790\t Accuracy 0.2197\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.3499\t Accuracy 0.2336\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3228\t Accuracy 0.2502\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2992\t Average training accuracy 0.2659\n",
      "Epoch [0]\t Average validation loss 2.0394\t Average validation accuracy 0.4534\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.0564\t Accuracy 0.4500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0380\t Accuracy 0.4502\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0243\t Accuracy 0.4589\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0174\t Accuracy 0.4632\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.0074\t Accuracy 0.4695\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.9965\t Accuracy 0.4772\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.9862\t Accuracy 0.4842\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.9783\t Accuracy 0.4903\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.9683\t Accuracy 0.4967\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.9602\t Accuracy 0.5024\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.9502\t Accuracy 0.5096\n",
      "\n",
      "Epoch [1]\t Average training loss 1.9405\t Average training accuracy 0.5158\n",
      "Epoch [1]\t Average validation loss 1.8137\t Average validation accuracy 0.6118\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.8334\t Accuracy 0.6200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.8228\t Accuracy 0.5900\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.8117\t Accuracy 0.5970\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.8078\t Accuracy 0.5947\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.8002\t Accuracy 0.5970\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.7906\t Accuracy 0.6012\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.7814\t Accuracy 0.6042\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.7761\t Accuracy 0.6072\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.7676\t Accuracy 0.6108\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.7612\t Accuracy 0.6136\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.7527\t Accuracy 0.6176\n",
      "\n",
      "Epoch [2]\t Average training loss 1.7442\t Average training accuracy 0.6218\n",
      "Epoch [2]\t Average validation loss 1.6221\t Average validation accuracy 0.6970\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.6441\t Accuracy 0.6900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.6400\t Accuracy 0.6700\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.6311\t Accuracy 0.6762\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.6295\t Accuracy 0.6693\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.6240\t Accuracy 0.6702\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.6154\t Accuracy 0.6732\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.6072\t Accuracy 0.6753\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.6041\t Accuracy 0.6763\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.5968\t Accuracy 0.6790\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.5918\t Accuracy 0.6810\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.5846\t Accuracy 0.6832\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5772\t Average training accuracy 0.6864\n",
      "Epoch [3]\t Average validation loss 1.4592\t Average validation accuracy 0.7526\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.4840\t Accuracy 0.7600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.4850\t Accuracy 0.7182\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.4780\t Accuracy 0.7228\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.4786\t Accuracy 0.7148\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.4746\t Accuracy 0.7146\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.4670\t Accuracy 0.7170\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.4597\t Accuracy 0.7178\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.4584\t Accuracy 0.7181\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.4521\t Accuracy 0.7202\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.4483\t Accuracy 0.7218\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.4422\t Accuracy 0.7232\n",
      "\n",
      "Epoch [4]\t Average training loss 1.4357\t Average training accuracy 0.7256\n",
      "Epoch [4]\t Average validation loss 1.3215\t Average validation accuracy 0.7874\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.3493\t Accuracy 0.7900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.3541\t Accuracy 0.7480\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.3489\t Accuracy 0.7508\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.3512\t Accuracy 0.7434\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.3485\t Accuracy 0.7440\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.3417\t Accuracy 0.7460\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.3353\t Accuracy 0.7466\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.3353\t Accuracy 0.7469\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.3300\t Accuracy 0.7485\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.3271\t Accuracy 0.7495\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.3220\t Accuracy 0.7505\n",
      "\n",
      "Epoch [5]\t Average training loss 1.3163\t Average training accuracy 0.7525\n",
      "Epoch [5]\t Average validation loss 1.2054\t Average validation accuracy 0.8076\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.2361\t Accuracy 0.8100\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.2437\t Accuracy 0.7704\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.2402\t Accuracy 0.7738\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.2439\t Accuracy 0.7658\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.2422\t Accuracy 0.7664\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.2360\t Accuracy 0.7675\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.2304\t Accuracy 0.7682\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.2316\t Accuracy 0.7679\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.2270\t Accuracy 0.7695\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.2248\t Accuracy 0.7701\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.2205\t Accuracy 0.7708\n",
      "\n",
      "Epoch [6]\t Average training loss 1.2155\t Average training accuracy 0.7727\n",
      "Epoch [6]\t Average validation loss 1.1074\t Average validation accuracy 0.8280\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.1409\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.1505\t Accuracy 0.7878\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.1484\t Accuracy 0.7904\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.1533\t Accuracy 0.7823\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.1523\t Accuracy 0.7829\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.1468\t Accuracy 0.7834\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.1419\t Accuracy 0.7838\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.1439\t Accuracy 0.7830\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.1399\t Accuracy 0.7844\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.1383\t Accuracy 0.7847\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.1347\t Accuracy 0.7851\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1303\t Average training accuracy 0.7867\n",
      "Epoch [7]\t Average validation loss 1.0245\t Average validation accuracy 0.8384\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.0606\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.0715\t Accuracy 0.8012\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.0707\t Accuracy 0.8023\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.0766\t Accuracy 0.7946\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.0762\t Accuracy 0.7949\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.0711\t Accuracy 0.7956\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.0668\t Accuracy 0.7963\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.0695\t Accuracy 0.7957\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.0660\t Accuracy 0.7971\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.0649\t Accuracy 0.7971\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.0619\t Accuracy 0.7974\n",
      "\n",
      "Epoch [8]\t Average training loss 1.0579\t Average training accuracy 0.7986\n",
      "Epoch [8]\t Average validation loss 0.9540\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.9925\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.0043\t Accuracy 0.8118\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.0045\t Accuracy 0.8130\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.0112\t Accuracy 0.8048\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.0112\t Accuracy 0.8051\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.0066\t Accuracy 0.8049\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.0027\t Accuracy 0.8055\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.0060\t Accuracy 0.8050\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.0029\t Accuracy 0.8068\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.0022\t Accuracy 0.8063\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.9996\t Accuracy 0.8064\n",
      "\n",
      "Epoch [9]\t Average training loss 0.9961\t Average training accuracy 0.8074\n",
      "Epoch [9]\t Average validation loss 0.8937\t Average validation accuracy 0.8596\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.9343\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.9466\t Accuracy 0.8206\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.9478\t Accuracy 0.8212\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.9552\t Accuracy 0.8130\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.9555\t Accuracy 0.8129\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.9512\t Accuracy 0.8122\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.9477\t Accuracy 0.8132\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.9514\t Accuracy 0.8124\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.9487\t Accuracy 0.8141\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.9483\t Accuracy 0.8135\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.9461\t Accuracy 0.8137\n",
      "\n",
      "Epoch [10]\t Average training loss 0.9429\t Average training accuracy 0.8145\n",
      "Epoch [10]\t Average validation loss 0.8418\t Average validation accuracy 0.8634\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8842\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8967\t Accuracy 0.8269\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8987\t Accuracy 0.8271\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.9067\t Accuracy 0.8193\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.9072\t Accuracy 0.8195\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.9032\t Accuracy 0.8189\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.9002\t Accuracy 0.8196\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.9042\t Accuracy 0.8187\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.9017\t Accuracy 0.8202\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.9015\t Accuracy 0.8195\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8997\t Accuracy 0.8195\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8968\t Average training accuracy 0.8204\n",
      "Epoch [11]\t Average validation loss 0.7967\t Average validation accuracy 0.8682\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8408\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8534\t Accuracy 0.8314\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8561\t Accuracy 0.8309\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8646\t Accuracy 0.8239\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8652\t Accuracy 0.8248\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8615\t Accuracy 0.8244\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8587\t Accuracy 0.8252\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8630\t Accuracy 0.8243\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8607\t Accuracy 0.8258\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8608\t Accuracy 0.8252\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8593\t Accuracy 0.8250\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8566\t Average training accuracy 0.8256\n",
      "Epoch [12]\t Average validation loss 0.7573\t Average validation accuracy 0.8718\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8029\t Accuracy 0.8600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8153\t Accuracy 0.8355\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8187\t Accuracy 0.8350\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8276\t Accuracy 0.8287\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8283\t Accuracy 0.8298\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8248\t Accuracy 0.8292\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8224\t Accuracy 0.8302\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8268\t Accuracy 0.8295\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8248\t Accuracy 0.8307\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8250\t Accuracy 0.8302\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8237\t Accuracy 0.8301\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8212\t Average training accuracy 0.8307\n",
      "Epoch [13]\t Average validation loss 0.7226\t Average validation accuracy 0.8740\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7695\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7818\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7857\t Accuracy 0.8396\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.7950\t Accuracy 0.8333\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.7958\t Accuracy 0.8339\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.7924\t Accuracy 0.8331\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7902\t Accuracy 0.8342\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.7948\t Accuracy 0.8335\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.7929\t Accuracy 0.8347\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.7933\t Accuracy 0.8343\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.7922\t Accuracy 0.8340\n",
      "\n",
      "Epoch [14]\t Average training loss 0.7899\t Average training accuracy 0.8345\n",
      "Epoch [14]\t Average validation loss 0.6920\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7399\t Accuracy 0.8600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7519\t Accuracy 0.8435\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7563\t Accuracy 0.8425\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7659\t Accuracy 0.8367\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7668\t Accuracy 0.8371\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7636\t Accuracy 0.8364\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7616\t Accuracy 0.8375\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7663\t Accuracy 0.8368\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7646\t Accuracy 0.8379\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.7650\t Accuracy 0.8373\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.7642\t Accuracy 0.8370\n",
      "\n",
      "Epoch [15]\t Average training loss 0.7620\t Average training accuracy 0.8376\n",
      "Epoch [15]\t Average validation loss 0.6646\t Average validation accuracy 0.8796\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7136\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7252\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7301\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7400\t Accuracy 0.8397\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.7408\t Accuracy 0.8404\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7378\t Accuracy 0.8394\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7360\t Accuracy 0.8402\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.7408\t Accuracy 0.8395\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.7392\t Accuracy 0.8408\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.7397\t Accuracy 0.8404\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.7390\t Accuracy 0.8398\n",
      "\n",
      "Epoch [16]\t Average training loss 0.7371\t Average training accuracy 0.8403\n",
      "Epoch [16]\t Average validation loss 0.6401\t Average validation accuracy 0.8824\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.6899\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7012\t Accuracy 0.8504\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7065\t Accuracy 0.8487\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.7166\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.7175\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.7146\t Accuracy 0.8429\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.7130\t Accuracy 0.8435\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.7178\t Accuracy 0.8426\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.7163\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.7169\t Accuracy 0.8431\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.7164\t Accuracy 0.8428\n",
      "\n",
      "Epoch [17]\t Average training loss 0.7145\t Average training accuracy 0.8432\n",
      "Epoch [17]\t Average validation loss 0.6181\t Average validation accuracy 0.8844\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.6685\t Accuracy 0.8700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.6795\t Accuracy 0.8533\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.6851\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.6955\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.6964\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.6936\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.6921\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.6970\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.6956\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.6963\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.6959\t Accuracy 0.8452\n",
      "\n",
      "Epoch [18]\t Average training loss 0.6941\t Average training accuracy 0.8455\n",
      "Epoch [18]\t Average validation loss 0.5981\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.6492\t Accuracy 0.8700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.6598\t Accuracy 0.8586\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.6657\t Accuracy 0.8555\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.6763\t Accuracy 0.8493\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.6771\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.6745\t Accuracy 0.8489\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.6731\t Accuracy 0.8493\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.6781\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.6768\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.6775\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.6772\t Accuracy 0.8482\n",
      "\n",
      "Epoch [19]\t Average training loss 0.6756\t Average training accuracy 0.8485\n",
      "Epoch [19]\t Average validation loss 0.5799\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5474\t Accuracy 0.0500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.0314\t Accuracy 0.3131\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4613\t Accuracy 0.5419\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1675\t Accuracy 0.6351\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9938\t Accuracy 0.6913\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8792\t Accuracy 0.7286\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.7998\t Accuracy 0.7541\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7439\t Accuracy 0.7722\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6949\t Accuracy 0.7881\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6551\t Accuracy 0.8005\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6226\t Accuracy 0.8103\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5944\t Average training accuracy 0.8192\n",
      "Epoch [0]\t Average validation loss 0.2215\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2762\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2550\t Accuracy 0.9284\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2631\t Accuracy 0.9256\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2690\t Accuracy 0.9226\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2625\t Accuracy 0.9251\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2598\t Accuracy 0.9263\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2584\t Accuracy 0.9265\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2566\t Accuracy 0.9266\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2540\t Accuracy 0.9274\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2519\t Accuracy 0.9276\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2512\t Accuracy 0.9274\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2487\t Average training accuracy 0.9283\n",
      "Epoch [1]\t Average validation loss 0.1644\t Average validation accuracy 0.9572\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1684\t Accuracy 0.9700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1844\t Accuracy 0.9508\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1932\t Accuracy 0.9456\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1988\t Accuracy 0.9434\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1957\t Accuracy 0.9446\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1953\t Accuracy 0.9444\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1954\t Accuracy 0.9446\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1948\t Accuracy 0.9446\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1937\t Accuracy 0.9448\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1931\t Accuracy 0.9449\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1936\t Accuracy 0.9446\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1925\t Average training accuracy 0.9452\n",
      "Epoch [2]\t Average validation loss 0.1400\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1443\t Accuracy 0.9700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1531\t Accuracy 0.9598\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1601\t Accuracy 0.9557\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1638\t Accuracy 0.9541\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1620\t Accuracy 0.9541\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1616\t Accuracy 0.9538\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1620\t Accuracy 0.9541\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1618\t Accuracy 0.9540\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1609\t Accuracy 0.9544\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1608\t Accuracy 0.9546\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1616\t Accuracy 0.9542\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1608\t Average training accuracy 0.9545\n",
      "Epoch [3]\t Average validation loss 0.1221\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1282\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1316\t Accuracy 0.9645\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1373\t Accuracy 0.9615\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1395\t Accuracy 0.9605\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1384\t Accuracy 0.9613\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1378\t Accuracy 0.9612\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1386\t Accuracy 0.9608\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1386\t Accuracy 0.9608\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1379\t Accuracy 0.9613\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1379\t Accuracy 0.9616\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1387\t Accuracy 0.9610\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1382\t Average training accuracy 0.9613\n",
      "Epoch [4]\t Average validation loss 0.1106\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1154\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1152\t Accuracy 0.9680\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1206\t Accuracy 0.9663\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1217\t Accuracy 0.9654\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1210\t Accuracy 0.9659\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1203\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1214\t Accuracy 0.9651\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1214\t Accuracy 0.9651\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1210\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1210\t Accuracy 0.9657\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1218\t Accuracy 0.9653\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1215\t Average training accuracy 0.9655\n",
      "Epoch [5]\t Average validation loss 0.1025\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1045\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1030\t Accuracy 0.9722\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1079\t Accuracy 0.9705\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1080\t Accuracy 0.9695\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1076\t Accuracy 0.9695\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1069\t Accuracy 0.9692\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1080\t Accuracy 0.9688\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1081\t Accuracy 0.9688\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1079\t Accuracy 0.9693\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1079\t Accuracy 0.9695\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1088\t Accuracy 0.9690\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1085\t Average training accuracy 0.9692\n",
      "Epoch [6]\t Average validation loss 0.0960\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0941\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0931\t Accuracy 0.9747\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0977\t Accuracy 0.9739\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0972\t Accuracy 0.9733\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0969\t Accuracy 0.9730\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0963\t Accuracy 0.9726\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0973\t Accuracy 0.9723\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0975\t Accuracy 0.9722\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0975\t Accuracy 0.9724\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0975\t Accuracy 0.9726\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0983\t Accuracy 0.9722\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0981\t Average training accuracy 0.9722\n",
      "Epoch [7]\t Average validation loss 0.0910\t Average validation accuracy 0.9734\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0853\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0851\t Accuracy 0.9771\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0892\t Accuracy 0.9759\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0884\t Accuracy 0.9759\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0881\t Accuracy 0.9756\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0877\t Accuracy 0.9755\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0886\t Accuracy 0.9751\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0887\t Accuracy 0.9750\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0888\t Accuracy 0.9751\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0889\t Accuracy 0.9753\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0897\t Accuracy 0.9749\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0895\t Average training accuracy 0.9750\n",
      "Epoch [8]\t Average validation loss 0.0870\t Average validation accuracy 0.9732\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0770\t Accuracy 0.9800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0781\t Accuracy 0.9796\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0819\t Accuracy 0.9784\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0809\t Accuracy 0.9782\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0807\t Accuracy 0.9779\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0803\t Accuracy 0.9779\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0811\t Accuracy 0.9775\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0813\t Accuracy 0.9774\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0815\t Accuracy 0.9774\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0816\t Accuracy 0.9776\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0823\t Accuracy 0.9774\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0821\t Average training accuracy 0.9774\n",
      "Epoch [9]\t Average validation loss 0.0839\t Average validation accuracy 0.9744\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0708\t Accuracy 0.9800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0723\t Accuracy 0.9808\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0758\t Accuracy 0.9801\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0746\t Accuracy 0.9799\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0745\t Accuracy 0.9793\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0740\t Accuracy 0.9794\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0748\t Accuracy 0.9793\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0750\t Accuracy 0.9793\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0753\t Accuracy 0.9792\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0753\t Accuracy 0.9795\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0760\t Accuracy 0.9793\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0758\t Average training accuracy 0.9793\n",
      "Epoch [10]\t Average validation loss 0.0815\t Average validation accuracy 0.9752\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0655\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0670\t Accuracy 0.9816\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0703\t Accuracy 0.9811\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0691\t Accuracy 0.9811\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0690\t Accuracy 0.9807\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0685\t Accuracy 0.9810\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0693\t Accuracy 0.9808\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0695\t Accuracy 0.9808\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0698\t Accuracy 0.9805\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0699\t Accuracy 0.9809\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0705\t Accuracy 0.9807\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0703\t Average training accuracy 0.9806\n",
      "Epoch [11]\t Average validation loss 0.0794\t Average validation accuracy 0.9750\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0614\t Accuracy 0.9800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0625\t Accuracy 0.9833\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0654\t Accuracy 0.9826\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0642\t Accuracy 0.9826\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0641\t Accuracy 0.9824\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0637\t Accuracy 0.9826\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.9825\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.9825\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0650\t Accuracy 0.9822\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0651\t Accuracy 0.9824\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0656\t Accuracy 0.9822\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0654\t Average training accuracy 0.9821\n",
      "Epoch [12]\t Average validation loss 0.0778\t Average validation accuracy 0.9756\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0575\t Accuracy 0.9800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0585\t Accuracy 0.9849\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0611\t Accuracy 0.9843\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0597\t Accuracy 0.9842\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0598\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0594\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0602\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0603\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0607\t Accuracy 0.9837\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0608\t Accuracy 0.9838\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0613\t Accuracy 0.9837\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0611\t Average training accuracy 0.9836\n",
      "Epoch [13]\t Average validation loss 0.0765\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0536\t Accuracy 0.9800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0549\t Accuracy 0.9853\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0572\t Accuracy 0.9849\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0559\t Accuracy 0.9851\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0559\t Accuracy 0.9848\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0555\t Accuracy 0.9848\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0563\t Accuracy 0.9850\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0565\t Accuracy 0.9850\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0569\t Accuracy 0.9848\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0569\t Accuracy 0.9849\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0574\t Accuracy 0.9848\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0572\t Average training accuracy 0.9847\n",
      "Epoch [14]\t Average validation loss 0.0749\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0505\t Accuracy 0.9800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0518\t Accuracy 0.9859\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0538\t Accuracy 0.9856\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0524\t Accuracy 0.9859\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0525\t Accuracy 0.9857\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0521\t Accuracy 0.9857\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0529\t Accuracy 0.9858\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0530\t Accuracy 0.9859\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0535\t Accuracy 0.9858\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0535\t Accuracy 0.9858\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0540\t Accuracy 0.9857\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0537\t Average training accuracy 0.9857\n",
      "Epoch [15]\t Average validation loss 0.0737\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0483\t Accuracy 0.9800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0490\t Accuracy 0.9875\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0507\t Accuracy 0.9866\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0493\t Accuracy 0.9869\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0494\t Accuracy 0.9867\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0490\t Accuracy 0.9868\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0498\t Accuracy 0.9870\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0499\t Accuracy 0.9870\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0504\t Accuracy 0.9868\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0504\t Accuracy 0.9868\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0508\t Accuracy 0.9867\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0505\t Average training accuracy 0.9868\n",
      "Epoch [16]\t Average validation loss 0.0726\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0464\t Accuracy 0.9800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0465\t Accuracy 0.9880\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0480\t Accuracy 0.9880\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0465\t Accuracy 0.9881\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0466\t Accuracy 0.9880\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0461\t Accuracy 0.9880\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0469\t Accuracy 0.9881\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0471\t Accuracy 0.9881\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0475\t Accuracy 0.9879\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0475\t Accuracy 0.9879\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0480\t Accuracy 0.9878\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0476\t Average training accuracy 0.9880\n",
      "Epoch [17]\t Average validation loss 0.0718\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0449\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0441\t Accuracy 0.9890\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0455\t Accuracy 0.9888\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0439\t Accuracy 0.9889\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0440\t Accuracy 0.9888\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0435\t Accuracy 0.9889\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0443\t Accuracy 0.9889\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0445\t Accuracy 0.9889\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0449\t Accuracy 0.9888\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0449\t Accuracy 0.9888\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0453\t Accuracy 0.9886\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0450\t Average training accuracy 0.9887\n",
      "Epoch [18]\t Average validation loss 0.0710\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0436\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0419\t Accuracy 0.9894\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0432\t Accuracy 0.9895\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0416\t Accuracy 0.9897\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0417\t Accuracy 0.9895\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0411\t Accuracy 0.9896\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0420\t Accuracy 0.9896\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0421\t Accuracy 0.9897\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0425\t Accuracy 0.9896\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0425\t Accuracy 0.9896\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0429\t Accuracy 0.9894\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0426\t Average training accuracy 0.9895\n",
      "Epoch [19]\t Average validation loss 0.0703\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [0][20]\t Batch [0][183]\t Training Loss 2.7512\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][183]\t Training Loss 0.8989\t Accuracy 0.0887\n",
      "Epoch [0][20]\t Batch [100][183]\t Training Loss 0.7440\t Accuracy 0.0994\n",
      "Epoch [0][20]\t Batch [150][183]\t Training Loss 0.6817\t Accuracy 0.1148\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6557\t Average training accuracy 0.1246\n",
      "Epoch [0]\t Average validation loss 0.5203\t Average validation accuracy 0.1929\n",
      "\n",
      "Epoch [1][20]\t Batch [0][183]\t Training Loss 0.5167\t Accuracy 0.2133\n",
      "Epoch [1][20]\t Batch [50][183]\t Training Loss 0.5124\t Accuracy 0.2169\n",
      "Epoch [1][20]\t Batch [100][183]\t Training Loss 0.5048\t Accuracy 0.2326\n",
      "Epoch [1][20]\t Batch [150][183]\t Training Loss 0.4966\t Accuracy 0.2501\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4910\t Average training accuracy 0.2622\n",
      "Epoch [1]\t Average validation loss 0.4510\t Average validation accuracy 0.3406\n",
      "\n",
      "Epoch [2][20]\t Batch [0][183]\t Training Loss 0.4525\t Accuracy 0.3533\n",
      "Epoch [2][20]\t Batch [50][183]\t Training Loss 0.4505\t Accuracy 0.3639\n",
      "Epoch [2][20]\t Batch [100][183]\t Training Loss 0.4465\t Accuracy 0.3710\n",
      "Epoch [2][20]\t Batch [150][183]\t Training Loss 0.4419\t Accuracy 0.3846\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4381\t Average training accuracy 0.3936\n",
      "Epoch [2]\t Average validation loss 0.4060\t Average validation accuracy 0.4681\n",
      "\n",
      "Epoch [3][20]\t Batch [0][183]\t Training Loss 0.4144\t Accuracy 0.4433\n",
      "Epoch [3][20]\t Batch [50][183]\t Training Loss 0.4104\t Accuracy 0.4674\n",
      "Epoch [3][20]\t Batch [100][183]\t Training Loss 0.4081\t Accuracy 0.4716\n",
      "Epoch [3][20]\t Batch [150][183]\t Training Loss 0.4054\t Accuracy 0.4801\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4028\t Average training accuracy 0.4876\n",
      "Epoch [3]\t Average validation loss 0.3742\t Average validation accuracy 0.5563\n",
      "\n",
      "Epoch [4][20]\t Batch [0][183]\t Training Loss 0.3653\t Accuracy 0.5833\n",
      "Epoch [4][20]\t Batch [50][183]\t Training Loss 0.3821\t Accuracy 0.5420\n",
      "Epoch [4][20]\t Batch [100][183]\t Training Loss 0.3807\t Accuracy 0.5429\n",
      "Epoch [4][20]\t Batch [150][183]\t Training Loss 0.3794\t Accuracy 0.5462\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3774\t Average training accuracy 0.5520\n",
      "Epoch [4]\t Average validation loss 0.3544\t Average validation accuracy 0.6121\n",
      "\n",
      "Epoch [5][20]\t Batch [0][183]\t Training Loss 0.3424\t Accuracy 0.6633\n",
      "Epoch [5][20]\t Batch [50][183]\t Training Loss 0.3610\t Accuracy 0.5929\n",
      "Epoch [5][20]\t Batch [100][183]\t Training Loss 0.3604\t Accuracy 0.5915\n",
      "Epoch [5][20]\t Batch [150][183]\t Training Loss 0.3599\t Accuracy 0.5922\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3584\t Average training accuracy 0.5963\n",
      "Epoch [5]\t Average validation loss 0.3369\t Average validation accuracy 0.6519\n",
      "\n",
      "Epoch [6][20]\t Batch [0][183]\t Training Loss 0.3298\t Accuracy 0.6767\n",
      "Epoch [6][20]\t Batch [50][183]\t Training Loss 0.3447\t Accuracy 0.6310\n",
      "Epoch [6][20]\t Batch [100][183]\t Training Loss 0.3446\t Accuracy 0.6290\n",
      "Epoch [6][20]\t Batch [150][183]\t Training Loss 0.3446\t Accuracy 0.6282\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3435\t Average training accuracy 0.6311\n",
      "Epoch [6]\t Average validation loss 0.3213\t Average validation accuracy 0.6883\n",
      "\n",
      "Epoch [7][20]\t Batch [0][183]\t Training Loss 0.3262\t Accuracy 0.7033\n",
      "Epoch [7][20]\t Batch [50][183]\t Training Loss 0.3316\t Accuracy 0.6612\n",
      "Epoch [7][20]\t Batch [100][183]\t Training Loss 0.3320\t Accuracy 0.6571\n",
      "Epoch [7][20]\t Batch [150][183]\t Training Loss 0.3324\t Accuracy 0.6552\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3315\t Average training accuracy 0.6579\n",
      "Epoch [7]\t Average validation loss 0.3101\t Average validation accuracy 0.7175\n",
      "\n",
      "Epoch [8][20]\t Batch [0][183]\t Training Loss 0.3116\t Accuracy 0.7000\n",
      "Epoch [8][20]\t Batch [50][183]\t Training Loss 0.3208\t Accuracy 0.6830\n",
      "Epoch [8][20]\t Batch [100][183]\t Training Loss 0.3217\t Accuracy 0.6781\n",
      "Epoch [8][20]\t Batch [150][183]\t Training Loss 0.3224\t Accuracy 0.6760\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3215\t Average training accuracy 0.6783\n",
      "Epoch [8]\t Average validation loss 0.3012\t Average validation accuracy 0.7304\n",
      "\n",
      "Epoch [9][20]\t Batch [0][183]\t Training Loss 0.3056\t Accuracy 0.7167\n",
      "Epoch [9][20]\t Batch [50][183]\t Training Loss 0.3120\t Accuracy 0.7018\n",
      "Epoch [9][20]\t Batch [100][183]\t Training Loss 0.3131\t Accuracy 0.6954\n",
      "Epoch [9][20]\t Batch [150][183]\t Training Loss 0.3139\t Accuracy 0.6937\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3130\t Average training accuracy 0.6958\n",
      "Epoch [9]\t Average validation loss 0.2922\t Average validation accuracy 0.7494\n",
      "\n",
      "Epoch [10][20]\t Batch [0][183]\t Training Loss 0.3089\t Accuracy 0.7033\n",
      "Epoch [10][20]\t Batch [50][183]\t Training Loss 0.3042\t Accuracy 0.7164\n",
      "Epoch [10][20]\t Batch [100][183]\t Training Loss 0.3058\t Accuracy 0.7096\n",
      "Epoch [10][20]\t Batch [150][183]\t Training Loss 0.3066\t Accuracy 0.7081\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3058\t Average training accuracy 0.7100\n",
      "Epoch [10]\t Average validation loss 0.2851\t Average validation accuracy 0.7633\n",
      "\n",
      "Epoch [11][20]\t Batch [0][183]\t Training Loss 0.3175\t Accuracy 0.7167\n",
      "Epoch [11][20]\t Batch [50][183]\t Training Loss 0.2977\t Accuracy 0.7302\n",
      "Epoch [11][20]\t Batch [100][183]\t Training Loss 0.2994\t Accuracy 0.7231\n",
      "Epoch [11][20]\t Batch [150][183]\t Training Loss 0.3003\t Accuracy 0.7211\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2997\t Average training accuracy 0.7226\n",
      "Epoch [11]\t Average validation loss 0.2783\t Average validation accuracy 0.7777\n",
      "\n",
      "Epoch [12][20]\t Batch [0][183]\t Training Loss 0.3061\t Accuracy 0.7367\n",
      "Epoch [12][20]\t Batch [50][183]\t Training Loss 0.2916\t Accuracy 0.7407\n",
      "Epoch [12][20]\t Batch [100][183]\t Training Loss 0.2936\t Accuracy 0.7338\n",
      "Epoch [12][20]\t Batch [150][183]\t Training Loss 0.2947\t Accuracy 0.7314\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2942\t Average training accuracy 0.7327\n",
      "Epoch [12]\t Average validation loss 0.2730\t Average validation accuracy 0.7850\n",
      "\n",
      "Epoch [13][20]\t Batch [0][183]\t Training Loss 0.2834\t Accuracy 0.7700\n",
      "Epoch [13][20]\t Batch [50][183]\t Training Loss 0.2863\t Accuracy 0.7498\n",
      "Epoch [13][20]\t Batch [100][183]\t Training Loss 0.2885\t Accuracy 0.7427\n",
      "Epoch [13][20]\t Batch [150][183]\t Training Loss 0.2897\t Accuracy 0.7400\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2894\t Average training accuracy 0.7413\n",
      "Epoch [13]\t Average validation loss 0.2681\t Average validation accuracy 0.7967\n",
      "\n",
      "Epoch [14][20]\t Batch [0][183]\t Training Loss 0.2655\t Accuracy 0.8100\n",
      "Epoch [14][20]\t Batch [50][183]\t Training Loss 0.2815\t Accuracy 0.7593\n",
      "Epoch [14][20]\t Batch [100][183]\t Training Loss 0.2840\t Accuracy 0.7515\n",
      "Epoch [14][20]\t Batch [150][183]\t Training Loss 0.2853\t Accuracy 0.7485\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2850\t Average training accuracy 0.7496\n",
      "Epoch [14]\t Average validation loss 0.2650\t Average validation accuracy 0.8006\n",
      "\n",
      "Epoch [15][20]\t Batch [0][183]\t Training Loss 0.2798\t Accuracy 0.7600\n",
      "Epoch [15][20]\t Batch [50][183]\t Training Loss 0.2777\t Accuracy 0.7660\n",
      "Epoch [15][20]\t Batch [100][183]\t Training Loss 0.2802\t Accuracy 0.7578\n",
      "Epoch [15][20]\t Batch [150][183]\t Training Loss 0.2815\t Accuracy 0.7553\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2812\t Average training accuracy 0.7560\n",
      "Epoch [15]\t Average validation loss 0.2614\t Average validation accuracy 0.8060\n",
      "\n",
      "Epoch [16][20]\t Batch [0][183]\t Training Loss 0.2814\t Accuracy 0.7667\n",
      "Epoch [16][20]\t Batch [50][183]\t Training Loss 0.2740\t Accuracy 0.7727\n",
      "Epoch [16][20]\t Batch [100][183]\t Training Loss 0.2767\t Accuracy 0.7643\n",
      "Epoch [16][20]\t Batch [150][183]\t Training Loss 0.2779\t Accuracy 0.7613\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2777\t Average training accuracy 0.7620\n",
      "Epoch [16]\t Average validation loss 0.2574\t Average validation accuracy 0.8135\n",
      "\n",
      "Epoch [17][20]\t Batch [0][183]\t Training Loss 0.2722\t Accuracy 0.7867\n",
      "Epoch [17][20]\t Batch [50][183]\t Training Loss 0.2704\t Accuracy 0.7769\n",
      "Epoch [17][20]\t Batch [100][183]\t Training Loss 0.2733\t Accuracy 0.7689\n",
      "Epoch [17][20]\t Batch [150][183]\t Training Loss 0.2747\t Accuracy 0.7661\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2744\t Average training accuracy 0.7669\n",
      "Epoch [17]\t Average validation loss 0.2541\t Average validation accuracy 0.8160\n",
      "\n",
      "Epoch [18][20]\t Batch [0][183]\t Training Loss 0.2582\t Accuracy 0.8200\n",
      "Epoch [18][20]\t Batch [50][183]\t Training Loss 0.2677\t Accuracy 0.7809\n",
      "Epoch [18][20]\t Batch [100][183]\t Training Loss 0.2703\t Accuracy 0.7737\n",
      "Epoch [18][20]\t Batch [150][183]\t Training Loss 0.2717\t Accuracy 0.7707\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2716\t Average training accuracy 0.7714\n",
      "Epoch [18]\t Average validation loss 0.2506\t Average validation accuracy 0.8217\n",
      "\n",
      "Epoch [19][20]\t Batch [0][183]\t Training Loss 0.2564\t Accuracy 0.8133\n",
      "Epoch [19][20]\t Batch [50][183]\t Training Loss 0.2652\t Accuracy 0.7842\n",
      "Epoch [19][20]\t Batch [100][183]\t Training Loss 0.2676\t Accuracy 0.7779\n",
      "Epoch [19][20]\t Batch [150][183]\t Training Loss 0.2691\t Accuracy 0.7746\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2689\t Average training accuracy 0.7753\n",
      "Epoch [19]\t Average validation loss 0.2497\t Average validation accuracy 0.8227\n",
      "\n",
      "Epoch [0][20]\t Batch [0][183]\t Training Loss 2.9225\t Accuracy 0.0367\n",
      "Epoch [0][20]\t Batch [50][183]\t Training Loss 1.2981\t Accuracy 0.3044\n",
      "Epoch [0][20]\t Batch [100][183]\t Training Loss 0.8636\t Accuracy 0.4066\n",
      "Epoch [0][20]\t Batch [150][183]\t Training Loss 0.6971\t Accuracy 0.4507\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6302\t Average training accuracy 0.4809\n",
      "Epoch [0]\t Average validation loss 0.2847\t Average validation accuracy 0.6863\n",
      "\n",
      "Epoch [1][20]\t Batch [0][183]\t Training Loss 0.3078\t Accuracy 0.6567\n",
      "Epoch [1][20]\t Batch [50][183]\t Training Loss 0.2793\t Accuracy 0.6963\n",
      "Epoch [1][20]\t Batch [100][183]\t Training Loss 0.2624\t Accuracy 0.7151\n",
      "Epoch [1][20]\t Batch [150][183]\t Training Loss 0.2501\t Accuracy 0.7296\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2422\t Average training accuracy 0.7419\n",
      "Epoch [1]\t Average validation loss 0.1806\t Average validation accuracy 0.8448\n",
      "\n",
      "Epoch [2][20]\t Batch [0][183]\t Training Loss 0.2056\t Accuracy 0.7733\n",
      "Epoch [2][20]\t Batch [50][183]\t Training Loss 0.1878\t Accuracy 0.8195\n",
      "Epoch [2][20]\t Batch [100][183]\t Training Loss 0.1812\t Accuracy 0.8260\n",
      "Epoch [2][20]\t Batch [150][183]\t Training Loss 0.1760\t Accuracy 0.8326\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1726\t Average training accuracy 0.8353\n",
      "Epoch [2]\t Average validation loss 0.1329\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [3][20]\t Batch [0][183]\t Training Loss 0.1594\t Accuracy 0.8367\n",
      "Epoch [3][20]\t Batch [50][183]\t Training Loss 0.1461\t Accuracy 0.8676\n",
      "Epoch [3][20]\t Batch [100][183]\t Training Loss 0.1430\t Accuracy 0.8685\n",
      "Epoch [3][20]\t Batch [150][183]\t Training Loss 0.1411\t Accuracy 0.8701\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1399\t Average training accuracy 0.8701\n",
      "Epoch [3]\t Average validation loss 0.1108\t Average validation accuracy 0.9052\n",
      "\n",
      "Epoch [4][20]\t Batch [0][183]\t Training Loss 0.1101\t Accuracy 0.8933\n",
      "Epoch [4][20]\t Batch [50][183]\t Training Loss 0.1266\t Accuracy 0.8841\n",
      "Epoch [4][20]\t Batch [100][183]\t Training Loss 0.1247\t Accuracy 0.8854\n",
      "Epoch [4][20]\t Batch [150][183]\t Training Loss 0.1243\t Accuracy 0.8858\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1239\t Average training accuracy 0.8856\n",
      "Epoch [4]\t Average validation loss 0.1028\t Average validation accuracy 0.9150\n",
      "\n",
      "Epoch [5][20]\t Batch [0][183]\t Training Loss 0.1051\t Accuracy 0.9000\n",
      "Epoch [5][20]\t Batch [50][183]\t Training Loss 0.1160\t Accuracy 0.8954\n",
      "Epoch [5][20]\t Batch [100][183]\t Training Loss 0.1148\t Accuracy 0.8966\n",
      "Epoch [5][20]\t Batch [150][183]\t Training Loss 0.1150\t Accuracy 0.8966\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1150\t Average training accuracy 0.8958\n",
      "Epoch [5]\t Average validation loss 0.0969\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [6][20]\t Batch [0][183]\t Training Loss 0.1038\t Accuracy 0.9167\n",
      "Epoch [6][20]\t Batch [50][183]\t Training Loss 0.1091\t Accuracy 0.9034\n",
      "Epoch [6][20]\t Batch [100][183]\t Training Loss 0.1085\t Accuracy 0.9036\n",
      "Epoch [6][20]\t Batch [150][183]\t Training Loss 0.1090\t Accuracy 0.9033\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1091\t Average training accuracy 0.9023\n",
      "Epoch [6]\t Average validation loss 0.0922\t Average validation accuracy 0.9285\n",
      "\n",
      "Epoch [7][20]\t Batch [0][183]\t Training Loss 0.0992\t Accuracy 0.9333\n",
      "Epoch [7][20]\t Batch [50][183]\t Training Loss 0.1040\t Accuracy 0.9092\n",
      "Epoch [7][20]\t Batch [100][183]\t Training Loss 0.1039\t Accuracy 0.9096\n",
      "Epoch [7][20]\t Batch [150][183]\t Training Loss 0.1046\t Accuracy 0.9087\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1047\t Average training accuracy 0.9074\n",
      "Epoch [7]\t Average validation loss 0.0884\t Average validation accuracy 0.9321\n",
      "\n",
      "Epoch [8][20]\t Batch [0][183]\t Training Loss 0.0799\t Accuracy 0.9633\n",
      "Epoch [8][20]\t Batch [50][183]\t Training Loss 0.0999\t Accuracy 0.9152\n",
      "Epoch [8][20]\t Batch [100][183]\t Training Loss 0.1002\t Accuracy 0.9145\n",
      "Epoch [8][20]\t Batch [150][183]\t Training Loss 0.1010\t Accuracy 0.9130\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1012\t Average training accuracy 0.9119\n",
      "Epoch [8]\t Average validation loss 0.0866\t Average validation accuracy 0.9331\n",
      "\n",
      "Epoch [9][20]\t Batch [0][183]\t Training Loss 0.0781\t Accuracy 0.9467\n",
      "Epoch [9][20]\t Batch [50][183]\t Training Loss 0.0968\t Accuracy 0.9173\n",
      "Epoch [9][20]\t Batch [100][183]\t Training Loss 0.0972\t Accuracy 0.9174\n",
      "Epoch [9][20]\t Batch [150][183]\t Training Loss 0.0980\t Accuracy 0.9159\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0982\t Average training accuracy 0.9149\n",
      "Epoch [9]\t Average validation loss 0.0842\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [10][20]\t Batch [0][183]\t Training Loss 0.0851\t Accuracy 0.9400\n",
      "Epoch [10][20]\t Batch [50][183]\t Training Loss 0.0940\t Accuracy 0.9208\n",
      "Epoch [10][20]\t Batch [100][183]\t Training Loss 0.0945\t Accuracy 0.9205\n",
      "Epoch [10][20]\t Batch [150][183]\t Training Loss 0.0953\t Accuracy 0.9190\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0956\t Average training accuracy 0.9179\n",
      "Epoch [10]\t Average validation loss 0.0818\t Average validation accuracy 0.9379\n",
      "\n",
      "Epoch [11][20]\t Batch [0][183]\t Training Loss 0.0998\t Accuracy 0.9067\n",
      "Epoch [11][20]\t Batch [50][183]\t Training Loss 0.0917\t Accuracy 0.9233\n",
      "Epoch [11][20]\t Batch [100][183]\t Training Loss 0.0922\t Accuracy 0.9227\n",
      "Epoch [11][20]\t Batch [150][183]\t Training Loss 0.0930\t Accuracy 0.9212\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0934\t Average training accuracy 0.9202\n",
      "Epoch [11]\t Average validation loss 0.0787\t Average validation accuracy 0.9425\n",
      "\n",
      "Epoch [12][20]\t Batch [0][183]\t Training Loss 0.0960\t Accuracy 0.9033\n",
      "Epoch [12][20]\t Batch [50][183]\t Training Loss 0.0894\t Accuracy 0.9248\n",
      "Epoch [12][20]\t Batch [100][183]\t Training Loss 0.0901\t Accuracy 0.9244\n",
      "Epoch [12][20]\t Batch [150][183]\t Training Loss 0.0910\t Accuracy 0.9232\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0914\t Average training accuracy 0.9221\n",
      "Epoch [12]\t Average validation loss 0.0777\t Average validation accuracy 0.9425\n",
      "\n",
      "Epoch [13][20]\t Batch [0][183]\t Training Loss 0.0843\t Accuracy 0.9067\n",
      "Epoch [13][20]\t Batch [50][183]\t Training Loss 0.0874\t Accuracy 0.9270\n",
      "Epoch [13][20]\t Batch [100][183]\t Training Loss 0.0882\t Accuracy 0.9264\n",
      "Epoch [13][20]\t Batch [150][183]\t Training Loss 0.0892\t Accuracy 0.9253\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0896\t Average training accuracy 0.9244\n",
      "Epoch [13]\t Average validation loss 0.0761\t Average validation accuracy 0.9440\n",
      "\n",
      "Epoch [14][20]\t Batch [0][183]\t Training Loss 0.0795\t Accuracy 0.9067\n",
      "Epoch [14][20]\t Batch [50][183]\t Training Loss 0.0858\t Accuracy 0.9280\n",
      "Epoch [14][20]\t Batch [100][183]\t Training Loss 0.0866\t Accuracy 0.9278\n",
      "Epoch [14][20]\t Batch [150][183]\t Training Loss 0.0875\t Accuracy 0.9269\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0879\t Average training accuracy 0.9260\n",
      "Epoch [14]\t Average validation loss 0.0758\t Average validation accuracy 0.9446\n",
      "\n",
      "Epoch [15][20]\t Batch [0][183]\t Training Loss 0.0846\t Accuracy 0.9167\n",
      "Epoch [15][20]\t Batch [50][183]\t Training Loss 0.0844\t Accuracy 0.9293\n",
      "Epoch [15][20]\t Batch [100][183]\t Training Loss 0.0852\t Accuracy 0.9289\n",
      "Epoch [15][20]\t Batch [150][183]\t Training Loss 0.0860\t Accuracy 0.9284\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0865\t Average training accuracy 0.9275\n",
      "Epoch [15]\t Average validation loss 0.0747\t Average validation accuracy 0.9463\n",
      "\n",
      "Epoch [16][20]\t Batch [0][183]\t Training Loss 0.0806\t Accuracy 0.9267\n",
      "Epoch [16][20]\t Batch [50][183]\t Training Loss 0.0828\t Accuracy 0.9315\n",
      "Epoch [16][20]\t Batch [100][183]\t Training Loss 0.0837\t Accuracy 0.9306\n",
      "Epoch [16][20]\t Batch [150][183]\t Training Loss 0.0846\t Accuracy 0.9302\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0851\t Average training accuracy 0.9293\n",
      "Epoch [16]\t Average validation loss 0.0733\t Average validation accuracy 0.9477\n",
      "\n",
      "Epoch [17][20]\t Batch [0][183]\t Training Loss 0.0702\t Accuracy 0.9533\n",
      "Epoch [17][20]\t Batch [50][183]\t Training Loss 0.0813\t Accuracy 0.9322\n",
      "Epoch [17][20]\t Batch [100][183]\t Training Loss 0.0823\t Accuracy 0.9316\n",
      "Epoch [17][20]\t Batch [150][183]\t Training Loss 0.0833\t Accuracy 0.9313\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0838\t Average training accuracy 0.9305\n",
      "Epoch [17]\t Average validation loss 0.0717\t Average validation accuracy 0.9487\n",
      "\n",
      "Epoch [18][20]\t Batch [0][183]\t Training Loss 0.0590\t Accuracy 0.9667\n",
      "Epoch [18][20]\t Batch [50][183]\t Training Loss 0.0801\t Accuracy 0.9338\n",
      "Epoch [18][20]\t Batch [100][183]\t Training Loss 0.0812\t Accuracy 0.9330\n",
      "Epoch [18][20]\t Batch [150][183]\t Training Loss 0.0821\t Accuracy 0.9325\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0826\t Average training accuracy 0.9317\n",
      "Epoch [18]\t Average validation loss 0.0707\t Average validation accuracy 0.9498\n",
      "\n",
      "Epoch [19][20]\t Batch [0][183]\t Training Loss 0.0677\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][183]\t Training Loss 0.0791\t Accuracy 0.9349\n",
      "Epoch [19][20]\t Batch [100][183]\t Training Loss 0.0801\t Accuracy 0.9344\n",
      "Epoch [19][20]\t Batch [150][183]\t Training Loss 0.0810\t Accuracy 0.9337\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0814\t Average training accuracy 0.9330\n",
      "Epoch [19]\t Average validation loss 0.0707\t Average validation accuracy 0.9496\n",
      "\n",
      "Epoch [0][20]\t Batch [0][183]\t Training Loss 2.7960\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][183]\t Training Loss 2.5992\t Accuracy 0.1178\n",
      "Epoch [0][20]\t Batch [100][183]\t Training Loss 2.4990\t Accuracy 0.1219\n",
      "Epoch [0][20]\t Batch [150][183]\t Training Loss 2.4310\t Accuracy 0.1294\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3977\t Average training accuracy 0.1382\n",
      "Epoch [0]\t Average validation loss 2.2238\t Average validation accuracy 0.1719\n",
      "\n",
      "Epoch [1][20]\t Batch [0][183]\t Training Loss 2.2115\t Accuracy 0.2233\n",
      "Epoch [1][20]\t Batch [50][183]\t Training Loss 2.2024\t Accuracy 0.2061\n",
      "Epoch [1][20]\t Batch [100][183]\t Training Loss 2.1851\t Accuracy 0.2340\n",
      "Epoch [1][20]\t Batch [150][183]\t Training Loss 2.1706\t Accuracy 0.2597\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1614\t Average training accuracy 0.2770\n",
      "Epoch [1]\t Average validation loss 2.1061\t Average validation accuracy 0.3650\n",
      "\n",
      "Epoch [2][20]\t Batch [0][183]\t Training Loss 2.0949\t Accuracy 0.4000\n",
      "Epoch [2][20]\t Batch [50][183]\t Training Loss 2.0964\t Accuracy 0.3935\n",
      "Epoch [2][20]\t Batch [100][183]\t Training Loss 2.0858\t Accuracy 0.4139\n",
      "Epoch [2][20]\t Batch [150][183]\t Training Loss 2.0762\t Accuracy 0.4297\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0688\t Average training accuracy 0.4404\n",
      "Epoch [2]\t Average validation loss 2.0202\t Average validation accuracy 0.4958\n",
      "\n",
      "Epoch [3][20]\t Batch [0][183]\t Training Loss 2.0331\t Accuracy 0.4767\n",
      "Epoch [3][20]\t Batch [50][183]\t Training Loss 2.0142\t Accuracy 0.5120\n",
      "Epoch [3][20]\t Batch [100][183]\t Training Loss 2.0046\t Accuracy 0.5195\n",
      "Epoch [3][20]\t Batch [150][183]\t Training Loss 1.9964\t Accuracy 0.5285\n",
      "\n",
      "Epoch [3]\t Average training loss 1.9896\t Average training accuracy 0.5344\n",
      "Epoch [3]\t Average validation loss 1.9380\t Average validation accuracy 0.5781\n",
      "\n",
      "Epoch [4][20]\t Batch [0][183]\t Training Loss 1.9302\t Accuracy 0.5700\n",
      "Epoch [4][20]\t Batch [50][183]\t Training Loss 1.9372\t Accuracy 0.5753\n",
      "Epoch [4][20]\t Batch [100][183]\t Training Loss 1.9281\t Accuracy 0.5779\n",
      "Epoch [4][20]\t Batch [150][183]\t Training Loss 1.9213\t Accuracy 0.5830\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9149\t Average training accuracy 0.5873\n",
      "Epoch [4]\t Average validation loss 1.8634\t Average validation accuracy 0.6267\n",
      "\n",
      "Epoch [5][20]\t Batch [0][183]\t Training Loss 1.8534\t Accuracy 0.6367\n",
      "Epoch [5][20]\t Batch [50][183]\t Training Loss 1.8647\t Accuracy 0.6178\n",
      "Epoch [5][20]\t Batch [100][183]\t Training Loss 1.8558\t Accuracy 0.6200\n",
      "Epoch [5][20]\t Batch [150][183]\t Training Loss 1.8504\t Accuracy 0.6231\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8445\t Average training accuracy 0.6260\n",
      "Epoch [5]\t Average validation loss 1.7911\t Average validation accuracy 0.6608\n",
      "\n",
      "Epoch [6][20]\t Batch [0][183]\t Training Loss 1.7637\t Accuracy 0.7000\n",
      "Epoch [6][20]\t Batch [50][183]\t Training Loss 1.7951\t Accuracy 0.6515\n",
      "Epoch [6][20]\t Batch [100][183]\t Training Loss 1.7874\t Accuracy 0.6508\n",
      "Epoch [6][20]\t Batch [150][183]\t Training Loss 1.7834\t Accuracy 0.6530\n",
      "\n",
      "Epoch [6]\t Average training loss 1.7778\t Average training accuracy 0.6560\n",
      "Epoch [6]\t Average validation loss 1.7192\t Average validation accuracy 0.6958\n",
      "\n",
      "Epoch [7][20]\t Batch [0][183]\t Training Loss 1.7078\t Accuracy 0.6867\n",
      "Epoch [7][20]\t Batch [50][183]\t Training Loss 1.7289\t Accuracy 0.6753\n",
      "Epoch [7][20]\t Batch [100][183]\t Training Loss 1.7229\t Accuracy 0.6746\n",
      "Epoch [7][20]\t Batch [150][183]\t Training Loss 1.7201\t Accuracy 0.6768\n",
      "\n",
      "Epoch [7]\t Average training loss 1.7149\t Average training accuracy 0.6793\n",
      "Epoch [7]\t Average validation loss 1.6585\t Average validation accuracy 0.7217\n",
      "\n",
      "Epoch [8][20]\t Batch [0][183]\t Training Loss 1.6334\t Accuracy 0.7333\n",
      "Epoch [8][20]\t Batch [50][183]\t Training Loss 1.6671\t Accuracy 0.6952\n",
      "Epoch [8][20]\t Batch [100][183]\t Training Loss 1.6621\t Accuracy 0.6945\n",
      "Epoch [8][20]\t Batch [150][183]\t Training Loss 1.6605\t Accuracy 0.6954\n",
      "\n",
      "Epoch [8]\t Average training loss 1.6554\t Average training accuracy 0.6977\n",
      "Epoch [8]\t Average validation loss 1.5945\t Average validation accuracy 0.7396\n",
      "\n",
      "Epoch [9][20]\t Batch [0][183]\t Training Loss 1.5988\t Accuracy 0.7533\n",
      "Epoch [9][20]\t Batch [50][183]\t Training Loss 1.6094\t Accuracy 0.7120\n",
      "Epoch [9][20]\t Batch [100][183]\t Training Loss 1.6055\t Accuracy 0.7106\n",
      "Epoch [9][20]\t Batch [150][183]\t Training Loss 1.6043\t Accuracy 0.7113\n",
      "\n",
      "Epoch [9]\t Average training loss 1.5992\t Average training accuracy 0.7134\n",
      "Epoch [9]\t Average validation loss 1.5370\t Average validation accuracy 0.7542\n",
      "\n",
      "Epoch [10][20]\t Batch [0][183]\t Training Loss 1.5956\t Accuracy 0.7267\n",
      "Epoch [10][20]\t Batch [50][183]\t Training Loss 1.5551\t Accuracy 0.7271\n",
      "Epoch [10][20]\t Batch [100][183]\t Training Loss 1.5520\t Accuracy 0.7252\n",
      "Epoch [10][20]\t Batch [150][183]\t Training Loss 1.5513\t Accuracy 0.7249\n",
      "\n",
      "Epoch [10]\t Average training loss 1.5463\t Average training accuracy 0.7270\n",
      "Epoch [10]\t Average validation loss 1.4826\t Average validation accuracy 0.7700\n",
      "\n",
      "Epoch [11][20]\t Batch [0][183]\t Training Loss 1.5940\t Accuracy 0.6833\n",
      "Epoch [11][20]\t Batch [50][183]\t Training Loss 1.5038\t Accuracy 0.7390\n",
      "Epoch [11][20]\t Batch [100][183]\t Training Loss 1.5015\t Accuracy 0.7364\n",
      "Epoch [11][20]\t Batch [150][183]\t Training Loss 1.5012\t Accuracy 0.7362\n",
      "\n",
      "Epoch [11]\t Average training loss 1.4967\t Average training accuracy 0.7379\n",
      "Epoch [11]\t Average validation loss 1.4281\t Average validation accuracy 0.7817\n",
      "\n",
      "Epoch [12][20]\t Batch [0][183]\t Training Loss 1.5289\t Accuracy 0.7000\n",
      "Epoch [12][20]\t Batch [50][183]\t Training Loss 1.4549\t Accuracy 0.7516\n",
      "Epoch [12][20]\t Batch [100][183]\t Training Loss 1.4530\t Accuracy 0.7482\n",
      "Epoch [12][20]\t Batch [150][183]\t Training Loss 1.4536\t Accuracy 0.7471\n",
      "\n",
      "Epoch [12]\t Average training loss 1.4496\t Average training accuracy 0.7482\n",
      "Epoch [12]\t Average validation loss 1.3795\t Average validation accuracy 0.7894\n",
      "\n",
      "Epoch [13][20]\t Batch [0][183]\t Training Loss 1.4292\t Accuracy 0.7567\n",
      "Epoch [13][20]\t Batch [50][183]\t Training Loss 1.4092\t Accuracy 0.7601\n",
      "Epoch [13][20]\t Batch [100][183]\t Training Loss 1.4076\t Accuracy 0.7568\n",
      "Epoch [13][20]\t Batch [150][183]\t Training Loss 1.4089\t Accuracy 0.7556\n",
      "\n",
      "Epoch [13]\t Average training loss 1.4052\t Average training accuracy 0.7570\n",
      "Epoch [13]\t Average validation loss 1.3327\t Average validation accuracy 0.8013\n",
      "\n",
      "Epoch [14][20]\t Batch [0][183]\t Training Loss 1.3517\t Accuracy 0.7867\n",
      "Epoch [14][20]\t Batch [50][183]\t Training Loss 1.3652\t Accuracy 0.7674\n",
      "Epoch [14][20]\t Batch [100][183]\t Training Loss 1.3649\t Accuracy 0.7638\n",
      "Epoch [14][20]\t Batch [150][183]\t Training Loss 1.3667\t Accuracy 0.7629\n",
      "\n",
      "Epoch [14]\t Average training loss 1.3633\t Average training accuracy 0.7642\n",
      "Epoch [14]\t Average validation loss 1.2912\t Average validation accuracy 0.8077\n",
      "\n",
      "Epoch [15][20]\t Batch [0][183]\t Training Loss 1.3458\t Accuracy 0.7633\n",
      "Epoch [15][20]\t Batch [50][183]\t Training Loss 1.3249\t Accuracy 0.7737\n",
      "Epoch [15][20]\t Batch [100][183]\t Training Loss 1.3253\t Accuracy 0.7698\n",
      "Epoch [15][20]\t Batch [150][183]\t Training Loss 1.3271\t Accuracy 0.7694\n",
      "\n",
      "Epoch [15]\t Average training loss 1.3240\t Average training accuracy 0.7706\n",
      "Epoch [15]\t Average validation loss 1.2546\t Average validation accuracy 0.8169\n",
      "\n",
      "Epoch [16][20]\t Batch [0][183]\t Training Loss 1.3165\t Accuracy 0.7600\n",
      "Epoch [16][20]\t Batch [50][183]\t Training Loss 1.2860\t Accuracy 0.7803\n",
      "Epoch [16][20]\t Batch [100][183]\t Training Loss 1.2874\t Accuracy 0.7757\n",
      "Epoch [16][20]\t Batch [150][183]\t Training Loss 1.2898\t Accuracy 0.7750\n",
      "\n",
      "Epoch [16]\t Average training loss 1.2871\t Average training accuracy 0.7760\n",
      "Epoch [16]\t Average validation loss 1.2162\t Average validation accuracy 0.8229\n",
      "\n",
      "Epoch [17][20]\t Batch [0][183]\t Training Loss 1.2296\t Accuracy 0.7900\n",
      "Epoch [17][20]\t Batch [50][183]\t Training Loss 1.2484\t Accuracy 0.7867\n",
      "Epoch [17][20]\t Batch [100][183]\t Training Loss 1.2511\t Accuracy 0.7813\n",
      "Epoch [17][20]\t Batch [150][183]\t Training Loss 1.2544\t Accuracy 0.7801\n",
      "\n",
      "Epoch [17]\t Average training loss 1.2518\t Average training accuracy 0.7810\n",
      "Epoch [17]\t Average validation loss 1.1787\t Average validation accuracy 0.8296\n",
      "\n",
      "Epoch [18][20]\t Batch [0][183]\t Training Loss 1.1764\t Accuracy 0.8167\n",
      "Epoch [18][20]\t Batch [50][183]\t Training Loss 1.2154\t Accuracy 0.7920\n",
      "Epoch [18][20]\t Batch [100][183]\t Training Loss 1.2177\t Accuracy 0.7864\n",
      "Epoch [18][20]\t Batch [150][183]\t Training Loss 1.2211\t Accuracy 0.7851\n",
      "\n",
      "Epoch [18]\t Average training loss 1.2188\t Average training accuracy 0.7857\n",
      "Epoch [18]\t Average validation loss 1.1408\t Average validation accuracy 0.8344\n",
      "\n",
      "Epoch [19][20]\t Batch [0][183]\t Training Loss 1.1533\t Accuracy 0.8133\n",
      "Epoch [19][20]\t Batch [50][183]\t Training Loss 1.1844\t Accuracy 0.7961\n",
      "Epoch [19][20]\t Batch [100][183]\t Training Loss 1.1864\t Accuracy 0.7907\n",
      "Epoch [19][20]\t Batch [150][183]\t Training Loss 1.1901\t Accuracy 0.7890\n",
      "\n",
      "Epoch [19]\t Average training loss 1.1877\t Average training accuracy 0.7898\n",
      "Epoch [19]\t Average validation loss 1.1132\t Average validation accuracy 0.8381\n",
      "\n",
      "Epoch [0][20]\t Batch [0][183]\t Training Loss 2.6105\t Accuracy 0.0667\n",
      "Epoch [0][20]\t Batch [50][183]\t Training Loss 2.0003\t Accuracy 0.3352\n",
      "Epoch [0][20]\t Batch [100][183]\t Training Loss 1.4656\t Accuracy 0.5299\n",
      "Epoch [0][20]\t Batch [150][183]\t Training Loss 1.1738\t Accuracy 0.6252\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0541\t Average training accuracy 0.6647\n",
      "Epoch [0]\t Average validation loss 0.3479\t Average validation accuracy 0.8967\n",
      "\n",
      "Epoch [1][20]\t Batch [0][183]\t Training Loss 0.5959\t Accuracy 0.8167\n",
      "Epoch [1][20]\t Batch [50][183]\t Training Loss 0.4303\t Accuracy 0.8765\n",
      "Epoch [1][20]\t Batch [100][183]\t Training Loss 0.4098\t Accuracy 0.8813\n",
      "Epoch [1][20]\t Batch [150][183]\t Training Loss 0.3979\t Accuracy 0.8845\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3900\t Average training accuracy 0.8864\n",
      "Epoch [1]\t Average validation loss 0.2473\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [2][20]\t Batch [0][183]\t Training Loss 0.4283\t Accuracy 0.8767\n",
      "Epoch [2][20]\t Batch [50][183]\t Training Loss 0.3260\t Accuracy 0.9068\n",
      "Epoch [2][20]\t Batch [100][183]\t Training Loss 0.3145\t Accuracy 0.9093\n",
      "Epoch [2][20]\t Batch [150][183]\t Training Loss 0.3097\t Accuracy 0.9096\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3067\t Average training accuracy 0.9105\n",
      "Epoch [2]\t Average validation loss 0.2194\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [3][20]\t Batch [0][183]\t Training Loss 0.3025\t Accuracy 0.9000\n",
      "Epoch [3][20]\t Batch [50][183]\t Training Loss 0.2729\t Accuracy 0.9214\n",
      "Epoch [3][20]\t Batch [100][183]\t Training Loss 0.2669\t Accuracy 0.9229\n",
      "Epoch [3][20]\t Batch [150][183]\t Training Loss 0.2650\t Accuracy 0.9229\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2642\t Average training accuracy 0.9232\n",
      "Epoch [3]\t Average validation loss 0.1910\t Average validation accuracy 0.9485\n",
      "\n",
      "Epoch [4][20]\t Batch [0][183]\t Training Loss 0.1865\t Accuracy 0.9367\n",
      "Epoch [4][20]\t Batch [50][183]\t Training Loss 0.2424\t Accuracy 0.9299\n",
      "Epoch [4][20]\t Batch [100][183]\t Training Loss 0.2374\t Accuracy 0.9325\n",
      "Epoch [4][20]\t Batch [150][183]\t Training Loss 0.2368\t Accuracy 0.9320\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2368\t Average training accuracy 0.9322\n",
      "Epoch [4]\t Average validation loss 0.1804\t Average validation accuracy 0.9515\n",
      "\n",
      "Epoch [5][20]\t Batch [0][183]\t Training Loss 0.1791\t Accuracy 0.9433\n",
      "Epoch [5][20]\t Batch [50][183]\t Training Loss 0.2193\t Accuracy 0.9364\n",
      "Epoch [5][20]\t Batch [100][183]\t Training Loss 0.2155\t Accuracy 0.9386\n",
      "Epoch [5][20]\t Batch [150][183]\t Training Loss 0.2154\t Accuracy 0.9382\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2164\t Average training accuracy 0.9380\n",
      "Epoch [5]\t Average validation loss 0.1644\t Average validation accuracy 0.9558\n",
      "\n",
      "Epoch [6][20]\t Batch [0][183]\t Training Loss 0.1660\t Accuracy 0.9567\n",
      "Epoch [6][20]\t Batch [50][183]\t Training Loss 0.1992\t Accuracy 0.9431\n",
      "Epoch [6][20]\t Batch [100][183]\t Training Loss 0.1978\t Accuracy 0.9441\n",
      "Epoch [6][20]\t Batch [150][183]\t Training Loss 0.1983\t Accuracy 0.9434\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1995\t Average training accuracy 0.9432\n",
      "Epoch [6]\t Average validation loss 0.1552\t Average validation accuracy 0.9590\n",
      "\n",
      "Epoch [7][20]\t Batch [0][183]\t Training Loss 0.1675\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][183]\t Training Loss 0.1838\t Accuracy 0.9469\n",
      "Epoch [7][20]\t Batch [100][183]\t Training Loss 0.1835\t Accuracy 0.9479\n",
      "Epoch [7][20]\t Batch [150][183]\t Training Loss 0.1841\t Accuracy 0.9475\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1855\t Average training accuracy 0.9472\n",
      "Epoch [7]\t Average validation loss 0.1457\t Average validation accuracy 0.9619\n",
      "\n",
      "Epoch [8][20]\t Batch [0][183]\t Training Loss 0.1124\t Accuracy 0.9767\n",
      "Epoch [8][20]\t Batch [50][183]\t Training Loss 0.1706\t Accuracy 0.9508\n",
      "Epoch [8][20]\t Batch [100][183]\t Training Loss 0.1710\t Accuracy 0.9517\n",
      "Epoch [8][20]\t Batch [150][183]\t Training Loss 0.1719\t Accuracy 0.9511\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1734\t Average training accuracy 0.9509\n",
      "Epoch [8]\t Average validation loss 0.1397\t Average validation accuracy 0.9615\n",
      "\n",
      "Epoch [9][20]\t Batch [0][183]\t Training Loss 0.1181\t Accuracy 0.9767\n",
      "Epoch [9][20]\t Batch [50][183]\t Training Loss 0.1599\t Accuracy 0.9533\n",
      "Epoch [9][20]\t Batch [100][183]\t Training Loss 0.1604\t Accuracy 0.9541\n",
      "Epoch [9][20]\t Batch [150][183]\t Training Loss 0.1611\t Accuracy 0.9541\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1629\t Average training accuracy 0.9538\n",
      "Epoch [9]\t Average validation loss 0.1317\t Average validation accuracy 0.9637\n",
      "\n",
      "Epoch [10][20]\t Batch [0][183]\t Training Loss 0.1266\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][183]\t Training Loss 0.1498\t Accuracy 0.9568\n",
      "Epoch [10][20]\t Batch [100][183]\t Training Loss 0.1511\t Accuracy 0.9572\n",
      "Epoch [10][20]\t Batch [150][183]\t Training Loss 0.1519\t Accuracy 0.9572\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1536\t Average training accuracy 0.9567\n",
      "Epoch [10]\t Average validation loss 0.1277\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [11][20]\t Batch [0][183]\t Training Loss 0.1741\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][183]\t Training Loss 0.1422\t Accuracy 0.9588\n",
      "Epoch [11][20]\t Batch [100][183]\t Training Loss 0.1433\t Accuracy 0.9590\n",
      "Epoch [11][20]\t Batch [150][183]\t Training Loss 0.1441\t Accuracy 0.9591\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1457\t Average training accuracy 0.9585\n",
      "Epoch [11]\t Average validation loss 0.1163\t Average validation accuracy 0.9671\n",
      "\n",
      "Epoch [12][20]\t Batch [0][183]\t Training Loss 0.1898\t Accuracy 0.9533\n",
      "Epoch [12][20]\t Batch [50][183]\t Training Loss 0.1352\t Accuracy 0.9608\n",
      "Epoch [12][20]\t Batch [100][183]\t Training Loss 0.1361\t Accuracy 0.9615\n",
      "Epoch [12][20]\t Batch [150][183]\t Training Loss 0.1369\t Accuracy 0.9613\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1386\t Average training accuracy 0.9607\n",
      "Epoch [12]\t Average validation loss 0.1178\t Average validation accuracy 0.9667\n",
      "\n",
      "Epoch [13][20]\t Batch [0][183]\t Training Loss 0.1601\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][183]\t Training Loss 0.1280\t Accuracy 0.9628\n",
      "Epoch [13][20]\t Batch [100][183]\t Training Loss 0.1294\t Accuracy 0.9630\n",
      "Epoch [13][20]\t Batch [150][183]\t Training Loss 0.1303\t Accuracy 0.9630\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1318\t Average training accuracy 0.9626\n",
      "Epoch [13]\t Average validation loss 0.1144\t Average validation accuracy 0.9667\n",
      "\n",
      "Epoch [14][20]\t Batch [0][183]\t Training Loss 0.1569\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [50][183]\t Training Loss 0.1223\t Accuracy 0.9649\n",
      "Epoch [14][20]\t Batch [100][183]\t Training Loss 0.1239\t Accuracy 0.9648\n",
      "Epoch [14][20]\t Batch [150][183]\t Training Loss 0.1245\t Accuracy 0.9648\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1260\t Average training accuracy 0.9643\n",
      "Epoch [14]\t Average validation loss 0.1108\t Average validation accuracy 0.9675\n",
      "\n",
      "Epoch [15][20]\t Batch [0][183]\t Training Loss 0.1555\t Accuracy 0.9467\n",
      "Epoch [15][20]\t Batch [50][183]\t Training Loss 0.1172\t Accuracy 0.9670\n",
      "Epoch [15][20]\t Batch [100][183]\t Training Loss 0.1188\t Accuracy 0.9667\n",
      "Epoch [15][20]\t Batch [150][183]\t Training Loss 0.1192\t Accuracy 0.9668\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1207\t Average training accuracy 0.9663\n",
      "Epoch [15]\t Average validation loss 0.1080\t Average validation accuracy 0.9696\n",
      "\n",
      "Epoch [16][20]\t Batch [0][183]\t Training Loss 0.1556\t Accuracy 0.9533\n",
      "Epoch [16][20]\t Batch [50][183]\t Training Loss 0.1119\t Accuracy 0.9690\n",
      "Epoch [16][20]\t Batch [100][183]\t Training Loss 0.1138\t Accuracy 0.9685\n",
      "Epoch [16][20]\t Batch [150][183]\t Training Loss 0.1143\t Accuracy 0.9683\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1159\t Average training accuracy 0.9677\n",
      "Epoch [16]\t Average validation loss 0.1050\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [17][20]\t Batch [0][183]\t Training Loss 0.0956\t Accuracy 0.9767\n",
      "Epoch [17][20]\t Batch [50][183]\t Training Loss 0.1068\t Accuracy 0.9705\n",
      "Epoch [17][20]\t Batch [100][183]\t Training Loss 0.1090\t Accuracy 0.9697\n",
      "Epoch [17][20]\t Batch [150][183]\t Training Loss 0.1097\t Accuracy 0.9696\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1113\t Average training accuracy 0.9690\n",
      "Epoch [17]\t Average validation loss 0.1011\t Average validation accuracy 0.9723\n",
      "\n",
      "Epoch [18][20]\t Batch [0][183]\t Training Loss 0.0760\t Accuracy 0.9867\n",
      "Epoch [18][20]\t Batch [50][183]\t Training Loss 0.1027\t Accuracy 0.9716\n",
      "Epoch [18][20]\t Batch [100][183]\t Training Loss 0.1047\t Accuracy 0.9709\n",
      "Epoch [18][20]\t Batch [150][183]\t Training Loss 0.1054\t Accuracy 0.9707\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1071\t Average training accuracy 0.9701\n",
      "Epoch [18]\t Average validation loss 0.0984\t Average validation accuracy 0.9725\n",
      "\n",
      "Epoch [19][20]\t Batch [0][183]\t Training Loss 0.0707\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][183]\t Training Loss 0.0988\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [100][183]\t Training Loss 0.1008\t Accuracy 0.9719\n",
      "Epoch [19][20]\t Batch [150][183]\t Training Loss 0.1016\t Accuracy 0.9717\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1032\t Average training accuracy 0.9710\n",
      "Epoch [19]\t Average validation loss 0.0979\t Average validation accuracy 0.9733\n",
      "\n",
      "Epoch [0][20]\t Batch [0][91]\t Training Loss 9.2093\t Accuracy 0.0717\n",
      "Epoch [0][20]\t Batch [50][91]\t Training Loss 1.7670\t Accuracy 0.1045\n",
      "\n",
      "Epoch [0]\t Average training loss 1.2695\t Average training accuracy 0.1110\n",
      "Epoch [0]\t Average validation loss 0.6095\t Average validation accuracy 0.1467\n",
      "\n",
      "Epoch [1][20]\t Batch [0][91]\t Training Loss 0.6242\t Accuracy 0.1267\n",
      "Epoch [1][20]\t Batch [50][91]\t Training Loss 0.6000\t Accuracy 0.1486\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5893\t Average training accuracy 0.1571\n",
      "Epoch [1]\t Average validation loss 0.5543\t Average validation accuracy 0.1973\n",
      "\n",
      "Epoch [2][20]\t Batch [0][91]\t Training Loss 0.5681\t Accuracy 0.1717\n",
      "Epoch [2][20]\t Batch [50][91]\t Training Loss 0.5473\t Accuracy 0.2036\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5397\t Average training accuracy 0.2137\n",
      "Epoch [2]\t Average validation loss 0.5094\t Average validation accuracy 0.2648\n",
      "\n",
      "Epoch [3][20]\t Batch [0][91]\t Training Loss 0.5256\t Accuracy 0.2300\n",
      "Epoch [3][20]\t Batch [50][91]\t Training Loss 0.5064\t Accuracy 0.2645\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5010\t Average training accuracy 0.2738\n",
      "Epoch [3]\t Average validation loss 0.4745\t Average validation accuracy 0.3260\n",
      "\n",
      "Epoch [4][20]\t Batch [0][91]\t Training Loss 0.4835\t Accuracy 0.3000\n",
      "Epoch [4][20]\t Batch [50][91]\t Training Loss 0.4739\t Accuracy 0.3276\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4702\t Average training accuracy 0.3363\n",
      "Epoch [4]\t Average validation loss 0.4483\t Average validation accuracy 0.3902\n",
      "\n",
      "Epoch [5][20]\t Batch [0][91]\t Training Loss 0.4521\t Accuracy 0.3533\n",
      "Epoch [5][20]\t Batch [50][91]\t Training Loss 0.4475\t Accuracy 0.3890\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4451\t Average training accuracy 0.3961\n",
      "Epoch [5]\t Average validation loss 0.4231\t Average validation accuracy 0.4456\n",
      "\n",
      "Epoch [6][20]\t Batch [0][91]\t Training Loss 0.4329\t Accuracy 0.4117\n",
      "Epoch [6][20]\t Batch [50][91]\t Training Loss 0.4262\t Accuracy 0.4396\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4246\t Average training accuracy 0.4459\n",
      "Epoch [6]\t Average validation loss 0.4050\t Average validation accuracy 0.4908\n",
      "\n",
      "Epoch [7][20]\t Batch [0][91]\t Training Loss 0.3989\t Accuracy 0.4933\n",
      "Epoch [7][20]\t Batch [50][91]\t Training Loss 0.4083\t Accuracy 0.4821\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4072\t Average training accuracy 0.4875\n",
      "Epoch [7]\t Average validation loss 0.3887\t Average validation accuracy 0.5308\n",
      "\n",
      "Epoch [8][20]\t Batch [0][91]\t Training Loss 0.4045\t Accuracy 0.4533\n",
      "Epoch [8][20]\t Batch [50][91]\t Training Loss 0.3933\t Accuracy 0.5163\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3927\t Average training accuracy 0.5195\n",
      "Epoch [8]\t Average validation loss 0.3747\t Average validation accuracy 0.5604\n",
      "\n",
      "Epoch [9][20]\t Batch [0][91]\t Training Loss 0.3858\t Accuracy 0.5333\n",
      "Epoch [9][20]\t Batch [50][91]\t Training Loss 0.3804\t Accuracy 0.5437\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3805\t Average training accuracy 0.5459\n",
      "Epoch [9]\t Average validation loss 0.3632\t Average validation accuracy 0.5883\n",
      "\n",
      "Epoch [10][20]\t Batch [0][91]\t Training Loss 0.3599\t Accuracy 0.6067\n",
      "Epoch [10][20]\t Batch [50][91]\t Training Loss 0.3690\t Accuracy 0.5682\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3696\t Average training accuracy 0.5686\n",
      "Epoch [10]\t Average validation loss 0.3521\t Average validation accuracy 0.6115\n",
      "\n",
      "Epoch [11][20]\t Batch [0][91]\t Training Loss 0.3579\t Accuracy 0.6067\n",
      "Epoch [11][20]\t Batch [50][91]\t Training Loss 0.3591\t Accuracy 0.5897\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3601\t Average training accuracy 0.5887\n",
      "Epoch [11]\t Average validation loss 0.3426\t Average validation accuracy 0.6367\n",
      "\n",
      "Epoch [12][20]\t Batch [0][91]\t Training Loss 0.3727\t Accuracy 0.5600\n",
      "Epoch [12][20]\t Batch [50][91]\t Training Loss 0.3509\t Accuracy 0.6089\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3519\t Average training accuracy 0.6073\n",
      "Epoch [12]\t Average validation loss 0.3347\t Average validation accuracy 0.6558\n",
      "\n",
      "Epoch [13][20]\t Batch [0][91]\t Training Loss 0.3513\t Accuracy 0.6233\n",
      "Epoch [13][20]\t Batch [50][91]\t Training Loss 0.3435\t Accuracy 0.6251\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3443\t Average training accuracy 0.6236\n",
      "Epoch [13]\t Average validation loss 0.3268\t Average validation accuracy 0.6721\n",
      "\n",
      "Epoch [14][20]\t Batch [0][91]\t Training Loss 0.3734\t Accuracy 0.5750\n",
      "Epoch [14][20]\t Batch [50][91]\t Training Loss 0.3375\t Accuracy 0.6378\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3380\t Average training accuracy 0.6367\n",
      "Epoch [14]\t Average validation loss 0.3216\t Average validation accuracy 0.6873\n",
      "\n",
      "Epoch [15][20]\t Batch [0][91]\t Training Loss 0.3557\t Accuracy 0.5967\n",
      "Epoch [15][20]\t Batch [50][91]\t Training Loss 0.3315\t Accuracy 0.6511\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3322\t Average training accuracy 0.6489\n",
      "Epoch [15]\t Average validation loss 0.3159\t Average validation accuracy 0.6975\n",
      "\n",
      "Epoch [16][20]\t Batch [0][91]\t Training Loss 0.3277\t Accuracy 0.6367\n",
      "Epoch [16][20]\t Batch [50][91]\t Training Loss 0.3259\t Accuracy 0.6636\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3269\t Average training accuracy 0.6609\n",
      "Epoch [16]\t Average validation loss 0.3106\t Average validation accuracy 0.7110\n",
      "\n",
      "Epoch [17][20]\t Batch [0][91]\t Training Loss 0.3098\t Accuracy 0.6883\n",
      "Epoch [17][20]\t Batch [50][91]\t Training Loss 0.3207\t Accuracy 0.6755\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3221\t Average training accuracy 0.6714\n",
      "Epoch [17]\t Average validation loss 0.3053\t Average validation accuracy 0.7223\n",
      "\n",
      "Epoch [18][20]\t Batch [0][91]\t Training Loss 0.3054\t Accuracy 0.7133\n",
      "Epoch [18][20]\t Batch [50][91]\t Training Loss 0.3159\t Accuracy 0.6853\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3174\t Average training accuracy 0.6816\n",
      "Epoch [18]\t Average validation loss 0.3005\t Average validation accuracy 0.7323\n",
      "\n",
      "Epoch [19][20]\t Batch [0][91]\t Training Loss 0.3232\t Accuracy 0.6700\n",
      "Epoch [19][20]\t Batch [50][91]\t Training Loss 0.3121\t Accuracy 0.6928\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3134\t Average training accuracy 0.6896\n",
      "Epoch [19]\t Average validation loss 0.2969\t Average validation accuracy 0.7421\n",
      "\n",
      "Epoch [0][20]\t Batch [0][91]\t Training Loss 3.9834\t Accuracy 0.0617\n",
      "Epoch [0][20]\t Batch [50][91]\t Training Loss 1.3832\t Accuracy 0.2719\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9658\t Average training accuracy 0.3557\n",
      "Epoch [0]\t Average validation loss 0.3769\t Average validation accuracy 0.5437\n",
      "\n",
      "Epoch [1][20]\t Batch [0][91]\t Training Loss 0.3893\t Accuracy 0.5317\n",
      "Epoch [1][20]\t Batch [50][91]\t Training Loss 0.3578\t Accuracy 0.5537\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3382\t Average training accuracy 0.5839\n",
      "Epoch [1]\t Average validation loss 0.2845\t Average validation accuracy 0.6804\n",
      "\n",
      "Epoch [2][20]\t Batch [0][91]\t Training Loss 0.2927\t Accuracy 0.6483\n",
      "Epoch [2][20]\t Batch [50][91]\t Training Loss 0.2839\t Accuracy 0.6597\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2742\t Average training accuracy 0.6823\n",
      "Epoch [2]\t Average validation loss 0.2396\t Average validation accuracy 0.7592\n",
      "\n",
      "Epoch [3][20]\t Batch [0][91]\t Training Loss 0.2505\t Accuracy 0.7300\n",
      "Epoch [3][20]\t Batch [50][91]\t Training Loss 0.2388\t Accuracy 0.7488\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2311\t Average training accuracy 0.7612\n",
      "Epoch [3]\t Average validation loss 0.1985\t Average validation accuracy 0.8173\n",
      "\n",
      "Epoch [4][20]\t Batch [0][91]\t Training Loss 0.2060\t Accuracy 0.7933\n",
      "Epoch [4][20]\t Batch [50][91]\t Training Loss 0.2028\t Accuracy 0.7970\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1979\t Average training accuracy 0.7989\n",
      "Epoch [4]\t Average validation loss 0.1697\t Average validation accuracy 0.8367\n",
      "\n",
      "Epoch [5][20]\t Batch [0][91]\t Training Loss 0.1716\t Accuracy 0.8433\n",
      "Epoch [5][20]\t Batch [50][91]\t Training Loss 0.1759\t Accuracy 0.8230\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1729\t Average training accuracy 0.8286\n",
      "Epoch [5]\t Average validation loss 0.1465\t Average validation accuracy 0.8748\n",
      "\n",
      "Epoch [6][20]\t Batch [0][91]\t Training Loss 0.1795\t Accuracy 0.7983\n",
      "Epoch [6][20]\t Batch [50][91]\t Training Loss 0.1569\t Accuracy 0.8545\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1550\t Average training accuracy 0.8555\n",
      "Epoch [6]\t Average validation loss 0.1306\t Average validation accuracy 0.8923\n",
      "\n",
      "Epoch [7][20]\t Batch [0][91]\t Training Loss 0.1454\t Accuracy 0.8667\n",
      "Epoch [7][20]\t Batch [50][91]\t Training Loss 0.1419\t Accuracy 0.8725\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1406\t Average training accuracy 0.8722\n",
      "Epoch [7]\t Average validation loss 0.1165\t Average validation accuracy 0.9052\n",
      "\n",
      "Epoch [8][20]\t Batch [0][91]\t Training Loss 0.1466\t Accuracy 0.8550\n",
      "Epoch [8][20]\t Batch [50][91]\t Training Loss 0.1299\t Accuracy 0.8817\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1293\t Average training accuracy 0.8810\n",
      "Epoch [8]\t Average validation loss 0.1075\t Average validation accuracy 0.9112\n",
      "\n",
      "Epoch [9][20]\t Batch [0][91]\t Training Loss 0.1078\t Accuracy 0.9100\n",
      "Epoch [9][20]\t Batch [50][91]\t Training Loss 0.1201\t Accuracy 0.8911\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1205\t Average training accuracy 0.8894\n",
      "Epoch [9]\t Average validation loss 0.0999\t Average validation accuracy 0.9185\n",
      "\n",
      "Epoch [10][20]\t Batch [0][91]\t Training Loss 0.1071\t Accuracy 0.9150\n",
      "Epoch [10][20]\t Batch [50][91]\t Training Loss 0.1135\t Accuracy 0.8974\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1145\t Average training accuracy 0.8955\n",
      "Epoch [10]\t Average validation loss 0.0951\t Average validation accuracy 0.9252\n",
      "\n",
      "Epoch [11][20]\t Batch [0][91]\t Training Loss 0.1045\t Accuracy 0.9083\n",
      "Epoch [11][20]\t Batch [50][91]\t Training Loss 0.1085\t Accuracy 0.9033\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1097\t Average training accuracy 0.9008\n",
      "Epoch [11]\t Average validation loss 0.0905\t Average validation accuracy 0.9285\n",
      "\n",
      "Epoch [12][20]\t Batch [0][91]\t Training Loss 0.1181\t Accuracy 0.8967\n",
      "Epoch [12][20]\t Batch [50][91]\t Training Loss 0.1053\t Accuracy 0.9067\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1063\t Average training accuracy 0.9043\n",
      "Epoch [12]\t Average validation loss 0.0879\t Average validation accuracy 0.9333\n",
      "\n",
      "Epoch [13][20]\t Batch [0][91]\t Training Loss 0.1162\t Accuracy 0.8983\n",
      "Epoch [13][20]\t Batch [50][91]\t Training Loss 0.1026\t Accuracy 0.9100\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1033\t Average training accuracy 0.9078\n",
      "Epoch [13]\t Average validation loss 0.0856\t Average validation accuracy 0.9369\n",
      "\n",
      "Epoch [14][20]\t Batch [0][91]\t Training Loss 0.1240\t Accuracy 0.8717\n",
      "Epoch [14][20]\t Batch [50][91]\t Training Loss 0.1006\t Accuracy 0.9111\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1011\t Average training accuracy 0.9100\n",
      "Epoch [14]\t Average validation loss 0.0846\t Average validation accuracy 0.9367\n",
      "\n",
      "Epoch [15][20]\t Batch [0][91]\t Training Loss 0.1206\t Accuracy 0.8750\n",
      "Epoch [15][20]\t Batch [50][91]\t Training Loss 0.0986\t Accuracy 0.9136\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0992\t Average training accuracy 0.9122\n",
      "Epoch [15]\t Average validation loss 0.0833\t Average validation accuracy 0.9377\n",
      "\n",
      "Epoch [16][20]\t Batch [0][91]\t Training Loss 0.1043\t Accuracy 0.8883\n",
      "Epoch [16][20]\t Batch [50][91]\t Training Loss 0.0967\t Accuracy 0.9152\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0975\t Average training accuracy 0.9137\n",
      "Epoch [16]\t Average validation loss 0.0819\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [17][20]\t Batch [0][91]\t Training Loss 0.0859\t Accuracy 0.9300\n",
      "Epoch [17][20]\t Batch [50][91]\t Training Loss 0.0950\t Accuracy 0.9171\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0959\t Average training accuracy 0.9154\n",
      "Epoch [17]\t Average validation loss 0.0802\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [18][20]\t Batch [0][91]\t Training Loss 0.0844\t Accuracy 0.9317\n",
      "Epoch [18][20]\t Batch [50][91]\t Training Loss 0.0935\t Accuracy 0.9184\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0943\t Average training accuracy 0.9170\n",
      "Epoch [18]\t Average validation loss 0.0787\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [19][20]\t Batch [0][91]\t Training Loss 0.1009\t Accuracy 0.9067\n",
      "Epoch [19][20]\t Batch [50][91]\t Training Loss 0.0925\t Accuracy 0.9191\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0931\t Average training accuracy 0.9180\n",
      "Epoch [19]\t Average validation loss 0.0787\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [0][20]\t Batch [0][91]\t Training Loss 3.2675\t Accuracy 0.1067\n",
      "Epoch [0][20]\t Batch [50][91]\t Training Loss 2.9517\t Accuracy 0.0948\n",
      "\n",
      "Epoch [0]\t Average training loss 2.7971\t Average training accuracy 0.0984\n",
      "Epoch [0]\t Average validation loss 2.5032\t Average validation accuracy 0.1065\n",
      "\n",
      "Epoch [1][20]\t Batch [0][91]\t Training Loss 2.4617\t Accuracy 0.1217\n",
      "Epoch [1][20]\t Batch [50][91]\t Training Loss 2.4273\t Accuracy 0.1176\n",
      "\n",
      "Epoch [1]\t Average training loss 2.3873\t Average training accuracy 0.1273\n",
      "Epoch [1]\t Average validation loss 2.2987\t Average validation accuracy 0.1596\n",
      "\n",
      "Epoch [2][20]\t Batch [0][91]\t Training Loss 2.2775\t Accuracy 0.1633\n",
      "Epoch [2][20]\t Batch [50][91]\t Training Loss 2.2705\t Accuracy 0.1783\n",
      "\n",
      "Epoch [2]\t Average training loss 2.2553\t Average training accuracy 0.1932\n",
      "Epoch [2]\t Average validation loss 2.2111\t Average validation accuracy 0.2508\n",
      "\n",
      "Epoch [3][20]\t Batch [0][91]\t Training Loss 2.2235\t Accuracy 0.2300\n",
      "Epoch [3][20]\t Batch [50][91]\t Training Loss 2.1994\t Accuracy 0.2622\n",
      "\n",
      "Epoch [3]\t Average training loss 2.1909\t Average training accuracy 0.2705\n",
      "Epoch [3]\t Average validation loss 2.1573\t Average validation accuracy 0.3175\n",
      "\n",
      "Epoch [4][20]\t Batch [0][91]\t Training Loss 2.1789\t Accuracy 0.2817\n",
      "Epoch [4][20]\t Batch [50][91]\t Training Loss 2.1513\t Accuracy 0.3210\n",
      "\n",
      "Epoch [4]\t Average training loss 2.1452\t Average training accuracy 0.3273\n",
      "Epoch [4]\t Average validation loss 2.1140\t Average validation accuracy 0.3638\n",
      "\n",
      "Epoch [5][20]\t Batch [0][91]\t Training Loss 2.1123\t Accuracy 0.3650\n",
      "Epoch [5][20]\t Batch [50][91]\t Training Loss 2.1097\t Accuracy 0.3676\n",
      "\n",
      "Epoch [5]\t Average training loss 2.1046\t Average training accuracy 0.3721\n",
      "Epoch [5]\t Average validation loss 2.0738\t Average validation accuracy 0.4129\n",
      "\n",
      "Epoch [6][20]\t Batch [0][91]\t Training Loss 2.0928\t Accuracy 0.3650\n",
      "Epoch [6][20]\t Batch [50][91]\t Training Loss 2.0712\t Accuracy 0.4081\n",
      "\n",
      "Epoch [6]\t Average training loss 2.0665\t Average training accuracy 0.4123\n",
      "Epoch [6]\t Average validation loss 2.0325\t Average validation accuracy 0.4544\n",
      "\n",
      "Epoch [7][20]\t Batch [0][91]\t Training Loss 2.0392\t Accuracy 0.4100\n",
      "Epoch [7][20]\t Batch [50][91]\t Training Loss 2.0338\t Accuracy 0.4451\n",
      "\n",
      "Epoch [7]\t Average training loss 2.0291\t Average training accuracy 0.4491\n",
      "Epoch [7]\t Average validation loss 1.9947\t Average validation accuracy 0.4910\n",
      "\n",
      "Epoch [8][20]\t Batch [0][91]\t Training Loss 2.0213\t Accuracy 0.4233\n",
      "Epoch [8][20]\t Batch [50][91]\t Training Loss 1.9977\t Accuracy 0.4773\n",
      "\n",
      "Epoch [8]\t Average training loss 1.9932\t Average training accuracy 0.4807\n",
      "Epoch [8]\t Average validation loss 1.9563\t Average validation accuracy 0.5281\n",
      "\n",
      "Epoch [9][20]\t Batch [0][91]\t Training Loss 1.9663\t Accuracy 0.4933\n",
      "Epoch [9][20]\t Batch [50][91]\t Training Loss 1.9617\t Accuracy 0.5094\n",
      "\n",
      "Epoch [9]\t Average training loss 1.9581\t Average training accuracy 0.5109\n",
      "Epoch [9]\t Average validation loss 1.9191\t Average validation accuracy 0.5565\n",
      "\n",
      "Epoch [10][20]\t Batch [0][91]\t Training Loss 1.9206\t Accuracy 0.5417\n",
      "Epoch [10][20]\t Batch [50][91]\t Training Loss 1.9267\t Accuracy 0.5372\n",
      "\n",
      "Epoch [10]\t Average training loss 1.9237\t Average training accuracy 0.5385\n",
      "Epoch [10]\t Average validation loss 1.8815\t Average validation accuracy 0.5885\n",
      "\n",
      "Epoch [11][20]\t Batch [0][91]\t Training Loss 1.8946\t Accuracy 0.5533\n",
      "Epoch [11][20]\t Batch [50][91]\t Training Loss 1.8924\t Accuracy 0.5621\n",
      "\n",
      "Epoch [11]\t Average training loss 1.8901\t Average training accuracy 0.5623\n",
      "Epoch [11]\t Average validation loss 1.8478\t Average validation accuracy 0.6123\n",
      "\n",
      "Epoch [12][20]\t Batch [0][91]\t Training Loss 1.8907\t Accuracy 0.5550\n",
      "Epoch [12][20]\t Batch [50][91]\t Training Loss 1.8597\t Accuracy 0.5828\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8572\t Average training accuracy 0.5843\n",
      "Epoch [12]\t Average validation loss 1.8126\t Average validation accuracy 0.6362\n",
      "\n",
      "Epoch [13][20]\t Batch [0][91]\t Training Loss 1.8596\t Accuracy 0.5750\n",
      "Epoch [13][20]\t Batch [50][91]\t Training Loss 1.8286\t Accuracy 0.6013\n",
      "\n",
      "Epoch [13]\t Average training loss 1.8250\t Average training accuracy 0.6035\n",
      "Epoch [13]\t Average validation loss 1.7789\t Average validation accuracy 0.6552\n",
      "\n",
      "Epoch [14][20]\t Batch [0][91]\t Training Loss 1.8792\t Accuracy 0.5283\n",
      "Epoch [14][20]\t Batch [50][91]\t Training Loss 1.7985\t Accuracy 0.6175\n",
      "\n",
      "Epoch [14]\t Average training loss 1.7944\t Average training accuracy 0.6195\n",
      "Epoch [14]\t Average validation loss 1.7466\t Average validation accuracy 0.6729\n",
      "\n",
      "Epoch [15][20]\t Batch [0][91]\t Training Loss 1.8119\t Accuracy 0.6167\n",
      "Epoch [15][20]\t Batch [50][91]\t Training Loss 1.7675\t Accuracy 0.6329\n",
      "\n",
      "Epoch [15]\t Average training loss 1.7642\t Average training accuracy 0.6336\n",
      "Epoch [15]\t Average validation loss 1.7171\t Average validation accuracy 0.6846\n",
      "\n",
      "Epoch [16][20]\t Batch [0][91]\t Training Loss 1.7516\t Accuracy 0.6617\n",
      "Epoch [16][20]\t Batch [50][91]\t Training Loss 1.7371\t Accuracy 0.6473\n",
      "\n",
      "Epoch [16]\t Average training loss 1.7348\t Average training accuracy 0.6474\n",
      "Epoch [16]\t Average validation loss 1.6856\t Average validation accuracy 0.6992\n",
      "\n",
      "Epoch [17][20]\t Batch [0][91]\t Training Loss 1.6587\t Accuracy 0.7050\n",
      "Epoch [17][20]\t Batch [50][91]\t Training Loss 1.7076\t Accuracy 0.6603\n",
      "\n",
      "Epoch [17]\t Average training loss 1.7059\t Average training accuracy 0.6597\n",
      "Epoch [17]\t Average validation loss 1.6539\t Average validation accuracy 0.7102\n",
      "\n",
      "Epoch [18][20]\t Batch [0][91]\t Training Loss 1.6427\t Accuracy 0.7200\n",
      "Epoch [18][20]\t Batch [50][91]\t Training Loss 1.6781\t Accuracy 0.6728\n",
      "\n",
      "Epoch [18]\t Average training loss 1.6774\t Average training accuracy 0.6713\n",
      "Epoch [18]\t Average validation loss 1.6232\t Average validation accuracy 0.7210\n",
      "\n",
      "Epoch [19][20]\t Batch [0][91]\t Training Loss 1.6652\t Accuracy 0.6700\n",
      "Epoch [19][20]\t Batch [50][91]\t Training Loss 1.6509\t Accuracy 0.6832\n",
      "\n",
      "Epoch [19]\t Average training loss 1.6500\t Average training accuracy 0.6812\n",
      "Epoch [19]\t Average validation loss 1.5970\t Average validation accuracy 0.7317\n",
      "\n",
      "Epoch [0][20]\t Batch [0][91]\t Training Loss 2.5197\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][91]\t Training Loss 1.9730\t Accuracy 0.3388\n",
      "\n",
      "Epoch [0]\t Average training loss 1.5116\t Average training accuracy 0.5165\n",
      "Epoch [0]\t Average validation loss 0.5761\t Average validation accuracy 0.8456\n",
      "\n",
      "Epoch [1][20]\t Batch [0][91]\t Training Loss 0.6676\t Accuracy 0.8083\n",
      "Epoch [1][20]\t Batch [50][91]\t Training Loss 0.5615\t Accuracy 0.8319\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5262\t Average training accuracy 0.8429\n",
      "Epoch [1]\t Average validation loss 0.3367\t Average validation accuracy 0.9006\n",
      "\n",
      "Epoch [2][20]\t Batch [0][91]\t Training Loss 0.3181\t Accuracy 0.9083\n",
      "Epoch [2][20]\t Batch [50][91]\t Training Loss 0.4189\t Accuracy 0.8789\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4147\t Average training accuracy 0.8809\n",
      "Epoch [2]\t Average validation loss 0.2948\t Average validation accuracy 0.9131\n",
      "\n",
      "Epoch [3][20]\t Batch [0][91]\t Training Loss 0.3545\t Accuracy 0.8867\n",
      "Epoch [3][20]\t Batch [50][91]\t Training Loss 0.3633\t Accuracy 0.8973\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3597\t Average training accuracy 0.8971\n",
      "Epoch [3]\t Average validation loss 0.2511\t Average validation accuracy 0.9271\n",
      "\n",
      "Epoch [4][20]\t Batch [0][91]\t Training Loss 0.3156\t Accuracy 0.9067\n",
      "Epoch [4][20]\t Batch [50][91]\t Training Loss 0.3201\t Accuracy 0.9095\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3200\t Average training accuracy 0.9082\n",
      "Epoch [4]\t Average validation loss 0.2382\t Average validation accuracy 0.9323\n",
      "\n",
      "Epoch [5][20]\t Batch [0][91]\t Training Loss 0.2539\t Accuracy 0.9217\n",
      "Epoch [5][20]\t Batch [50][91]\t Training Loss 0.2894\t Accuracy 0.9176\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2899\t Average training accuracy 0.9163\n",
      "Epoch [5]\t Average validation loss 0.2191\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [6][20]\t Batch [0][91]\t Training Loss 0.3777\t Accuracy 0.8867\n",
      "Epoch [6][20]\t Batch [50][91]\t Training Loss 0.2708\t Accuracy 0.9228\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2712\t Average training accuracy 0.9222\n",
      "Epoch [6]\t Average validation loss 0.2054\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [7][20]\t Batch [0][91]\t Training Loss 0.2627\t Accuracy 0.9283\n",
      "Epoch [7][20]\t Batch [50][91]\t Training Loss 0.2546\t Accuracy 0.9275\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2546\t Average training accuracy 0.9273\n",
      "Epoch [7]\t Average validation loss 0.1942\t Average validation accuracy 0.9492\n",
      "\n",
      "Epoch [8][20]\t Batch [0][91]\t Training Loss 0.2981\t Accuracy 0.9150\n",
      "Epoch [8][20]\t Batch [50][91]\t Training Loss 0.2414\t Accuracy 0.9313\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2420\t Average training accuracy 0.9305\n",
      "Epoch [8]\t Average validation loss 0.1887\t Average validation accuracy 0.9496\n",
      "\n",
      "Epoch [9][20]\t Batch [0][91]\t Training Loss 0.1868\t Accuracy 0.9483\n",
      "Epoch [9][20]\t Batch [50][91]\t Training Loss 0.2281\t Accuracy 0.9350\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2302\t Average training accuracy 0.9337\n",
      "Epoch [9]\t Average validation loss 0.1787\t Average validation accuracy 0.9535\n",
      "\n",
      "Epoch [10][20]\t Batch [0][91]\t Training Loss 0.1628\t Accuracy 0.9467\n",
      "Epoch [10][20]\t Batch [50][91]\t Training Loss 0.2158\t Accuracy 0.9385\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2198\t Average training accuracy 0.9369\n",
      "Epoch [10]\t Average validation loss 0.1724\t Average validation accuracy 0.9569\n",
      "\n",
      "Epoch [11][20]\t Batch [0][91]\t Training Loss 0.1763\t Accuracy 0.9450\n",
      "Epoch [11][20]\t Batch [50][91]\t Training Loss 0.2054\t Accuracy 0.9413\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2091\t Average training accuracy 0.9400\n",
      "Epoch [11]\t Average validation loss 0.1595\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [12][20]\t Batch [0][91]\t Training Loss 0.3201\t Accuracy 0.9183\n",
      "Epoch [12][20]\t Batch [50][91]\t Training Loss 0.1994\t Accuracy 0.9434\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2017\t Average training accuracy 0.9424\n",
      "Epoch [12]\t Average validation loss 0.1603\t Average validation accuracy 0.9606\n",
      "\n",
      "Epoch [13][20]\t Batch [0][91]\t Training Loss 0.2608\t Accuracy 0.9267\n",
      "Epoch [13][20]\t Batch [50][91]\t Training Loss 0.1922\t Accuracy 0.9453\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1936\t Average training accuracy 0.9446\n",
      "Epoch [13]\t Average validation loss 0.1550\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [14][20]\t Batch [0][91]\t Training Loss 0.2588\t Accuracy 0.9233\n",
      "Epoch [14][20]\t Batch [50][91]\t Training Loss 0.1866\t Accuracy 0.9465\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1871\t Average training accuracy 0.9462\n",
      "Epoch [14]\t Average validation loss 0.1499\t Average validation accuracy 0.9623\n",
      "\n",
      "Epoch [15][20]\t Batch [0][91]\t Training Loss 0.2453\t Accuracy 0.9300\n",
      "Epoch [15][20]\t Batch [50][91]\t Training Loss 0.1796\t Accuracy 0.9484\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1808\t Average training accuracy 0.9479\n",
      "Epoch [15]\t Average validation loss 0.1461\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [16][20]\t Batch [0][91]\t Training Loss 0.1978\t Accuracy 0.9400\n",
      "Epoch [16][20]\t Batch [50][91]\t Training Loss 0.1736\t Accuracy 0.9502\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1751\t Average training accuracy 0.9497\n",
      "Epoch [16]\t Average validation loss 0.1416\t Average validation accuracy 0.9665\n",
      "\n",
      "Epoch [17][20]\t Batch [0][91]\t Training Loss 0.1236\t Accuracy 0.9550\n",
      "Epoch [17][20]\t Batch [50][91]\t Training Loss 0.1675\t Accuracy 0.9515\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1697\t Average training accuracy 0.9512\n",
      "Epoch [17]\t Average validation loss 0.1363\t Average validation accuracy 0.9665\n",
      "\n",
      "Epoch [18][20]\t Batch [0][91]\t Training Loss 0.1363\t Accuracy 0.9533\n",
      "Epoch [18][20]\t Batch [50][91]\t Training Loss 0.1618\t Accuracy 0.9528\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1641\t Average training accuracy 0.9525\n",
      "Epoch [18]\t Average validation loss 0.1329\t Average validation accuracy 0.9683\n",
      "\n",
      "Epoch [19][20]\t Batch [0][91]\t Training Loss 0.1786\t Accuracy 0.9417\n",
      "Epoch [19][20]\t Batch [50][91]\t Training Loss 0.1579\t Accuracy 0.9541\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1595\t Average training accuracy 0.9540\n",
      "Epoch [19]\t Average validation loss 0.1324\t Average validation accuracy 0.9675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate_SGD = 0.001\n",
    "\n",
    "for batch_size in [10, 50, 100, 300, 600]:\n",
    "    #Euclidean+Sigmoid\n",
    "    momentum = 0.55\n",
    "    weight_decay= 0.0001\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]   \n",
    "\n",
    "    #Euclidean+ReLU\n",
    "    momentum = 0.99\n",
    "    weight_decay= 0.0001\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    reluMLP = Network()\n",
    "    # 使用FCLayer和ReLULayer构建多层感知机\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]     \n",
    "\n",
    "    #CrossEntropy+Sigmoid\n",
    "    momentum = 0.55\n",
    "    weight_decay= 0.00001\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]         \n",
    "\n",
    "    #CrossEntropy+ReLU\n",
    "    momentum = 0.99\n",
    "    weight_decay= 0.00001\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    reluMLP = Network()\n",
    "    t1=time.time()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>202.992379</td>\n",
       "      <td>[0.23696529033491118, 0.21295889748507354, 0.2...</td>\n",
       "      <td>[0.8358000000000001, 0.8702000000000001, 0.879...</td>\n",
       "      <td>0.893160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>202.865551</td>\n",
       "      <td>[0.07850324827173032, 0.06918276771896614, 0.0...</td>\n",
       "      <td>[0.9466, 0.9546, 0.9596000000000001, 0.9642000...</td>\n",
       "      <td>0.967250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>220.172044</td>\n",
       "      <td>[0.8776581632144806, 0.5738326347256689, 0.457...</td>\n",
       "      <td>[0.8642000000000002, 0.8942, 0.905, 0.9134, 0....</td>\n",
       "      <td>0.922080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>213.903324</td>\n",
       "      <td>[0.12273972706776996, 0.09220356944746999, 0.0...</td>\n",
       "      <td>[0.9658000000000001, 0.971, 0.9770000000000001...</td>\n",
       "      <td>0.981260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>84.205458</td>\n",
       "      <td>[0.33334949700556904, 0.28014550534958116, 0.2...</td>\n",
       "      <td>[0.6496, 0.7664, 0.8048000000000001, 0.8263999...</td>\n",
       "      <td>0.844620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>103.404958</td>\n",
       "      <td>[0.09226160089623736, 0.07735723972868697, 0.0...</td>\n",
       "      <td>[0.9314, 0.9450000000000002, 0.948000000000000...</td>\n",
       "      <td>0.958730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>106.531004</td>\n",
       "      <td>[1.832304787036741, 1.461818502020953, 1.20390...</td>\n",
       "      <td>[0.6298, 0.7616000000000003, 0.8164, 0.8456, 0...</td>\n",
       "      <td>0.868030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>110.083097</td>\n",
       "      <td>[0.17114324509297812, 0.12833095406247166, 0.1...</td>\n",
       "      <td>[0.9540000000000001, 0.9658, 0.97, 0.972399999...</td>\n",
       "      <td>0.975220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>86.230360</td>\n",
       "      <td>[0.4049494244660077, 0.3307651218876163, 0.296...</td>\n",
       "      <td>[0.46959999999999996, 0.6522, 0.73, 0.77139999...</td>\n",
       "      <td>0.810190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>86.719688</td>\n",
       "      <td>[0.1452353369806154, 0.10322973457606605, 0.09...</td>\n",
       "      <td>[0.8859999999999999, 0.9181999999999999, 0.932...</td>\n",
       "      <td>0.942630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>87.260598</td>\n",
       "      <td>[2.0394249122976387, 1.813701121788175, 1.6220...</td>\n",
       "      <td>[0.4534, 0.6118, 0.6969999999999998, 0.7525999...</td>\n",
       "      <td>0.813120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>73.201181</td>\n",
       "      <td>[0.22148675894657038, 0.16435740396122633, 0.1...</td>\n",
       "      <td>[0.9390000000000001, 0.9571999999999999, 0.962...</td>\n",
       "      <td>0.971240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>60.541882</td>\n",
       "      <td>[0.5203300683697728, 0.4509735832075066, 0.405...</td>\n",
       "      <td>[0.19291666666666668, 0.34062499999999996, 0.4...</td>\n",
       "      <td>0.685542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.566508</td>\n",
       "      <td>[0.2846998772028879, 0.18061703583946317, 0.13...</td>\n",
       "      <td>[0.68625, 0.8447916666666666, 0.88583333333333...</td>\n",
       "      <td>0.917062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>79.935867</td>\n",
       "      <td>[2.2238404375975236, 2.1061308941983325, 2.020...</td>\n",
       "      <td>[0.171875, 0.365, 0.49583333333333335, 0.57812...</td>\n",
       "      <td>0.695073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>89.487252</td>\n",
       "      <td>[0.34793722116382103, 0.24725993974281432, 0.2...</td>\n",
       "      <td>[0.8966666666666666, 0.9239583333333333, 0.941...</td>\n",
       "      <td>0.957708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>89.956403</td>\n",
       "      <td>[0.6095010316344058, 0.5543381130682878, 0.509...</td>\n",
       "      <td>[0.14666666666666667, 0.19729166666666667, 0.2...</td>\n",
       "      <td>0.540479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>90.437684</td>\n",
       "      <td>[0.37691816545944806, 0.28450462760429074, 0.2...</td>\n",
       "      <td>[0.54375, 0.6804166666666667, 0.75916666666666...</td>\n",
       "      <td>0.874927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>79.771831</td>\n",
       "      <td>[2.503238239069959, 2.298720323406733, 2.21107...</td>\n",
       "      <td>[0.10645833333333332, 0.15958333333333333, 0.2...</td>\n",
       "      <td>0.517646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>80.134106</td>\n",
       "      <td>[0.5761123979013656, 0.3366554806395818, 0.294...</td>\n",
       "      <td>[0.8456250000000001, 0.900625, 0.913125, 0.927...</td>\n",
       "      <td>0.944479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0      Euclidean_Sigmoid          10              0.001      0.55   \n",
       "1         Euclidean_ReLU          10              0.001      0.99   \n",
       "2   CrossEntropy_Sigmoid          10              0.001      0.55   \n",
       "3      CrossEntropy_ReLU          10              0.001      0.99   \n",
       "4      Euclidean_Sigmoid          50              0.001      0.55   \n",
       "5         Euclidean_ReLU          50              0.001      0.99   \n",
       "6   CrossEntropy_Sigmoid          50              0.001      0.55   \n",
       "7      CrossEntropy_ReLU          50              0.001      0.99   \n",
       "8      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "9         Euclidean_ReLU         100              0.001      0.99   \n",
       "10  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "11     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "12     Euclidean_Sigmoid         300              0.001      0.55   \n",
       "13        Euclidean_ReLU         300              0.001      0.99   \n",
       "14  CrossEntropy_Sigmoid         300              0.001      0.55   \n",
       "15     CrossEntropy_ReLU         300              0.001      0.99   \n",
       "16     Euclidean_Sigmoid         600              0.001      0.55   \n",
       "17        Euclidean_ReLU         600              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         600              0.001      0.55   \n",
       "19     CrossEntropy_ReLU         600              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "0        0.00010  202.992379   \n",
       "1        0.00010  202.865551   \n",
       "2        0.00001  220.172044   \n",
       "3        0.00001  213.903324   \n",
       "4        0.00010   84.205458   \n",
       "5        0.00010  103.404958   \n",
       "6        0.00001  106.531004   \n",
       "7        0.00001  110.083097   \n",
       "8        0.00010   86.230360   \n",
       "9        0.00010   86.719688   \n",
       "10       0.00001   87.260598   \n",
       "11       0.00001   73.201181   \n",
       "12       0.00010   60.541882   \n",
       "13       0.00010   88.566508   \n",
       "14       0.00001   79.935867   \n",
       "15       0.00001   89.487252   \n",
       "16       0.00010   89.956403   \n",
       "17       0.00010   90.437684   \n",
       "18       0.00001   79.771831   \n",
       "19       0.00001   80.134106   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "0   [0.23696529033491118, 0.21295889748507354, 0.2...   \n",
       "1   [0.07850324827173032, 0.06918276771896614, 0.0...   \n",
       "2   [0.8776581632144806, 0.5738326347256689, 0.457...   \n",
       "3   [0.12273972706776996, 0.09220356944746999, 0.0...   \n",
       "4   [0.33334949700556904, 0.28014550534958116, 0.2...   \n",
       "5   [0.09226160089623736, 0.07735723972868697, 0.0...   \n",
       "6   [1.832304787036741, 1.461818502020953, 1.20390...   \n",
       "7   [0.17114324509297812, 0.12833095406247166, 0.1...   \n",
       "8   [0.4049494244660077, 0.3307651218876163, 0.296...   \n",
       "9   [0.1452353369806154, 0.10322973457606605, 0.09...   \n",
       "10  [2.0394249122976387, 1.813701121788175, 1.6220...   \n",
       "11  [0.22148675894657038, 0.16435740396122633, 0.1...   \n",
       "12  [0.5203300683697728, 0.4509735832075066, 0.405...   \n",
       "13  [0.2846998772028879, 0.18061703583946317, 0.13...   \n",
       "14  [2.2238404375975236, 2.1061308941983325, 2.020...   \n",
       "15  [0.34793722116382103, 0.24725993974281432, 0.2...   \n",
       "16  [0.6095010316344058, 0.5543381130682878, 0.509...   \n",
       "17  [0.37691816545944806, 0.28450462760429074, 0.2...   \n",
       "18  [2.503238239069959, 2.298720323406733, 2.21107...   \n",
       "19  [0.5761123979013656, 0.3366554806395818, 0.294...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "0   [0.8358000000000001, 0.8702000000000001, 0.879...            0.893160  \n",
       "1   [0.9466, 0.9546, 0.9596000000000001, 0.9642000...            0.967250  \n",
       "2   [0.8642000000000002, 0.8942, 0.905, 0.9134, 0....            0.922080  \n",
       "3   [0.9658000000000001, 0.971, 0.9770000000000001...            0.981260  \n",
       "4   [0.6496, 0.7664, 0.8048000000000001, 0.8263999...            0.844620  \n",
       "5   [0.9314, 0.9450000000000002, 0.948000000000000...            0.958730  \n",
       "6   [0.6298, 0.7616000000000003, 0.8164, 0.8456, 0...            0.868030  \n",
       "7   [0.9540000000000001, 0.9658, 0.97, 0.972399999...            0.975220  \n",
       "8   [0.46959999999999996, 0.6522, 0.73, 0.77139999...            0.810190  \n",
       "9   [0.8859999999999999, 0.9181999999999999, 0.932...            0.942630  \n",
       "10  [0.4534, 0.6118, 0.6969999999999998, 0.7525999...            0.813120  \n",
       "11  [0.9390000000000001, 0.9571999999999999, 0.962...            0.971240  \n",
       "12  [0.19291666666666668, 0.34062499999999996, 0.4...            0.685542  \n",
       "13  [0.68625, 0.8447916666666666, 0.88583333333333...            0.917062  \n",
       "14  [0.171875, 0.365, 0.49583333333333335, 0.57812...            0.695073  \n",
       "15  [0.8966666666666666, 0.9239583333333333, 0.941...            0.957708  \n",
       "16  [0.14666666666666667, 0.19729166666666667, 0.2...            0.540479  \n",
       "17  [0.54375, 0.6804166666666667, 0.75916666666666...            0.874927  \n",
       "18  [0.10645833333333332, 0.15958333333333333, 0.2...            0.517646  \n",
       "19  [0.8456250000000001, 0.900625, 0.913125, 0.927...            0.944479  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result['acc_validate_float'] = exec_result['acc_validate'].map(lambda x: np.average(x))\n",
    "exec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe04233dfd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6), dpi=100)\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['batch_size'], exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'], marker='s', label = 'Euclidean+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['batch_size'], exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'], marker='s', label = 'Euclidean+ReLU')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['batch_size'], exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['batch_size'], exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+ReLU')\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('acc_validate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result.to_csv('./result/result_batch_size.csv',index=False)\n",
    "# # euclidean_sigmoid.learning_rate_SGD, euclidean_relu.learning_rate_SGD, crossEntropy_sigmoid.learning_rate_SGD, crossEntropy_relu.learning_rate_SGD\n",
    "\n",
    "# exec_result=pd.read_csv('./result_learning_rate_SGD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001, 0.0001, 1e-05, 1e-05)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index = exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "euclidean_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'].idxmax()\n",
    "euclidean_relu= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_relu= exec_result.loc[best_index]\n",
    "euclidean_sigmoid.weight_decay, euclidean_relu.weight_decay, crossEntropy_sigmoid.weight_decay, crossEntropy_relu.weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>202.992379</td>\n",
       "      <td>[0.23696529033491118, 0.21295889748507354, 0.2...</td>\n",
       "      <td>[0.8358000000000001, 0.8702000000000001, 0.879...</td>\n",
       "      <td>0.893160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>202.865551</td>\n",
       "      <td>[0.07850324827173032, 0.06918276771896614, 0.0...</td>\n",
       "      <td>[0.9466, 0.9546, 0.9596000000000001, 0.9642000...</td>\n",
       "      <td>0.967250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>220.172044</td>\n",
       "      <td>[0.8776581632144806, 0.5738326347256689, 0.457...</td>\n",
       "      <td>[0.8642000000000002, 0.8942, 0.905, 0.9134, 0....</td>\n",
       "      <td>0.922080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>213.903324</td>\n",
       "      <td>[0.12273972706776996, 0.09220356944746999, 0.0...</td>\n",
       "      <td>[0.9658000000000001, 0.971, 0.9770000000000001...</td>\n",
       "      <td>0.981260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>84.205458</td>\n",
       "      <td>[0.33334949700556904, 0.28014550534958116, 0.2...</td>\n",
       "      <td>[0.6496, 0.7664, 0.8048000000000001, 0.8263999...</td>\n",
       "      <td>0.844620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>103.404958</td>\n",
       "      <td>[0.09226160089623736, 0.07735723972868697, 0.0...</td>\n",
       "      <td>[0.9314, 0.9450000000000002, 0.948000000000000...</td>\n",
       "      <td>0.958730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>106.531004</td>\n",
       "      <td>[1.832304787036741, 1.461818502020953, 1.20390...</td>\n",
       "      <td>[0.6298, 0.7616000000000003, 0.8164, 0.8456, 0...</td>\n",
       "      <td>0.868030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>110.083097</td>\n",
       "      <td>[0.17114324509297812, 0.12833095406247166, 0.1...</td>\n",
       "      <td>[0.9540000000000001, 0.9658, 0.97, 0.972399999...</td>\n",
       "      <td>0.975220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>86.230360</td>\n",
       "      <td>[0.4049494244660077, 0.3307651218876163, 0.296...</td>\n",
       "      <td>[0.46959999999999996, 0.6522, 0.73, 0.77139999...</td>\n",
       "      <td>0.810190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>86.719688</td>\n",
       "      <td>[0.1452353369806154, 0.10322973457606605, 0.09...</td>\n",
       "      <td>[0.8859999999999999, 0.9181999999999999, 0.932...</td>\n",
       "      <td>0.942630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>87.260598</td>\n",
       "      <td>[2.0394249122976387, 1.813701121788175, 1.6220...</td>\n",
       "      <td>[0.4534, 0.6118, 0.6969999999999998, 0.7525999...</td>\n",
       "      <td>0.813120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>73.201181</td>\n",
       "      <td>[0.22148675894657038, 0.16435740396122633, 0.1...</td>\n",
       "      <td>[0.9390000000000001, 0.9571999999999999, 0.962...</td>\n",
       "      <td>0.971240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>60.541882</td>\n",
       "      <td>[0.5203300683697728, 0.4509735832075066, 0.405...</td>\n",
       "      <td>[0.19291666666666668, 0.34062499999999996, 0.4...</td>\n",
       "      <td>0.685542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.566508</td>\n",
       "      <td>[0.2846998772028879, 0.18061703583946317, 0.13...</td>\n",
       "      <td>[0.68625, 0.8447916666666666, 0.88583333333333...</td>\n",
       "      <td>0.917062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>79.935867</td>\n",
       "      <td>[2.2238404375975236, 2.1061308941983325, 2.020...</td>\n",
       "      <td>[0.171875, 0.365, 0.49583333333333335, 0.57812...</td>\n",
       "      <td>0.695073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>89.487252</td>\n",
       "      <td>[0.34793722116382103, 0.24725993974281432, 0.2...</td>\n",
       "      <td>[0.8966666666666666, 0.9239583333333333, 0.941...</td>\n",
       "      <td>0.957708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>89.956403</td>\n",
       "      <td>[0.6095010316344058, 0.5543381130682878, 0.509...</td>\n",
       "      <td>[0.14666666666666667, 0.19729166666666667, 0.2...</td>\n",
       "      <td>0.540479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>90.437684</td>\n",
       "      <td>[0.37691816545944806, 0.28450462760429074, 0.2...</td>\n",
       "      <td>[0.54375, 0.6804166666666667, 0.75916666666666...</td>\n",
       "      <td>0.874927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>79.771831</td>\n",
       "      <td>[2.503238239069959, 2.298720323406733, 2.21107...</td>\n",
       "      <td>[0.10645833333333332, 0.15958333333333333, 0.2...</td>\n",
       "      <td>0.517646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>600</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>80.134106</td>\n",
       "      <td>[0.5761123979013656, 0.3366554806395818, 0.294...</td>\n",
       "      <td>[0.8456250000000001, 0.900625, 0.913125, 0.927...</td>\n",
       "      <td>0.944479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0      Euclidean_Sigmoid          10              0.001      0.55   \n",
       "1         Euclidean_ReLU          10              0.001      0.99   \n",
       "2   CrossEntropy_Sigmoid          10              0.001      0.55   \n",
       "3      CrossEntropy_ReLU          10              0.001      0.99   \n",
       "4      Euclidean_Sigmoid          50              0.001      0.55   \n",
       "5         Euclidean_ReLU          50              0.001      0.99   \n",
       "6   CrossEntropy_Sigmoid          50              0.001      0.55   \n",
       "7      CrossEntropy_ReLU          50              0.001      0.99   \n",
       "8      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "9         Euclidean_ReLU         100              0.001      0.99   \n",
       "10  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "11     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "12     Euclidean_Sigmoid         300              0.001      0.55   \n",
       "13        Euclidean_ReLU         300              0.001      0.99   \n",
       "14  CrossEntropy_Sigmoid         300              0.001      0.55   \n",
       "15     CrossEntropy_ReLU         300              0.001      0.99   \n",
       "16     Euclidean_Sigmoid         600              0.001      0.55   \n",
       "17        Euclidean_ReLU         600              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         600              0.001      0.55   \n",
       "19     CrossEntropy_ReLU         600              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "0        0.00010  202.992379   \n",
       "1        0.00010  202.865551   \n",
       "2        0.00001  220.172044   \n",
       "3        0.00001  213.903324   \n",
       "4        0.00010   84.205458   \n",
       "5        0.00010  103.404958   \n",
       "6        0.00001  106.531004   \n",
       "7        0.00001  110.083097   \n",
       "8        0.00010   86.230360   \n",
       "9        0.00010   86.719688   \n",
       "10       0.00001   87.260598   \n",
       "11       0.00001   73.201181   \n",
       "12       0.00010   60.541882   \n",
       "13       0.00010   88.566508   \n",
       "14       0.00001   79.935867   \n",
       "15       0.00001   89.487252   \n",
       "16       0.00010   89.956403   \n",
       "17       0.00010   90.437684   \n",
       "18       0.00001   79.771831   \n",
       "19       0.00001   80.134106   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "0   [0.23696529033491118, 0.21295889748507354, 0.2...   \n",
       "1   [0.07850324827173032, 0.06918276771896614, 0.0...   \n",
       "2   [0.8776581632144806, 0.5738326347256689, 0.457...   \n",
       "3   [0.12273972706776996, 0.09220356944746999, 0.0...   \n",
       "4   [0.33334949700556904, 0.28014550534958116, 0.2...   \n",
       "5   [0.09226160089623736, 0.07735723972868697, 0.0...   \n",
       "6   [1.832304787036741, 1.461818502020953, 1.20390...   \n",
       "7   [0.17114324509297812, 0.12833095406247166, 0.1...   \n",
       "8   [0.4049494244660077, 0.3307651218876163, 0.296...   \n",
       "9   [0.1452353369806154, 0.10322973457606605, 0.09...   \n",
       "10  [2.0394249122976387, 1.813701121788175, 1.6220...   \n",
       "11  [0.22148675894657038, 0.16435740396122633, 0.1...   \n",
       "12  [0.5203300683697728, 0.4509735832075066, 0.405...   \n",
       "13  [0.2846998772028879, 0.18061703583946317, 0.13...   \n",
       "14  [2.2238404375975236, 2.1061308941983325, 2.020...   \n",
       "15  [0.34793722116382103, 0.24725993974281432, 0.2...   \n",
       "16  [0.6095010316344058, 0.5543381130682878, 0.509...   \n",
       "17  [0.37691816545944806, 0.28450462760429074, 0.2...   \n",
       "18  [2.503238239069959, 2.298720323406733, 2.21107...   \n",
       "19  [0.5761123979013656, 0.3366554806395818, 0.294...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "0   [0.8358000000000001, 0.8702000000000001, 0.879...            0.893160  \n",
       "1   [0.9466, 0.9546, 0.9596000000000001, 0.9642000...            0.967250  \n",
       "2   [0.8642000000000002, 0.8942, 0.905, 0.9134, 0....            0.922080  \n",
       "3   [0.9658000000000001, 0.971, 0.9770000000000001...            0.981260  \n",
       "4   [0.6496, 0.7664, 0.8048000000000001, 0.8263999...            0.844620  \n",
       "5   [0.9314, 0.9450000000000002, 0.948000000000000...            0.958730  \n",
       "6   [0.6298, 0.7616000000000003, 0.8164, 0.8456, 0...            0.868030  \n",
       "7   [0.9540000000000001, 0.9658, 0.97, 0.972399999...            0.975220  \n",
       "8   [0.46959999999999996, 0.6522, 0.73, 0.77139999...            0.810190  \n",
       "9   [0.8859999999999999, 0.9181999999999999, 0.932...            0.942630  \n",
       "10  [0.4534, 0.6118, 0.6969999999999998, 0.7525999...            0.813120  \n",
       "11  [0.9390000000000001, 0.9571999999999999, 0.962...            0.971240  \n",
       "12  [0.19291666666666668, 0.34062499999999996, 0.4...            0.685542  \n",
       "13  [0.68625, 0.8447916666666666, 0.88583333333333...            0.917062  \n",
       "14  [0.171875, 0.365, 0.49583333333333335, 0.57812...            0.695073  \n",
       "15  [0.8966666666666666, 0.9239583333333333, 0.941...            0.957708  \n",
       "16  [0.14666666666666667, 0.19729166666666667, 0.2...            0.540479  \n",
       "17  [0.54375, 0.6804166666666667, 0.75916666666666...            0.874927  \n",
       "18  [0.10645833333333332, 0.15958333333333333, 0.2...            0.517646  \n",
       "19  [0.8456250000000001, 0.900625, 0.913125, 0.927...            0.944479  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result.query(\"weight_decay <= 0.00010\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cf1e4799745e9ccc5e4a5d8e027719ef2e43f9255145d7e4068adaea12ce15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
