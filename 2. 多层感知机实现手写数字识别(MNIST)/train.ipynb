{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、 背景介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 手写数字数据集是机器学习领域中广泛使用的图像分类数据集。它包含 60,000 个训练样本和 10,000 个测试样本。这些数字已进行尺寸规格化,并在固定尺寸的图像中居中。每个样本都是一个 784×1 的矩阵,是从原始的 28×28灰度图像转换而来的。MNIST 中的数字范围是 0 到 9。下面显示了一些示例。 \n",
    "\n",
    "![示例图片](../1.%20Softmax实现手写数字识别(MNIST)/img/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验目的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建自己的多层感知机，实现MNIST 手写数字识别\n",
    "\n",
    "### 本案例要求如下\n",
    "- #### 实现SGD优化器 (`./optimizer.py`)\n",
    "- #### 实现全连接层FCLayer前向和后向计算 (`layers/fc_layer.py`)\n",
    "- #### 实现激活层SigmoidLayer前向和后向计算 (`layers/sigmoid_layer.py`)\n",
    "- #### 实现激活层ReLULayer前向和后向计算 (`layers/relu_layer.py`)\n",
    "- #### 实现损失层EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### 实现损失层SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、评价指标\n",
    "* Accuracy 准确率: 分类正确的样本数除以总样本数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、实验过程\n",
    "见各个训练文件\n",
    "* train.ipynb                       本文件    \n",
    "* mlp_1 momentum.ipynb              训练过程，尝试加入动量  \n",
    "* mlp_2 learning_rate_SGD.ipynb     训练过程，尝试不同的学习率  \n",
    "* mlp_3 weight_decay.ipynb          训练过程，尝试不同的权重衰减率  \n",
    "* mlp_4 batch_size.ipynb            训练过程，尝试不同的正则化系数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1读入MNIST数据集\n",
    "\n",
    "* <font color=\"red\">使用tensorflow keras来加载，也可以使用torchvision.datasets。</font>\n",
    "* <font color=\"red\">下载有问题的也可以直接使用示例《1. Softmax实现手写数字识别(MNIST)》的本地文件加载方式</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# max_epoch = 20\n",
    "# init_std = 0.01\n",
    "\n",
    "# learning_rate_SGD = 0.001\n",
    "# weight_decay = 0.1\n",
    "\n",
    "# disp_freq = 50\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 使用欧式距离损失训练多层感知机(MLP with Euclidean Loss)\n",
    "第一部分将使用欧式距离损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 使用欧式距离损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和Sigmoid激活函数.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 7.0290\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.1590\t Accuracy 0.1263\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.3968\t Accuracy 0.1282\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1254\t Accuracy 0.1396\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9861\t Accuracy 0.1502\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8998\t Accuracy 0.1582\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8397\t Accuracy 0.1683\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7946\t Accuracy 0.1775\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7595\t Accuracy 0.1858\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7304\t Accuracy 0.1940\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7066\t Accuracy 0.2022\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6856\t Average training accuracy 0.2111\n",
      "Epoch [0]\t Average validation loss 0.4619\t Average validation accuracy 0.3268\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4761\t Accuracy 0.2900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4678\t Accuracy 0.2986\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4631\t Accuracy 0.3197\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4601\t Accuracy 0.3319\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4580\t Accuracy 0.3361\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4552\t Accuracy 0.3409\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4519\t Accuracy 0.3482\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4491\t Accuracy 0.3539\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4460\t Accuracy 0.3608\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4426\t Accuracy 0.3691\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4396\t Accuracy 0.3755\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4360\t Average training accuracy 0.3837\n",
      "Epoch [1]\t Average validation loss 0.3903\t Average validation accuracy 0.4962\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3997\t Accuracy 0.4700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3975\t Accuracy 0.4673\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3948\t Accuracy 0.4769\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3939\t Accuracy 0.4846\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3935\t Accuracy 0.4838\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3921\t Accuracy 0.4861\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3903\t Accuracy 0.4892\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3892\t Accuracy 0.4909\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3875\t Accuracy 0.4954\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3857\t Accuracy 0.5014\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3840\t Accuracy 0.5054\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3817\t Average training accuracy 0.5109\n",
      "Epoch [2]\t Average validation loss 0.3502\t Average validation accuracy 0.5968\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3570\t Accuracy 0.6000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3578\t Accuracy 0.5739\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3561\t Accuracy 0.5794\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3562\t Accuracy 0.5785\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3565\t Accuracy 0.5749\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3557\t Accuracy 0.5748\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3546\t Accuracy 0.5758\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3544\t Accuracy 0.5763\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3533\t Accuracy 0.5785\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3523\t Accuracy 0.5825\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3513\t Accuracy 0.5847\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3498\t Average training accuracy 0.5889\n",
      "Epoch [3]\t Average validation loss 0.3256\t Average validation accuracy 0.6542\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3309\t Accuracy 0.6500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3335\t Accuracy 0.6388\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3323\t Accuracy 0.6406\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3330\t Accuracy 0.6362\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3335\t Accuracy 0.6322\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3331\t Accuracy 0.6314\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3324\t Accuracy 0.6309\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3326\t Accuracy 0.6308\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3320\t Accuracy 0.6326\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3314\t Accuracy 0.6352\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3308\t Accuracy 0.6363\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3296\t Average training accuracy 0.6402\n",
      "Epoch [4]\t Average validation loss 0.3099\t Average validation accuracy 0.6994\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.3142\t Accuracy 0.6800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3178\t Accuracy 0.6792\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3170\t Accuracy 0.6782\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3180\t Accuracy 0.6716\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3187\t Accuracy 0.6686\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3184\t Accuracy 0.6672\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3180\t Accuracy 0.6665\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3185\t Accuracy 0.6667\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3181\t Accuracy 0.6684\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3178\t Accuracy 0.6703\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3174\t Accuracy 0.6712\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3165\t Average training accuracy 0.6746\n",
      "Epoch [5]\t Average validation loss 0.2997\t Average validation accuracy 0.7356\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.3032\t Accuracy 0.7200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3076\t Accuracy 0.6996\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3070\t Accuracy 0.7005\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3082\t Accuracy 0.6956\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3090\t Accuracy 0.6929\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3088\t Accuracy 0.6921\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3085\t Accuracy 0.6914\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3092\t Accuracy 0.6912\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3089\t Accuracy 0.6930\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3089\t Accuracy 0.6947\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3086\t Accuracy 0.6952\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3080\t Average training accuracy 0.6982\n",
      "Epoch [6]\t Average validation loss 0.2931\t Average validation accuracy 0.7514\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2960\t Accuracy 0.7500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.3010\t Accuracy 0.7178\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.3005\t Accuracy 0.7187\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3019\t Accuracy 0.7150\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3027\t Accuracy 0.7128\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3026\t Accuracy 0.7116\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3024\t Accuracy 0.7106\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3032\t Accuracy 0.7103\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3030\t Accuracy 0.7119\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3031\t Accuracy 0.7130\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3030\t Accuracy 0.7136\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3024\t Average training accuracy 0.7159\n",
      "Epoch [7]\t Average validation loss 0.2891\t Average validation accuracy 0.7654\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2913\t Accuracy 0.7500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2969\t Accuracy 0.7327\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2965\t Accuracy 0.7329\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2980\t Accuracy 0.7293\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2988\t Accuracy 0.7272\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2987\t Accuracy 0.7257\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2986\t Accuracy 0.7243\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2995\t Accuracy 0.7234\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2994\t Accuracy 0.7248\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2996\t Accuracy 0.7257\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2995\t Accuracy 0.7264\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2991\t Average training accuracy 0.7286\n",
      "Epoch [8]\t Average validation loss 0.2869\t Average validation accuracy 0.7758\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2886\t Accuracy 0.7600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2945\t Accuracy 0.7422\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2943\t Accuracy 0.7422\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2958\t Accuracy 0.7387\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2966\t Accuracy 0.7364\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2966\t Accuracy 0.7349\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2965\t Accuracy 0.7336\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2975\t Accuracy 0.7328\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2974\t Accuracy 0.7342\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2977\t Accuracy 0.7350\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2977\t Accuracy 0.7358\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2973\t Average training accuracy 0.7380\n",
      "Epoch [9]\t Average validation loss 0.2861\t Average validation accuracy 0.7854\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2873\t Accuracy 0.7800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2936\t Accuracy 0.7492\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2934\t Accuracy 0.7500\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2949\t Accuracy 0.7452\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2957\t Accuracy 0.7436\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2957\t Accuracy 0.7421\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2957\t Accuracy 0.7414\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2967\t Accuracy 0.7407\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2967\t Accuracy 0.7421\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2970\t Accuracy 0.7427\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2971\t Accuracy 0.7433\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2968\t Average training accuracy 0.7451\n",
      "Epoch [10]\t Average validation loss 0.2863\t Average validation accuracy 0.7902\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2871\t Accuracy 0.7900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2936\t Accuracy 0.7547\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2935\t Accuracy 0.7546\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2950\t Accuracy 0.7502\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2959\t Accuracy 0.7491\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2959\t Accuracy 0.7481\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2959\t Accuracy 0.7474\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2969\t Accuracy 0.7467\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2969\t Accuracy 0.7480\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2973\t Accuracy 0.7484\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2974\t Accuracy 0.7488\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2971\t Average training accuracy 0.7505\n",
      "Epoch [11]\t Average validation loss 0.2873\t Average validation accuracy 0.7964\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2877\t Accuracy 0.7900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2945\t Accuracy 0.7586\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2944\t Accuracy 0.7587\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2959\t Accuracy 0.7542\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2967\t Accuracy 0.7535\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2967\t Accuracy 0.7529\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2968\t Accuracy 0.7525\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2978\t Accuracy 0.7518\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2979\t Accuracy 0.7527\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2983\t Accuracy 0.7531\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2984\t Accuracy 0.7534\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2982\t Average training accuracy 0.7548\n",
      "Epoch [12]\t Average validation loss 0.2890\t Average validation accuracy 0.8000\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2890\t Accuracy 0.7900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2959\t Accuracy 0.7602\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2959\t Accuracy 0.7601\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2974\t Accuracy 0.7549\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2982\t Accuracy 0.7547\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2982\t Accuracy 0.7547\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2983\t Accuracy 0.7543\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2993\t Accuracy 0.7538\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2994\t Accuracy 0.7549\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2998\t Accuracy 0.7552\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2999\t Accuracy 0.7556\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2998\t Average training accuracy 0.7572\n",
      "Epoch [13]\t Average validation loss 0.2911\t Average validation accuracy 0.7998\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2908\t Accuracy 0.7900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2979\t Accuracy 0.7633\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2978\t Accuracy 0.7638\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2993\t Accuracy 0.7580\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.3001\t Accuracy 0.7578\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.3002\t Accuracy 0.7576\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.3002\t Accuracy 0.7572\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.3013\t Accuracy 0.7565\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.3014\t Accuracy 0.7575\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.3018\t Accuracy 0.7576\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.3020\t Accuracy 0.7579\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3018\t Average training accuracy 0.7594\n",
      "Epoch [14]\t Average validation loss 0.2936\t Average validation accuracy 0.8008\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2930\t Accuracy 0.7800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.3002\t Accuracy 0.7651\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.3002\t Accuracy 0.7658\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.3017\t Accuracy 0.7596\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.3024\t Accuracy 0.7590\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.3025\t Accuracy 0.7589\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.3025\t Accuracy 0.7585\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3036\t Accuracy 0.7579\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3037\t Accuracy 0.7590\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3041\t Accuracy 0.7590\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3043\t Accuracy 0.7593\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3042\t Average training accuracy 0.7607\n",
      "Epoch [15]\t Average validation loss 0.2964\t Average validation accuracy 0.8012\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2956\t Accuracy 0.7800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.3028\t Accuracy 0.7661\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.3028\t Accuracy 0.7655\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3042\t Accuracy 0.7600\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3050\t Accuracy 0.7595\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3051\t Accuracy 0.7597\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3051\t Accuracy 0.7593\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3062\t Accuracy 0.7586\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3063\t Accuracy 0.7596\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3067\t Accuracy 0.7596\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3069\t Accuracy 0.7599\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3069\t Average training accuracy 0.7613\n",
      "Epoch [16]\t Average validation loss 0.2995\t Average validation accuracy 0.8024\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2984\t Accuracy 0.7800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3056\t Accuracy 0.7651\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3056\t Accuracy 0.7656\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3070\t Accuracy 0.7593\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3078\t Accuracy 0.7592\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3079\t Accuracy 0.7594\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3079\t Accuracy 0.7594\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3090\t Accuracy 0.7590\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3091\t Accuracy 0.7600\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3095\t Accuracy 0.7598\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3097\t Accuracy 0.7601\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3097\t Average training accuracy 0.7616\n",
      "Epoch [17]\t Average validation loss 0.3027\t Average validation accuracy 0.8038\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.3014\t Accuracy 0.7800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3086\t Accuracy 0.7661\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3086\t Accuracy 0.7660\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3100\t Accuracy 0.7599\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3108\t Accuracy 0.7599\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3108\t Accuracy 0.7601\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3109\t Accuracy 0.7598\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3119\t Accuracy 0.7592\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3120\t Accuracy 0.7600\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3125\t Accuracy 0.7600\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3127\t Accuracy 0.7603\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3127\t Average training accuracy 0.7617\n",
      "Epoch [18]\t Average validation loss 0.3060\t Average validation accuracy 0.8030\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.3045\t Accuracy 0.7800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3117\t Accuracy 0.7651\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3117\t Accuracy 0.7652\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3131\t Accuracy 0.7593\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3138\t Accuracy 0.7594\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3139\t Accuracy 0.7597\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3140\t Accuracy 0.7593\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3150\t Accuracy 0.7583\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3151\t Accuracy 0.7593\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3155\t Accuracy 0.7592\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3157\t Accuracy 0.7596\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3157\t Average training accuracy 0.7609\n",
      "Epoch [19]\t Average validation loss 0.3094\t Average validation accuracy 0.8020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss1, sigmoid_acc1 = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7774.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 使用欧式距离损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和ReLU激活函数.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# 使用FCLayer和ReLULayer构建多层感知机\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.6447\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.0115\t Accuracy 0.1102\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.7406\t Accuracy 0.1257\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.5791\t Accuracy 0.1444\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.4622\t Accuracy 0.1607\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.3717\t Accuracy 0.1767\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.2954\t Accuracy 0.1954\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.2334\t Accuracy 0.2088\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.1809\t Accuracy 0.2216\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.1348\t Accuracy 0.2339\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.0945\t Accuracy 0.2477\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0583\t Average training accuracy 0.2600\n",
      "Epoch [0]\t Average validation loss 0.6544\t Average validation accuracy 0.4140\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.6135\t Accuracy 0.4100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.6514\t Accuracy 0.4163\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.6398\t Accuracy 0.4255\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.6309\t Accuracy 0.4303\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.6206\t Accuracy 0.4346\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.6105\t Accuracy 0.4420\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.5998\t Accuracy 0.4492\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.5908\t Accuracy 0.4530\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.5819\t Accuracy 0.4588\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.5736\t Accuracy 0.4639\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.5660\t Accuracy 0.4696\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5583\t Average training accuracy 0.4757\n",
      "Epoch [1]\t Average validation loss 0.4566\t Average validation accuracy 0.5680\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.4336\t Accuracy 0.5600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4612\t Accuracy 0.5588\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4571\t Accuracy 0.5636\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4554\t Accuracy 0.5641\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4518\t Accuracy 0.5631\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4474\t Accuracy 0.5672\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4430\t Accuracy 0.5706\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4394\t Accuracy 0.5721\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4353\t Accuracy 0.5758\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4316\t Accuracy 0.5781\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4282\t Accuracy 0.5818\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4245\t Average training accuracy 0.5856\n",
      "Epoch [2]\t Average validation loss 0.3672\t Average validation accuracy 0.6532\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3526\t Accuracy 0.6100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3741\t Accuracy 0.6410\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3722\t Accuracy 0.6418\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3726\t Accuracy 0.6372\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3711\t Accuracy 0.6390\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3686\t Accuracy 0.6420\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3662\t Accuracy 0.6442\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3645\t Accuracy 0.6446\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3621\t Accuracy 0.6474\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3601\t Accuracy 0.6489\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3582\t Accuracy 0.6510\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3560\t Average training accuracy 0.6534\n",
      "Epoch [3]\t Average validation loss 0.3157\t Average validation accuracy 0.7168\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3050\t Accuracy 0.6600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3236\t Accuracy 0.6941\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3227\t Accuracy 0.6941\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3241\t Accuracy 0.6897\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3234\t Accuracy 0.6899\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3218\t Accuracy 0.6922\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3203\t Accuracy 0.6931\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3196\t Accuracy 0.6924\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3181\t Accuracy 0.6945\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3168\t Accuracy 0.6964\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3157\t Accuracy 0.6983\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3142\t Average training accuracy 0.6999\n",
      "Epoch [4]\t Average validation loss 0.2825\t Average validation accuracy 0.7548\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2735\t Accuracy 0.7000\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2909\t Accuracy 0.7278\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2906\t Accuracy 0.7309\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2925\t Accuracy 0.7267\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2922\t Accuracy 0.7263\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2911\t Accuracy 0.7292\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2901\t Accuracy 0.7293\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2900\t Accuracy 0.7283\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2890\t Accuracy 0.7296\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2882\t Accuracy 0.7310\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2875\t Accuracy 0.7321\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2863\t Average training accuracy 0.7334\n",
      "Epoch [5]\t Average validation loss 0.2596\t Average validation accuracy 0.7802\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2521\t Accuracy 0.7200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2682\t Accuracy 0.7543\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2684\t Accuracy 0.7573\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2705\t Accuracy 0.7521\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2706\t Accuracy 0.7512\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2697\t Accuracy 0.7535\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2691\t Accuracy 0.7537\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2693\t Accuracy 0.7526\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2686\t Accuracy 0.7532\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2681\t Accuracy 0.7546\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2677\t Accuracy 0.7557\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2668\t Average training accuracy 0.7565\n",
      "Epoch [6]\t Average validation loss 0.2433\t Average validation accuracy 0.8032\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2369\t Accuracy 0.8000\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2520\t Accuracy 0.7763\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2525\t Accuracy 0.7794\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2548\t Accuracy 0.7724\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2550\t Accuracy 0.7725\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2543\t Accuracy 0.7733\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2539\t Accuracy 0.7738\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2544\t Accuracy 0.7723\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2539\t Accuracy 0.7727\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2536\t Accuracy 0.7742\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2533\t Accuracy 0.7750\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2526\t Average training accuracy 0.7755\n",
      "Epoch [7]\t Average validation loss 0.2312\t Average validation accuracy 0.8230\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2258\t Accuracy 0.8100\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2401\t Accuracy 0.7920\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2408\t Accuracy 0.7950\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2431\t Accuracy 0.7881\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2435\t Accuracy 0.7883\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2429\t Accuracy 0.7888\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2427\t Accuracy 0.7889\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2433\t Accuracy 0.7874\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2430\t Accuracy 0.7877\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2428\t Accuracy 0.7892\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2426\t Accuracy 0.7899\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2421\t Average training accuracy 0.7905\n",
      "Epoch [8]\t Average validation loss 0.2222\t Average validation accuracy 0.8388\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2177\t Accuracy 0.8200\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2311\t Accuracy 0.8045\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2320\t Accuracy 0.8057\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2344\t Accuracy 0.7999\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2348\t Accuracy 0.8002\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2344\t Accuracy 0.8009\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2342\t Accuracy 0.8007\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2350\t Accuracy 0.7994\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2348\t Accuracy 0.7997\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2348\t Accuracy 0.8011\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2346\t Accuracy 0.8017\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2341\t Average training accuracy 0.8023\n",
      "Epoch [9]\t Average validation loss 0.2153\t Average validation accuracy 0.8500\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2116\t Accuracy 0.8200\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2245\t Accuracy 0.8155\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2254\t Accuracy 0.8159\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2278\t Accuracy 0.8093\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2283\t Accuracy 0.8102\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2279\t Accuracy 0.8104\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2279\t Accuracy 0.8101\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2288\t Accuracy 0.8089\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2286\t Accuracy 0.8091\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2286\t Accuracy 0.8101\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2286\t Accuracy 0.8109\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2282\t Average training accuracy 0.8114\n",
      "Epoch [10]\t Average validation loss 0.2101\t Average validation accuracy 0.8562\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2071\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2195\t Accuracy 0.8263\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2204\t Accuracy 0.8258\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2229\t Accuracy 0.8184\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2234\t Accuracy 0.8192\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2231\t Accuracy 0.8191\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2231\t Accuracy 0.8188\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2240\t Accuracy 0.8173\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2239\t Accuracy 0.8174\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2240\t Accuracy 0.8183\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2241\t Accuracy 0.8190\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2237\t Average training accuracy 0.8193\n",
      "Epoch [11]\t Average validation loss 0.2062\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2037\t Accuracy 0.8400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2157\t Accuracy 0.8318\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2167\t Accuracy 0.8321\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2192\t Accuracy 0.8249\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2197\t Accuracy 0.8253\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2194\t Accuracy 0.8251\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2195\t Accuracy 0.8250\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2205\t Accuracy 0.8236\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2204\t Accuracy 0.8241\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2206\t Accuracy 0.8245\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2206\t Accuracy 0.8250\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2203\t Average training accuracy 0.8252\n",
      "Epoch [12]\t Average validation loss 0.2033\t Average validation accuracy 0.8680\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2011\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2130\t Accuracy 0.8371\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2140\t Accuracy 0.8365\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2165\t Accuracy 0.8293\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2170\t Accuracy 0.8301\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2168\t Accuracy 0.8296\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2168\t Accuracy 0.8297\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2179\t Accuracy 0.8284\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2179\t Accuracy 0.8290\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2181\t Accuracy 0.8294\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2181\t Accuracy 0.8299\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2178\t Average training accuracy 0.8299\n",
      "Epoch [13]\t Average validation loss 0.2012\t Average validation accuracy 0.8718\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1992\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2110\t Accuracy 0.8433\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2120\t Accuracy 0.8412\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2145\t Accuracy 0.8341\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2150\t Accuracy 0.8341\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2148\t Accuracy 0.8335\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2149\t Accuracy 0.8336\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2160\t Accuracy 0.8323\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2160\t Accuracy 0.8327\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2163\t Accuracy 0.8329\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2164\t Accuracy 0.8332\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2161\t Average training accuracy 0.8331\n",
      "Epoch [14]\t Average validation loss 0.1997\t Average validation accuracy 0.8726\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1979\t Accuracy 0.8700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2096\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2106\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2131\t Accuracy 0.8368\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2137\t Accuracy 0.8374\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2135\t Accuracy 0.8369\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2136\t Accuracy 0.8365\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2148\t Accuracy 0.8352\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2148\t Accuracy 0.8355\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2150\t Accuracy 0.8358\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2152\t Accuracy 0.8359\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2149\t Average training accuracy 0.8358\n",
      "Epoch [15]\t Average validation loss 0.1989\t Average validation accuracy 0.8756\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1970\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2087\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2098\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2123\t Accuracy 0.8383\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2129\t Accuracy 0.8385\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2127\t Accuracy 0.8380\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2128\t Accuracy 0.8380\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2140\t Accuracy 0.8368\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2140\t Accuracy 0.8369\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2143\t Accuracy 0.8371\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2145\t Accuracy 0.8372\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2142\t Average training accuracy 0.8371\n",
      "Epoch [16]\t Average validation loss 0.1984\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1965\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2083\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2093\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2119\t Accuracy 0.8405\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2124\t Accuracy 0.8404\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2123\t Accuracy 0.8398\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2124\t Accuracy 0.8396\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2136\t Accuracy 0.8386\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2137\t Accuracy 0.8389\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2140\t Accuracy 0.8390\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2141\t Accuracy 0.8390\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2139\t Average training accuracy 0.8387\n",
      "Epoch [17]\t Average validation loss 0.1983\t Average validation accuracy 0.8778\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1963\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2082\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2092\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2118\t Accuracy 0.8413\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2124\t Accuracy 0.8408\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2122\t Accuracy 0.8404\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2123\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2136\t Accuracy 0.8393\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2136\t Accuracy 0.8395\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2139\t Accuracy 0.8399\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2141\t Accuracy 0.8398\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2139\t Average training accuracy 0.8395\n",
      "Epoch [18]\t Average validation loss 0.1984\t Average validation accuracy 0.8780\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1964\t Accuracy 0.8700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2083\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2094\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2120\t Accuracy 0.8417\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2125\t Accuracy 0.8413\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2124\t Accuracy 0.8410\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2125\t Accuracy 0.8406\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2138\t Accuracy 0.8398\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2138\t Accuracy 0.8403\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2141\t Accuracy 0.8406\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2143\t Accuracy 0.8406\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2141\t Average training accuracy 0.8403\n",
      "Epoch [19]\t Average validation loss 0.1988\t Average validation accuracy 0.8810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss1, relu_acc1 = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8565.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8565"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGsCAYAAAA7XWY9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUb0lEQVR4nO3dd3hUZd7G8e9kJp1USCghSECkCUoJIiqCYAFF0bUgrgX1FfvacMVdlXVdZNVF1rWtikTsvS9WBGyIQQFDQFoCIaEkkMwEkkySmfP+MSQkQJKZZEoyuT/XNdfMmTnlN+Bxbp7nOc8xGYZhICIiIhKkQgJdgIiIiIgvKeyIiIhIUFPYERERkaCmsCMiIiJBTWFHREREgprCjoiIiAQ1hR0REREJapZAF+APTqeTgoICYmJiMJlMgS5HRERE3GAYBqWlpXTr1o2QkOa3z7SLsFNQUEBqamqgyxAREZFmyMvLo3v37s3evl2EnZiYGMD1hxUbGxvgakRERMQdNpuN1NTU2t/x5moXYaem6yo2NlZhR0REpI1p6RAUDVAWERGRoKawIyIiIkFNYUdERESCWrsYsyMiItIQh8NBVVVVoMtol0JDQzGbzT4/jsKOiIi0S4ZhsHPnTkpKSgJdSrsWHx9Ply5dfDoPnsKOiIi0SzVBJzk5maioKE0662eGYVBWVsbu3bsB6Nq1q8+OpbAjIiLtjsPhqA06HTt2DHQ57VZkZCQAu3fvJjk52WddWhqgLCIi7U7NGJ2oqKgAVyI1fwe+HDelsCMiIu2Wuq4Czx9/Bwo7IiIiEtQ0ZkdERKQZ8kvKKd5f2eDnCdFhpMRH+rEiaYjCjoiIiIfyS8o57bEl2KudDa4Tbglh8V1jvB54fvrpJ2688UY2bNjACSecwEsvvURKSgoAS5Ys4aqrriI3N9erx2xIRkYGGRkZLFmypEXr+Jq6sURERDxUvL+y0aADYK92Ntry0xxlZWWcd9553HzzzWRnZxMTE8PNN99c+/nJJ5/MmjVrvHrMxkydOpVPPvnEb8drLrXsiIiI4Jr3pbzK4da6FR6sV1ZZ3eR6kaFmtwbqrlu3juLiYqZNmwbAAw88wIMPPlj7ucViITY21q3avCEsLIywsDC/Ha+5FHZERESA8ioHA+7/3Kv7vPDZH91aL/vBM4kKa/onOTU1FZPJxKxZs/jLX/7C8ccfz3vvvVf7eUPdWIsXL+aaa66hsrKSyy67jNdff52nnnqK8847j1tvvZXXX3+dO+64g48//pht27bxww8/kJqayjvvvMNf/vIX9uzZw5QpU3jssceIiIio3W9DXVR///vfeeKJJ4iLi2PixIlu/Rn4krqxRERE2ojk5GReeeUV5s2bR58+fVi4cGGT2xiGweWXX87f//53Xn31VZ5++mkWL17M2LFjAVfrzDXXXMP999/PnDlziIiIYNmyZWRmZnLllVfyz3/+k++++47MzEzuueeeJo/30Ucf8fjjj/Puu++ycOFCXn311RZ/75ZSy46IiAiurqTsB890a93sAptbrTbvXH8iA7o13a0UGer+zMEXXnghp59+Oo8//jjTp09n1apVzJ07t8H1CwsLKSgo4OKLLyYsLIyYmBiKioro06cPANdccw3Lly9n6NChnHLKKaSkpFBVVcXzzz/PZZddxuTJkwGYO3cu48eP5/HHH2+0y+3999/nsssuY/To0QBce+21/PTTT25/P19Qy46IiAiuye2iwixuPSLcDCcRoWa39ufuxHoFBQVs3ryZuLg4Zs2axaJFi3j88cfZunVrg9t07NiR+Ph4fvzxRzZv3ozVauXoo48+WOOBbqm63VMAeXl59OrVq3a5V69elJeXU1RU1GiNO3bsIDU1td52gaawIyIi0ka8+eabXHvttbXLo0ePJjQ0FKvV2uA2TqeTYcOGMXHiRAYMGMA//vEPkpKSmjxWjx492LJlS+3y5s2biYqKolOnTo1ul5ycTEFBQe3ytm3bmjyWr6kbS0RExEMJ0WGEW0KanGcnIdq7VyqNHz+ee+65h9dff53Ro0fz3//+l65du9KvX78Gt/n222/Zu3cvmZmZJCYm0rlzZ7eOde2113LqqacyceJE+vfvz5133sl1113XZCvUeeedx/XXX8/FF1+M2Wzm+eefZ8CAAR59T29T2BEREfFQSnwki+8a4/cZlAcNGsSCBQt44IEHKCgoYMiQIXz44YeNXv49YsQICgsLOfnkkykuLqZDhw7cdddd3H///Y0ea/jw4bz00kv8+c9/Zs+ePVxyySU8/PDDTdZ4wQUXsGbNGs477zw6duzIeeedx8aNGz3+rt5kMgzDCGgFfmCz2YiLi8Nqtfp1/gEREWmdKioqyMnJIS0t7bCxKsHmvvvuY/v27fzjH/8gLCyML7/8kptvvpk9e/YEujSg8b8Lb/1+q2VHREQkiE2ePJmbbrqJY445hurqao455hieffbZQJflVwo7IiIiQWzYsGEsX7480GUElK7GEhERkaCmsCMiIiJBTWFHREREgprCjoiIiAQ1hR0REREJaroaS0REpDlK8qCskblqojpCfGrDn/vZkiVLuOqqq8jNzQ10KX6nsCMiIuKpkjx4chhU2xtexxION69sVYGnvVI3loiIiKfK9jQedMD1eWMtP+I3CjsiIiIAhgGV+917VJe7t8/qcvf258Gdm6666ipmzZrFK6+8Qt++fXnyyScB+PnnnznhhBOIi4vjggsuaPRO6DUyMjIYM2ZM7XJubm6TN/psiwLSjZWVlcW0adPYtGkT1157LY888kijf7iGYfDoo4/y/PPPU1xczCWXXMIjjzxCdHS0H6sWEZGgVlUGs7t5d58vnuXeevcWQJj7v2mff/45X3zxBXPnzuW4446jpKSECRMmcOutt/L2229z3XXXceedd/LCCy80s/Dg4veWHbvdzqRJkxg2bBiZmZlkZ2eTkZHR6Dbz58/niSee4NVXX+X7779nxYoVXH/99f4pWEREpJXZsmULixYt4uyzz6Z79+588sknhIaGct9999GjRw9uv/12Pvroo0CX2Wr4vWVn0aJFWK1W5s6dS1RUFLNnz+amm25i2rRpDW6zcOFCZsyYwYgRIwD429/+xpQpU/xVsoiItAehUa4WFnfsXONeq83Vn0GXwe4d2wNXXHEFcXFxtcv5+fkUFhaSkJAAgNPppLS0lIqKCo/u6l5WVuZRHW2F38PO6tWrGTlyJFFRrr/YwYMHk52d3eg2RUVF9OjRo3bZbDZjNpsbXN9ut2O3Hxw4ZrPZWli1iIgEPZPJ/a4kS6T763nQPeWuQ4dxdO/eneHDh/PGG28AruEfVquV0NDQRvdjMplwOBy1y5mZmV6vtTXwezeWzWYjLS2tdtlkMmE2mykuLm5wm+OPP54PPvigdnnBggWcccYZDa7/8MMPExcXV/tITdVlfyIiErzOPvtstm7dyooVKzCbzbzxxhucddZZGE0MfO7evTtr166luLiYXbt28dhjj/mpYv/ye8uOxWIhPDy83nsRERGUlZXVNr8davbs2UyYMIFTTjkFm83GmjVrWLZsWYPHmDlzJnfccUftss1mU+ARERHvieromkenqXl2ojr6pZz4+Hg++ugjbr75ZrKyshg4cCAfffQRFkvjP/Njx47lrLPOYtCgQXTr1o2HHnqI8847zy81+5PJaCr2edk///lPsrKyePnll2vfi4+PZ+PGjSQlJTW4nWEYrF+/nrvvvhu73c4XX3zh9jFtNhtxcXFYrVZiY2NbVL+IiLR9FRUV5OTkkJaW5tGYlnra2AzKrVVjfxfe+v32e8tOenp6vUvhcnNzsdvtJCYmNrqdyWQiNjaWr776iu+//97XZYqIiDQuPlVhpo3w+5id0aNHY7VaWbhwIQBz5sxh/PjxmM1mbDYbVVVVDW770EMPcdFFFzF06FB/lSsiIiL+VJIHBatcjx1rvLLLgIzZee6555g6dSozZszA4XCwdOlSwHVl1rx585g8efJh223atInXXnuNrKwsP1csIiIifnHoPcfs3hlpE5AZlCdPnszGjRvJzMxk1KhRtWN1GrsT69FHH+3W1NciIiLu8uuw1epKcFY3/HmIBSxhrfsYPth/vb8Dd+451gwBu+t5SkoKKSkpgTq8iIi0YzXzz5SVlREZGemfkLA7G2gsXJkgeUDzj+PrY/ho/zUTGTY1J1BLBCzsiIiIuMUHVz2ZzWbi4+PZvXs3OKqJKi/AZGriRzyxd/ODSGU5VDubWMmAsv0Q1tR6ATpGc/dvGGA4welwPRuuZ8PhoKyinN2FRcSXbsC86Hko2ep5XW5Q2BERkebz9eXXh47hOBJLONy80uPjdOnSBYDdu3bAvt1Nb2ALAXMzw46jEkoL3TiGueljGAZgHP7sqGr876LGnkpXS9WhLTRGgwsuTgfY3bgjwc4DQ04M54GHceT9ATgqid+6iC4bX2t4HS9Q2BERCWa+DCM+DCK13BnDUW13refhMUwmE127diW5uoCqz+5w3S6iMSNvgthuB1sonA7gwPMhrRYHlw889u2GdR82XVTSAAgxg8Pu6jZyHHhU2w+89v54Fr8xWVy3zgjvAOExEBpNqNmEOSoSTpgOEXFgL4XlT3v90Ao7IiLBytdhxIdBxGP7dsLOLFfLQ4XtwLPV9aj3Xt3PXK/Nlfto+G6LdXx1j2+/A8C+PA9WNoElwvV3aDkwGd++nU1v1n2EK1iYTK59HPGZw9+vsMKWb5re/7gHoOtgCI+DiFjXscJjITSy6UBZsEphR0REPODLMGIYUFXu3rq/vAS//8/VzeKsPtASUl3ncehynffKG75vYj2vXeJZ/c3R5XiIjAWT2dX6UvsccsjyEd4vL4a17zV9jHEPQFLfgwGmbpg5dNkcWj88FKyC505t+hgTH4Vux3v+/QtWwXNuhJ3epzVv/z6ksCMiEiit5XYDhgFle12P8rrPew55r7j+e45K9/af+aJv6wfXGJTIBFcLQkTsweeIuIMtDPU+q/OeNQ8WunE/qHP/3fwf8YJV7oWdVhgU/Mqde441g8KOiEgg+LqLqaocbPnurfv8WHw5OJT+k6BDF1cgCTEfeK77OPS9A8vmULDmwzcPNX2Ma79ufkiwlzZvO/G++FTXf/M1/wgo3QdzTmnxbhV2REQCobldTI5q2LcLSndC6Y46jwPLtgPLFSUeFHMg6ITFQFSC61/XkYkQlXjgueOB1wn137NthxfPanr3p9zVshYRd8JOa+ePu6T7+hj+utN73XuO2dy4+ssNCjsiIkfSWrqYvn3MFXBKC1yBZt9u3G6FMYe7d/XOH9+Hnid7PoeMO5c5twX++BE/tMWioTpa8t+Ur4/hj+/gIwo7IiKH8lUXU3mJa9K04lzY+qN726z7+PD3TGaI6QIxXQ8+x3ats9zN9VycA8+NafoYUYktv02BrwRLEKk5jq+DgK+P0Ubv9K6wIyJyqOZ2MVXbXUGpOBdKcqH4QLAp2ep67VHX0gFDr4RuQw4GmdhuENUJQkKa3ra4ict8W0pBRNoIhR0RaXtaSxfTygz4qcIVZEq2gq2AJruYojpBQk/XlUCbFzd9jOFXN3+8i6/DiIKItBEKOyLStvjyKian8+DkdO5YueDw90KjXGEm/ihIOKrO654Q38M1eyy4Bt66E3Zawl/jRBREpJVT2BGRtqUlE+U5ql2XY1vzoGSbKzhZt9V5vR2cVe7X0ncipAyrH2iiOzU9S6w/KYyIKOyISJBa9wms/8QVYkq2uQKOrcB176LGmMwQneTetPun/rn1djGJSC2FHREJTt8+euT3zWEQ1x3iUl3dSvE96rxOdV3JtCvLvWn3W6INX8Yr0tYo7IhI61deDNtXwvafYfPX7m3TfQR0OfbwUNOhs3tXMvmDuphE/EJhR0S8q6VXSjkdUPg7bF8BeT+7Ak7R757X0dybHdbUqC4mkaChsCMi3tOcK6XK9kL+Sshb4Qo4+b+A/QhTxCekQeoI13wz38/zSfm11MUkElQUdkTEe9y9Umrli1C6yxVw9mw8fJ3QaEgZCt3TXQGne7rrKidwXbLt67AD6mISCSIKOyLif9/Orb+c2PtgqOmeDskDwNzA/57UxSQiHlLYERH/6zYUep92MOBEJbq/rbqYRMRDCjsi0nJ7NrvmtFnzlnvrn/N48wcPg7qYRMQjCjsi4jmnEwp+dQWc3/8HhesDXZGISIMUdkTEPdWVkPstrP/UFXBKdxz8LMQCPU+Brsf5Z/CwiIgHFHZE2htP5sGpsMGmr1wBZ+MX9S8JD+sAfU6HfufA0eMhMt5/V0qJiHhAYUekPXFnHhxzGIy5B7b+ADnLwFF58LPoZOg30RVw0ka7rnqqS1dKiUgrpLAj0p64Mw+OoxK+fvDgcmJv6H+OK+CkDG/8Vgu6UkpEWiGFHRE5XFJ/GHyRK+B0OgZMJve31ZVSItLKKOyItCfV5e6td/6zLbs0XESkFVHYEQl2huG6TPyXhbD6zUBXIyLidwo7IsGqvBjWvO0KObt+C3Q1IiIBo7AjEkwMA7Z+7wo42R9CdYXrfXM4DDgXjhoFn9we2BpFRPxMYUckGJTugtWvwS8vw97NB99PHgBDr4TBF7vuP1WwKmAliogEisKOSGviyYR/Todrwr9fFsLvi8BwuN4P6wDH/sEVclKG1r+SSvPgiAS1/JJyivdXNvh5QnQYKfGRrXb/vqKwI9JauDPhnyUcLv8INn8Nv74CpQUHP+s+AoZeAQPPh/AOR95e8+BIG+SPH9hgOEZ+STmnPbYEe7WzwXXCLSEsvmtMs47j6/3XPU7Nn9O+UlsTa7tHYUektXBnwr9qOyw48+ByZAIcdykMuRw6D3DvOJoHp11p6//S98cPbLAco3h/ZaP7B7BXOyneX9msY/h6/3D4n5PTXtas/RwqIGEnKyuLadOmsWnTJq699loeeeQRTE1MWvboo4/y2GOPUV5ezumnn85zzz1Hx45qapd2qtcYVytOv3MOv2WDyAHB8C99f/zABssxgoE7f07N4fewY7fbmTRpEmeeeSZvvPEGt956KxkZGUybNq3BbZYtW8ZLL73EsmXLMJvN/OlPf+LOO+8kIyPDf4WLtBZTXnfdn0raPF+3igTDv/RbK4fToMrhpNLhpKq65tlwPdd5VFa71tu0u9St/T63bAuxkRYqq52uh8P1bD/wOPT9uq8rqhxuHWPSf74j1ByCOcSEJcSE2XzgOcSEJaTO+wceNeva3dz/U99sonNsBJYQExZzCKFm134tZlPt61Cz6zPLgf1bDrxfUOKdlpxD+T3sLFq0CKvVyty5c4mKimL27NncdNNNjYadFStWMHHiRPr27QvApZdeytNPP+2vkkX8o+Yy8abEdvNtHeIX/hr/4I6Nu/bVvjaZwITJ9Vz3NTVj3Q8u55e4NyP3uh1W9tmrqXYYVDmdVDsMqh1Oqpyu57rvVzmcVDsNqqpdn+9w8xj/XLSe2MhQnIZx4AHGgef6ywZOp+s948Bn++xVbh3j4md/xGG4anQabm3isY9WFzS9UgsZQKXDCe5lF48tytrpmx23gN/DzurVqxk5ciRRUVEADB48mOzs7Ea3OfbYY7n55puZPn06MTExzJ8/n9NPP73B9e12O3b7wbEPNpt3BjiJ+MS+QljxHPz0bKArET/ytFWk2uHEWl5FSXkVJWVVWMsrKd7vWraWVVJSXkVxWRUlZZWu9cqq2LOviTFgB9z+1iovfKOGzXjH95NafrupyOfHKGukZSPU7GqhqHmEmU2EWg4uVzucbNy9r8Hta/xhaHdSEiIJt4QQbgkhzBJCmPnA8yGvwy0hhJnNtcs5Rfu5OuPnJo+x8Op0jk6OweE0qHYaOJyugFntMOq8Z1DtdB5cdhhsKdzP7EXrmtz/1BGpJEaHHxZsq6oPBNkDAbfa6aSq7rPDia2imk1u/Dl5yu9hx2azkZaWVrtsMpkwm80UFxeTkJBwxG3OOuss+vTpw9FHHw1Aeno699xzT4PHePjhh/nb3/7m3cJFvK1oI/z4JKx6HRzu/SiJ//i6i8kw3GsauOalnymrdFBaUd3sYzWlU4cwLCEhGLhaOwxc81NSb9mofb/mtcNhNBoAanSJiyA6zFyvu6J+18bh3Rmubg4T1rIqPl6zo8ljXHdKGikJUYSYXL8rISYTISYIMblaokJMJkJCapYPfhZigry95fzjf03/iP/38mEM7BZLWE2oORA+Qs2mJsedZuVbOec/3zV5jGkn9eTYlLgm1zuS/Xb3/htJjA6nWzP+283Kt7q13tQTjmr2d3D3z8lTfg87FouF8PD6AyojIiIoKytrMOy89dZbbN26lfXr15OUlMRdd93FH//4R959990jrj9z5kzuuOOO2mWbzUZqqq4+kVbAMGDbcvjhP/D7/3D9jAApw6D/ufDVAwEtT1y80cVkGAZ79leyvbic/OJy8kvK6rwuZ+se98Ym7LLVD8IxERYSosKIjwolLjKU+KgwEqJCiY8MJS4qjPjIUOKjXO/vLq3ghld+afIYGdNGNOvHyd0fpheuGN6iHz93ws65x6e06BjuSImPpHtCVLOOIYHl97CTmJhIVlZWvfdKS0sJCwtrcJvXX3+dG264oXbMzrx584iLi6OkpIT4+PjD1g8PDz8sUIkElNMB6z9xhZztdZqZ+06EUbdAjxPBuh2WzNaEf25oLQN7N+4qZUeJK7xsL3Y9XK/LKCgpp6Kq5VeVPHrhYIYdlUB8VBixERYs5hC3t3X3R1yCQ0J0GOGWkCZDekJ0w7+3gdy/L/k97KSnp/PCCy/ULufm5mK320lMTGxwm+rqanbt2lW7vGOHK+U7HD4aXSXiLZVlsOpVV3dVca7rPXM4HDcFTrwZko45uK4m/HOLPwb2OtwcfXrVgsbHR5hM0DkmgpSEyAOtApG1r8srHdzwatOtLv27xtIrqYFJItsBf/zABssxUuIjWXzXGJ/9Q8DX+6/Zvqk/p+bwe9gZPXo0VquVhQsXcsUVVzBnzhzGjx+P2WzGZrMRGRlJaGhovW1OOukk5s6dS/fu3YmMjGTevHmceOKJmmdHWq+aQcc/vwDle13vRSZA+rUw4jrokHzk7TThX5Oae7mzYRjsr3Sw21ZBYamdwn12dttcz4WlrsfuA8/uDuwNMUG3+Mja7o2UhEi61wk1XeMiCbMcuSXGH60uwfAvfX/8wAbLMWqO48ur9/yx/7p/TvtKbZw4r+X7NRnujpLzog8++ICpU6cSExODw+Fg6dKlDBw4kJ49ezJv3jwmT55cb/2Kigruvvtu3n33XYqKijjxxBOZP38+vXv3dut4NpuNuLg4rFYrsbGxPvhG0i64c9+q6orDBx0n9HS14hw/FcKi/VJqMHN3nMjZg7rgNKgXYsrdnCfEXR/cOIrjexx5rGFT3P0en9xycrPHokDbn0FZ2jdv/X4HZAblyZMns3HjRjIzMxk1ahRJSUmAq0vrSCIiInjiiSd44okn/FilSB3u3LfKFAJGnX/hpgyDUbdC/0kQYvZ9je1ARZWDjbvcm5zt09+OPNdHh3ALSTHhBx8dwkmOdT3XvLd3XyWXv7iiyWN4Mn7mUP4a/9DW/6Uv4g0BuzdWSkoKKSkpgTq8iGfcuW9VTdCpO+i4ictRg5G3/qVfUlZJdoGN7B021hbYyC6wsalwn9vjaS5J787AbnG1ISY5JoJOMWFEhTX9vz1/dDH5q1tDRHQjUBHvuvhlGHBuoKsImOYMHjYMgwJrBWvzrfWCTUOz88ZGWLC5MefM5SObP1+Jv6hVRMQ/FHZEvCm+R6ArCCh3Bw+/uzKP0opqV7DZYaOk7MjT9acmRjKwaxwDusUysFssA7vFUVhawaQnv/dF+bXa8iW2InI4hR2RplTbIfuDQFcRVOZ+ubHesiXExNHJHRjYLY6B3WIZ0C2W/l1jiYsMPWzbIjevlGoJdTGJBBeFHZGGVFfCry/Dt3PBtj3Q1bRq1Q4nG3bt44u17t0AcEDXGNJ7JjKwm6vVpk/nDoRb3BvEHSwDe0XEfxR2RA5VXQmrXnGFHGue672oTlDm+xsN+kNLBxA7nQY5e/azZnsJq/Os/JZvZW2B1aPZgh+58Lhmj6dRq4uIeEphR6RGdaVrtuNv/3Uw5HToAqfcAV2HwIunB7Y+L/B0ALFhGGwvLmfNditrtpewZruVrHwrpUe44WBMuIWenaL5zU9XMinMiIi7FHZEHFWukLPsX2Dd5nqvQ2c4+Q4YdiWERrrm2bGEt/n7Vrk7gPiJrzew02rnt3wre4/QghIRGsLAbnEM7l7ziCetYzTZO2w+uWOxiEhLKOxI++WoglWvwbePQUndkHM7DLvKFXJqtLP7Vr3588ExSqFmE/26xNYLNn2SO7RoQj0REX9S2JH2x1EFq1+HZY8eDDnRya6QM3xa/ZBTVxu/b5VhGGwp3O/WuuP6JzPmmCQGd4+nX9eYVjd4WETEEwo7EhzcuW9VTBdY/caBkLPV9X50Mpx8GwybBmFRfinVn4r22fluYxHLNhSybGOR25dt3z7+mGYNINbgYRFpjRR2pO1z575VIRZX2LEe6J6JToKTboPhVwdVyKmsdrJyazHfbixk2cZCsvJt9T5vqtXFGzR4WERaG4UdafvcuW+Vs9oVdKI6uVpyhl/dKu9A3pzLwnOL9rNsYyHLNhTy4+Y97K+sf2fvAV1jGX1MEqP7dCIiLIQLnv7RJ7WLiLRWCjvSfpxwA4y7r1WGHHD/svCPbz6JnD1lLNtQyLcbi9i2t6zeOh2jwzilTydGH5PEyX06kRwTUfuZP25wKSLS2ijsSPtx3JRWG3TA/cvCJ/z7Wxx1bvwdajYx7KiEA603SQzoGktIyJHvtq4BxCLSHinsiLQxDgPSOkUz+kDrzcheHYkOd+9U1gBiEWmPFHak7atu+Ie7LdlprXBrvReuGM74AZ2bfRwNIBaR9kZhR9q2PZvhwxsCXUWzGIbB2gIbX2Tv4ou1O1m/s9St7brERTS9koiI1FLYkbYr61346E9Q6V5IaA2qHU5W5O7li7W7+DJ7F/kl5bWfhZjAaTSysYiINIvCjrQ9VRXw+b2QOd+1nDIcdq4BRyPdWV64b1Vz7xZeVlnNsg1FfJG9k8Xrd1NSVlX7WURoCKcek8QZA7rQNS6CqS/81KIaRUTkcAo70rbs2QxvXwk7f3Mtn3IXjJkJpTt8et8qT+8Wvmefna/X7eaL7J18u7Go3nYJUaGM79+ZMwZ24eSjOxEZ5roVgy4LFxHxDYUdaTuy3oOPbnV1W0V1hAueg6PHuz7z8X2r3L0s/MXvtvDbdhuZW/fW65JKTYzkjAFdOGNAZ4YdlXDEm2jqsnAREd9Q2JHWr6oCvvgL/PyCa7nHKLhwPsR2C2xdRzD/u9za18emxLoCzsDO9O0cg8l05LlvauiycBER31DYkdZtz2Z4+yrXmByAU+6EMfeCuXX+pzu4exwXDElh/IDOdE/w/J5buixcRMT7WucvhgjA2vfhw1sOdlud/xz0GR/oqho1+/xBzbpbuIiI+I7CjrQ+VRXwxV/h5+ddyz1OhD/Mh7iUgJVkGLomXESkrVLYkdZl7xZXt9WO1a7lk2+HsX8NaLdVYamdBz5aG7Dji4hIyyjsSOux9n3X1VZ2G0Qmuq626nN6QEtavH4XM95ew55GBg2LiEjrprAjgVdth8//crDbKnUkXPhiQLutKqocPPy/dbz041YAeidFs21vGVWOhruzdFm4iEjrpLAjvleS1/CEf7Z8WPx32L3OtXzy7TD2L2AO9V99h/h9Zym3vv4rv+9y3YZi2kk9+fNZ/dizv1KXhYuItEEKO+JbJXnw5DBX601jwuPgDy/AMWf4p64jMAyDl37IZfai9VRWO+nUIYxHLzqOsX2TAV0WLiLSVinsiG+V7Wk66EDAg07RPjsz3l7NN78XAjC2bxKPXHgcSTHhAatJRES8Q2FHWocOyQE79De/72bG26sp2ldJmCWEeyf048pRPZuc8VhERNoGhR1ptyqqHMxZtJ6MH3IB6Ns5hn9fejz9usQGtjAREfEqhR1pl37fWcqf3viV9Ttdg5CvGtWTeyb0IyLUHODKRETE2xR2pF0xDIOFP25l9v/WYa8ZhHzhcYztF7huNBER8S2FHWk3ivbZ+fM7a/h6/W4AxvRN4lENQhYRCXoKO+JbJXl+OUx+SXmjc+Bs2FXK7P+tp2ifnTBzCDMn9uMqDUIWEWkXAhJ2srKymDZtGps2beLaa6/lkUceafRHZ9asWfztb3877P1vvvmGMWPG+LBSaRF7KXx5f9PrWcJddzVvpvySck57bAn2ameT6/ZJ7sATlw6hf1cNQhYRaS9Mhp9v52y32+nXrx9nnnkmM2bM4NZbb+XCCy9k2rRpDW5TUVFBRUVF7fK2bdsYP348GzduJC4urslj2mw24uLisFqtxMbqR84vnE5463JY/wlEJ8P5/4WoxCOvG9UR4lObfaisfCvn/Oe7Jtc7e1AX/nXx8RqELCLSRnjr99vvLTuLFi3CarUyd+5coqKimD17NjfddFOjYSciIoKIiIja5bvvvpvbb7/draAjAfLdXFfQMYfBlNcgNT3QFXHDmKMVdERE2iG/h53Vq1czcuRIoqKiABg8eDDZ2dlub19QUMD7779PTk5Og+vY7Xbs9oOz9tpstuYXLJ7b+BUsfsj1euKjrSLoiIhI+xXi7wPabDbS0tJql00mE2azmeLiYre2f/bZZ5k6dSodOnRocJ2HH36YuLi42kdqavO7SMRDe7fAu1cDBgy7yvUQEREJIL+HHYvFQnh4/Ut9IyIiKCsra3Jbh8PB888/z/XXX9/oejNnzsRqtdY+8vL8c0VQu1e5H974I1RYoXs6THgk0BWJiIj4vxsrMTGRrKyseu+VlpYSFhbW5LbffPMNnTp1on///o2uFx4efligEh8zDPjwZti9Fjp0hotfdl1l5QdbCvf75TgiItI2+b1lJz09neXLl9cu5+bmYrfbSUxs4EqdOt566y3OP/98X5YnzfXDf2DtexBigYtegtiu/jnspiL+/O4avxxLRETaJr+HndGjR2O1Wlm4cCEAc+bMYfz48ZjNZmw2G1VVVQ1u+9lnnzF27Fh/lSru2vwNfPWA6/VZc+CoE/1y2I9WF3DlghWUVzloam7AcEsICdFNtx6KiEjw8Xs3lsVi4bnnnmPq1KnMmDEDh8PB0qVLAdeVWfPmzWPy5MmHbbd582YKCgpIT9eVPa1K8VZ452ownHD8HyH9Wr8c9oVvt/DQp+sAOHtQV+484xjKKh0Nrp8QHUZKfKRfahMRkdbF75MK1sjPzyczM5NRo0aRlJTk02NpUkEfqSyDF8+Anb9BtyEw7TMIjWh6uxZwOg1m/28dL3znmnrgqlE9uf+cAYSE6LYPIiLBps1OKlgjJSWFlJSUQB1eWsow4OM/uYJOVCe45BWfBx17tYO73l7Dx6sLAJg5oR/Xje6l+1uJiEijdCNQaZ6fnoXf3gKTGS7KgLjuPj2craKK6QtX8uOWPVhCTDx60WDOH+LbY4qISHBQ2BHP5X4Hn//F9frMf0DaKT493C5bBVe+uIL1O0uJDjPz7OXDOKWPb7s+RUQkeCjsiGes2+GtK8FwwKCL4YTGJ3hsqU27S7nyxZ/JLymnU4dwMqalc2yK7okmIiLuU9gR91VVwJt/hLIi6DIIJv2bJq/5boGVW/dydUYm1vIq0jpFs/DqEaQmRvnseCIiEpwUdsQ9hgGf3gkFv0JkAlzyKoT5Lnh8vnYnt77+K/ZqJ8enxvPiVekkap4cERFpBoUdcU/mfFj1CphC4MIFkHCUzw71yvKt3P9hFk4DxvVL5smpQ4kMM/vseCIiEtwUdqRpW3+ERX92vR4/C3r7ZhZrwzCY++UG/rN4EwBT0lN5aPKxWMx+n+hbRESCiMKONM62A96+EpzVMPB8GHWrTw5T5XBy73u/8fbK7QDcNr4PfxrXR3PoiIhIiynsSMOq7fDWFbBvFyQPhPOe8smA5LLKam589ReW/F5IiAn+cf4gLh3Rw+vHERGR9klhRxq26M+wfQVExMGUVyAsulm7yS8pp3h/5RE/Kymr5B+frmPdzlIiQkN48tKhjB/QuSVVi4iI1KOwI1CSB2V76r+37hNYucD1+qw5kNirWbvOLynntMeWYK92NrpeXISFBVePYGiPhGYdR0REpCEKO+1dSR48OczVZdWQT26DnqdAfKrHuy/eX9lk0AGYc+FgBR0REfEJXebS3pXtaTzogOvzQ1t+vCw1QZMFioiIbyjsiIiISFDzOOxs2LDBF3WIiIiI+ITHYee4445j6NCh/POf/yQnJ8cXNYmIiIh4jcdhp6ioiHvvvZfffvuNYcOGccIJJ/D444+zfft2X9QnIiIi0iIeh53o6GguvPBCXnnlFXbv3s21117Lgw8+SM+ePRk9ejQ//PCDL+qUNmqfvTrQJYiISDvXrEvPN27cyLvvvst7773H2rVrmTBhApdccgllZWVceOGFFBQUeLtO8ZXqI0/25w32agf//Gxdk+uFW0JI0B3NRUTERzwOO4MGDWLz5s2ceeaZ3H777Zx77rlER7tm1s3JySEpKcnrRYoPrf+46XUs4RDV0aPdOp0GM95ew6/brESGhjDnD4PpndThiOsmRIeREh/p0f5FRETc5XHY+fOf/8x5551HTEzMYZ+lpaWxevVqrxQmflCSByued70eex/0GX/k9aI6ejyh4COf/85HqwuwhJj47+XDGX2MQrCIiASGx2Hnj3/8Y73l3bt3k5yc7LWCxI+++CtUl0OPUTD6Tq/d5HPhj7k8u3QzAHP+MFhBR0REAsrjAcrZ2dkMHTqUt99+G4Bx48YxcOBAzb/T1mxZAtkfgCkEJj7qtaDzxdqdzPpoLQB3nn4MFw7r7pX9ioiINJfHYWf69OmcdtppnHHGGQAsX76cSZMmcf3113u9OPERRxX8727X6/T/gy7HemW3v2wr5tY3fsVpwJT0VG4+7Wiv7FdERKQlPO7GWrVqFW+99RZxcXGA61L0W265hQEDBni9OPGRn/4LRb9DVCcYe69XdplTtJ9rX8qkosrJ2L5JPDT5WExeai0SERFpCY9bdgYNGsTLL79c772XX36ZgQMHeq0o8aHSnbBkjuv1+FkQGd/iXRbts3PVghXs3V/JoJQ4npw6FItZt10TEZHWweOWnaeeeooJEybw0ksv0bNnT3JyciguLuazzz7zRX3ibV8+AJWlkDIMjr+sxbsrr3RwzUuZbN1TRveESOZfNZzo8GZN3yQiIuITHv8qDRkyhI0bN/Lxxx+Tn5/P5Zdfztlnn33ES9Glldn6I6x5AzC5BiWHtKz1pdrh5JbXf2F1XgnxUaG8dPUIkmMivFOriIiIlzTrn+AxMTFMnTq13nuFhYWaULA1czrgfzNcr4de4WrZaQHDMJj18Vq+WrebMEsIL1wxvMFJA0VERALJ47CTnZ3NjBkz2LBhAw6HA3D98BUUFGC3271eoHhJ5ouw6zeIiIdxD7R4d88s3cwry7dhMsG/Lzme4T0TW16jiIiID3jcjzFt2jT69OnD6NGjGTZsGE899RQRERHMmTPHF/WJN+wvgsV/d70+7a8Q7dmtHw71wa/5PPLZ7wDcd/YAJgzq2tIKRUREfMbjsJOVlcW9997Lddddx9atW5kwYQIvvPACGRkZPihPvOLrB6HCCl0GwfCrW7SrHzYVMeMd1y1Brj05jatPTvNGhSIiIj7jcdg55phjePHFFznuuOPYvHkzRUVFJCcnk5OT44v6pKXyV8IvC12vJz4GIeZm72r9ThvTX15JlcPg7MFduXdify8VKSIi4jseh50nnniCefPmYbPZuOaaa+jVqxfDhw/nvPPO80V90hJOJ3x6F2DA4CnQY2Szd7XDWs60BT9Taq9mRM9E/nXRcYSEaNJAERFp/UyGYRieblSziclkYunSpezbt4+zzjoLs7n5rQa+ZLPZiIuLw2q1EhsbG+hy/OeXhfDRLRAWA7dkQkyXZu3GVlHFxc/+yPqdpfROiubdG0YRHxXm5WJFRETq89bvd7MuPa97G4BTTz212QcXHyovhq9muV6PuafZQaey2skNr6xk/c5SkmLCyZg2QkFHRETaFI+7sZ5++mkKCgp8UYt40zezoWwPdOoLJ0xv1i4Mw+DP767h+017iA4zs+CqdFITo7xcqIiIiG81a8zOmjVrWnTQrKws0tPTSUhIYMaMGXjSkzZlyhRuueWWFh0/6O38DX5+wfV64iNgDm109fyScrLyrYc9/vzuGt7/NZ8QEzz9x2EcmxLnh+JFRES8y+NurPvuu4+HHnqIk08+mQ4dPJ8x1263M2nSJM4880zeeOMNbr31VjIyMpg2bVqT237++ecsXryYDRs2eHzcdsMwXDMlG04YMBl6jWl09fySck57bAn2ameD64SYTBydrNmRRUSkbfI47GzatAmn00mfPn244ooriI6Orv3s/vvvb3L7RYsWYbVamTt3LlFRUcyePZubbrqpybBTXl7OjTfeyJw5c4iPj/e07Pbjt7dh248QGgVn/qPJ1Yv3VzYadACqnQbF+ytJiY/0VpUiIiJ+43HYyc3NpW/fvvTt25fdu3d7fMDVq1czcuRIoqJcYz8GDx5MdnZ2k9v9/e9/p7y8HIvFwuLFixk7dmy9gdJ12e32ereusNlsHtfZJlXY4Iu/ul6Pvgviuge2HhERkVbA47CzYMGCFh3QZrORlnZw1l2TyYTZbKa4uJiEhIQjbrNt2zbmzp3LiBEj2LZtG//+97/p0aMH77333hEDz8MPP8zf/va3FtXZJi17BPbtgsRecOLNga5GRESkVfA47Gzbtq3Bz3r06NH0AS0WwsPD670XERFBWVlZg2EnIyODzp078+WXXxIeHs6f/vQnjjrqKL788kvOOOOMw9afOXMmd9xxR+2yzWYjNTW1ydratMLfYfkzrtcTHgFLeOPri4iItBMeh52ePXtiMpnqTSxYo+Yu6I1JTEwkKyur3nulpaWEhTU8d8v27dsZN25cbUiKiYmhT58+Dd6iIjw8/LBAFdQMAxbdDc5q6DsR+pwe6IpERERaDY8vPXc6nTgcDpxOJ/v37+ebb75hzJgxfP31125tn56ezvLly2uXc3NzsdvtJCYmNrhNamoq5eXl9WrYvn07Rx11lKflB6d1H8GWJWAOhzNnB7oaERGRVsXjsFNXZGQko0eP5qOPPuLOO+90a5vRo0djtVpZuNB1c8o5c+Ywfvx4zGYzNpuNqqqqw7a5+OKL+fjjj3n33XfZvn07M2fOxG63c9JJJ7Wk/OBQWQaf3et6ffJtkKi7kIuIiNTVorBTY/fu3ezYscOtdS0WC8899xzXX389nTt35p133mHOnDmA68qsTz/99LBt+vbty5tvvslDDz1Enz59+PTTT/nwww+JiYnxRvlt23dzwbYd4nrASbd5vHlCdBhN3c8z3BJCQrRuESEiIm2Tx2N20tLSDhuns2PHDm677Ta39zF58mQ2btxIZmYmo0aNIikpCXB1aTXk7LPP5uyzz/a03OC2ZzN8/2/X67NmQ5jnt3IoKavEeWAC66enDqFHx+jD1kmIDtMcOyIi0mZ5HHYyMjLqLZtMJrp3706vXr082k9KSgopKSmeHl7q+vxecFRC79Og3znN2sW8rzYCcO5x3Zg4uJs3qxMREWkVPA47h97lfPfu3SQnJ3utIHHT75/Bhs8gJNR1qXkDEyw2JivfypfZuzCZ4NZxfXxQpIiISOB5PGYnOzuboUOH8vbbbwMwbtw4Bg4cqPtV+VNVBXz2Z9frE2+ETs0LKnVbdXTvKxERCVYeh53p06dz2mmn1U7mt3z5ciZNmsT111/v9eKkAT/8B4pzIaYrjJ7RrF38tt3KV+t2EaJWHRERCXIed2OtWrWKt956i7i4OACio6O55ZZbGDBggNeLE6AkD8r2HFzetxOWPep6nf5/UF4C4Z5flTbvK1dL3HnHp9A7Sa06IiISvDwOO4MGDeLll1/m7rvvrn3v5ZdfZuDAgV4tTHAFnSeHQbX9yJ8vfhCW/RNuXgnx7t8OY832Er5ev5sQE9xy2tFeKlZERKR18jjsPPXUU0yYMIGXXnqJnj17kpOTQ3FxMZ999pkv6mvfyvY0HHRqVNtd63kQdmrG6kwekkIvteqIiEiQ8zjsDBkyhI0bN/LJJ5+wfft2Lr/8cs4++2xN8NdGrMorYfH63ZhDTNxymsbqiIhI8PM47IDrRpyXXnop4Lr0XEGn7agZqzP5+BTSOh0+gaCIiEiw0aXn7cgv24pZ8nsh5hATt47TWB0REWkfdOl5O/LvA2N1LhiSwlFHuC2EiIhIMNKl5+3Eyq3FLN1QqLE6IiLS7njcslNz6XlduvS89asZq/OHoSn06Oj5DUNFRETaqhZfer5lyxZKSkp06bkvRHUEczg4Grn83BLuWq8RK7fu5duNRVjUqiMiIu1Qiy89nzJlCnFxcbzxxhscd9xxvqix/YpPdd3k85M/QYfOMPVN4JAbfkZ1bHKOnZp5dS4c1p3URLXqiIhI+9KsS8+3bt3Kjh07WLx4Md9++y3V1dWMHDnS27UJwNbvXc+DLoJuQzzePDP3YKvOTWN1BZaIiLQ/boWdnTt38uWXX9Y+iouLGTJkCL/++isvvPAC559/PtHRurrH66oq4PdFrtcDzmvWLh4/MFbnouFq1RERkfbJrbDTrVs3TCYTY8aMYf78+YwfP56wsDASEhIYPXq0go6vbPkGKkshphukDPd48xU5e/l+0x5CzWrVERGR9sutsFO3Vee8887jmGOOYeTIkdjtdnbv3k2PHj18XWf7lP2h63nAuRDi8YVzPP5lTatOKt0T1KojIiLtk1u/oOPGjWPOnDmsXLmSHTt28Ne//hXDMOjYsSMnnHACffv25cYbb/R1re1LdSWs/5/rdTO6sJZv2cOPW9SqIyIi4vEA5U6dOnHppZfW3htr3bp1fP7553z11VdeL65dy1kKdqvrKqzUEzzevGZenUvSU0mJj/R2dSIiIm1Gs67Gqqt///7079+f2267zQvlSK3sD1zP/SdBiNmjTX/cvIflW/YSZg7hxjFq1RERkfbN84Eg4nuOKlj/qeu1h11YhmHUXoF1SXoq3dSqIyIi7ZzCTmuU+y2UF0NUJzjqJI82/XHLHlbkHGjVGdvbRwWKiIi0HQo7rdHaD1zPHnZhGYbBvC9dsyVfOiKVrnFq1REREVHYaW0c1bD+E9drD7uwfti8hxW5ewmzhHCDxuqIiIgACjutz9bvoWwPRCZCz5Pd3swwjNp5daaO6EGXuAhfVSgiItKmKOy0NjUTCfY7G8yhbm/23aYiMrcWE24J4YYxGqsjIiJSQ2GnNXE6YN3HrtcDJru9mWEYtXc2n3pCDzrHqlVHRESkhsJOa7JtOezfDRFxkDba7c2+3VjEyppWnVPVqiMiIlKXwk5rUtOF1fdssIS5tUndeXUuO+EoktWqIyIiUo/CTmvhdMK6j1yvB052e7OlGwr5dVsJEaEhXD+ml29qExERacMUdlqL7SugdAeEx0KvMW5t4mrVcY3V+eMJR5Eco1YdERGRQynstBa1XVgTwBLu1iZLNhSyOs/VqjNdY3VERESOSGGnNXA6D4YdNycSdM2W7Bqrc8WJPUmKcS8giYiItDcKO61BwS9gy4ewDtD7NLc2+eb33azebiUy1Mx1ozVWR0REpCEKO61B9geu52POhNCm72dVd16dK048ik4d1KojIiLSEIWdQDMMj7uwFq/fzZrtVqLC1KojIiLSlICEnaysLNLT00lISGDGjBkYhtHkNpMmTcJkMtU+xo8f74dK/WDHKijZBqFRcPTpTa5ev1WnJx3VqiMiItIov4cdu93OpEmTGDZsGJmZmWRnZ5ORkdHkditXruS3336juLiY4uJiPvzwQ98X6w81rTp9zoCwqMM+zi8pJyvfWvuY/10Ov+VbibCEcEqfTuSXlPu5YBERkbbF4u8DLlq0CKvVyty5c4mKimL27NncdNNNTJs2rcFttm/fjmEYHHvssX6s1A8MA9Z+4Hp9hC6s/JJyTntsCfZq52GfVVQ7ueyFnwi3hLD4rjGkxDc91kdERKQ98nvLzurVqxk5ciRRUa5WjMGDB5Odnd3oNitWrMDhcNC9e3eio6OZMmUKxcXFDa5vt9ux2Wz1Hq3Szt+gOAcsEa6WnUMU7688YtCpy17tpHh/pa8qFBERafP8HnZsNhtpaWm1yyaTCbPZ3Gh42bBhA8OGDePzzz8nMzOT3Nxc7r333gbXf/jhh4mLi6t9pKamevU7eE1NF9bR4yG8Q2BrERERCVJ+DzsWi4Xw8PqDaiMiIigrK2twm3vuuYdFixYxcOBA+vfvzz//+U/eeeedBtefOXMmVqu19pGXl+e1+r3GMA5ecj5gciArERERCWp+H7OTmJhIVlZWvfdKS0sJC3PvLt8A8fHxFBUVYbfbDwtOAOHh4Ud8v1XZvQ72bAJzmGt+HREREfEJv7fspKens3z58trl3Nxc7HY7iYmJDW5z4YUX1tvm559/pkuXLq0/0DSmpgur9ziIiA1sLSIiIkHM72Fn9OjRWK1WFi5cCMCcOXMYP348ZrMZm81GVVXVYdsMHjyY22+/nZ9++olPPvmE++67jxtvvNHfpXtXTdgZODmgZYiIiAQ7v3djWSwWnnvuOaZOncqMGTNwOBwsXboUcIWaefPmMXny5HrbzJw5k61bt3L66aeTnJzMDTfcwMyZM/1duvcU/g6F6yAkFI45K9DViIiIBDW/hx2AyZMns3HjRjIzMxk1ahRJSUmAq0vrSEJDQ5k/fz7z58/3Y5U+VNuFNRYi4xtcLSE6jHBLSKOXn4dbQkiIdn+8k4iISHsTkLADkJKSQkpKSqAOH1hu3gsrJT6Sd28YxTn/+Q6AjGnph930MyE6TBMKioiINCJgYafdKtoEu7IgxAJ9Jza5+vqdpQAM7BbLmL7Jvq5OREQk6Oiu5/627kCrTtpoiGr4CrQai9fvAmBcPwUdERGR5lDY8Tc3u7AAKqudLNtQBMC4/p19WZWIiEjQUtjxp705sGM1mMzQ75wmV1+Rs5d99mqSYsIZlBLnhwJFRESCj8KOP637yPXc82SI7tTk6l8f6MI6rW8yISEmX1YmIiIStBR2/MmDLizDMPh63W4ATuuv8ToiIiLNpbDjLyXbIH8lYIL+k5pcfXPhPrbtLSPMEsLJRzfdCiQiIiJHprDjL9kHurCOOgk6NN1S89WBVp0Te3UkOlwzBIiIiDSXwo6/eNCFBbD4QNgZry4sERGRFlHY8QdrPmxf4XrtRhdW8f5KMrfuBWCs5tcRERFpEYUdf1j3ses5dSTEdm1y9aUbCnEa0K9LDN0TonxcnIiISHBT2PEHD7uwvlp3YNZkdWGJiIi0mMKOr5XuhG0/ul4POLfJ1ascTpZuKATgtH6aNVlERKSlFHZ8bd3HgAHd0yGue5Or/5y7l9KKajpGh3F8arzPyxMREQl2Cju+1syrsMb0TcasWZNFRERaTGHHl/bthq3fu173b7oLC+Dr9brkXERExJsUdnxp/SdgOKHbEEg4qsnVtxTuI6doP6FmEyf30azJIiIi3qCw40sedmHV3AtrZK+OxESE+qoqERGRdkVhx1f274Gcb12v3e7COnCXc00kKCIi4jUKO77y+6dgOKDLIOjYu8nVrWVV/JxbDMA4XXIuIiLiNQo7vlLbhTXZrdWXbizE4TTok9yBHh01a7KIiIi3KOz4QnkxbFnieu1m2FlcO2uyWnVERES8SWHHF9b/D5zVkDwQOh3d5OrVDiff/O6aNVm3iBAREfEuhR1f8PAqrF+2lWAtryI+KpShPRJ8WJiIiEj7o7DjbRVW2LzY9drtS85dXVhjNWuyiIiI1ynseNvvn4GzCjr1heR+bm1SM2uyurBERES8T2HH2zzswtq6Zz+bdu/DEmJi9DFJPixMRESkfVLY8SZ7KWz6yvXaw1mTR6QlEqtZk0VERLxOYcebNnwODjt0PBo6D3RrE82aLCIi4luWQBfQppXkQdmeg8srX3I99xgJO1ZDVEeIT21w89KKKn7ashfQ/DoiIiK+orDTXCV58OQwqLYf/tmvr7gelnC4eWWDgWfZhiKqnQa9kqJJ6xTt44JFRETaJ3VjNVfZniMHnbqq7fVbfg5R04U1Tl1YIiIiPqOwEyAOp8GS2lmT1YUlIiLiKwo7AbIqr5i9+yuJjbAw7CjNmiwiIuIrCjsB8tWBS87H9E0m1Ky/BhEREV/Rr2yALF6nWZNFRET8QWEnAPL2lvH7rlLMISZO1azJIiIiPhWQsJOVlUV6ejoJCQnMmDEDwzDc3raqqopBgwaxZMkS3xXoY4sP3Atr2FEJxEeFBbgaERGR4Ob3sGO325k0aRLDhg0jMzOT7OxsMjIy3N7+kUceISsry3cFuiuqo2sencZYwl3rHeKrA3c5H68uLBEREZ/z+6SCixYtwmq1MnfuXKKiopg9ezY33XQT06ZNa3LbjRs38thjj9GzZ0/fF9qU+FTXhIGNzKNzpBmU99mra2dNPq2fLjkXERHxNb+HndWrVzNy5EiioqIAGDx4MNnZ2W5tO336dO655x4WLVrU6Hp2ux27/eCEfzabrfkFNyY+tdHbQRzJdxuLqHQ46dkxit5JmjVZRETE1/zejWWz2UhLS6tdNplMmM1miouLG91uwYIFWK1W7rzzziaP8fDDDxMXF1f7SE31LJD40tfram782RmTyRTgakRERIKf38OOxWIhPLz+WJeIiAjKysoa3KawsJCZM2cyf/58LJamG6NmzpyJ1WqtfeTl5bW4bm9wOg2++d01OFnjdURERPzD791YiYmJhw0wLi0tJSys4auSbrvtNq655hqOP/54t44RHh5+WKBqDVZvL6FoXyUx4RaG90wMdDkiIiLtgt9bdtLT01m+fHntcm5uLna7ncTEhn/8X3vtNf7zn/8QHx9PfHw83333Heeccw5z5szxR8leU3PJ+ei+SYRZNMWRiIiIP/i9ZWf06NFYrVYWLlzIFVdcwZw5cxg/fjxmsxmbzUZkZCShoaH1tsnJyam3PGXKFG677TbOOussf5beYjW3iNBdzkVERPzH72HHYrHw3HPPMXXqVGbMmIHD4WDp0qWA68qsefPmMXny5HrbHHqpeUREBF26dCE+Pt4/RXtBQUk563bYCDG57oclIiIi/uH3sAMwefJkNm7cSGZmJqNGjSIpyXXLhNzcXLe2b4uzJ399oAtraI8EEqM1a7KIiIi/BCTsAKSkpJCSkhKow/vd4ppLznUVloiIiF9plKwflFVW8/1m10zL4/tr1mQRERF/Utjxg+837aGy2kn3hEj6JHcIdDkiIiLtisKOH3xde+NPzZosIiLibwo7PuZ0GrXz65ymS85FRET8TmHHx7IKrOwutRMdZuaEXpo1WURExN8Udnzs6wMTCZ7SJ4lwiznA1YiIiLQ/Cjs+9vV613idcbrkXEREJCAUdnxop7WCrHwbJhOM1XgdERGRgFDY8aGagcnHp8bTqUPruwu7iIhIe6Cw40OLa7qw1KojIiISMAo7PlJR5eC7TUUAjNOsySIiIgGjsOMjP2wuoqLKSbe4CPp1iQl0OSIiIu2Wwo6P1FxyPk6zJouIiASUwo4PGEadWZN1ybmIiEhAKez4QPYOGzusFUSGmjmxV8dAlyMiItKuKez4QE0X1sl9OhERqlmTRUREAklhxwe+PtCFpUvORUREAk9hx8t2l1awOq8E0F3ORUREWgOFHS9bsr4QgMHd40iOjQhwNSIiIqKw42VfrauZNVkTCYqIiLQGCjteVH/WZHVhiYiItAYKO160fMseyioddI4NZ2C32ECXIyIiIijseFXtRIL9NGuyiIhIa2EJdAFtWX5JOcX7KwHXrMmLftsJQJ/kaLLyrSREh5ESHxnIEkVERNo9hZ1myi8p57THlmCvdh722YOfrAMg3BLC4rvGKPCIiIgEkLqxmql4f+URg05d9mpnbcuPiIiIBIbCjoiIiAQ1hR0REREJago7IiIiEtQUdkRERCSoKeyIiIhIUFPYERERkaCmsNNMCdFhhFsa/+MLt4SQEB3mp4pERETkSDSpYDOlxEey+K4xjc6joxmURUREAk9hpwVS4iMVZkRERFo5dWOJiIhIUGtTYaegoIAffviB0tLSQJciIiIibURAwk5WVhbp6ekkJCQwY8YMDMNocpt//etfDBw4kOuvv57u3buzdOlSP1QqIiIibZ3fw47dbmfSpEkMGzaMzMxMsrOzycjIaHSbDRs28Oijj5Kdnc2aNWu46667uP/++/1TsIiIiLRpfg87ixYtwmq1MnfuXHr37s3s2bOZP39+o9tUV1fz/PPP07VrVwCOO+44iouL/VGuiIiItHF+vxpr9erVjBw5kqioKAAGDx5MdnZ2o9sMGDCAAQMGALBv3z7+85//cMEFFzS4vt1ux2631y7bbDYvVC4iIiJtkd9bdmw2G2lpabXLJpMJs9nsVkvN//73P7p27crOnTv5y1/+0uB6Dz/8MHFxcbWP1NRUr9QuIiIibY/fw47FYiE8PLzeexEREZSVlTW57RlnnMGiRYuwWCzcfffdDa43c+ZMrFZr7SMvL6/FdYuIiEjb5Pewk5iYSGFhYb33SktLCQtr+rYKFouFk08+mSeeeIIFCxY0uF54eDixsbH1HiIiItI++T3spKens3z58trl3Nxc7HY7iYmJDW7z2muv8a9//at22WKxYDabfVqniIiIBAe/h53Ro0djtVpZuHAhAHPmzGH8+PGYzWZsNhtVVVWHbdOvXz9mzZrF+++/T25uLg888AAXXXSRv0sXERGRNiggY3aee+45rr/+ejp37sw777zDnDlzANeVWZ9++ulh2wwdOpRnnnmGO+64gyFDhnDUUUcxd+5cf5cuIiIibZDJcGf6Yh/Iz88nMzOTUaNGkZSU5NNj2Ww24uLisFqtGr8jIiLSRnjr9ztgdz1PSUkhJSUlUIcXERGRdqJN3QhURERExFMKOyIiIhLUFHZEREQkqCnsiIiISFBT2BEREZGgprAjIiIiQU1hR0RERIKawo6IiIgENYUdERERCWoKOyIiIhLUFHZEREQkqCnsiIiISFBT2BEREZGgprAjIiIiQU1hR0RERIKawo6IiIgENYUdERERCWoKOyIiIhLUFHZEREQkqCnsiIiISFBT2BEREZGgprAjIiIiQU1hR0RERIKawo6IiIgENYUdERERCWoKOyIiIhLUFHZEREQkqFkCXYA/GIYBgM1mC3AlIiIi4q6a3+2a3/HmahdhZ8+ePQCkpqYGuBIRERHx1J49e4iLi2v29u0i7CQmJgKwbdu2Fv1hBZrNZiM1NZW8vDxiY2MDXU6zBMN3AH2P1iQYvgMEx/cIhu8A+h6tidVqpUePHrW/483VLsJOSIhraFJcXFyb/QuvKzY2ts1/j2D4DqDv0ZoEw3eA4PgewfAdQN+jNan5HW/29l6qQ0RERKRVUtgRERGRoNYuwk54eDgPPPAA4eHhgS6lRYLhewTDdwB9j9YkGL4DBMf3CIbvAPoerYm3voPJaOn1XCIiIiKtWLto2REREZH2S2FHREREgprCjoiIiAQ1hZ024sMPP6RXr15YLBZOOOEE1q1bF+iSWuSss84iIyMj0GW0yD333MOkSZMCXUazvPzyy/To0YMOHTowfvx4cnNzA11Su7Nnzx7S0tLq/dm3xfP8SN+jrrZwrjf2HdrSeX6k76Fz/QAjyP3222/G8OHDjfj4eOOuu+4ynE5noEvy2KZNm4yEhATjzTffNHbu3GlcdNFFxqhRowJdVrO98sorBmAsWLAg0KU022+//WbExMQYmzZtCnQpHtu0aZORmppqrFy50ti6datx9dVXG6eeemqgy3JbUVGR0bNnTyMnJ6f2vbZ2nhcWFhojR440gNrv0RbP8yN9j7rawrne2HdoS+d5Q/9NtbVz/YMPPjDS0tIMs9lsjBgxwsjOzjYMo+XneFC37NjtdiZNmsSwYcPIzMwkOzu71f8L40jWrVvH7Nmzufjii+ncuTM33HADmZmZgS6rWfbu3cudd95J3759A11KsxmGwfTp07ntttvo3bt3oMvx2K+//srIkSMZOnQoPXr0YNq0aWzYsCHQZbmlqKiIc845p96/TtvieT5lyhSmTJlS7722eJ4f6XvUaCvnekPfoa2d50f6Hm3tXN+8eTPTpk1jzpw55Ofnc9RRR3Httdd65xz3fi5rPd5//30jISHB2L9/v2EYhrFq1SrjpJNOCnBVLffMM88YAwYMCHQZzXLVVVcZ119/vXHllVe26n/tNea///2vERUVZbz44ovGxx9/bFRWVga6JI+sXbvW6Nixo/HLL78YJSUlxpQpU4wrrrgi0GW5Zdy4cca8efPq/eu1LZ7nmzdvNgzDaLBFxDDaxnne2PdoK+d6Q9+hrZ3nR/oebe1c//jjj41nnnmmdnnx4sVGWFiYV87xoA47s2bNMiZMmFC77HQ6jYSEhABW1HJ2u93o3bu38eSTTwa6FI8tXrzYSE1NNaxWa6v/H2BDSktLjaSkJOO4444zHnzwQWPs2LHGyJEjjfLy8kCX5pHp06cbgAEYaWlpxu7duwNdkluO9D/0tnyeNxR22tp5fuj3aIvnet3v0JbP80P/LtrquW4YBwO/N87xoO7GstlspKWl1S6bTCbMZjPFxcUBrKpl/vrXv9KhQweuu+66QJfikYqKCqZPn84zzzzTpm9I995777F//34WL17MfffdxxdffEFJSQkLFy4MdGluW758OR9//DE//fQTpaWlXHrppUycOBGjDcwv2qtXr8Pe03neugTDuR4M5zm07XO9srKSxx57jBtvvNEr53hQhx2LxXLYFNMRERGUlZUFqKKW+fLLL3n22Wd57bXXCA0NDXQ5Hvn73/9Oeno6Z599dqBLaZHt27dzwgknkJiYCLj+Gxs8eDA5OTkBrsx9b775JlOmTGHEiBF06NCBhx56iC1btrB69epAl9YsOs9bl2A414PhPIe2fa7XDfzeOMct3i6wNUlMTCQrK6vee6WlpYSFhQWooubbsmULl112Gc888wwDBgwIdDkee+211ygsLCQ+Ph6AsrIy3nrrLVasWMHTTz8d2OI8kJqaSnl5eb33tm7dytixYwNUkeeqq6vr/YuotLSU/fv343A4AlhV8+k8b12C4VwPhvMc2u65XhP4ly9fTmhoqHfOce/2sLUuX3/9tXH00UfXLufk5BgRERFGdXV1AKvyXFlZmdG/f3/j//7v/4zS0tLaR2u/vLauvLw8Iycnp/bxhz/8wXj00UeNwsLCQJfmkT179hhxcXHGM888Y+Tl5Rn//ve/jfDw8AYHmbZGr7/+uhEZGWnMnTvXePXVV42xY8caPXr0aPUDMOuizriEtnye1/0ebfk8r/s92uq5Xvc7tOXzvO73aIvn+ubNm42kpCTjlVdeqX3PG+d4UIedqqoqIykpyXjppZcMw3AN1DrnnHMCXJXn3n///doBZnUfbeHEa0hbGbR4JD/++KMxatQoIzIy0khLSzPef//9QJfkEafTacyaNcvo0aOHERoaagwZMsTIzMwMdFkeqfvff1s+zznkqrK2ep43VmdbOdcP/Q5t9Tyv+z3a2rneUOCvrKxs8Tke9Hc9/+CDD5g6dSoxMTE4HA6WLl3KwIEDA12WiLSAyWQiJyeHnj17AjrPRYLBBx98wPnnn3/Y+zk5OaxatapF53jQhx2A/Px8MjMzGTVqFElJSYEuR0R8QOe5SHBryTneLsKOiIiItF9Bfem5iIiIiMKOiIiIBDWFHREREQlqCjsiIiIS1BR2REREJKgp7IhIQC1ZsgSTyVTv0aFDB58cKyMjgzFjxvhk3yLSegX1vbFEpG2IjY1l69attcsmkymA1YhIsFHYEZGAM5lMtTeOFBHxNnVjiUirNGvWLCZMmMCpp55KXFwcU6ZMwWaz1X6+bNkyjj/+eBISEpg6dSolJSW1n3399dcMHjyYmJgYJkyYwPbt2+vt+/nnn6dz584kJyfzzjvv+OsriUiAKOyISMBZrVbi4+NrH9OnTwfgs88+45prriEzM5Pc3Fzuu+8+APLy8pg4cSI33XQTK1euZN++fVx11VUA5Obmcu6553LHHXewbt064uPjufnmm2uPtXbtWt59912+++47rrrqKu644w6/f18R8S/dLkJEAmrJkiWce+65rFmzpva9Dh068OSTT/LVV1/x3XffAfD+++9z++23k5uby8MPP8w333zDF198AUBBQQEpKSns2LGDF198kW+//ZZFixYBsH37dlatWsU555xDRkYGN9xwA7m5uXTu3JkNGzbQt29f9L9BkeCmMTsiEnAhISG1dzCvKzU1tfZ1SkoKu3btAlwtO7169ar9rFu3boSHh5OXl8f27dvr7at79+507969drl///507twZgLCwMC9/ExFpjdSNJSKtVm5ubu3rbdu20bVrVwB69OjBli1baj/Lz8/HbrfTo0cPUlNTycnJqf1sw4YNDBkyBKfTCbiu/BKR9kVhR0QCzjAMSkpK6j0cDgfLly/npZdeYuPGjTzyyCNccMEFAPzxj3/khx9+4PnnnycnJ4cbbriByZMn07lzZy699FK+/fZbMjIyyMvL46GHHiI5OZmQEP3vTqS90tkvIgFns9lISEio9/jxxx+ZNGkSCxcuZPjw4fTu3ZsHHngAcHVNffrppzz11FMMGTKE6OhoFixYAEDPnj358MMPmTt3LgMHDqSkpKT2MxFpnzRAWURapVmzZpGbm0tGRkagSxGRNk4tOyIiIhLU1LIjIiIiQU0tOyIiIhLUFHZEREQkqCnsiIiISFBT2BEREZGgprAjIiIiQU1hR0RERIKawo6IiIgENYUdERERCWr/D/I+byoVFXN5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss1, sigmoid_acc1],\n",
    "                   'relu': [relu_loss1, relu_acc1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 使用Softmax交叉熵损失训练多层感知机(MLP with Softmax Cross-Entropy Loss)\n",
    "第二部分将使用Softmax交叉熵损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 使用Softmax交叉熵损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和Sigmoid激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7918\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.7811\t Accuracy 0.0986\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.6891\t Accuracy 0.0981\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.6266\t Accuracy 0.1008\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5775\t Accuracy 0.1028\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.5397\t Accuracy 0.1065\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.5104\t Accuracy 0.1109\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.4841\t Accuracy 0.1158\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.4606\t Accuracy 0.1232\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.4406\t Accuracy 0.1297\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.4215\t Accuracy 0.1378\n",
      "\n",
      "Epoch [0]\t Average training loss 2.4051\t Average training accuracy 0.1461\n",
      "Epoch [0]\t Average validation loss 2.2343\t Average validation accuracy 0.2256\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1840\t Accuracy 0.2800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.2245\t Accuracy 0.2253\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.2145\t Accuracy 0.2404\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.2092\t Accuracy 0.2502\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.2024\t Accuracy 0.2584\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1966\t Accuracy 0.2660\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1923\t Accuracy 0.2728\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1889\t Accuracy 0.2781\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1841\t Accuracy 0.2856\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1801\t Accuracy 0.2921\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1751\t Accuracy 0.2992\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1705\t Average training accuracy 0.3062\n",
      "Epoch [1]\t Average validation loss 2.1175\t Average validation accuracy 0.3754\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0861\t Accuracy 0.4300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.1168\t Accuracy 0.3802\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.1111\t Accuracy 0.3915\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.1091\t Accuracy 0.3943\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.1055\t Accuracy 0.4013\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.1017\t Accuracy 0.4077\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.0988\t Accuracy 0.4111\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.0976\t Accuracy 0.4125\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0944\t Accuracy 0.4166\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0920\t Accuracy 0.4204\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0884\t Accuracy 0.4258\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0852\t Average training accuracy 0.4304\n",
      "Epoch [2]\t Average validation loss 2.0417\t Average validation accuracy 0.4890\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0153\t Accuracy 0.6000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0451\t Accuracy 0.4831\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0407\t Accuracy 0.4867\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0401\t Accuracy 0.4866\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0378\t Accuracy 0.4919\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0348\t Accuracy 0.4962\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0326\t Accuracy 0.4990\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0325\t Accuracy 0.4990\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0300\t Accuracy 0.5012\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0285\t Accuracy 0.5040\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 2.0258\t Accuracy 0.5081\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0232\t Average training accuracy 0.5123\n",
      "Epoch [3]\t Average validation loss 1.9848\t Average validation accuracy 0.5690\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9608\t Accuracy 0.6400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9910\t Accuracy 0.5567\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9875\t Accuracy 0.5539\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9879\t Accuracy 0.5517\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9866\t Accuracy 0.5556\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9842\t Accuracy 0.5573\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9824\t Accuracy 0.5591\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9833\t Accuracy 0.5584\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9813\t Accuracy 0.5600\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9805\t Accuracy 0.5613\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9784\t Accuracy 0.5643\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9764\t Average training accuracy 0.5671\n",
      "Epoch [4]\t Average validation loss 1.9418\t Average validation accuracy 0.6204\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9198\t Accuracy 0.6700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9501\t Accuracy 0.6047\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9473\t Accuracy 0.6033\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9485\t Accuracy 0.5991\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9480\t Accuracy 0.6002\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9461\t Accuracy 0.6013\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9446\t Accuracy 0.6023\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9461\t Accuracy 0.6015\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9446\t Accuracy 0.6026\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9443\t Accuracy 0.6030\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9427\t Accuracy 0.6048\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9411\t Average training accuracy 0.6070\n",
      "Epoch [5]\t Average validation loss 1.9095\t Average validation accuracy 0.6554\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8891\t Accuracy 0.7000\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9193\t Accuracy 0.6375\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9172\t Accuracy 0.6371\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9190\t Accuracy 0.6317\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9191\t Accuracy 0.6314\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9175\t Accuracy 0.6312\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.9163\t Accuracy 0.6324\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9183\t Accuracy 0.6305\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9171\t Accuracy 0.6315\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9172\t Accuracy 0.6317\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.9160\t Accuracy 0.6328\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9147\t Average training accuracy 0.6344\n",
      "Epoch [6]\t Average validation loss 1.8855\t Average validation accuracy 0.6796\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8665\t Accuracy 0.7600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.8965\t Accuracy 0.6616\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.8948\t Accuracy 0.6620\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.8971\t Accuracy 0.6546\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.8977\t Accuracy 0.6533\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.8963\t Accuracy 0.6518\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.8954\t Accuracy 0.6525\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.8978\t Accuracy 0.6508\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.8968\t Accuracy 0.6515\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.8973\t Accuracy 0.6516\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.8963\t Accuracy 0.6523\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8953\t Average training accuracy 0.6536\n",
      "Epoch [7]\t Average validation loss 1.8681\t Average validation accuracy 0.6974\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8502\t Accuracy 0.7800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8799\t Accuracy 0.6763\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8786\t Accuracy 0.6770\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8812\t Accuracy 0.6689\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8821\t Accuracy 0.6677\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8810\t Accuracy 0.6658\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8802\t Accuracy 0.6659\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8829\t Accuracy 0.6640\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8822\t Accuracy 0.6641\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8828\t Accuracy 0.6645\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8821\t Accuracy 0.6650\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8813\t Average training accuracy 0.6661\n",
      "Epoch [8]\t Average validation loss 1.8557\t Average validation accuracy 0.7028\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8389\t Accuracy 0.7800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8682\t Accuracy 0.6869\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8671\t Accuracy 0.6888\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8699\t Accuracy 0.6798\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8712\t Accuracy 0.6788\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8703\t Accuracy 0.6760\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8696\t Accuracy 0.6756\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8725\t Accuracy 0.6737\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8719\t Accuracy 0.6737\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8728\t Accuracy 0.6739\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8723\t Accuracy 0.6743\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8716\t Average training accuracy 0.6751\n",
      "Epoch [9]\t Average validation loss 1.8474\t Average validation accuracy 0.7112\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8313\t Accuracy 0.7700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8602\t Accuracy 0.6931\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8594\t Accuracy 0.6944\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8624\t Accuracy 0.6850\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8639\t Accuracy 0.6840\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8631\t Accuracy 0.6812\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8626\t Accuracy 0.6811\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8656\t Accuracy 0.6791\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8652\t Accuracy 0.6790\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8661\t Accuracy 0.6791\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8658\t Accuracy 0.6799\n",
      "\n",
      "Epoch [10]\t Average training loss 1.8653\t Average training accuracy 0.6806\n",
      "Epoch [10]\t Average validation loss 1.8423\t Average validation accuracy 0.7140\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8268\t Accuracy 0.7700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8553\t Accuracy 0.6951\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8546\t Accuracy 0.6964\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8578\t Accuracy 0.6872\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8595\t Accuracy 0.6862\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8588\t Accuracy 0.6843\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8583\t Accuracy 0.6840\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.8615\t Accuracy 0.6822\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.8611\t Accuracy 0.6824\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.8622\t Accuracy 0.6827\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.8620\t Accuracy 0.6835\n",
      "\n",
      "Epoch [11]\t Average training loss 1.8616\t Average training accuracy 0.6841\n",
      "Epoch [11]\t Average validation loss 1.8396\t Average validation accuracy 0.7176\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8247\t Accuracy 0.7700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.8527\t Accuracy 0.6980\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.8522\t Accuracy 0.7006\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.8554\t Accuracy 0.6911\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.8573\t Accuracy 0.6891\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.8567\t Accuracy 0.6871\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.8563\t Accuracy 0.6863\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.8595\t Accuracy 0.6847\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.8592\t Accuracy 0.6847\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.8604\t Accuracy 0.6851\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.8603\t Accuracy 0.6857\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8599\t Average training accuracy 0.6862\n",
      "Epoch [12]\t Average validation loss 1.8388\t Average validation accuracy 0.7182\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8244\t Accuracy 0.7700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.8519\t Accuracy 0.6986\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.8516\t Accuracy 0.7014\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.8548\t Accuracy 0.6921\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.8568\t Accuracy 0.6900\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.8563\t Accuracy 0.6885\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.8560\t Accuracy 0.6875\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.8592\t Accuracy 0.6858\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.8590\t Accuracy 0.6860\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.8602\t Accuracy 0.6865\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.8602\t Accuracy 0.6870\n",
      "\n",
      "Epoch [13]\t Average training loss 1.8599\t Average training accuracy 0.6875\n",
      "Epoch [13]\t Average validation loss 1.8396\t Average validation accuracy 0.7184\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8256\t Accuracy 0.7700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.8526\t Accuracy 0.6988\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.8524\t Accuracy 0.7011\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.8557\t Accuracy 0.6919\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.8577\t Accuracy 0.6898\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.8573\t Accuracy 0.6887\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.8570\t Accuracy 0.6876\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.8603\t Accuracy 0.6860\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.8601\t Accuracy 0.6861\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.8614\t Accuracy 0.6865\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.8614\t Accuracy 0.6872\n",
      "\n",
      "Epoch [14]\t Average training loss 1.8612\t Average training accuracy 0.6876\n",
      "Epoch [14]\t Average validation loss 1.8416\t Average validation accuracy 0.7180\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.8278\t Accuracy 0.7700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.8544\t Accuracy 0.6965\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.8543\t Accuracy 0.6991\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.8576\t Accuracy 0.6903\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.8597\t Accuracy 0.6884\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.8594\t Accuracy 0.6873\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.8591\t Accuracy 0.6862\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.8623\t Accuracy 0.6846\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.8622\t Accuracy 0.6849\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.8635\t Accuracy 0.6854\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.8636\t Accuracy 0.6861\n",
      "\n",
      "Epoch [15]\t Average training loss 1.8634\t Average training accuracy 0.6864\n",
      "Epoch [15]\t Average validation loss 1.8444\t Average validation accuracy 0.7160\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.8309\t Accuracy 0.7700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.8571\t Accuracy 0.6955\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.8570\t Accuracy 0.6973\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.8603\t Accuracy 0.6889\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.8625\t Accuracy 0.6872\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.8622\t Accuracy 0.6859\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.8619\t Accuracy 0.6849\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.8652\t Accuracy 0.6830\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.8651\t Accuracy 0.6831\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.8664\t Accuracy 0.6837\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.8665\t Accuracy 0.6844\n",
      "\n",
      "Epoch [16]\t Average training loss 1.8664\t Average training accuracy 0.6846\n",
      "Epoch [16]\t Average validation loss 1.8480\t Average validation accuracy 0.7148\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.8347\t Accuracy 0.7700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.8604\t Accuracy 0.6943\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.8604\t Accuracy 0.6955\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.8637\t Accuracy 0.6874\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.8659\t Accuracy 0.6852\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.8656\t Accuracy 0.6843\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.8654\t Accuracy 0.6829\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.8686\t Accuracy 0.6811\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.8686\t Accuracy 0.6813\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.8699\t Accuracy 0.6818\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.8700\t Accuracy 0.6824\n",
      "\n",
      "Epoch [17]\t Average training loss 1.8700\t Average training accuracy 0.6824\n",
      "Epoch [17]\t Average validation loss 1.8520\t Average validation accuracy 0.7128\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.8388\t Accuracy 0.7600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.8643\t Accuracy 0.6918\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.8642\t Accuracy 0.6933\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.8675\t Accuracy 0.6854\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.8698\t Accuracy 0.6826\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.8695\t Accuracy 0.6817\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.8693\t Accuracy 0.6805\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.8725\t Accuracy 0.6785\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.8725\t Accuracy 0.6787\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.8738\t Accuracy 0.6789\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.8739\t Accuracy 0.6795\n",
      "\n",
      "Epoch [18]\t Average training loss 1.8739\t Average training accuracy 0.6796\n",
      "Epoch [18]\t Average validation loss 1.8565\t Average validation accuracy 0.7114\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.8434\t Accuracy 0.7500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.8684\t Accuracy 0.6888\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.8684\t Accuracy 0.6909\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.8716\t Accuracy 0.6834\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.8739\t Accuracy 0.6801\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.8737\t Accuracy 0.6794\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.8735\t Accuracy 0.6779\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.8767\t Accuracy 0.6758\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.8767\t Accuracy 0.6758\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.8780\t Accuracy 0.6759\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.8782\t Accuracy 0.6765\n",
      "\n",
      "Epoch [19]\t Average training loss 1.8782\t Average training accuracy 0.6767\n",
      "Epoch [19]\t Average validation loss 1.8611\t Average validation accuracy 0.7080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss2, sigmoid_acc2 = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.6943.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6942999999999999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 使用Softmax交叉熵损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和ReLU激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4728\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4086\t Accuracy 0.0769\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3452\t Accuracy 0.0962\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2873\t Accuracy 0.1211\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2389\t Accuracy 0.1473\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.1932\t Accuracy 0.1754\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1483\t Accuracy 0.2072\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1120\t Accuracy 0.2336\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.0781\t Accuracy 0.2594\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0473\t Accuracy 0.2831\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0165\t Accuracy 0.3055\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9870\t Average training accuracy 0.3275\n",
      "Epoch [0]\t Average validation loss 1.6407\t Average validation accuracy 0.5750\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6187\t Accuracy 0.6400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.6510\t Accuracy 0.5673\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6303\t Accuracy 0.5777\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6119\t Accuracy 0.5847\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.5968\t Accuracy 0.5919\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.5779\t Accuracy 0.6008\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.5568\t Accuracy 0.6104\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.5446\t Accuracy 0.6164\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5297\t Accuracy 0.6228\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5169\t Accuracy 0.6283\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5025\t Accuracy 0.6339\n",
      "\n",
      "Epoch [1]\t Average training loss 1.4877\t Average training accuracy 0.6403\n",
      "Epoch [1]\t Average validation loss 1.2834\t Average validation accuracy 0.7390\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.2829\t Accuracy 0.7500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3150\t Accuracy 0.7161\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3050\t Accuracy 0.7190\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.2972\t Accuracy 0.7183\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.2910\t Accuracy 0.7202\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.2796\t Accuracy 0.7216\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.2667\t Accuracy 0.7250\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2627\t Accuracy 0.7265\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2542\t Accuracy 0.7282\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2478\t Accuracy 0.7300\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2396\t Accuracy 0.7324\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2305\t Average training accuracy 0.7350\n",
      "Epoch [2]\t Average validation loss 1.0789\t Average validation accuracy 0.8018\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.0910\t Accuracy 0.7900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1212\t Accuracy 0.7702\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1175\t Accuracy 0.7692\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1154\t Accuracy 0.7677\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1137\t Accuracy 0.7678\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1062\t Accuracy 0.7675\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.0978\t Accuracy 0.7702\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.0979\t Accuracy 0.7706\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.0929\t Accuracy 0.7710\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0898\t Accuracy 0.7720\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0849\t Accuracy 0.7737\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0790\t Average training accuracy 0.7759\n",
      "Epoch [3]\t Average validation loss 0.9553\t Average validation accuracy 0.8326\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.9770\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.0036\t Accuracy 0.8049\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.0035\t Accuracy 0.8015\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0049\t Accuracy 0.7985\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0054\t Accuracy 0.7978\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.0001\t Accuracy 0.7977\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.9944\t Accuracy 0.7992\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.9967\t Accuracy 0.7987\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.9936\t Accuracy 0.7985\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.9923\t Accuracy 0.7984\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.9895\t Accuracy 0.7995\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9853\t Average training accuracy 0.8011\n",
      "Epoch [4]\t Average validation loss 0.8776\t Average validation accuracy 0.8528\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.9042\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9292\t Accuracy 0.8198\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9311\t Accuracy 0.8158\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9347\t Accuracy 0.8135\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9365\t Accuracy 0.8126\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9325\t Accuracy 0.8130\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9284\t Accuracy 0.8145\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9320\t Accuracy 0.8140\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9300\t Accuracy 0.8138\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9298\t Accuracy 0.8135\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9281\t Accuracy 0.8142\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9251\t Average training accuracy 0.8157\n",
      "Epoch [5]\t Average validation loss 0.8271\t Average validation accuracy 0.8620\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8566\t Accuracy 0.8900\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8805\t Accuracy 0.8349\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8837\t Accuracy 0.8292\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8887\t Accuracy 0.8252\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8912\t Accuracy 0.8253\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8879\t Accuracy 0.8253\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8849\t Accuracy 0.8265\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8892\t Accuracy 0.8255\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8880\t Accuracy 0.8256\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8884\t Accuracy 0.8251\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8875\t Accuracy 0.8253\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8852\t Average training accuracy 0.8265\n",
      "Epoch [6]\t Average validation loss 0.7934\t Average validation accuracy 0.8720\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8255\t Accuracy 0.8900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8479\t Accuracy 0.8420\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8518\t Accuracy 0.8369\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8578\t Accuracy 0.8328\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8607\t Accuracy 0.8336\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8579\t Accuracy 0.8333\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8556\t Accuracy 0.8344\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8604\t Accuracy 0.8332\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8596\t Accuracy 0.8332\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8605\t Accuracy 0.8326\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8601\t Accuracy 0.8327\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8583\t Average training accuracy 0.8337\n",
      "Epoch [7]\t Average validation loss 0.7708\t Average validation accuracy 0.8766\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8048\t Accuracy 0.9000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8258\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8302\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8368\t Accuracy 0.8405\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8399\t Accuracy 0.8406\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8375\t Accuracy 0.8405\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8356\t Accuracy 0.8414\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8409\t Accuracy 0.8396\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8404\t Accuracy 0.8397\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8415\t Accuracy 0.8393\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8415\t Accuracy 0.8392\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8400\t Average training accuracy 0.8399\n",
      "Epoch [8]\t Average validation loss 0.7556\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7909\t Accuracy 0.8900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8107\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8154\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8225\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8258\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8236\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8221\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8276\t Accuracy 0.8440\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8273\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8286\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8289\t Accuracy 0.8434\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8276\t Average training accuracy 0.8441\n",
      "Epoch [9]\t Average validation loss 0.7455\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7816\t Accuracy 0.9000\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8005\t Accuracy 0.8559\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8055\t Accuracy 0.8526\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8129\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8163\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8143\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8131\t Accuracy 0.8493\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8187\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8186\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8201\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8205\t Accuracy 0.8468\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8194\t Average training accuracy 0.8473\n",
      "Epoch [10]\t Average validation loss 0.7391\t Average validation accuracy 0.8884\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7758\t Accuracy 0.9000\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7939\t Accuracy 0.8600\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.7990\t Accuracy 0.8556\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8067\t Accuracy 0.8516\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8102\t Accuracy 0.8521\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8084\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8073\t Accuracy 0.8523\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8131\t Accuracy 0.8506\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8131\t Accuracy 0.8504\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8146\t Accuracy 0.8502\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8152\t Accuracy 0.8499\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8142\t Average training accuracy 0.8501\n",
      "Epoch [11]\t Average validation loss 0.7354\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7724\t Accuracy 0.9000\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7899\t Accuracy 0.8627\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.7951\t Accuracy 0.8577\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8029\t Accuracy 0.8530\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8065\t Accuracy 0.8532\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8048\t Accuracy 0.8533\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8039\t Accuracy 0.8535\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8098\t Accuracy 0.8521\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8098\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8115\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8121\t Accuracy 0.8516\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8112\t Average training accuracy 0.8519\n",
      "Epoch [12]\t Average validation loss 0.7337\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7709\t Accuracy 0.9000\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7878\t Accuracy 0.8639\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.7931\t Accuracy 0.8597\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8010\t Accuracy 0.8548\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8047\t Accuracy 0.8548\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8030\t Accuracy 0.8551\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8023\t Accuracy 0.8554\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8082\t Accuracy 0.8539\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8083\t Accuracy 0.8537\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8100\t Accuracy 0.8535\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8108\t Accuracy 0.8532\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8099\t Average training accuracy 0.8535\n",
      "Epoch [13]\t Average validation loss 0.7334\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7706\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7871\t Accuracy 0.8647\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7925\t Accuracy 0.8611\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8005\t Accuracy 0.8558\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8042\t Accuracy 0.8555\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8026\t Accuracy 0.8561\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8019\t Accuracy 0.8563\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8079\t Accuracy 0.8550\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8080\t Accuracy 0.8549\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8097\t Accuracy 0.8548\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8106\t Accuracy 0.8544\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8098\t Average training accuracy 0.8546\n",
      "Epoch [14]\t Average validation loss 0.7342\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7712\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7875\t Accuracy 0.8651\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7928\t Accuracy 0.8618\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8009\t Accuracy 0.8564\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8047\t Accuracy 0.8560\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8031\t Accuracy 0.8565\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8025\t Accuracy 0.8567\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8085\t Accuracy 0.8556\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8086\t Accuracy 0.8558\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8104\t Accuracy 0.8557\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8112\t Accuracy 0.8552\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8105\t Average training accuracy 0.8554\n",
      "Epoch [15]\t Average validation loss 0.7356\t Average validation accuracy 0.8926\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7725\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7885\t Accuracy 0.8645\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7939\t Accuracy 0.8618\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8020\t Accuracy 0.8570\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8058\t Accuracy 0.8565\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8042\t Accuracy 0.8571\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8037\t Accuracy 0.8573\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8097\t Accuracy 0.8562\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8099\t Accuracy 0.8564\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8116\t Accuracy 0.8565\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8125\t Accuracy 0.8560\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8118\t Average training accuracy 0.8561\n",
      "Epoch [16]\t Average validation loss 0.7375\t Average validation accuracy 0.8934\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7741\t Accuracy 0.9000\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7901\t Accuracy 0.8649\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7954\t Accuracy 0.8623\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8035\t Accuracy 0.8573\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8074\t Accuracy 0.8572\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8058\t Accuracy 0.8578\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8053\t Accuracy 0.8583\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8114\t Accuracy 0.8572\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8115\t Accuracy 0.8575\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8133\t Accuracy 0.8573\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8142\t Accuracy 0.8568\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8135\t Average training accuracy 0.8570\n",
      "Epoch [17]\t Average validation loss 0.7398\t Average validation accuracy 0.8942\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7760\t Accuracy 0.9000\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7919\t Accuracy 0.8655\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.7973\t Accuracy 0.8628\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8054\t Accuracy 0.8575\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8092\t Accuracy 0.8576\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8077\t Accuracy 0.8582\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8073\t Accuracy 0.8588\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8133\t Accuracy 0.8577\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8135\t Accuracy 0.8581\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8152\t Accuracy 0.8578\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8161\t Accuracy 0.8574\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8155\t Average training accuracy 0.8576\n",
      "Epoch [18]\t Average validation loss 0.7422\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7781\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.7939\t Accuracy 0.8647\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.7993\t Accuracy 0.8623\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8074\t Accuracy 0.8574\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8113\t Accuracy 0.8576\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8097\t Accuracy 0.8584\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8093\t Accuracy 0.8588\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8154\t Accuracy 0.8578\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8155\t Accuracy 0.8582\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8173\t Accuracy 0.8580\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8182\t Accuracy 0.8576\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8176\t Average training accuracy 0.8578\n",
      "Epoch [19]\t Average validation loss 0.7447\t Average validation accuracy 0.8934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss2, relu_acc2 = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8692.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8692000000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss2, sigmoid_acc2],\n",
    "                   'relu': [relu_loss2, relu_acc2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认参数下对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Euclidean+Sigmoid': [sigmoid_loss1, sigmoid_acc1],\n",
    "                   'Euclidean+ReLU': [relu_loss1, relu_acc1],\n",
    "                   'CrossEntropy+Sigmoid':  [sigmoid_loss2, sigmoid_acc2],\n",
    "                   'CrossEntropy+ReLU': [relu_loss2, relu_acc2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3127026087945578, 0.72933, 0.2630535916988853, 0.7961000000000001)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sigmoid_loss1),np.mean(sigmoid_acc1),np.mean(relu_loss1),np.mean(relu_acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9075082560290375, 0.64375, 0.8551942800106801, 0.85483)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sigmoid_loss2),np.mean(sigmoid_acc2),np.mean(relu_loss2),np.mean(relu_acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_loss_and_acc({'Euclidean+Sigmoid': [sigmoid_loss1, sigmoid_acc1],\n",
    "                   'Euclidean+ReLU': [relu_loss1, relu_acc1],\n",
    "                   'CrossEntropy+Sigmoid':  [sigmoid_loss2, sigmoid_acc2],\n",
    "                   'CrossEntropy+ReLU': [relu_loss2, relu_acc2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 具有单层隐含层的多层感知机最佳模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 4.1226\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 1.0670\t Accuracy 0.1020\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 0.8354\t Accuracy 0.1168\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 0.7555\t Accuracy 0.1265\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 0.7079\t Accuracy 0.1358\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 0.6743\t Accuracy 0.1494\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 0.6460\t Accuracy 0.1718\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.6232\t Accuracy 0.1866\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.6051\t Accuracy 0.2020\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.5899\t Accuracy 0.2173\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.5759\t Accuracy 0.2355\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.5614\t Accuracy 0.2584\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.5498\t Accuracy 0.2744\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.5385\t Accuracy 0.2931\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.5283\t Accuracy 0.3074\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.5205\t Accuracy 0.3162\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.5127\t Accuracy 0.3276\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.5052\t Accuracy 0.3408\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.4987\t Accuracy 0.3514\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.4919\t Accuracy 0.3645\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.4855\t Accuracy 0.3753\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.4795\t Accuracy 0.3859\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.4736\t Accuracy 0.3960\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.4680\t Accuracy 0.4061\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.4635\t Accuracy 0.4122\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.4587\t Accuracy 0.4207\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.4549\t Accuracy 0.4280\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.4508\t Accuracy 0.4352\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.4473\t Accuracy 0.4403\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.4441\t Accuracy 0.4461\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.4414\t Accuracy 0.4500\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.4374\t Accuracy 0.4569\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.4345\t Accuracy 0.4620\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.4311\t Accuracy 0.4678\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.4282\t Accuracy 0.4724\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.4253\t Accuracy 0.4780\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.4230\t Accuracy 0.4821\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.4198\t Accuracy 0.4881\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.4169\t Accuracy 0.4936\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.4143\t Accuracy 0.4991\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.4114\t Accuracy 0.5047\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.4090\t Accuracy 0.5089\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.4066\t Accuracy 0.5130\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.4041\t Accuracy 0.5182\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.4014\t Accuracy 0.5228\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.3996\t Accuracy 0.5263\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.3975\t Accuracy 0.5304\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.3954\t Accuracy 0.5346\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.3935\t Accuracy 0.5379\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.3914\t Accuracy 0.5417\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.3897\t Accuracy 0.5445\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.3878\t Accuracy 0.5479\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.3858\t Accuracy 0.5517\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.3841\t Accuracy 0.5547\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.3826\t Accuracy 0.5573\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.3812\t Accuracy 0.5603\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.3792\t Accuracy 0.5640\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.3774\t Accuracy 0.5676\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.3758\t Accuracy 0.5707\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.3744\t Accuracy 0.5737\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.3732\t Accuracy 0.5761\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.3719\t Accuracy 0.5782\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.3707\t Accuracy 0.5804\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.3696\t Accuracy 0.5825\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.3685\t Accuracy 0.5846\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.3676\t Accuracy 0.5862\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.3664\t Accuracy 0.5885\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.3651\t Accuracy 0.5908\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.3635\t Accuracy 0.5939\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.3622\t Accuracy 0.5966\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.3611\t Accuracy 0.5986\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.3599\t Accuracy 0.6012\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.3587\t Accuracy 0.6034\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.3577\t Accuracy 0.6055\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.3564\t Accuracy 0.6081\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.3555\t Accuracy 0.6096\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.3545\t Accuracy 0.6113\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.3533\t Accuracy 0.6136\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.3522\t Accuracy 0.6157\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.3510\t Accuracy 0.6179\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.3502\t Accuracy 0.6193\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.3491\t Accuracy 0.6214\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.3480\t Accuracy 0.6235\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.3472\t Accuracy 0.6249\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.3462\t Accuracy 0.6268\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.3455\t Accuracy 0.6279\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.3445\t Accuracy 0.6293\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.3435\t Accuracy 0.6313\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.3426\t Accuracy 0.6332\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.3419\t Accuracy 0.6345\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.3409\t Accuracy 0.6364\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.3402\t Accuracy 0.6377\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.3395\t Accuracy 0.6392\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.3388\t Accuracy 0.6405\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.3378\t Accuracy 0.6422\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.3372\t Accuracy 0.6432\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.3363\t Accuracy 0.6449\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.3353\t Accuracy 0.6468\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.3345\t Accuracy 0.6481\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.3339\t Accuracy 0.6490\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.3335\t Accuracy 0.6500\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.3327\t Accuracy 0.6513\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.3320\t Accuracy 0.6527\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.3312\t Accuracy 0.6541\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.3304\t Accuracy 0.6558\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.3297\t Accuracy 0.6568\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.3290\t Accuracy 0.6580\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.3282\t Accuracy 0.6594\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.3275\t Accuracy 0.6606\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.3268\t Accuracy 0.6618\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3262\t Average training accuracy 0.6630\n",
      "Epoch [0]\t Average validation loss 0.2384\t Average validation accuracy 0.8380\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.1779\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.2422\t Accuracy 0.8235\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.2519\t Accuracy 0.7990\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.2567\t Accuracy 0.7907\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.2529\t Accuracy 0.7955\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.2493\t Accuracy 0.8036\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.2472\t Accuracy 0.8090\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.2475\t Accuracy 0.8120\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.2476\t Accuracy 0.8115\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.2487\t Accuracy 0.8102\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.2483\t Accuracy 0.8092\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.2473\t Accuracy 0.8109\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.2465\t Accuracy 0.8120\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.2451\t Accuracy 0.8141\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.2444\t Accuracy 0.8141\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.2456\t Accuracy 0.8103\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.2462\t Accuracy 0.8101\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.2468\t Accuracy 0.8098\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.2478\t Accuracy 0.8081\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.2472\t Accuracy 0.8086\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.2466\t Accuracy 0.8095\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.2459\t Accuracy 0.8106\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.2453\t Accuracy 0.8118\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.2447\t Accuracy 0.8128\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.2455\t Accuracy 0.8121\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.2456\t Accuracy 0.8117\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.2461\t Accuracy 0.8112\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.2461\t Accuracy 0.8110\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.2467\t Accuracy 0.8089\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.2476\t Accuracy 0.8074\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.2486\t Accuracy 0.8046\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.2481\t Accuracy 0.8050\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.2486\t Accuracy 0.8041\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.2482\t Accuracy 0.8039\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.2483\t Accuracy 0.8032\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.2483\t Accuracy 0.8031\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.2488\t Accuracy 0.8026\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.2482\t Accuracy 0.8038\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.2476\t Accuracy 0.8054\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.2476\t Accuracy 0.8053\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.2472\t Accuracy 0.8059\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.2470\t Accuracy 0.8062\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.2471\t Accuracy 0.8060\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.2467\t Accuracy 0.8069\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.2461\t Accuracy 0.8081\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.2463\t Accuracy 0.8078\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.2465\t Accuracy 0.8075\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.2463\t Accuracy 0.8080\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.2463\t Accuracy 0.8076\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.2461\t Accuracy 0.8075\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.2463\t Accuracy 0.8072\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.2460\t Accuracy 0.8073\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.2457\t Accuracy 0.8075\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.2456\t Accuracy 0.8076\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.2457\t Accuracy 0.8074\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.2459\t Accuracy 0.8076\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.2455\t Accuracy 0.8083\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.2453\t Accuracy 0.8089\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.2451\t Accuracy 0.8090\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.2450\t Accuracy 0.8088\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.2452\t Accuracy 0.8085\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.2454\t Accuracy 0.8085\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.2455\t Accuracy 0.8080\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.2457\t Accuracy 0.8076\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.2458\t Accuracy 0.8077\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.2461\t Accuracy 0.8068\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.2460\t Accuracy 0.8070\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.2459\t Accuracy 0.8072\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.2454\t Accuracy 0.8080\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.2452\t Accuracy 0.8086\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.2453\t Accuracy 0.8084\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.2451\t Accuracy 0.8088\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.2450\t Accuracy 0.8090\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.2450\t Accuracy 0.8092\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.2448\t Accuracy 0.8097\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.2449\t Accuracy 0.8094\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.2449\t Accuracy 0.8093\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.2447\t Accuracy 0.8097\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.2445\t Accuracy 0.8099\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.2443\t Accuracy 0.8102\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.2444\t Accuracy 0.8098\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.2442\t Accuracy 0.8101\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.2440\t Accuracy 0.8103\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.2441\t Accuracy 0.8100\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.2440\t Accuracy 0.8103\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.2441\t Accuracy 0.8100\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.2440\t Accuracy 0.8100\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.2437\t Accuracy 0.8104\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.2436\t Accuracy 0.8107\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.2437\t Accuracy 0.8106\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.2435\t Accuracy 0.8111\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.2434\t Accuracy 0.8110\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.2435\t Accuracy 0.8112\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.2435\t Accuracy 0.8111\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.2433\t Accuracy 0.8116\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.2433\t Accuracy 0.8112\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.2432\t Accuracy 0.8114\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.2429\t Accuracy 0.8120\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.2428\t Accuracy 0.8122\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.2428\t Accuracy 0.8118\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.2430\t Accuracy 0.8116\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.2429\t Accuracy 0.8116\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.2428\t Accuracy 0.8118\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.2426\t Accuracy 0.8121\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.2423\t Accuracy 0.8126\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.2423\t Accuracy 0.8125\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.2422\t Accuracy 0.8126\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.2420\t Accuracy 0.8130\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.2418\t Accuracy 0.8131\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.2417\t Accuracy 0.8132\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2416\t Average training accuracy 0.8133\n",
      "Epoch [1]\t Average validation loss 0.2136\t Average validation accuracy 0.8686\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.1561\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.2182\t Accuracy 0.8451\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.2284\t Accuracy 0.8307\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.2342\t Accuracy 0.8205\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.2301\t Accuracy 0.8269\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.2264\t Accuracy 0.8351\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.2244\t Accuracy 0.8375\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.2245\t Accuracy 0.8399\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.2246\t Accuracy 0.8406\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.2256\t Accuracy 0.8390\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.2250\t Accuracy 0.8403\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.2244\t Accuracy 0.8414\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.2238\t Accuracy 0.8426\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.2227\t Accuracy 0.8444\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.2222\t Accuracy 0.8444\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.2234\t Accuracy 0.8411\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.2241\t Accuracy 0.8404\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.2249\t Accuracy 0.8400\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.2262\t Accuracy 0.8374\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.2257\t Accuracy 0.8377\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.2250\t Accuracy 0.8385\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.2245\t Accuracy 0.8387\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.2239\t Accuracy 0.8401\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.2235\t Accuracy 0.8408\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.2244\t Accuracy 0.8397\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.2247\t Accuracy 0.8390\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.2253\t Accuracy 0.8379\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.2255\t Accuracy 0.8377\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.2261\t Accuracy 0.8356\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.2271\t Accuracy 0.8338\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.2282\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.2278\t Accuracy 0.8312\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.2284\t Accuracy 0.8300\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.2281\t Accuracy 0.8296\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.2282\t Accuracy 0.8290\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.2283\t Accuracy 0.8288\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.2289\t Accuracy 0.8279\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.2284\t Accuracy 0.8293\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.2278\t Accuracy 0.8309\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.2279\t Accuracy 0.8308\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.2276\t Accuracy 0.8312\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.2275\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.2276\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.2273\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.2268\t Accuracy 0.8325\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.2270\t Accuracy 0.8323\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.2273\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.2273\t Accuracy 0.8319\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.2273\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.2272\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.2274\t Accuracy 0.8311\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.2271\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.2269\t Accuracy 0.8311\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.2269\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.2271\t Accuracy 0.8308\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.2273\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.2270\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.2268\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.2266\t Accuracy 0.8323\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.2267\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.2269\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.2271\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.2273\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.2276\t Accuracy 0.8304\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.2277\t Accuracy 0.8304\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.2280\t Accuracy 0.8295\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.2280\t Accuracy 0.8295\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.2280\t Accuracy 0.8297\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.2275\t Accuracy 0.8304\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.2273\t Accuracy 0.8310\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.2275\t Accuracy 0.8307\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.2274\t Accuracy 0.8311\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.2274\t Accuracy 0.8312\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.2274\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.2272\t Accuracy 0.8319\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.2275\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.2275\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.2274\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.2272\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.2271\t Accuracy 0.8318\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.2273\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.2271\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.2269\t Accuracy 0.8318\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.2271\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.2271\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.2273\t Accuracy 0.8312\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.2272\t Accuracy 0.8311\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.2270\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.2270\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.2271\t Accuracy 0.8313\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.2269\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.2270\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.2270\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.2272\t Accuracy 0.8313\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.2269\t Accuracy 0.8317\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.2270\t Accuracy 0.8314\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.2269\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.2267\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.2266\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.2267\t Accuracy 0.8318\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.2270\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.2270\t Accuracy 0.8315\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.2269\t Accuracy 0.8316\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.2267\t Accuracy 0.8318\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.2265\t Accuracy 0.8322\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.2264\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.2264\t Accuracy 0.8321\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.2262\t Accuracy 0.8323\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.2261\t Accuracy 0.8325\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.2261\t Accuracy 0.8325\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2260\t Average training accuracy 0.8325\n",
      "Epoch [2]\t Average validation loss 0.2029\t Average validation accuracy 0.8796\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.1458\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.2071\t Accuracy 0.8549\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.2174\t Accuracy 0.8416\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.2239\t Accuracy 0.8305\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.2196\t Accuracy 0.8363\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.2159\t Accuracy 0.8426\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.2139\t Accuracy 0.8458\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.2138\t Accuracy 0.8479\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.2139\t Accuracy 0.8499\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.2148\t Accuracy 0.8483\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.2142\t Accuracy 0.8511\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.2136\t Accuracy 0.8515\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.2132\t Accuracy 0.8527\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.2122\t Accuracy 0.8542\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.2118\t Accuracy 0.8535\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.2130\t Accuracy 0.8515\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.2137\t Accuracy 0.8509\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.2145\t Accuracy 0.8504\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.2160\t Accuracy 0.8477\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.2155\t Accuracy 0.8478\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.2148\t Accuracy 0.8486\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.2143\t Accuracy 0.8489\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.2137\t Accuracy 0.8502\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.2134\t Accuracy 0.8510\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.2144\t Accuracy 0.8495\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.2147\t Accuracy 0.8488\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.2153\t Accuracy 0.8473\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.2156\t Accuracy 0.8470\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.2162\t Accuracy 0.8449\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.2172\t Accuracy 0.8432\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.2184\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.2180\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.2186\t Accuracy 0.8397\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.2183\t Accuracy 0.8392\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.2185\t Accuracy 0.8389\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.2185\t Accuracy 0.8384\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.2192\t Accuracy 0.8375\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.2187\t Accuracy 0.8389\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.2181\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.2182\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.2179\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.2178\t Accuracy 0.8412\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.2180\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.2177\t Accuracy 0.8411\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.2172\t Accuracy 0.8419\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.2174\t Accuracy 0.8416\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.2178\t Accuracy 0.8412\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.2178\t Accuracy 0.8415\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.2178\t Accuracy 0.8414\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.2177\t Accuracy 0.8413\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.2179\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.2176\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.2175\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.2175\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.2176\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.2179\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.2176\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.2174\t Accuracy 0.8415\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.2173\t Accuracy 0.8415\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.2173\t Accuracy 0.8412\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.2176\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.2178\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.2181\t Accuracy 0.8400\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.2183\t Accuracy 0.8394\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.2184\t Accuracy 0.8393\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.2188\t Accuracy 0.8385\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.2187\t Accuracy 0.8386\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.2188\t Accuracy 0.8387\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.2183\t Accuracy 0.8395\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.2182\t Accuracy 0.8401\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.2184\t Accuracy 0.8397\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.2183\t Accuracy 0.8402\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.2182\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.2183\t Accuracy 0.8406\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.2181\t Accuracy 0.8412\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.2184\t Accuracy 0.8406\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.2185\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.2184\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.2182\t Accuracy 0.8409\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.2181\t Accuracy 0.8409\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.2183\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.2182\t Accuracy 0.8409\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.2180\t Accuracy 0.8410\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.2182\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.2182\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.2185\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.2184\t Accuracy 0.8402\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.2182\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.2182\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.2183\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.2182\t Accuracy 0.8406\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.2182\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.2183\t Accuracy 0.8404\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.2185\t Accuracy 0.8401\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.2183\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.2184\t Accuracy 0.8402\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.2183\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.2181\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.2180\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.2182\t Accuracy 0.8405\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.2184\t Accuracy 0.8402\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.2184\t Accuracy 0.8401\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.2183\t Accuracy 0.8401\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.2182\t Accuracy 0.8403\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.2180\t Accuracy 0.8407\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.2179\t Accuracy 0.8406\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.2179\t Accuracy 0.8406\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.2178\t Accuracy 0.8408\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.2177\t Accuracy 0.8409\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.2176\t Accuracy 0.8410\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2176\t Average training accuracy 0.8409\n",
      "Epoch [3]\t Average validation loss 0.1964\t Average validation accuracy 0.8842\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.1386\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.1999\t Accuracy 0.8529\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.2104\t Accuracy 0.8495\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.2172\t Accuracy 0.8371\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.2129\t Accuracy 0.8438\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.2092\t Accuracy 0.8498\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.2071\t Accuracy 0.8528\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.2070\t Accuracy 0.8550\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.2071\t Accuracy 0.8566\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.2079\t Accuracy 0.8554\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.2072\t Accuracy 0.8583\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.2067\t Accuracy 0.8586\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.2063\t Accuracy 0.8592\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.2054\t Accuracy 0.8610\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.2051\t Accuracy 0.8603\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.2063\t Accuracy 0.8585\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.2070\t Accuracy 0.8577\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.2079\t Accuracy 0.8573\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.2093\t Accuracy 0.8546\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.2089\t Accuracy 0.8545\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.2082\t Accuracy 0.8554\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.2077\t Accuracy 0.8559\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.2071\t Accuracy 0.8570\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.2068\t Accuracy 0.8577\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.2079\t Accuracy 0.8562\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.2082\t Accuracy 0.8556\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.2088\t Accuracy 0.8540\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.2091\t Accuracy 0.8537\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.2097\t Accuracy 0.8515\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.2108\t Accuracy 0.8500\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.2119\t Accuracy 0.8475\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.2116\t Accuracy 0.8477\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.2121\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.2119\t Accuracy 0.8464\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.2120\t Accuracy 0.8463\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.2121\t Accuracy 0.8457\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.2128\t Accuracy 0.8449\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.2123\t Accuracy 0.8462\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.2117\t Accuracy 0.8476\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.2118\t Accuracy 0.8473\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.2115\t Accuracy 0.8479\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.2115\t Accuracy 0.8483\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.2117\t Accuracy 0.8477\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.2113\t Accuracy 0.8480\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.2108\t Accuracy 0.8487\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.2111\t Accuracy 0.8482\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.2114\t Accuracy 0.8477\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.2114\t Accuracy 0.8479\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.2115\t Accuracy 0.8481\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.2114\t Accuracy 0.8479\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.2116\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.2113\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.2112\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.2112\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.2114\t Accuracy 0.8471\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.2116\t Accuracy 0.8470\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.2114\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.2111\t Accuracy 0.8481\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.2111\t Accuracy 0.8482\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.2111\t Accuracy 0.8478\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.2114\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.2116\t Accuracy 0.8472\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.2118\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.2121\t Accuracy 0.8461\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.2122\t Accuracy 0.8460\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.2126\t Accuracy 0.8452\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.2125\t Accuracy 0.8455\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.2126\t Accuracy 0.8456\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.2121\t Accuracy 0.8463\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.2120\t Accuracy 0.8470\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.2122\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.2121\t Accuracy 0.8469\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.2121\t Accuracy 0.8471\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.2121\t Accuracy 0.8472\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.2119\t Accuracy 0.8478\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.2123\t Accuracy 0.8472\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.2123\t Accuracy 0.8471\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.2123\t Accuracy 0.8473\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.2121\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.2120\t Accuracy 0.8475\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.2122\t Accuracy 0.8473\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.2121\t Accuracy 0.8474\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.2120\t Accuracy 0.8475\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.2122\t Accuracy 0.8470\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.2121\t Accuracy 0.8473\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.2124\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.2124\t Accuracy 0.8467\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.2122\t Accuracy 0.8469\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.2122\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.2123\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.2122\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.2123\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.2124\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.2125\t Accuracy 0.8463\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.2123\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.2124\t Accuracy 0.8463\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.2124\t Accuracy 0.8464\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.2122\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.2121\t Accuracy 0.8468\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.2122\t Accuracy 0.8465\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.2125\t Accuracy 0.8462\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.2125\t Accuracy 0.8459\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.2124\t Accuracy 0.8460\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.2123\t Accuracy 0.8462\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.2121\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.2121\t Accuracy 0.8464\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.2121\t Accuracy 0.8464\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.2119\t Accuracy 0.8466\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.2119\t Accuracy 0.8467\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.2118\t Accuracy 0.8468\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2118\t Average training accuracy 0.8468\n",
      "Epoch [4]\t Average validation loss 0.1915\t Average validation accuracy 0.8862\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.1330\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.1945\t Accuracy 0.8608\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.2051\t Accuracy 0.8584\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.2122\t Accuracy 0.8470\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.2078\t Accuracy 0.8512\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.2041\t Accuracy 0.8578\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.2020\t Accuracy 0.8598\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.2018\t Accuracy 0.8618\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.2019\t Accuracy 0.8638\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.2027\t Accuracy 0.8636\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.2020\t Accuracy 0.8659\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.2015\t Accuracy 0.8657\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.2011\t Accuracy 0.8659\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.2003\t Accuracy 0.8671\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.2000\t Accuracy 0.8663\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.2012\t Accuracy 0.8640\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.2019\t Accuracy 0.8629\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.2028\t Accuracy 0.8623\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.2043\t Accuracy 0.8596\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.2039\t Accuracy 0.8595\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.2031\t Accuracy 0.8603\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.2026\t Accuracy 0.8610\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.2021\t Accuracy 0.8619\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.2018\t Accuracy 0.8623\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.2029\t Accuracy 0.8609\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.2032\t Accuracy 0.8606\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.2039\t Accuracy 0.8590\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.2041\t Accuracy 0.8587\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.2048\t Accuracy 0.8566\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.2058\t Accuracy 0.8552\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.2069\t Accuracy 0.8526\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.2066\t Accuracy 0.8528\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.2072\t Accuracy 0.8515\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.2070\t Accuracy 0.8514\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.2071\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.2072\t Accuracy 0.8503\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.2079\t Accuracy 0.8496\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.2074\t Accuracy 0.8509\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.2068\t Accuracy 0.8523\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.2069\t Accuracy 0.8520\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.2066\t Accuracy 0.8527\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.2065\t Accuracy 0.8531\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.2067\t Accuracy 0.8525\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.2064\t Accuracy 0.8528\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.2059\t Accuracy 0.8534\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.2062\t Accuracy 0.8530\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.2065\t Accuracy 0.8525\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.2066\t Accuracy 0.8527\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.2066\t Accuracy 0.8528\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.2065\t Accuracy 0.8525\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.2067\t Accuracy 0.8521\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.2064\t Accuracy 0.8522\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.2063\t Accuracy 0.8522\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.2063\t Accuracy 0.8521\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.2065\t Accuracy 0.8519\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.2067\t Accuracy 0.8519\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.2065\t Accuracy 0.8522\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.2063\t Accuracy 0.8528\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.2062\t Accuracy 0.8529\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.2062\t Accuracy 0.8526\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.2065\t Accuracy 0.8522\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.2067\t Accuracy 0.8520\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.2070\t Accuracy 0.8515\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.2073\t Accuracy 0.8507\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.2074\t Accuracy 0.8505\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.2078\t Accuracy 0.8496\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.2077\t Accuracy 0.8499\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.2077\t Accuracy 0.8500\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.2073\t Accuracy 0.8507\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.2072\t Accuracy 0.8513\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.2073\t Accuracy 0.8508\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.2073\t Accuracy 0.8511\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.2072\t Accuracy 0.8511\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.2073\t Accuracy 0.8511\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.2071\t Accuracy 0.8518\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.2075\t Accuracy 0.8512\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.2075\t Accuracy 0.8511\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.2074\t Accuracy 0.8513\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.2073\t Accuracy 0.8514\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.2072\t Accuracy 0.8515\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.2074\t Accuracy 0.8514\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.2073\t Accuracy 0.8515\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.2072\t Accuracy 0.8517\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.2074\t Accuracy 0.8512\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.2074\t Accuracy 0.8515\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.2077\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.2076\t Accuracy 0.8509\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.2075\t Accuracy 0.8512\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.2074\t Accuracy 0.8511\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.2076\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.2075\t Accuracy 0.8512\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.2075\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.2077\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.2078\t Accuracy 0.8507\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.2076\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.2077\t Accuracy 0.8506\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.2077\t Accuracy 0.8506\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.2075\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.2074\t Accuracy 0.8510\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.2075\t Accuracy 0.8507\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.2078\t Accuracy 0.8504\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.2078\t Accuracy 0.8501\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.2077\t Accuracy 0.8502\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.2076\t Accuracy 0.8504\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.2074\t Accuracy 0.8508\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.2074\t Accuracy 0.8506\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.2074\t Accuracy 0.8506\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.2073\t Accuracy 0.8506\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.2072\t Accuracy 0.8507\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.2072\t Accuracy 0.8508\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2071\t Average training accuracy 0.8507\n",
      "Epoch [5]\t Average validation loss 0.1874\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.1282\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.1901\t Accuracy 0.8647\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.2008\t Accuracy 0.8624\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.2080\t Accuracy 0.8523\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.2035\t Accuracy 0.8557\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.1998\t Accuracy 0.8618\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.1977\t Accuracy 0.8641\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.1975\t Accuracy 0.8658\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.1975\t Accuracy 0.8676\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.1983\t Accuracy 0.8685\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.1976\t Accuracy 0.8707\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.1971\t Accuracy 0.8708\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.1968\t Accuracy 0.8710\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.1959\t Accuracy 0.8720\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.1958\t Accuracy 0.8710\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.1970\t Accuracy 0.8684\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.1976\t Accuracy 0.8669\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.1985\t Accuracy 0.8660\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.2000\t Accuracy 0.8634\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.1996\t Accuracy 0.8630\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.1988\t Accuracy 0.8637\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.1984\t Accuracy 0.8644\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.1978\t Accuracy 0.8655\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.1975\t Accuracy 0.8658\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.1986\t Accuracy 0.8641\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.1990\t Accuracy 0.8638\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.1996\t Accuracy 0.8624\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.1999\t Accuracy 0.8620\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.2006\t Accuracy 0.8600\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.2016\t Accuracy 0.8588\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.2027\t Accuracy 0.8564\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.2024\t Accuracy 0.8565\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.2029\t Accuracy 0.8553\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.2027\t Accuracy 0.8555\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.2029\t Accuracy 0.8551\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.2030\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.2037\t Accuracy 0.8536\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.2032\t Accuracy 0.8548\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.2026\t Accuracy 0.8561\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.2027\t Accuracy 0.8558\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.2024\t Accuracy 0.8564\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.2023\t Accuracy 0.8568\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.2025\t Accuracy 0.8562\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.2022\t Accuracy 0.8566\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.2017\t Accuracy 0.8571\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.2020\t Accuracy 0.8567\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.2024\t Accuracy 0.8563\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.2024\t Accuracy 0.8564\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.2024\t Accuracy 0.8565\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.2024\t Accuracy 0.8562\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.2026\t Accuracy 0.8558\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.2022\t Accuracy 0.8559\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.2021\t Accuracy 0.8558\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.2021\t Accuracy 0.8558\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.2023\t Accuracy 0.8556\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.2025\t Accuracy 0.8556\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.2023\t Accuracy 0.8559\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.2021\t Accuracy 0.8564\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.2020\t Accuracy 0.8565\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.2021\t Accuracy 0.8561\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.2024\t Accuracy 0.8556\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.2026\t Accuracy 0.8554\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.2028\t Accuracy 0.8549\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.2031\t Accuracy 0.8541\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.2032\t Accuracy 0.8540\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.2036\t Accuracy 0.8532\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.2035\t Accuracy 0.8534\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.2036\t Accuracy 0.8535\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.2031\t Accuracy 0.8542\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.2030\t Accuracy 0.8547\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.2032\t Accuracy 0.8542\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.2031\t Accuracy 0.8545\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.2031\t Accuracy 0.8545\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.2031\t Accuracy 0.8546\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.2030\t Accuracy 0.8552\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.2033\t Accuracy 0.8545\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.2034\t Accuracy 0.8545\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.2033\t Accuracy 0.8547\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.2031\t Accuracy 0.8549\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.2031\t Accuracy 0.8549\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.2033\t Accuracy 0.8548\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.2032\t Accuracy 0.8550\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.2030\t Accuracy 0.8550\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.2033\t Accuracy 0.8546\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.2032\t Accuracy 0.8549\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.2035\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.2035\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.2034\t Accuracy 0.8547\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.2033\t Accuracy 0.8546\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.2035\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.2034\t Accuracy 0.8546\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.2034\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.2036\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.2037\t Accuracy 0.8541\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.2035\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.2036\t Accuracy 0.8540\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.2036\t Accuracy 0.8539\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.2034\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.2033\t Accuracy 0.8544\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.2034\t Accuracy 0.8541\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.2037\t Accuracy 0.8537\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.2038\t Accuracy 0.8535\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.2037\t Accuracy 0.8535\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.2035\t Accuracy 0.8537\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.2033\t Accuracy 0.8540\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.2033\t Accuracy 0.8539\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.2033\t Accuracy 0.8539\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.2032\t Accuracy 0.8540\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.2031\t Accuracy 0.8540\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.2031\t Accuracy 0.8541\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2031\t Average training accuracy 0.8539\n",
      "Epoch [6]\t Average validation loss 0.1837\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.1238\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.1862\t Accuracy 0.8667\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.1969\t Accuracy 0.8673\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.2043\t Accuracy 0.8556\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.1996\t Accuracy 0.8592\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.1960\t Accuracy 0.8649\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.1939\t Accuracy 0.8671\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.1936\t Accuracy 0.8684\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.1936\t Accuracy 0.8703\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.1944\t Accuracy 0.8714\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.1936\t Accuracy 0.8737\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.1932\t Accuracy 0.8735\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.1928\t Accuracy 0.8734\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.1920\t Accuracy 0.8743\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.1919\t Accuracy 0.8735\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.1931\t Accuracy 0.8715\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.1938\t Accuracy 0.8702\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.1947\t Accuracy 0.8693\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.1961\t Accuracy 0.8667\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.1958\t Accuracy 0.8666\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.1950\t Accuracy 0.8670\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.1945\t Accuracy 0.8676\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.1940\t Accuracy 0.8685\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.1937\t Accuracy 0.8687\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.1948\t Accuracy 0.8670\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.1951\t Accuracy 0.8667\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.1958\t Accuracy 0.8653\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.1961\t Accuracy 0.8650\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.1967\t Accuracy 0.8633\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.1977\t Accuracy 0.8621\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.1988\t Accuracy 0.8598\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.1986\t Accuracy 0.8602\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.1991\t Accuracy 0.8590\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.1989\t Accuracy 0.8591\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.1991\t Accuracy 0.8588\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.1992\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.1999\t Accuracy 0.8576\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.1994\t Accuracy 0.8587\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.1988\t Accuracy 0.8600\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.1989\t Accuracy 0.8598\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.1985\t Accuracy 0.8603\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.1985\t Accuracy 0.8608\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.1987\t Accuracy 0.8602\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.1984\t Accuracy 0.8606\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.1979\t Accuracy 0.8612\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.1981\t Accuracy 0.8609\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.1985\t Accuracy 0.8604\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.1985\t Accuracy 0.8605\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.1986\t Accuracy 0.8608\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.1985\t Accuracy 0.8604\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.1987\t Accuracy 0.8599\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.1984\t Accuracy 0.8600\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.1983\t Accuracy 0.8600\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.1983\t Accuracy 0.8600\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.1985\t Accuracy 0.8596\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.1987\t Accuracy 0.8595\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.1985\t Accuracy 0.8597\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.1983\t Accuracy 0.8602\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.1982\t Accuracy 0.8602\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.1982\t Accuracy 0.8598\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.1985\t Accuracy 0.8594\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.1987\t Accuracy 0.8593\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.1990\t Accuracy 0.8588\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.1993\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.1994\t Accuracy 0.8580\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.1998\t Accuracy 0.8572\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.1997\t Accuracy 0.8574\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.1997\t Accuracy 0.8574\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.1993\t Accuracy 0.8580\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.1992\t Accuracy 0.8586\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.1994\t Accuracy 0.8580\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.1993\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.1992\t Accuracy 0.8583\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.1993\t Accuracy 0.8584\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.1991\t Accuracy 0.8590\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.1995\t Accuracy 0.8584\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.1995\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.1995\t Accuracy 0.8583\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.1993\t Accuracy 0.8585\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.1993\t Accuracy 0.8586\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.1994\t Accuracy 0.8585\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.1994\t Accuracy 0.8586\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.1992\t Accuracy 0.8587\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.1994\t Accuracy 0.8583\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.1994\t Accuracy 0.8586\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.1997\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.1997\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.1996\t Accuracy 0.8585\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.1995\t Accuracy 0.8583\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.1997\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.1996\t Accuracy 0.8584\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.1997\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.1998\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.2000\t Accuracy 0.8578\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.1997\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.1999\t Accuracy 0.8578\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.1998\t Accuracy 0.8577\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.1996\t Accuracy 0.8581\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.1995\t Accuracy 0.8582\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.1997\t Accuracy 0.8579\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.1999\t Accuracy 0.8575\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.2000\t Accuracy 0.8573\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.1999\t Accuracy 0.8573\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.1997\t Accuracy 0.8575\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.1996\t Accuracy 0.8577\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.1996\t Accuracy 0.8576\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.1996\t Accuracy 0.8575\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.1994\t Accuracy 0.8576\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.1994\t Accuracy 0.8577\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.1993\t Accuracy 0.8578\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1993\t Average training accuracy 0.8576\n",
      "Epoch [7]\t Average validation loss 0.1802\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.1196\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.1826\t Accuracy 0.8667\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.1933\t Accuracy 0.8673\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.2007\t Accuracy 0.8570\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.1959\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.1923\t Accuracy 0.8665\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.1902\t Accuracy 0.8691\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.1899\t Accuracy 0.8698\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.1899\t Accuracy 0.8723\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.1907\t Accuracy 0.8732\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.1899\t Accuracy 0.8756\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.1895\t Accuracy 0.8753\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.1891\t Accuracy 0.8754\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.1883\t Accuracy 0.8763\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.1882\t Accuracy 0.8755\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.1894\t Accuracy 0.8740\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.1901\t Accuracy 0.8727\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.1910\t Accuracy 0.8720\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.1924\t Accuracy 0.8693\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.1921\t Accuracy 0.8691\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.1913\t Accuracy 0.8694\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.1908\t Accuracy 0.8702\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.1903\t Accuracy 0.8710\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.1900\t Accuracy 0.8712\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.1912\t Accuracy 0.8693\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.1915\t Accuracy 0.8691\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.1921\t Accuracy 0.8677\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.1925\t Accuracy 0.8675\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.1931\t Accuracy 0.8658\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.1941\t Accuracy 0.8645\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.1952\t Accuracy 0.8622\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.1949\t Accuracy 0.8626\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.1954\t Accuracy 0.8613\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.1952\t Accuracy 0.8613\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.1954\t Accuracy 0.8611\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.1955\t Accuracy 0.8605\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.1962\t Accuracy 0.8599\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.1957\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.1951\t Accuracy 0.8624\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.1952\t Accuracy 0.8623\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.1949\t Accuracy 0.8629\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.1948\t Accuracy 0.8635\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.1950\t Accuracy 0.8630\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.1947\t Accuracy 0.8633\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.1942\t Accuracy 0.8639\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.1945\t Accuracy 0.8636\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.1948\t Accuracy 0.8631\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.1948\t Accuracy 0.8633\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.1949\t Accuracy 0.8637\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.1948\t Accuracy 0.8633\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.1951\t Accuracy 0.8629\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.1947\t Accuracy 0.8631\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.1946\t Accuracy 0.8630\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.1946\t Accuracy 0.8630\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.1948\t Accuracy 0.8626\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.1950\t Accuracy 0.8625\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.1948\t Accuracy 0.8627\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.1946\t Accuracy 0.8631\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.1945\t Accuracy 0.8632\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.1946\t Accuracy 0.8628\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.1949\t Accuracy 0.8622\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.1951\t Accuracy 0.8620\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.1953\t Accuracy 0.8615\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.1956\t Accuracy 0.8609\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.1957\t Accuracy 0.8607\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.1961\t Accuracy 0.8600\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.1960\t Accuracy 0.8602\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.1961\t Accuracy 0.8603\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.1956\t Accuracy 0.8609\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.1955\t Accuracy 0.8614\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.1957\t Accuracy 0.8608\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.1956\t Accuracy 0.8610\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.1956\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.1956\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.1954\t Accuracy 0.8620\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.1958\t Accuracy 0.8614\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.1959\t Accuracy 0.8613\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.1958\t Accuracy 0.8614\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.1956\t Accuracy 0.8616\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.1956\t Accuracy 0.8616\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.1958\t Accuracy 0.8615\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.1957\t Accuracy 0.8616\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.1955\t Accuracy 0.8616\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.1958\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.1957\t Accuracy 0.8616\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.1960\t Accuracy 0.8610\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.1960\t Accuracy 0.8611\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.1959\t Accuracy 0.8615\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.1959\t Accuracy 0.8614\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.1960\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.1959\t Accuracy 0.8613\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.1960\t Accuracy 0.8611\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.1961\t Accuracy 0.8611\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.1963\t Accuracy 0.8608\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.1961\t Accuracy 0.8610\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.1962\t Accuracy 0.8608\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.1961\t Accuracy 0.8607\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.1959\t Accuracy 0.8611\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.1959\t Accuracy 0.8612\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.1960\t Accuracy 0.8610\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.1962\t Accuracy 0.8606\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.1963\t Accuracy 0.8603\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.1962\t Accuracy 0.8604\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.1961\t Accuracy 0.8606\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.1959\t Accuracy 0.8609\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.1959\t Accuracy 0.8607\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.1959\t Accuracy 0.8606\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.1958\t Accuracy 0.8607\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.1957\t Accuracy 0.8608\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.1957\t Accuracy 0.8609\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1956\t Average training accuracy 0.8608\n",
      "Epoch [8]\t Average validation loss 0.1767\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.1157\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.1791\t Accuracy 0.8706\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.1897\t Accuracy 0.8723\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.1972\t Accuracy 0.8616\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.1923\t Accuracy 0.8657\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.1888\t Accuracy 0.8713\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.1867\t Accuracy 0.8731\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.1864\t Accuracy 0.8746\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.1863\t Accuracy 0.8768\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.1870\t Accuracy 0.8774\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.1862\t Accuracy 0.8796\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.1858\t Accuracy 0.8795\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.1855\t Accuracy 0.8794\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.1846\t Accuracy 0.8803\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.1846\t Accuracy 0.8795\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.1858\t Accuracy 0.8779\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.1865\t Accuracy 0.8763\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.1874\t Accuracy 0.8756\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.1888\t Accuracy 0.8725\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.1885\t Accuracy 0.8725\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.1877\t Accuracy 0.8726\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.1872\t Accuracy 0.8735\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.1867\t Accuracy 0.8742\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.1864\t Accuracy 0.8743\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.1876\t Accuracy 0.8722\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.1879\t Accuracy 0.8718\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.1885\t Accuracy 0.8705\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.1888\t Accuracy 0.8702\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.1894\t Accuracy 0.8687\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.1904\t Accuracy 0.8673\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.1916\t Accuracy 0.8654\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.1913\t Accuracy 0.8657\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.1918\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.1916\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.1918\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.1919\t Accuracy 0.8640\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.1926\t Accuracy 0.8634\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.1921\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.1915\t Accuracy 0.8659\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.1916\t Accuracy 0.8659\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.1912\t Accuracy 0.8666\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.1911\t Accuracy 0.8673\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.1914\t Accuracy 0.8669\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.1911\t Accuracy 0.8673\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.1905\t Accuracy 0.8678\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.1908\t Accuracy 0.8675\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.1912\t Accuracy 0.8670\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.1912\t Accuracy 0.8672\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.1912\t Accuracy 0.8676\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.1912\t Accuracy 0.8672\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.1914\t Accuracy 0.8668\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.1911\t Accuracy 0.8670\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.1910\t Accuracy 0.8669\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.1909\t Accuracy 0.8669\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.1911\t Accuracy 0.8665\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.1914\t Accuracy 0.8664\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.1911\t Accuracy 0.8666\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.1909\t Accuracy 0.8670\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.1908\t Accuracy 0.8670\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.1909\t Accuracy 0.8667\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.1912\t Accuracy 0.8661\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.1914\t Accuracy 0.8658\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.1917\t Accuracy 0.8653\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.1919\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.1921\t Accuracy 0.8644\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.1925\t Accuracy 0.8637\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.1924\t Accuracy 0.8639\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.1924\t Accuracy 0.8639\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.1920\t Accuracy 0.8645\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.1918\t Accuracy 0.8650\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.1920\t Accuracy 0.8644\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.1919\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.1919\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.1919\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.1918\t Accuracy 0.8654\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.1922\t Accuracy 0.8648\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.1922\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.1921\t Accuracy 0.8649\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.1920\t Accuracy 0.8651\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.1919\t Accuracy 0.8650\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.1921\t Accuracy 0.8649\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.1920\t Accuracy 0.8649\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.1919\t Accuracy 0.8650\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.1921\t Accuracy 0.8647\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.1921\t Accuracy 0.8650\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.1924\t Accuracy 0.8645\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.1924\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.1922\t Accuracy 0.8649\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.1922\t Accuracy 0.8648\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.1924\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.1923\t Accuracy 0.8648\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.1923\t Accuracy 0.8644\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.1925\t Accuracy 0.8644\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.1927\t Accuracy 0.8641\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.1924\t Accuracy 0.8644\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.1925\t Accuracy 0.8642\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.1925\t Accuracy 0.8641\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.1923\t Accuracy 0.8645\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.1922\t Accuracy 0.8646\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.1923\t Accuracy 0.8643\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.1926\t Accuracy 0.8639\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.1927\t Accuracy 0.8637\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.1926\t Accuracy 0.8637\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.1924\t Accuracy 0.8638\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.1923\t Accuracy 0.8641\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.1923\t Accuracy 0.8640\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.1923\t Accuracy 0.8638\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.1921\t Accuracy 0.8639\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.1921\t Accuracy 0.8640\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.1920\t Accuracy 0.8640\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1920\t Average training accuracy 0.8639\n",
      "Epoch [9]\t Average validation loss 0.1731\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.1118\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.1756\t Accuracy 0.8725\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.1862\t Accuracy 0.8733\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.1936\t Accuracy 0.8623\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.1887\t Accuracy 0.8672\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.1852\t Accuracy 0.8729\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.1831\t Accuracy 0.8754\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.1828\t Accuracy 0.8766\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.1827\t Accuracy 0.8791\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.1833\t Accuracy 0.8794\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.1825\t Accuracy 0.8814\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.1821\t Accuracy 0.8811\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.1818\t Accuracy 0.8810\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.1810\t Accuracy 0.8819\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.1810\t Accuracy 0.8809\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.1822\t Accuracy 0.8794\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.1829\t Accuracy 0.8779\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.1837\t Accuracy 0.8773\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.1852\t Accuracy 0.8747\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.1848\t Accuracy 0.8746\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.1841\t Accuracy 0.8746\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.1836\t Accuracy 0.8755\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.1831\t Accuracy 0.8763\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.1828\t Accuracy 0.8764\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.1839\t Accuracy 0.8744\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.1842\t Accuracy 0.8739\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.1849\t Accuracy 0.8726\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.1852\t Accuracy 0.8723\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.1858\t Accuracy 0.8707\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.1868\t Accuracy 0.8693\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.1879\t Accuracy 0.8674\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.1876\t Accuracy 0.8678\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.1882\t Accuracy 0.8668\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.1879\t Accuracy 0.8669\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.1881\t Accuracy 0.8667\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.1882\t Accuracy 0.8662\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.1889\t Accuracy 0.8656\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.1885\t Accuracy 0.8668\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.1878\t Accuracy 0.8682\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.1879\t Accuracy 0.8683\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.1875\t Accuracy 0.8691\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.1875\t Accuracy 0.8698\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.1877\t Accuracy 0.8694\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.1874\t Accuracy 0.8698\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.1869\t Accuracy 0.8703\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.1872\t Accuracy 0.8699\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.1875\t Accuracy 0.8693\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.1875\t Accuracy 0.8695\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.1876\t Accuracy 0.8698\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.1875\t Accuracy 0.8695\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.1877\t Accuracy 0.8692\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.1874\t Accuracy 0.8694\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.1873\t Accuracy 0.8694\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.1872\t Accuracy 0.8694\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.1874\t Accuracy 0.8690\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.1877\t Accuracy 0.8689\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.1874\t Accuracy 0.8691\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.1872\t Accuracy 0.8696\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.1872\t Accuracy 0.8696\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.1872\t Accuracy 0.8693\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.1875\t Accuracy 0.8687\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.1877\t Accuracy 0.8685\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.1880\t Accuracy 0.8679\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.1882\t Accuracy 0.8673\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.1884\t Accuracy 0.8671\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.1888\t Accuracy 0.8664\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.1887\t Accuracy 0.8667\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.1887\t Accuracy 0.8668\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.1883\t Accuracy 0.8674\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.1881\t Accuracy 0.8679\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.1883\t Accuracy 0.8674\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.1882\t Accuracy 0.8676\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.1882\t Accuracy 0.8677\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.1882\t Accuracy 0.8677\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.1881\t Accuracy 0.8684\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.1884\t Accuracy 0.8678\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.1885\t Accuracy 0.8677\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.1884\t Accuracy 0.8679\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.1883\t Accuracy 0.8681\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.1882\t Accuracy 0.8681\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.1884\t Accuracy 0.8678\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.1883\t Accuracy 0.8679\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.1882\t Accuracy 0.8680\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.1884\t Accuracy 0.8676\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.1884\t Accuracy 0.8679\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.1887\t Accuracy 0.8673\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.1887\t Accuracy 0.8674\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.1885\t Accuracy 0.8678\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.1885\t Accuracy 0.8677\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.1887\t Accuracy 0.8674\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.1886\t Accuracy 0.8675\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.1887\t Accuracy 0.8672\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.1888\t Accuracy 0.8672\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.1890\t Accuracy 0.8668\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.1887\t Accuracy 0.8672\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.1888\t Accuracy 0.8670\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.1888\t Accuracy 0.8669\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.1886\t Accuracy 0.8673\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.1885\t Accuracy 0.8673\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.1886\t Accuracy 0.8671\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.1889\t Accuracy 0.8666\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.1890\t Accuracy 0.8664\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.1889\t Accuracy 0.8664\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.1887\t Accuracy 0.8666\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.1886\t Accuracy 0.8669\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.1886\t Accuracy 0.8667\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.1886\t Accuracy 0.8666\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.1885\t Accuracy 0.8667\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.1884\t Accuracy 0.8668\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.1883\t Accuracy 0.8668\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1883\t Average training accuracy 0.8667\n",
      "Epoch [10]\t Average validation loss 0.1695\t Average validation accuracy 0.8974\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.1079\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.1720\t Accuracy 0.8824\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.1826\t Accuracy 0.8782\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.1900\t Accuracy 0.8662\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.1850\t Accuracy 0.8711\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.1816\t Accuracy 0.8761\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.1795\t Accuracy 0.8784\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.1791\t Accuracy 0.8795\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.1790\t Accuracy 0.8815\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.1796\t Accuracy 0.8818\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.1788\t Accuracy 0.8840\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.1784\t Accuracy 0.8835\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.1781\t Accuracy 0.8837\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.1772\t Accuracy 0.8845\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.1773\t Accuracy 0.8836\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.1786\t Accuracy 0.8820\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.1792\t Accuracy 0.8806\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.1800\t Accuracy 0.8798\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.1814\t Accuracy 0.8771\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.1811\t Accuracy 0.8769\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.1804\t Accuracy 0.8771\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.1799\t Accuracy 0.8781\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.1794\t Accuracy 0.8792\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.1791\t Accuracy 0.8794\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.1802\t Accuracy 0.8774\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.1805\t Accuracy 0.8770\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.1812\t Accuracy 0.8756\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.1815\t Accuracy 0.8754\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.1821\t Accuracy 0.8737\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.1830\t Accuracy 0.8724\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.1842\t Accuracy 0.8704\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.1839\t Accuracy 0.8708\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.1844\t Accuracy 0.8697\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.1842\t Accuracy 0.8700\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.1844\t Accuracy 0.8698\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.1845\t Accuracy 0.8693\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.1852\t Accuracy 0.8686\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.1847\t Accuracy 0.8699\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.1841\t Accuracy 0.8712\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.1841\t Accuracy 0.8714\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.1838\t Accuracy 0.8721\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.1837\t Accuracy 0.8727\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.1839\t Accuracy 0.8724\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.1836\t Accuracy 0.8727\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.1831\t Accuracy 0.8732\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.1834\t Accuracy 0.8729\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.1837\t Accuracy 0.8723\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.1837\t Accuracy 0.8724\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.1838\t Accuracy 0.8728\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.1837\t Accuracy 0.8725\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.1840\t Accuracy 0.8722\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.1836\t Accuracy 0.8723\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.1835\t Accuracy 0.8722\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.1834\t Accuracy 0.8723\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.1836\t Accuracy 0.8720\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.1839\t Accuracy 0.8719\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.1837\t Accuracy 0.8720\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.1834\t Accuracy 0.8725\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.1834\t Accuracy 0.8726\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.1834\t Accuracy 0.8722\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.1838\t Accuracy 0.8716\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.1839\t Accuracy 0.8715\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.1842\t Accuracy 0.8709\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.1845\t Accuracy 0.8704\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.1846\t Accuracy 0.8701\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.1850\t Accuracy 0.8695\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.1849\t Accuracy 0.8698\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.1849\t Accuracy 0.8699\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.1845\t Accuracy 0.8704\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.1844\t Accuracy 0.8708\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.1845\t Accuracy 0.8703\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.1845\t Accuracy 0.8705\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.1844\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.1844\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.1843\t Accuracy 0.8712\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.1847\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.1847\t Accuracy 0.8705\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.1847\t Accuracy 0.8707\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.1845\t Accuracy 0.8709\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.1844\t Accuracy 0.8708\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.1846\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.1845\t Accuracy 0.8707\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.1844\t Accuracy 0.8708\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.1846\t Accuracy 0.8704\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.1846\t Accuracy 0.8707\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.1849\t Accuracy 0.8702\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.1849\t Accuracy 0.8703\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.1848\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.1847\t Accuracy 0.8706\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.1849\t Accuracy 0.8702\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.1848\t Accuracy 0.8703\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.1849\t Accuracy 0.8700\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.1850\t Accuracy 0.8700\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.1852\t Accuracy 0.8697\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.1850\t Accuracy 0.8700\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.1851\t Accuracy 0.8698\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.1850\t Accuracy 0.8698\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.1848\t Accuracy 0.8702\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.1847\t Accuracy 0.8702\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.1849\t Accuracy 0.8699\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.1851\t Accuracy 0.8695\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.1852\t Accuracy 0.8693\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.1851\t Accuracy 0.8693\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.1849\t Accuracy 0.8694\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.1848\t Accuracy 0.8697\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.1848\t Accuracy 0.8695\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.1848\t Accuracy 0.8694\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.1847\t Accuracy 0.8695\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.1846\t Accuracy 0.8696\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.1846\t Accuracy 0.8696\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1845\t Average training accuracy 0.8695\n",
      "Epoch [11]\t Average validation loss 0.1658\t Average validation accuracy 0.9002\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.1041\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.1685\t Accuracy 0.8882\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.1789\t Accuracy 0.8812\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.1863\t Accuracy 0.8689\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.1812\t Accuracy 0.8731\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.1779\t Accuracy 0.8781\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.1758\t Accuracy 0.8804\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.1754\t Accuracy 0.8812\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.1752\t Accuracy 0.8835\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.1758\t Accuracy 0.8843\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.1750\t Accuracy 0.8866\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.1746\t Accuracy 0.8860\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.1743\t Accuracy 0.8865\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.1734\t Accuracy 0.8874\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.1736\t Accuracy 0.8867\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.1748\t Accuracy 0.8851\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.1754\t Accuracy 0.8836\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.1763\t Accuracy 0.8826\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.1776\t Accuracy 0.8799\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.1773\t Accuracy 0.8796\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.1766\t Accuracy 0.8799\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.1761\t Accuracy 0.8808\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.1756\t Accuracy 0.8818\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.1753\t Accuracy 0.8821\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.1765\t Accuracy 0.8801\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.1767\t Accuracy 0.8796\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.1774\t Accuracy 0.8783\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.1777\t Accuracy 0.8779\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.1783\t Accuracy 0.8764\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.1792\t Accuracy 0.8750\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.1803\t Accuracy 0.8730\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.1801\t Accuracy 0.8734\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.1806\t Accuracy 0.8724\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.1803\t Accuracy 0.8730\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.1805\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.1807\t Accuracy 0.8721\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.1813\t Accuracy 0.8713\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.1809\t Accuracy 0.8725\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.1803\t Accuracy 0.8739\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.1803\t Accuracy 0.8742\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.1799\t Accuracy 0.8749\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.1798\t Accuracy 0.8755\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.1801\t Accuracy 0.8751\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.1798\t Accuracy 0.8755\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.1792\t Accuracy 0.8760\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.1795\t Accuracy 0.8756\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.1799\t Accuracy 0.8751\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.1799\t Accuracy 0.8753\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.1799\t Accuracy 0.8756\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.1799\t Accuracy 0.8754\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.1801\t Accuracy 0.8752\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.1797\t Accuracy 0.8752\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.1796\t Accuracy 0.8752\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.1796\t Accuracy 0.8753\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.1798\t Accuracy 0.8751\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.1800\t Accuracy 0.8750\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.1798\t Accuracy 0.8753\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.1796\t Accuracy 0.8757\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.1795\t Accuracy 0.8757\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.1796\t Accuracy 0.8754\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.1799\t Accuracy 0.8747\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.1801\t Accuracy 0.8747\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.1803\t Accuracy 0.8741\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.1806\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.1808\t Accuracy 0.8733\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.1811\t Accuracy 0.8727\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.1810\t Accuracy 0.8729\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.1811\t Accuracy 0.8730\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.1806\t Accuracy 0.8735\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.1805\t Accuracy 0.8739\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.1807\t Accuracy 0.8733\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.1806\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.1806\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.1806\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.1804\t Accuracy 0.8742\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.1808\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.1808\t Accuracy 0.8735\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.1808\t Accuracy 0.8737\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.1806\t Accuracy 0.8738\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.1805\t Accuracy 0.8738\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.1807\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.1806\t Accuracy 0.8738\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.1805\t Accuracy 0.8739\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.1807\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.1807\t Accuracy 0.8738\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.1810\t Accuracy 0.8733\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.1810\t Accuracy 0.8734\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.1809\t Accuracy 0.8737\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.1809\t Accuracy 0.8736\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.1810\t Accuracy 0.8733\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.1809\t Accuracy 0.8734\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.1810\t Accuracy 0.8731\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.1811\t Accuracy 0.8732\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.1813\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.1811\t Accuracy 0.8731\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.1812\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.1811\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.1809\t Accuracy 0.8732\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.1809\t Accuracy 0.8733\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.1810\t Accuracy 0.8730\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.1812\t Accuracy 0.8726\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.1813\t Accuracy 0.8724\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.1812\t Accuracy 0.8724\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.1811\t Accuracy 0.8725\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.1809\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.1809\t Accuracy 0.8727\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.1809\t Accuracy 0.8726\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.1808\t Accuracy 0.8727\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.1807\t Accuracy 0.8728\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.1807\t Accuracy 0.8728\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1806\t Average training accuracy 0.8728\n",
      "Epoch [12]\t Average validation loss 0.1620\t Average validation accuracy 0.9040\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.1003\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.1648\t Accuracy 0.8902\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.1751\t Accuracy 0.8822\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.1825\t Accuracy 0.8709\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.1773\t Accuracy 0.8751\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.1741\t Accuracy 0.8801\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.1720\t Accuracy 0.8821\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.1716\t Accuracy 0.8832\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.1714\t Accuracy 0.8853\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.1719\t Accuracy 0.8867\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.1711\t Accuracy 0.8886\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.1707\t Accuracy 0.8878\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.1704\t Accuracy 0.8885\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.1696\t Accuracy 0.8894\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.1697\t Accuracy 0.8887\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.1710\t Accuracy 0.8876\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.1716\t Accuracy 0.8861\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.1724\t Accuracy 0.8854\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.1737\t Accuracy 0.8828\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.1735\t Accuracy 0.8824\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.1727\t Accuracy 0.8828\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.1723\t Accuracy 0.8835\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.1717\t Accuracy 0.8847\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.1714\t Accuracy 0.8848\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.1726\t Accuracy 0.8828\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.1729\t Accuracy 0.8823\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.1735\t Accuracy 0.8810\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.1738\t Accuracy 0.8805\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.1744\t Accuracy 0.8790\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.1753\t Accuracy 0.8777\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.1764\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.1762\t Accuracy 0.8763\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.1767\t Accuracy 0.8751\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.1764\t Accuracy 0.8758\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.1766\t Accuracy 0.8755\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.1767\t Accuracy 0.8746\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.1774\t Accuracy 0.8738\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.1769\t Accuracy 0.8751\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.1763\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.1764\t Accuracy 0.8767\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.1760\t Accuracy 0.8774\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.1759\t Accuracy 0.8782\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.1761\t Accuracy 0.8777\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.1758\t Accuracy 0.8781\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.1753\t Accuracy 0.8786\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.1756\t Accuracy 0.8783\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.1759\t Accuracy 0.8777\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.1759\t Accuracy 0.8779\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.1760\t Accuracy 0.8782\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.1759\t Accuracy 0.8780\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.1761\t Accuracy 0.8778\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.1758\t Accuracy 0.8780\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.1757\t Accuracy 0.8779\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.1756\t Accuracy 0.8780\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.1758\t Accuracy 0.8778\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.1760\t Accuracy 0.8776\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.1758\t Accuracy 0.8780\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.1756\t Accuracy 0.8784\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.1755\t Accuracy 0.8784\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.1756\t Accuracy 0.8781\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.1759\t Accuracy 0.8774\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.1761\t Accuracy 0.8774\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.1764\t Accuracy 0.8767\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.1767\t Accuracy 0.8762\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.1768\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.1772\t Accuracy 0.8753\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.1771\t Accuracy 0.8756\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.1771\t Accuracy 0.8756\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.1767\t Accuracy 0.8762\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.1765\t Accuracy 0.8765\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.1767\t Accuracy 0.8760\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.1766\t Accuracy 0.8762\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.1766\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.1766\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.1764\t Accuracy 0.8770\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.1768\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.1769\t Accuracy 0.8763\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.1768\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.1766\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.1766\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.1768\t Accuracy 0.8763\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.1767\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.1765\t Accuracy 0.8765\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.1767\t Accuracy 0.8762\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.1767\t Accuracy 0.8765\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.1770\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.1770\t Accuracy 0.8761\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.1769\t Accuracy 0.8764\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.1769\t Accuracy 0.8763\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.1771\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.1770\t Accuracy 0.8761\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.1770\t Accuracy 0.8758\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.1771\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.1774\t Accuracy 0.8755\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.1771\t Accuracy 0.8758\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.1772\t Accuracy 0.8756\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.1772\t Accuracy 0.8755\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.1770\t Accuracy 0.8759\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.1769\t Accuracy 0.8760\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.1770\t Accuracy 0.8757\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.1773\t Accuracy 0.8753\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.1773\t Accuracy 0.8752\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.1772\t Accuracy 0.8752\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.1771\t Accuracy 0.8753\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.1769\t Accuracy 0.8756\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.1769\t Accuracy 0.8755\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.1770\t Accuracy 0.8753\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.1768\t Accuracy 0.8754\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.1768\t Accuracy 0.8755\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.1767\t Accuracy 0.8756\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1767\t Average training accuracy 0.8755\n",
      "Epoch [13]\t Average validation loss 0.1581\t Average validation accuracy 0.9044\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0965\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.1611\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.1713\t Accuracy 0.8891\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.1787\t Accuracy 0.8755\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.1734\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.1702\t Accuracy 0.8833\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.1682\t Accuracy 0.8850\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.1678\t Accuracy 0.8860\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.1675\t Accuracy 0.8888\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.1680\t Accuracy 0.8898\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.1671\t Accuracy 0.8912\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.1668\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.1665\t Accuracy 0.8907\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.1656\t Accuracy 0.8917\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.1658\t Accuracy 0.8910\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.1671\t Accuracy 0.8899\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.1677\t Accuracy 0.8885\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.1684\t Accuracy 0.8877\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.1698\t Accuracy 0.8853\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.1695\t Accuracy 0.8851\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.1688\t Accuracy 0.8856\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.1683\t Accuracy 0.8862\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.1678\t Accuracy 0.8873\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.1675\t Accuracy 0.8875\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.1687\t Accuracy 0.8854\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.1689\t Accuracy 0.8851\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.1696\t Accuracy 0.8839\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.1699\t Accuracy 0.8835\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.1704\t Accuracy 0.8819\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.1713\t Accuracy 0.8806\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.1724\t Accuracy 0.8790\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.1722\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.1727\t Accuracy 0.8781\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.1724\t Accuracy 0.8787\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.1726\t Accuracy 0.8783\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.1727\t Accuracy 0.8775\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.1734\t Accuracy 0.8768\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.1729\t Accuracy 0.8780\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.1724\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.1724\t Accuracy 0.8797\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.1719\t Accuracy 0.8804\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.1719\t Accuracy 0.8810\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.1721\t Accuracy 0.8805\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.1718\t Accuracy 0.8809\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.1713\t Accuracy 0.8814\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.1716\t Accuracy 0.8811\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.1719\t Accuracy 0.8805\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.1719\t Accuracy 0.8806\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.1719\t Accuracy 0.8809\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.1719\t Accuracy 0.8807\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.1721\t Accuracy 0.8806\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.1718\t Accuracy 0.8808\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.1717\t Accuracy 0.8808\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.1716\t Accuracy 0.8811\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.1718\t Accuracy 0.8808\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.1720\t Accuracy 0.8806\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.1718\t Accuracy 0.8809\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.1716\t Accuracy 0.8813\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.1715\t Accuracy 0.8813\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.1716\t Accuracy 0.8810\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.1719\t Accuracy 0.8802\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.1721\t Accuracy 0.8802\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.1724\t Accuracy 0.8798\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.1726\t Accuracy 0.8792\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.1728\t Accuracy 0.8790\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.1732\t Accuracy 0.8783\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.1731\t Accuracy 0.8786\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.1731\t Accuracy 0.8786\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.1727\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.1725\t Accuracy 0.8795\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.1727\t Accuracy 0.8789\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.1726\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.1726\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.1726\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.1724\t Accuracy 0.8799\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.1728\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.1728\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.1728\t Accuracy 0.8792\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.1726\t Accuracy 0.8792\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.1726\t Accuracy 0.8792\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.1727\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.1726\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.1725\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.1727\t Accuracy 0.8790\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.1727\t Accuracy 0.8793\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.1730\t Accuracy 0.8788\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.1730\t Accuracy 0.8789\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.1729\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.1729\t Accuracy 0.8791\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.1731\t Accuracy 0.8788\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.1729\t Accuracy 0.8790\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.1730\t Accuracy 0.8787\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.1731\t Accuracy 0.8787\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.1733\t Accuracy 0.8783\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.1731\t Accuracy 0.8786\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.1732\t Accuracy 0.8784\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.1731\t Accuracy 0.8784\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.1729\t Accuracy 0.8788\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.1729\t Accuracy 0.8788\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.1730\t Accuracy 0.8785\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.1732\t Accuracy 0.8782\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.1733\t Accuracy 0.8781\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.1732\t Accuracy 0.8781\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.1731\t Accuracy 0.8783\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.1729\t Accuracy 0.8785\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.1729\t Accuracy 0.8784\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.1730\t Accuracy 0.8783\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.1728\t Accuracy 0.8784\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.1728\t Accuracy 0.8785\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.1727\t Accuracy 0.8786\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1727\t Average training accuracy 0.8786\n",
      "Epoch [14]\t Average validation loss 0.1542\t Average validation accuracy 0.9066\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0929\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.1574\t Accuracy 0.9020\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.1675\t Accuracy 0.8911\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.1747\t Accuracy 0.8788\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.1694\t Accuracy 0.8826\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.1664\t Accuracy 0.8865\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.1643\t Accuracy 0.8884\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.1639\t Accuracy 0.8897\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.1635\t Accuracy 0.8923\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.1640\t Accuracy 0.8927\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.1631\t Accuracy 0.8940\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.1628\t Accuracy 0.8927\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.1625\t Accuracy 0.8935\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.1617\t Accuracy 0.8949\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.1619\t Accuracy 0.8942\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.1631\t Accuracy 0.8927\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.1637\t Accuracy 0.8916\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.1645\t Accuracy 0.8907\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.1658\t Accuracy 0.8885\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.1656\t Accuracy 0.8884\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.1649\t Accuracy 0.8889\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.1644\t Accuracy 0.8893\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.1638\t Accuracy 0.8905\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.1636\t Accuracy 0.8907\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.1647\t Accuracy 0.8886\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.1649\t Accuracy 0.8880\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.1656\t Accuracy 0.8870\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.1659\t Accuracy 0.8865\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.1664\t Accuracy 0.8852\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.1673\t Accuracy 0.8838\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.1684\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.1682\t Accuracy 0.8826\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.1687\t Accuracy 0.8813\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.1684\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.1686\t Accuracy 0.8815\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.1687\t Accuracy 0.8809\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.1694\t Accuracy 0.8802\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.1689\t Accuracy 0.8813\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.1683\t Accuracy 0.8824\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.1683\t Accuracy 0.8828\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.1679\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.1678\t Accuracy 0.8841\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.1681\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.1678\t Accuracy 0.8840\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.1672\t Accuracy 0.8846\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.1676\t Accuracy 0.8842\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.1679\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.1679\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.1679\t Accuracy 0.8838\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.1678\t Accuracy 0.8837\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.1681\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.1677\t Accuracy 0.8838\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.1676\t Accuracy 0.8838\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.1675\t Accuracy 0.8840\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.1677\t Accuracy 0.8838\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.1679\t Accuracy 0.8836\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.1678\t Accuracy 0.8839\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.1675\t Accuracy 0.8843\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.1675\t Accuracy 0.8842\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.1676\t Accuracy 0.8839\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.1679\t Accuracy 0.8832\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.1680\t Accuracy 0.8832\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.1683\t Accuracy 0.8827\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.1686\t Accuracy 0.8822\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.1688\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.1691\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.1690\t Accuracy 0.8816\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.1690\t Accuracy 0.8816\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.1686\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.1685\t Accuracy 0.8824\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.1686\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.1686\t Accuracy 0.8820\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.1685\t Accuracy 0.8822\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.1685\t Accuracy 0.8822\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.1683\t Accuracy 0.8827\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.1687\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.1688\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.1687\t Accuracy 0.8820\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.1686\t Accuracy 0.8820\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.1685\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.1687\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.1686\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.1684\t Accuracy 0.8822\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.1687\t Accuracy 0.8819\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.1687\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.1690\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.1690\t Accuracy 0.8818\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.1688\t Accuracy 0.8821\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.1688\t Accuracy 0.8820\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.1690\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.1689\t Accuracy 0.8820\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.1690\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.1691\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.1693\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.1691\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.1692\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.1691\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.1689\t Accuracy 0.8818\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.1688\t Accuracy 0.8817\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.1689\t Accuracy 0.8815\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.1692\t Accuracy 0.8811\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.1693\t Accuracy 0.8810\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.1692\t Accuracy 0.8811\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.1690\t Accuracy 0.8812\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.1689\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.1689\t Accuracy 0.8813\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.1689\t Accuracy 0.8812\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.1688\t Accuracy 0.8813\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.1688\t Accuracy 0.8814\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.1687\t Accuracy 0.8816\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1686\t Average training accuracy 0.8816\n",
      "Epoch [15]\t Average validation loss 0.1503\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0894\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.1537\t Accuracy 0.9039\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.1636\t Accuracy 0.8941\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.1708\t Accuracy 0.8808\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.1655\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.1625\t Accuracy 0.8892\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.1604\t Accuracy 0.8904\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.1600\t Accuracy 0.8920\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.1596\t Accuracy 0.8945\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.1600\t Accuracy 0.8953\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.1591\t Accuracy 0.8964\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.1589\t Accuracy 0.8951\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.1586\t Accuracy 0.8960\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.1577\t Accuracy 0.8975\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.1580\t Accuracy 0.8971\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.1592\t Accuracy 0.8956\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.1598\t Accuracy 0.8946\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.1605\t Accuracy 0.8935\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.1618\t Accuracy 0.8913\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.1616\t Accuracy 0.8913\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.1609\t Accuracy 0.8917\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.1604\t Accuracy 0.8921\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.1599\t Accuracy 0.8932\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.1596\t Accuracy 0.8932\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.1607\t Accuracy 0.8913\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.1610\t Accuracy 0.8906\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.1616\t Accuracy 0.8896\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.1619\t Accuracy 0.8891\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.1624\t Accuracy 0.8879\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.1633\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.1644\t Accuracy 0.8850\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.1642\t Accuracy 0.8854\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.1647\t Accuracy 0.8843\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.1644\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.1646\t Accuracy 0.8844\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.1647\t Accuracy 0.8839\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.1654\t Accuracy 0.8830\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.1649\t Accuracy 0.8841\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.1643\t Accuracy 0.8853\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.1643\t Accuracy 0.8857\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.1639\t Accuracy 0.8864\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.1638\t Accuracy 0.8869\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.1640\t Accuracy 0.8863\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.1637\t Accuracy 0.8868\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.1632\t Accuracy 0.8873\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.1635\t Accuracy 0.8869\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.1638\t Accuracy 0.8863\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.1638\t Accuracy 0.8864\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.1638\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.1638\t Accuracy 0.8865\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.1640\t Accuracy 0.8864\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.1637\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.1636\t Accuracy 0.8865\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.1635\t Accuracy 0.8869\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.1637\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.1639\t Accuracy 0.8863\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.1637\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.1635\t Accuracy 0.8870\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.1634\t Accuracy 0.8870\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.1635\t Accuracy 0.8866\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.1638\t Accuracy 0.8859\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.1640\t Accuracy 0.8859\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.1643\t Accuracy 0.8855\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.1646\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.1648\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.1651\t Accuracy 0.8842\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.1650\t Accuracy 0.8843\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.1650\t Accuracy 0.8842\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.1646\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.1644\t Accuracy 0.8851\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.1646\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.1645\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.1645\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.1645\t Accuracy 0.8850\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.1643\t Accuracy 0.8854\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.1647\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.1647\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.1647\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.1645\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.1645\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.1646\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.1645\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.1644\t Accuracy 0.8851\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.1646\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.1646\t Accuracy 0.8851\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.1649\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.1649\t Accuracy 0.8847\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.1648\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.1648\t Accuracy 0.8849\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.1650\t Accuracy 0.8847\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.1649\t Accuracy 0.8848\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.1649\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.1651\t Accuracy 0.8847\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.1653\t Accuracy 0.8843\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.1650\t Accuracy 0.8846\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.1651\t Accuracy 0.8844\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.1651\t Accuracy 0.8844\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.1649\t Accuracy 0.8847\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.1648\t Accuracy 0.8847\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.1649\t Accuracy 0.8844\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.1652\t Accuracy 0.8841\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.1653\t Accuracy 0.8839\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.1652\t Accuracy 0.8841\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.1650\t Accuracy 0.8842\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.1649\t Accuracy 0.8844\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.1649\t Accuracy 0.8843\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.1649\t Accuracy 0.8841\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.1648\t Accuracy 0.8842\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.1647\t Accuracy 0.8842\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.1647\t Accuracy 0.8845\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1646\t Average training accuracy 0.8845\n",
      "Epoch [16]\t Average validation loss 0.1464\t Average validation accuracy 0.9120\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0860\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.1500\t Accuracy 0.9059\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.1598\t Accuracy 0.8950\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.1669\t Accuracy 0.8815\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.1616\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.1586\t Accuracy 0.8904\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.1566\t Accuracy 0.8920\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.1562\t Accuracy 0.8940\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.1557\t Accuracy 0.8960\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.1561\t Accuracy 0.8967\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.1552\t Accuracy 0.8978\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.1550\t Accuracy 0.8964\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.1547\t Accuracy 0.8972\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.1539\t Accuracy 0.8991\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.1541\t Accuracy 0.8987\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.1553\t Accuracy 0.8971\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.1559\t Accuracy 0.8960\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.1566\t Accuracy 0.8949\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.1579\t Accuracy 0.8930\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.1577\t Accuracy 0.8931\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.1570\t Accuracy 0.8935\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.1565\t Accuracy 0.8940\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.1559\t Accuracy 0.8952\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.1557\t Accuracy 0.8952\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.1568\t Accuracy 0.8933\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.1571\t Accuracy 0.8928\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.1577\t Accuracy 0.8919\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.1580\t Accuracy 0.8913\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.1585\t Accuracy 0.8902\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.1594\t Accuracy 0.8890\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.1605\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.1603\t Accuracy 0.8876\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.1607\t Accuracy 0.8864\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.1604\t Accuracy 0.8870\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.1606\t Accuracy 0.8867\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.1607\t Accuracy 0.8861\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.1614\t Accuracy 0.8852\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.1609\t Accuracy 0.8862\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.1603\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.1603\t Accuracy 0.8877\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.1599\t Accuracy 0.8886\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.1598\t Accuracy 0.8892\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.1601\t Accuracy 0.8885\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.1598\t Accuracy 0.8890\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.1592\t Accuracy 0.8896\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.1596\t Accuracy 0.8892\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.1599\t Accuracy 0.8885\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.1598\t Accuracy 0.8887\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.1599\t Accuracy 0.8888\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.1598\t Accuracy 0.8887\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.1600\t Accuracy 0.8886\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.1597\t Accuracy 0.8888\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.1596\t Accuracy 0.8888\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.1595\t Accuracy 0.8891\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.1597\t Accuracy 0.8889\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.1599\t Accuracy 0.8887\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.1597\t Accuracy 0.8890\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.1595\t Accuracy 0.8893\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.1595\t Accuracy 0.8892\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.1596\t Accuracy 0.8889\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.1599\t Accuracy 0.8883\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.1600\t Accuracy 0.8884\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.1603\t Accuracy 0.8880\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.1606\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.1608\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.1611\t Accuracy 0.8867\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.1610\t Accuracy 0.8868\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.1610\t Accuracy 0.8868\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.1606\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.1605\t Accuracy 0.8878\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.1606\t Accuracy 0.8873\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.1606\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.1605\t Accuracy 0.8876\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.1605\t Accuracy 0.8877\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.1603\t Accuracy 0.8882\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.1607\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.1608\t Accuracy 0.8873\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.1607\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.1606\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.1605\t Accuracy 0.8876\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.1607\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.1606\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.1604\t Accuracy 0.8878\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.1607\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.1607\t Accuracy 0.8877\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.1610\t Accuracy 0.8872\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.1610\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.1609\t Accuracy 0.8876\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.1609\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.1610\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.1609\t Accuracy 0.8875\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.1610\t Accuracy 0.8873\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.1611\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.1613\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.1611\t Accuracy 0.8873\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.1612\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.1611\t Accuracy 0.8870\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.1609\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.1609\t Accuracy 0.8874\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.1610\t Accuracy 0.8872\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.1612\t Accuracy 0.8869\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.1613\t Accuracy 0.8867\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.1612\t Accuracy 0.8868\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.1611\t Accuracy 0.8869\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.1609\t Accuracy 0.8871\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.1609\t Accuracy 0.8869\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.1610\t Accuracy 0.8867\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.1608\t Accuracy 0.8869\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.1608\t Accuracy 0.8870\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.1607\t Accuracy 0.8872\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1607\t Average training accuracy 0.8871\n",
      "Epoch [17]\t Average validation loss 0.1427\t Average validation accuracy 0.9126\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0828\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.1464\t Accuracy 0.9078\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.1560\t Accuracy 0.8990\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.1631\t Accuracy 0.8854\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.1577\t Accuracy 0.8905\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.1548\t Accuracy 0.8928\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.1528\t Accuracy 0.8947\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.1524\t Accuracy 0.8966\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.1519\t Accuracy 0.8988\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.1523\t Accuracy 0.8998\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.1514\t Accuracy 0.9008\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.1512\t Accuracy 0.8991\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.1509\t Accuracy 0.8998\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.1501\t Accuracy 0.9017\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.1503\t Accuracy 0.9011\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.1515\t Accuracy 0.8995\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.1521\t Accuracy 0.8986\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.1528\t Accuracy 0.8978\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.1541\t Accuracy 0.8960\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.1538\t Accuracy 0.8959\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.1532\t Accuracy 0.8962\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.1527\t Accuracy 0.8965\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.1521\t Accuracy 0.8977\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.1519\t Accuracy 0.8978\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.1530\t Accuracy 0.8960\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.1532\t Accuracy 0.8957\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.1539\t Accuracy 0.8947\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.1541\t Accuracy 0.8940\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.1547\t Accuracy 0.8929\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.1555\t Accuracy 0.8917\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.1566\t Accuracy 0.8899\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.1564\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.1568\t Accuracy 0.8892\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.1565\t Accuracy 0.8897\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.1567\t Accuracy 0.8894\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.1569\t Accuracy 0.8889\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.1575\t Accuracy 0.8882\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.1570\t Accuracy 0.8891\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.1565\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.1564\t Accuracy 0.8905\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.1560\t Accuracy 0.8914\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.1559\t Accuracy 0.8919\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.1562\t Accuracy 0.8911\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.1559\t Accuracy 0.8917\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.1554\t Accuracy 0.8923\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.1557\t Accuracy 0.8920\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.1560\t Accuracy 0.8914\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.1560\t Accuracy 0.8915\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.1560\t Accuracy 0.8916\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.1559\t Accuracy 0.8914\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.1561\t Accuracy 0.8913\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.1558\t Accuracy 0.8915\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.1557\t Accuracy 0.8915\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.1556\t Accuracy 0.8920\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.1558\t Accuracy 0.8918\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.1560\t Accuracy 0.8916\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.1559\t Accuracy 0.8919\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.1556\t Accuracy 0.8921\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.1556\t Accuracy 0.8921\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.1557\t Accuracy 0.8918\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.1560\t Accuracy 0.8912\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.1562\t Accuracy 0.8912\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.1564\t Accuracy 0.8908\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.1567\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.1569\t Accuracy 0.8899\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.1573\t Accuracy 0.8894\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.1572\t Accuracy 0.8895\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.1571\t Accuracy 0.8896\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.1567\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.1566\t Accuracy 0.8905\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.1567\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.1567\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.1566\t Accuracy 0.8903\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.1566\t Accuracy 0.8904\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.1564\t Accuracy 0.8908\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.1568\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.1569\t Accuracy 0.8899\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.1568\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.1567\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.1566\t Accuracy 0.8904\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.1568\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.1567\t Accuracy 0.8903\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.1565\t Accuracy 0.8907\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.1568\t Accuracy 0.8904\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.1568\t Accuracy 0.8906\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.1571\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.1571\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.1570\t Accuracy 0.8904\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.1570\t Accuracy 0.8903\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.1572\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.1570\t Accuracy 0.8903\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.1571\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.1572\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.1575\t Accuracy 0.8898\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.1572\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.1573\t Accuracy 0.8899\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.1573\t Accuracy 0.8898\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.1571\t Accuracy 0.8902\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.1570\t Accuracy 0.8901\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.1571\t Accuracy 0.8899\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.1573\t Accuracy 0.8896\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.1574\t Accuracy 0.8894\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.1573\t Accuracy 0.8895\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.1572\t Accuracy 0.8896\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.1570\t Accuracy 0.8898\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.1571\t Accuracy 0.8896\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.1571\t Accuracy 0.8894\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.1570\t Accuracy 0.8896\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.1570\t Accuracy 0.8897\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.1569\t Accuracy 0.8900\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1569\t Average training accuracy 0.8899\n",
      "Epoch [18]\t Average validation loss 0.1390\t Average validation accuracy 0.9154\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0798\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.1430\t Accuracy 0.9098\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.1524\t Accuracy 0.9010\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.1594\t Accuracy 0.8874\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.1540\t Accuracy 0.8930\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.1512\t Accuracy 0.8956\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.1492\t Accuracy 0.8973\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.1488\t Accuracy 0.8989\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.1483\t Accuracy 0.9012\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.1485\t Accuracy 0.9027\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.1477\t Accuracy 0.9034\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.1475\t Accuracy 0.9015\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.1473\t Accuracy 0.9020\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.1464\t Accuracy 0.9038\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.1467\t Accuracy 0.9034\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.1479\t Accuracy 0.9017\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.1484\t Accuracy 0.9009\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.1490\t Accuracy 0.9001\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.1503\t Accuracy 0.8980\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.1501\t Accuracy 0.8981\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.1495\t Accuracy 0.8985\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.1490\t Accuracy 0.8990\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.1484\t Accuracy 0.9002\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.1482\t Accuracy 0.9003\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.1493\t Accuracy 0.8987\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.1495\t Accuracy 0.8983\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.1502\t Accuracy 0.8974\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.1504\t Accuracy 0.8967\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.1509\t Accuracy 0.8959\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.1518\t Accuracy 0.8949\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.1528\t Accuracy 0.8934\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.1526\t Accuracy 0.8937\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.1531\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.1528\t Accuracy 0.8934\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.1530\t Accuracy 0.8931\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.1531\t Accuracy 0.8928\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.1537\t Accuracy 0.8921\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.1532\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.1527\t Accuracy 0.8940\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.1526\t Accuracy 0.8944\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.1522\t Accuracy 0.8952\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.1521\t Accuracy 0.8956\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.1524\t Accuracy 0.8948\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.1521\t Accuracy 0.8953\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.1516\t Accuracy 0.8959\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.1519\t Accuracy 0.8955\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.1522\t Accuracy 0.8949\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.1522\t Accuracy 0.8950\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.1522\t Accuracy 0.8951\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.1521\t Accuracy 0.8947\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.1524\t Accuracy 0.8946\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.1521\t Accuracy 0.8947\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.1520\t Accuracy 0.8948\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.1519\t Accuracy 0.8951\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.1521\t Accuracy 0.8950\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.1523\t Accuracy 0.8948\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.1521\t Accuracy 0.8950\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.1519\t Accuracy 0.8952\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.1518\t Accuracy 0.8952\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.1519\t Accuracy 0.8949\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.1523\t Accuracy 0.8943\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.1524\t Accuracy 0.8943\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.1527\t Accuracy 0.8940\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.1530\t Accuracy 0.8934\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.1532\t Accuracy 0.8931\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.1535\t Accuracy 0.8925\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.1534\t Accuracy 0.8926\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.1534\t Accuracy 0.8927\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.1530\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.1528\t Accuracy 0.8935\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.1530\t Accuracy 0.8931\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.1529\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.1529\t Accuracy 0.8933\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.1529\t Accuracy 0.8933\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.1527\t Accuracy 0.8937\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.1531\t Accuracy 0.8931\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.1531\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.1531\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.1529\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.1529\t Accuracy 0.8933\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.1530\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.1529\t Accuracy 0.8933\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.1528\t Accuracy 0.8936\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.1530\t Accuracy 0.8934\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.1530\t Accuracy 0.8935\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.1534\t Accuracy 0.8930\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.1534\t Accuracy 0.8931\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.1532\t Accuracy 0.8933\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.1532\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.1534\t Accuracy 0.8930\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.1533\t Accuracy 0.8932\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.1534\t Accuracy 0.8930\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.1535\t Accuracy 0.8930\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.1537\t Accuracy 0.8926\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.1535\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.1536\t Accuracy 0.8927\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.1535\t Accuracy 0.8926\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.1533\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.1533\t Accuracy 0.8929\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.1534\t Accuracy 0.8927\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.1536\t Accuracy 0.8924\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.1537\t Accuracy 0.8922\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.1536\t Accuracy 0.8923\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.1535\t Accuracy 0.8924\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.1533\t Accuracy 0.8926\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.1534\t Accuracy 0.8923\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.1534\t Accuracy 0.8922\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.1533\t Accuracy 0.8924\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.1533\t Accuracy 0.8924\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.1532\t Accuracy 0.8927\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1532\t Average training accuracy 0.8925\n",
      "Epoch [19]\t Average validation loss 0.1355\t Average validation accuracy 0.9180\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0770\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.1396\t Accuracy 0.9098\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.1489\t Accuracy 0.9030\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.1558\t Accuracy 0.8887\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.1504\t Accuracy 0.8940\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.1477\t Accuracy 0.8968\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.1457\t Accuracy 0.8990\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.1453\t Accuracy 0.9006\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.1447\t Accuracy 0.9030\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.1450\t Accuracy 0.9042\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.1441\t Accuracy 0.9052\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.1440\t Accuracy 0.9034\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.1438\t Accuracy 0.9038\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.1429\t Accuracy 0.9057\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.1432\t Accuracy 0.9051\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.1443\t Accuracy 0.9033\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.1448\t Accuracy 0.9025\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.1455\t Accuracy 0.9018\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.1467\t Accuracy 0.8994\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.1466\t Accuracy 0.8996\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.1459\t Accuracy 0.8999\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.1455\t Accuracy 0.9006\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.1449\t Accuracy 0.9019\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.1446\t Accuracy 0.9021\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.1457\t Accuracy 0.9003\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.1460\t Accuracy 0.8999\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.1466\t Accuracy 0.8990\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.1468\t Accuracy 0.8984\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.1473\t Accuracy 0.8978\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.1482\t Accuracy 0.8968\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.1492\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.1490\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.1495\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.1491\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.1494\t Accuracy 0.8954\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.1495\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.1501\t Accuracy 0.8941\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.1496\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.1491\t Accuracy 0.8961\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.1490\t Accuracy 0.8966\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.1486\t Accuracy 0.8974\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.1485\t Accuracy 0.8980\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.1488\t Accuracy 0.8971\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.1485\t Accuracy 0.8976\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.1480\t Accuracy 0.8982\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.1483\t Accuracy 0.8978\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.1486\t Accuracy 0.8973\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.1486\t Accuracy 0.8973\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.1486\t Accuracy 0.8974\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.1485\t Accuracy 0.8971\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.1488\t Accuracy 0.8969\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.1485\t Accuracy 0.8972\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.1484\t Accuracy 0.8972\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.1483\t Accuracy 0.8975\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.1484\t Accuracy 0.8974\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.1486\t Accuracy 0.8972\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.1485\t Accuracy 0.8975\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.1483\t Accuracy 0.8977\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.1482\t Accuracy 0.8977\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.1484\t Accuracy 0.8974\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.1487\t Accuracy 0.8968\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.1488\t Accuracy 0.8968\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.1491\t Accuracy 0.8964\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.1494\t Accuracy 0.8958\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.1496\t Accuracy 0.8954\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.1500\t Accuracy 0.8948\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.1498\t Accuracy 0.8949\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.1498\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.1494\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.1492\t Accuracy 0.8959\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.1494\t Accuracy 0.8955\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.1493\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.1493\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.1493\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.1491\t Accuracy 0.8961\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.1495\t Accuracy 0.8955\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.1495\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.1495\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.1494\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.1493\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.1494\t Accuracy 0.8956\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.1493\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.1492\t Accuracy 0.8960\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.1494\t Accuracy 0.8957\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.1495\t Accuracy 0.8958\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.1498\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.1498\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.1497\t Accuracy 0.8955\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.1497\t Accuracy 0.8955\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.1498\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.1497\t Accuracy 0.8955\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.1498\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.1499\t Accuracy 0.8954\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.1501\t Accuracy 0.8951\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.1499\t Accuracy 0.8954\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.1500\t Accuracy 0.8952\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.1499\t Accuracy 0.8951\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.1498\t Accuracy 0.8954\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.1497\t Accuracy 0.8953\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.1498\t Accuracy 0.8951\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.1500\t Accuracy 0.8948\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.1501\t Accuracy 0.8947\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.1501\t Accuracy 0.8947\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.1499\t Accuracy 0.8949\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.1498\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.1498\t Accuracy 0.8948\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.1499\t Accuracy 0.8946\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.1497\t Accuracy 0.8948\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.1497\t Accuracy 0.8948\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.1496\t Accuracy 0.8951\n",
      "\n",
      "Epoch [20]\t Average training loss 0.1496\t Average training accuracy 0.8950\n",
      "Epoch [20]\t Average validation loss 0.1322\t Average validation accuracy 0.9192\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0743\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.1363\t Accuracy 0.9118\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.1455\t Accuracy 0.9059\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.1524\t Accuracy 0.8921\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.1469\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.1443\t Accuracy 0.8992\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.1424\t Accuracy 0.9013\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.1419\t Accuracy 0.9031\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.1413\t Accuracy 0.9060\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.1416\t Accuracy 0.9069\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.1407\t Accuracy 0.9076\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.1406\t Accuracy 0.9060\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.1404\t Accuracy 0.9062\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.1395\t Accuracy 0.9081\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.1398\t Accuracy 0.9076\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.1410\t Accuracy 0.9057\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.1414\t Accuracy 0.9050\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.1421\t Accuracy 0.9043\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.1433\t Accuracy 0.9019\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.1431\t Accuracy 0.9021\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.1425\t Accuracy 0.9023\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.1421\t Accuracy 0.9031\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.1415\t Accuracy 0.9045\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.1412\t Accuracy 0.9045\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.1423\t Accuracy 0.9031\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.1425\t Accuracy 0.9026\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.1432\t Accuracy 0.9017\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.1434\t Accuracy 0.9011\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.1439\t Accuracy 0.9004\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.1448\t Accuracy 0.8992\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.1458\t Accuracy 0.8978\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.1456\t Accuracy 0.8981\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.1460\t Accuracy 0.8974\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.1457\t Accuracy 0.8980\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.1459\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.1460\t Accuracy 0.8973\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.1466\t Accuracy 0.8964\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.1462\t Accuracy 0.8973\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.1456\t Accuracy 0.8984\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.1456\t Accuracy 0.8990\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.1451\t Accuracy 0.8998\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.1450\t Accuracy 0.9003\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.1453\t Accuracy 0.8996\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.1451\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.1446\t Accuracy 0.9006\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.1449\t Accuracy 0.9002\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.1452\t Accuracy 0.8997\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.1451\t Accuracy 0.8998\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.1452\t Accuracy 0.8999\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.1451\t Accuracy 0.8996\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.1453\t Accuracy 0.8994\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.1450\t Accuracy 0.8996\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.1449\t Accuracy 0.8997\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.1448\t Accuracy 0.9001\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.1450\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.1452\t Accuracy 0.8997\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.1451\t Accuracy 0.8999\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.1448\t Accuracy 0.9002\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.1448\t Accuracy 0.9001\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.1449\t Accuracy 0.8997\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.1452\t Accuracy 0.8991\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.1454\t Accuracy 0.8991\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.1457\t Accuracy 0.8987\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.1460\t Accuracy 0.8981\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.1462\t Accuracy 0.8976\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.1465\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.1464\t Accuracy 0.8971\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.1464\t Accuracy 0.8972\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.1460\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.1458\t Accuracy 0.8980\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.1460\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.1459\t Accuracy 0.8978\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.1459\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.1459\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.1457\t Accuracy 0.8983\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.1460\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.1461\t Accuracy 0.8975\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.1461\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.1459\t Accuracy 0.8978\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.1459\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.1460\t Accuracy 0.8978\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.1459\t Accuracy 0.8980\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.1458\t Accuracy 0.8982\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.1460\t Accuracy 0.8980\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.1460\t Accuracy 0.8981\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.1463\t Accuracy 0.8976\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.1464\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.1463\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.1463\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.1464\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.1463\t Accuracy 0.8979\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.1464\t Accuracy 0.8976\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.1465\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.1467\t Accuracy 0.8974\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.1465\t Accuracy 0.8976\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.1466\t Accuracy 0.8975\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.1465\t Accuracy 0.8974\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.1464\t Accuracy 0.8977\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.1463\t Accuracy 0.8976\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.1464\t Accuracy 0.8974\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.1466\t Accuracy 0.8971\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.1467\t Accuracy 0.8969\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.1467\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.1465\t Accuracy 0.8971\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.1464\t Accuracy 0.8973\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.1464\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.1465\t Accuracy 0.8968\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.1464\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.1463\t Accuracy 0.8970\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.1462\t Accuracy 0.8973\n",
      "\n",
      "Epoch [21]\t Average training loss 0.1462\t Average training accuracy 0.8972\n",
      "Epoch [21]\t Average validation loss 0.1290\t Average validation accuracy 0.9208\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0719\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.1333\t Accuracy 0.9137\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.1423\t Accuracy 0.9069\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.1491\t Accuracy 0.8940\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.1437\t Accuracy 0.8985\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.1411\t Accuracy 0.9004\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.1392\t Accuracy 0.9030\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.1387\t Accuracy 0.9048\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.1381\t Accuracy 0.9077\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.1383\t Accuracy 0.9086\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.1375\t Accuracy 0.9094\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.1374\t Accuracy 0.9078\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.1372\t Accuracy 0.9080\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.1363\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.1366\t Accuracy 0.9093\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.1378\t Accuracy 0.9075\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.1382\t Accuracy 0.9069\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.1388\t Accuracy 0.9063\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.1401\t Accuracy 0.9039\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.1399\t Accuracy 0.9041\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.1393\t Accuracy 0.9044\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.1389\t Accuracy 0.9053\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.1383\t Accuracy 0.9068\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.1380\t Accuracy 0.9069\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.1391\t Accuracy 0.9053\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.1393\t Accuracy 0.9050\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.1399\t Accuracy 0.9039\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.1402\t Accuracy 0.9033\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.1407\t Accuracy 0.9029\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.1415\t Accuracy 0.9015\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.1425\t Accuracy 0.9001\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.1423\t Accuracy 0.9004\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.1428\t Accuracy 0.8996\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.1424\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.1426\t Accuracy 0.9001\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.1428\t Accuracy 0.8997\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.1434\t Accuracy 0.8989\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.1429\t Accuracy 0.8998\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.1424\t Accuracy 0.9009\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.1423\t Accuracy 0.9014\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.1419\t Accuracy 0.9021\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.1417\t Accuracy 0.9026\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.1421\t Accuracy 0.9018\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.1418\t Accuracy 0.9022\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.1413\t Accuracy 0.9028\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.1416\t Accuracy 0.9026\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.1419\t Accuracy 0.9021\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.1419\t Accuracy 0.9021\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.1419\t Accuracy 0.9022\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.1418\t Accuracy 0.9019\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.1421\t Accuracy 0.9017\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.1418\t Accuracy 0.9019\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.1417\t Accuracy 0.9021\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.1416\t Accuracy 0.9024\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.1418\t Accuracy 0.9023\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.1419\t Accuracy 0.9020\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.1418\t Accuracy 0.9021\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.1416\t Accuracy 0.9024\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.1416\t Accuracy 0.9022\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.1417\t Accuracy 0.9019\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.1420\t Accuracy 0.9013\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.1422\t Accuracy 0.9013\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.1424\t Accuracy 0.9009\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.1427\t Accuracy 0.9003\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.1429\t Accuracy 0.8998\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.1433\t Accuracy 0.8994\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.1432\t Accuracy 0.8995\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.1431\t Accuracy 0.8996\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.1427\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.1426\t Accuracy 0.9005\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.1427\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.1427\t Accuracy 0.9003\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.1426\t Accuracy 0.9005\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.1426\t Accuracy 0.9004\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.1424\t Accuracy 0.9008\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.1428\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.1429\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.1428\t Accuracy 0.9001\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.1427\t Accuracy 0.9001\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.1427\t Accuracy 0.9003\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.1428\t Accuracy 0.9001\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.1427\t Accuracy 0.9003\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.1425\t Accuracy 0.9006\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.1428\t Accuracy 0.9004\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.1428\t Accuracy 0.9004\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.1431\t Accuracy 0.8999\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.1431\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.1430\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.1430\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.1432\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.1431\t Accuracy 0.9002\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.1432\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.1433\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.1435\t Accuracy 0.8997\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.1433\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.1433\t Accuracy 0.8998\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.1433\t Accuracy 0.8997\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.1431\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.1431\t Accuracy 0.8999\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.1432\t Accuracy 0.8997\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.1434\t Accuracy 0.8994\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.1435\t Accuracy 0.8992\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.1434\t Accuracy 0.8993\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.1433\t Accuracy 0.8994\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.1432\t Accuracy 0.8996\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.1432\t Accuracy 0.8993\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.1433\t Accuracy 0.8991\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.1431\t Accuracy 0.8993\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.1431\t Accuracy 0.8993\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.1430\t Accuracy 0.8996\n",
      "\n",
      "Epoch [22]\t Average training loss 0.1430\t Average training accuracy 0.8995\n",
      "Epoch [22]\t Average validation loss 0.1261\t Average validation accuracy 0.9218\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0697\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.1303\t Accuracy 0.9176\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.1393\t Accuracy 0.9089\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.1460\t Accuracy 0.8967\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.1406\t Accuracy 0.9010\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.1381\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.1362\t Accuracy 0.9056\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.1357\t Accuracy 0.9071\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.1351\t Accuracy 0.9102\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.1353\t Accuracy 0.9109\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.1344\t Accuracy 0.9118\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.1344\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.1342\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.1333\t Accuracy 0.9118\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.1336\t Accuracy 0.9113\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.1347\t Accuracy 0.9097\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.1352\t Accuracy 0.9090\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.1358\t Accuracy 0.9083\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.1370\t Accuracy 0.9059\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.1369\t Accuracy 0.9061\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.1362\t Accuracy 0.9064\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.1359\t Accuracy 0.9074\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.1352\t Accuracy 0.9089\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.1350\t Accuracy 0.9089\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.1361\t Accuracy 0.9074\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.1363\t Accuracy 0.9070\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.1369\t Accuracy 0.9060\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.1371\t Accuracy 0.9055\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.1376\t Accuracy 0.9050\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.1384\t Accuracy 0.9037\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.1394\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.1393\t Accuracy 0.9026\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.1397\t Accuracy 0.9018\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.1393\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.1395\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.1397\t Accuracy 0.9019\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.1403\t Accuracy 0.9011\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.1398\t Accuracy 0.9018\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.1393\t Accuracy 0.9029\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.1392\t Accuracy 0.9034\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.1388\t Accuracy 0.9041\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.1386\t Accuracy 0.9045\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.1390\t Accuracy 0.9037\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.1387\t Accuracy 0.9041\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.1383\t Accuracy 0.9046\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.1386\t Accuracy 0.9045\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.1388\t Accuracy 0.9041\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.1388\t Accuracy 0.9041\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.1388\t Accuracy 0.9042\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.1387\t Accuracy 0.9040\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.1390\t Accuracy 0.9038\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.1387\t Accuracy 0.9040\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.1386\t Accuracy 0.9043\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.1385\t Accuracy 0.9047\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.1387\t Accuracy 0.9046\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.1389\t Accuracy 0.9043\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.1387\t Accuracy 0.9044\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.1385\t Accuracy 0.9047\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.1385\t Accuracy 0.9044\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.1386\t Accuracy 0.9041\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.1390\t Accuracy 0.9036\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.1391\t Accuracy 0.9036\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.1394\t Accuracy 0.9032\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.1397\t Accuracy 0.9026\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.1399\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.1402\t Accuracy 0.9016\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.1401\t Accuracy 0.9017\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.1401\t Accuracy 0.9018\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.1397\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.1395\t Accuracy 0.9026\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.1397\t Accuracy 0.9023\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.1396\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.1396\t Accuracy 0.9026\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.1396\t Accuracy 0.9026\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.1394\t Accuracy 0.9029\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.1397\t Accuracy 0.9023\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.1398\t Accuracy 0.9021\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.1398\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.1396\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.1396\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.1397\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.1396\t Accuracy 0.9023\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.1395\t Accuracy 0.9027\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.1397\t Accuracy 0.9024\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.1398\t Accuracy 0.9025\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.1401\t Accuracy 0.9021\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.1401\t Accuracy 0.9021\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.1400\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.1400\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.1401\t Accuracy 0.9020\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.1400\t Accuracy 0.9022\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.1401\t Accuracy 0.9020\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.1402\t Accuracy 0.9020\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.1404\t Accuracy 0.9017\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.1402\t Accuracy 0.9020\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.1403\t Accuracy 0.9018\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.1403\t Accuracy 0.9017\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.1401\t Accuracy 0.9020\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.1400\t Accuracy 0.9019\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.1401\t Accuracy 0.9017\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.1404\t Accuracy 0.9014\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.1405\t Accuracy 0.9012\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.1404\t Accuracy 0.9013\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.1403\t Accuracy 0.9014\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.1401\t Accuracy 0.9016\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.1402\t Accuracy 0.9013\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.1402\t Accuracy 0.9011\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.1401\t Accuracy 0.9013\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.1401\t Accuracy 0.9013\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.1400\t Accuracy 0.9016\n",
      "\n",
      "Epoch [23]\t Average training loss 0.1400\t Average training accuracy 0.9015\n",
      "Epoch [23]\t Average validation loss 0.1233\t Average validation accuracy 0.9234\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0677\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.1276\t Accuracy 0.9176\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.1364\t Accuracy 0.9109\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.1431\t Accuracy 0.8967\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.1377\t Accuracy 0.9015\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.1352\t Accuracy 0.9028\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.1333\t Accuracy 0.9066\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.1329\t Accuracy 0.9080\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.1322\t Accuracy 0.9110\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.1324\t Accuracy 0.9115\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.1316\t Accuracy 0.9134\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.1315\t Accuracy 0.9116\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.1314\t Accuracy 0.9116\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.1305\t Accuracy 0.9137\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.1308\t Accuracy 0.9130\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.1319\t Accuracy 0.9115\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.1323\t Accuracy 0.9107\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.1329\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.1342\t Accuracy 0.9077\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.1340\t Accuracy 0.9078\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.1334\t Accuracy 0.9080\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.1330\t Accuracy 0.9090\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.1324\t Accuracy 0.9105\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.1321\t Accuracy 0.9106\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.1332\t Accuracy 0.9089\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.1334\t Accuracy 0.9087\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.1340\t Accuracy 0.9078\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.1343\t Accuracy 0.9073\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.1347\t Accuracy 0.9067\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.1355\t Accuracy 0.9054\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.1365\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.1364\t Accuracy 0.9043\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.1368\t Accuracy 0.9035\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.1364\t Accuracy 0.9039\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.1366\t Accuracy 0.9039\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.1367\t Accuracy 0.9037\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.1373\t Accuracy 0.9028\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.1369\t Accuracy 0.9036\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.1364\t Accuracy 0.9047\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.1363\t Accuracy 0.9051\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.1358\t Accuracy 0.9057\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.1357\t Accuracy 0.9061\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.1361\t Accuracy 0.9053\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.1358\t Accuracy 0.9057\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.1354\t Accuracy 0.9062\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.1357\t Accuracy 0.9062\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.1360\t Accuracy 0.9057\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.1359\t Accuracy 0.9057\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.1359\t Accuracy 0.9059\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.1358\t Accuracy 0.9058\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.1361\t Accuracy 0.9056\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.1358\t Accuracy 0.9058\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.1357\t Accuracy 0.9062\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.1356\t Accuracy 0.9066\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.1358\t Accuracy 0.9064\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.1360\t Accuracy 0.9062\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.1358\t Accuracy 0.9064\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.1356\t Accuracy 0.9066\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.1356\t Accuracy 0.9063\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.1358\t Accuracy 0.9060\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.1361\t Accuracy 0.9055\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.1362\t Accuracy 0.9054\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.1365\t Accuracy 0.9050\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.1368\t Accuracy 0.9045\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.1370\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.1374\t Accuracy 0.9034\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.1372\t Accuracy 0.9035\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.1372\t Accuracy 0.9037\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.1368\t Accuracy 0.9042\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.1366\t Accuracy 0.9044\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.1368\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.1367\t Accuracy 0.9042\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.1367\t Accuracy 0.9043\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.1367\t Accuracy 0.9043\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.1365\t Accuracy 0.9047\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.1368\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.1369\t Accuracy 0.9039\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.1369\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.1368\t Accuracy 0.9040\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.1367\t Accuracy 0.9042\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.1368\t Accuracy 0.9040\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.1367\t Accuracy 0.9042\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.1366\t Accuracy 0.9044\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.1369\t Accuracy 0.9042\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.1369\t Accuracy 0.9044\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.1372\t Accuracy 0.9040\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.1372\t Accuracy 0.9040\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.1371\t Accuracy 0.9040\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.1371\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.1373\t Accuracy 0.9039\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.1371\t Accuracy 0.9041\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.1372\t Accuracy 0.9039\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.1373\t Accuracy 0.9038\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.1376\t Accuracy 0.9036\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.1374\t Accuracy 0.9038\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.1374\t Accuracy 0.9037\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.1374\t Accuracy 0.9035\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.1372\t Accuracy 0.9038\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.1372\t Accuracy 0.9037\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.1373\t Accuracy 0.9035\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.1375\t Accuracy 0.9032\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.1376\t Accuracy 0.9030\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.1375\t Accuracy 0.9031\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.1374\t Accuracy 0.9032\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.1373\t Accuracy 0.9034\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.1373\t Accuracy 0.9032\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.1374\t Accuracy 0.9030\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.1373\t Accuracy 0.9032\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.1372\t Accuracy 0.9031\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.1372\t Accuracy 0.9034\n",
      "\n",
      "Epoch [24]\t Average training loss 0.1372\t Average training accuracy 0.9033\n",
      "Epoch [24]\t Average validation loss 0.1207\t Average validation accuracy 0.9246\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0658\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.1250\t Accuracy 0.9176\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.1337\t Accuracy 0.9109\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.1403\t Accuracy 0.8980\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.1349\t Accuracy 0.9025\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.1325\t Accuracy 0.9048\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.1307\t Accuracy 0.9083\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.1302\t Accuracy 0.9094\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.1295\t Accuracy 0.9127\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.1297\t Accuracy 0.9131\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.1289\t Accuracy 0.9150\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.1288\t Accuracy 0.9134\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.1287\t Accuracy 0.9131\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.1278\t Accuracy 0.9151\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.1282\t Accuracy 0.9144\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.1292\t Accuracy 0.9133\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.1296\t Accuracy 0.9125\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.1302\t Accuracy 0.9118\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.1315\t Accuracy 0.9093\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.1313\t Accuracy 0.9094\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.1307\t Accuracy 0.9096\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.1303\t Accuracy 0.9107\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.1297\t Accuracy 0.9121\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.1294\t Accuracy 0.9121\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.1305\t Accuracy 0.9104\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.1307\t Accuracy 0.9103\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.1313\t Accuracy 0.9094\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.1316\t Accuracy 0.9088\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.1320\t Accuracy 0.9084\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.1328\t Accuracy 0.9071\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.1338\t Accuracy 0.9059\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.1336\t Accuracy 0.9060\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.1341\t Accuracy 0.9053\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.1337\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.1339\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.1340\t Accuracy 0.9057\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.1346\t Accuracy 0.9047\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.1341\t Accuracy 0.9053\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.1336\t Accuracy 0.9065\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.1335\t Accuracy 0.9069\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.1331\t Accuracy 0.9075\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.1330\t Accuracy 0.9079\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.1333\t Accuracy 0.9071\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.1331\t Accuracy 0.9073\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.1327\t Accuracy 0.9079\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.1330\t Accuracy 0.9078\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.1332\t Accuracy 0.9074\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.1332\t Accuracy 0.9074\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.1332\t Accuracy 0.9075\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.1331\t Accuracy 0.9074\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.1334\t Accuracy 0.9071\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.1331\t Accuracy 0.9073\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.1330\t Accuracy 0.9076\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.1329\t Accuracy 0.9081\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.1331\t Accuracy 0.9080\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.1333\t Accuracy 0.9078\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.1331\t Accuracy 0.9080\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.1329\t Accuracy 0.9081\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.1329\t Accuracy 0.9079\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.1331\t Accuracy 0.9077\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.1334\t Accuracy 0.9071\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.1335\t Accuracy 0.9072\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.1338\t Accuracy 0.9068\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.1341\t Accuracy 0.9062\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.1343\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.1347\t Accuracy 0.9051\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.1345\t Accuracy 0.9052\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.1345\t Accuracy 0.9055\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.1341\t Accuracy 0.9059\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.1339\t Accuracy 0.9062\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.1341\t Accuracy 0.9059\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.1340\t Accuracy 0.9060\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.1340\t Accuracy 0.9061\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.1340\t Accuracy 0.9061\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.1338\t Accuracy 0.9064\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.1341\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.1342\t Accuracy 0.9056\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.1342\t Accuracy 0.9057\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.1341\t Accuracy 0.9057\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.1340\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.1341\t Accuracy 0.9056\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.1340\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.1339\t Accuracy 0.9061\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.1342\t Accuracy 0.9058\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.1342\t Accuracy 0.9060\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.1345\t Accuracy 0.9055\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.1345\t Accuracy 0.9055\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.1344\t Accuracy 0.9056\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.1344\t Accuracy 0.9056\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.1346\t Accuracy 0.9054\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.1344\t Accuracy 0.9056\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.1345\t Accuracy 0.9054\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.1346\t Accuracy 0.9053\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.1349\t Accuracy 0.9051\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.1347\t Accuracy 0.9053\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.1347\t Accuracy 0.9052\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.1347\t Accuracy 0.9051\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.1345\t Accuracy 0.9053\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.1345\t Accuracy 0.9052\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.1346\t Accuracy 0.9050\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.1348\t Accuracy 0.9048\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.1349\t Accuracy 0.9046\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.1349\t Accuracy 0.9047\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.1348\t Accuracy 0.9048\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.1346\t Accuracy 0.9050\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.1347\t Accuracy 0.9047\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.1347\t Accuracy 0.9045\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.1346\t Accuracy 0.9047\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.1346\t Accuracy 0.9047\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.1345\t Accuracy 0.9050\n",
      "\n",
      "Epoch [25]\t Average training loss 0.1345\t Average training accuracy 0.9049\n",
      "Epoch [25]\t Average validation loss 0.1182\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0641\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.1225\t Accuracy 0.9176\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.1312\t Accuracy 0.9109\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.1377\t Accuracy 0.8980\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.1323\t Accuracy 0.9025\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.1300\t Accuracy 0.9056\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.1282\t Accuracy 0.9096\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.1277\t Accuracy 0.9105\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.1270\t Accuracy 0.9142\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.1271\t Accuracy 0.9146\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.1263\t Accuracy 0.9164\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.1263\t Accuracy 0.9147\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.1262\t Accuracy 0.9145\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.1254\t Accuracy 0.9163\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.1257\t Accuracy 0.9155\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.1267\t Accuracy 0.9142\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.1271\t Accuracy 0.9135\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.1276\t Accuracy 0.9126\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.1289\t Accuracy 0.9101\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.1288\t Accuracy 0.9102\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.1282\t Accuracy 0.9107\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.1278\t Accuracy 0.9117\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.1272\t Accuracy 0.9131\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.1269\t Accuracy 0.9133\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.1280\t Accuracy 0.9116\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.1282\t Accuracy 0.9114\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.1288\t Accuracy 0.9106\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.1290\t Accuracy 0.9101\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.1295\t Accuracy 0.9096\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.1303\t Accuracy 0.9083\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.1313\t Accuracy 0.9070\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.1311\t Accuracy 0.9072\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.1315\t Accuracy 0.9064\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.1311\t Accuracy 0.9069\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.1313\t Accuracy 0.9070\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.1314\t Accuracy 0.9069\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.1320\t Accuracy 0.9061\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.1316\t Accuracy 0.9069\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.1311\t Accuracy 0.9080\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.1310\t Accuracy 0.9084\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.1306\t Accuracy 0.9089\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.1304\t Accuracy 0.9094\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.1308\t Accuracy 0.9086\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.1306\t Accuracy 0.9088\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.1301\t Accuracy 0.9095\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.1304\t Accuracy 0.9094\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.1307\t Accuracy 0.9091\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.1307\t Accuracy 0.9091\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.1307\t Accuracy 0.9092\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.1306\t Accuracy 0.9091\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.1308\t Accuracy 0.9088\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.1306\t Accuracy 0.9090\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.1305\t Accuracy 0.9093\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.1304\t Accuracy 0.9099\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.1305\t Accuracy 0.9098\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.1307\t Accuracy 0.9096\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.1306\t Accuracy 0.9097\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.1304\t Accuracy 0.9098\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.1304\t Accuracy 0.9097\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.1306\t Accuracy 0.9095\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.1309\t Accuracy 0.9089\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.1310\t Accuracy 0.9090\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.1313\t Accuracy 0.9086\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.1315\t Accuracy 0.9081\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.1318\t Accuracy 0.9077\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.1321\t Accuracy 0.9070\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.1320\t Accuracy 0.9071\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.1319\t Accuracy 0.9073\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.1316\t Accuracy 0.9078\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.1314\t Accuracy 0.9081\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.1315\t Accuracy 0.9077\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.1315\t Accuracy 0.9078\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.1315\t Accuracy 0.9080\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.1315\t Accuracy 0.9079\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.1313\t Accuracy 0.9082\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.1316\t Accuracy 0.9076\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.1317\t Accuracy 0.9074\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.1316\t Accuracy 0.9075\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.1315\t Accuracy 0.9075\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.1315\t Accuracy 0.9076\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.1316\t Accuracy 0.9074\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.1315\t Accuracy 0.9077\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.1314\t Accuracy 0.9079\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.1316\t Accuracy 0.9077\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.1317\t Accuracy 0.9078\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.1320\t Accuracy 0.9074\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.1320\t Accuracy 0.9073\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.1319\t Accuracy 0.9074\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.1319\t Accuracy 0.9074\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.1320\t Accuracy 0.9072\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.1319\t Accuracy 0.9075\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.1320\t Accuracy 0.9072\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.1321\t Accuracy 0.9072\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.1324\t Accuracy 0.9069\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.1322\t Accuracy 0.9071\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.1322\t Accuracy 0.9070\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.1322\t Accuracy 0.9069\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.1320\t Accuracy 0.9072\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.1320\t Accuracy 0.9070\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.1321\t Accuracy 0.9068\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.1323\t Accuracy 0.9066\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.1324\t Accuracy 0.9064\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.1324\t Accuracy 0.9065\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.1322\t Accuracy 0.9066\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.1321\t Accuracy 0.9068\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.1322\t Accuracy 0.9065\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.1322\t Accuracy 0.9063\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.1321\t Accuracy 0.9065\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.1321\t Accuracy 0.9065\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.1320\t Accuracy 0.9068\n",
      "\n",
      "Epoch [26]\t Average training loss 0.1320\t Average training accuracy 0.9066\n",
      "Epoch [26]\t Average validation loss 0.1159\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0625\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.1202\t Accuracy 0.9196\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.1288\t Accuracy 0.9139\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.1353\t Accuracy 0.9007\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.1299\t Accuracy 0.9055\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.1276\t Accuracy 0.9080\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.1258\t Accuracy 0.9123\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.1254\t Accuracy 0.9128\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.1246\t Accuracy 0.9162\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.1247\t Accuracy 0.9169\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.1240\t Accuracy 0.9188\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.1240\t Accuracy 0.9174\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.1239\t Accuracy 0.9170\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.1230\t Accuracy 0.9186\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.1233\t Accuracy 0.9175\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.1244\t Accuracy 0.9162\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.1248\t Accuracy 0.9154\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.1253\t Accuracy 0.9143\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.1266\t Accuracy 0.9120\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.1264\t Accuracy 0.9120\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.1258\t Accuracy 0.9125\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.1255\t Accuracy 0.9134\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.1248\t Accuracy 0.9147\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.1246\t Accuracy 0.9149\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.1256\t Accuracy 0.9133\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.1258\t Accuracy 0.9131\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.1264\t Accuracy 0.9123\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.1267\t Accuracy 0.9118\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.1271\t Accuracy 0.9113\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.1279\t Accuracy 0.9103\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.1289\t Accuracy 0.9089\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.1287\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.1291\t Accuracy 0.9084\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.1287\t Accuracy 0.9088\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.1289\t Accuracy 0.9090\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.1290\t Accuracy 0.9090\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.1296\t Accuracy 0.9082\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.1292\t Accuracy 0.9090\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.1287\t Accuracy 0.9101\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.1286\t Accuracy 0.9105\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.1282\t Accuracy 0.9109\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.1280\t Accuracy 0.9113\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.1284\t Accuracy 0.9106\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.1282\t Accuracy 0.9108\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.1277\t Accuracy 0.9114\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.1280\t Accuracy 0.9114\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.1283\t Accuracy 0.9111\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.1283\t Accuracy 0.9111\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.1283\t Accuracy 0.9112\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.1282\t Accuracy 0.9110\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.1285\t Accuracy 0.9106\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.1282\t Accuracy 0.9109\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.1281\t Accuracy 0.9111\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.1280\t Accuracy 0.9117\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.1282\t Accuracy 0.9116\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.1284\t Accuracy 0.9113\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.1282\t Accuracy 0.9114\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.1281\t Accuracy 0.9115\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.1281\t Accuracy 0.9114\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.1282\t Accuracy 0.9113\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.1285\t Accuracy 0.9107\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.1286\t Accuracy 0.9108\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.1289\t Accuracy 0.9104\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.1292\t Accuracy 0.9099\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.1294\t Accuracy 0.9096\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.1298\t Accuracy 0.9089\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.1296\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.1296\t Accuracy 0.9093\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.1292\t Accuracy 0.9097\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.1290\t Accuracy 0.9099\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.1292\t Accuracy 0.9096\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.1291\t Accuracy 0.9096\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.1291\t Accuracy 0.9098\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.1291\t Accuracy 0.9098\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.1289\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.1292\t Accuracy 0.9093\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.1293\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.1293\t Accuracy 0.9092\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.1292\t Accuracy 0.9092\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.1291\t Accuracy 0.9093\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.1292\t Accuracy 0.9092\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.1291\t Accuracy 0.9094\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.1290\t Accuracy 0.9097\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.1293\t Accuracy 0.9094\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.1293\t Accuracy 0.9095\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.1296\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.1296\t Accuracy 0.9090\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.1295\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.1295\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.1297\t Accuracy 0.9089\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.1296\t Accuracy 0.9091\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.1296\t Accuracy 0.9089\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.1297\t Accuracy 0.9088\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.1300\t Accuracy 0.9085\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.1298\t Accuracy 0.9087\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.1298\t Accuracy 0.9086\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.1298\t Accuracy 0.9085\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.1297\t Accuracy 0.9088\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.1296\t Accuracy 0.9086\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.1297\t Accuracy 0.9084\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.1300\t Accuracy 0.9082\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.1301\t Accuracy 0.9080\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.1300\t Accuracy 0.9081\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.1299\t Accuracy 0.9082\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.1297\t Accuracy 0.9083\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.1298\t Accuracy 0.9080\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.1299\t Accuracy 0.9078\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.1298\t Accuracy 0.9081\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.1297\t Accuracy 0.9080\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.1297\t Accuracy 0.9083\n",
      "\n",
      "Epoch [27]\t Average training loss 0.1297\t Average training accuracy 0.9082\n",
      "Epoch [27]\t Average validation loss 0.1138\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0611\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.1180\t Accuracy 0.9176\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.1265\t Accuracy 0.9129\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.1330\t Accuracy 0.9020\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.1277\t Accuracy 0.9065\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.1254\t Accuracy 0.9092\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.1236\t Accuracy 0.9130\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.1232\t Accuracy 0.9137\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.1224\t Accuracy 0.9172\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.1225\t Accuracy 0.9182\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.1217\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.1218\t Accuracy 0.9189\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.1217\t Accuracy 0.9183\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.1208\t Accuracy 0.9198\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.1212\t Accuracy 0.9188\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.1222\t Accuracy 0.9177\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.1225\t Accuracy 0.9167\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.1231\t Accuracy 0.9156\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.1244\t Accuracy 0.9132\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.1242\t Accuracy 0.9132\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.1236\t Accuracy 0.9137\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.1233\t Accuracy 0.9145\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.1226\t Accuracy 0.9156\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.1224\t Accuracy 0.9161\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.1234\t Accuracy 0.9145\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.1236\t Accuracy 0.9142\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.1242\t Accuracy 0.9134\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.1245\t Accuracy 0.9129\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.1249\t Accuracy 0.9124\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.1256\t Accuracy 0.9112\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.1266\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.1265\t Accuracy 0.9101\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.1269\t Accuracy 0.9094\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.1265\t Accuracy 0.9098\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.1267\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.1268\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.1274\t Accuracy 0.9092\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.1269\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.1264\t Accuracy 0.9110\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.1263\t Accuracy 0.9113\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.1259\t Accuracy 0.9119\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.1258\t Accuracy 0.9123\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.1261\t Accuracy 0.9116\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.1259\t Accuracy 0.9119\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.1255\t Accuracy 0.9124\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.1258\t Accuracy 0.9124\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.1261\t Accuracy 0.9122\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.1260\t Accuracy 0.9122\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.1260\t Accuracy 0.9122\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.1260\t Accuracy 0.9121\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.1262\t Accuracy 0.9118\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.1260\t Accuracy 0.9120\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.1259\t Accuracy 0.9123\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.1258\t Accuracy 0.9128\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.1259\t Accuracy 0.9127\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.1261\t Accuracy 0.9124\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.1260\t Accuracy 0.9126\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.1258\t Accuracy 0.9127\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.1258\t Accuracy 0.9126\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.1260\t Accuracy 0.9125\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.1263\t Accuracy 0.9119\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.1264\t Accuracy 0.9120\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.1267\t Accuracy 0.9116\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.1269\t Accuracy 0.9111\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.1272\t Accuracy 0.9108\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.1275\t Accuracy 0.9101\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.1274\t Accuracy 0.9104\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.1273\t Accuracy 0.9105\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.1270\t Accuracy 0.9110\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.1268\t Accuracy 0.9112\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.1269\t Accuracy 0.9109\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.1269\t Accuracy 0.9109\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.1269\t Accuracy 0.9111\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.1269\t Accuracy 0.9111\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.1267\t Accuracy 0.9113\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.1270\t Accuracy 0.9106\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.1271\t Accuracy 0.9104\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.1270\t Accuracy 0.9105\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.1269\t Accuracy 0.9105\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.1269\t Accuracy 0.9106\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.1270\t Accuracy 0.9105\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.1269\t Accuracy 0.9107\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.1268\t Accuracy 0.9109\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.1270\t Accuracy 0.9107\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.1271\t Accuracy 0.9108\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.1274\t Accuracy 0.9104\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.1274\t Accuracy 0.9103\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.1273\t Accuracy 0.9104\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.1273\t Accuracy 0.9104\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.1274\t Accuracy 0.9102\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.1273\t Accuracy 0.9105\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.1274\t Accuracy 0.9102\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.1275\t Accuracy 0.9102\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.1278\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.1276\t Accuracy 0.9101\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.1276\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.1276\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.1275\t Accuracy 0.9101\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.1274\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.1275\t Accuracy 0.9097\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.1277\t Accuracy 0.9095\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.1279\t Accuracy 0.9093\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.1278\t Accuracy 0.9094\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.1277\t Accuracy 0.9094\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.1275\t Accuracy 0.9096\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.1276\t Accuracy 0.9093\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.1277\t Accuracy 0.9091\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.1276\t Accuracy 0.9093\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.1275\t Accuracy 0.9093\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.1275\t Accuracy 0.9095\n",
      "\n",
      "Epoch [28]\t Average training loss 0.1275\t Average training accuracy 0.9094\n",
      "Epoch [28]\t Average validation loss 0.1118\t Average validation accuracy 0.9284\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0597\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.1160\t Accuracy 0.9216\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.1244\t Accuracy 0.9149\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.1309\t Accuracy 0.9053\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.1256\t Accuracy 0.9095\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.1233\t Accuracy 0.9120\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.1216\t Accuracy 0.9150\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.1211\t Accuracy 0.9157\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.1203\t Accuracy 0.9190\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.1204\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.1197\t Accuracy 0.9216\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.1197\t Accuracy 0.9205\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.1196\t Accuracy 0.9201\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.1188\t Accuracy 0.9217\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.1191\t Accuracy 0.9207\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.1201\t Accuracy 0.9194\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.1205\t Accuracy 0.9184\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.1210\t Accuracy 0.9172\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.1223\t Accuracy 0.9147\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.1221\t Accuracy 0.9146\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.1215\t Accuracy 0.9150\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.1212\t Accuracy 0.9157\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.1206\t Accuracy 0.9168\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.1203\t Accuracy 0.9174\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.1213\t Accuracy 0.9157\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.1215\t Accuracy 0.9154\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.1221\t Accuracy 0.9146\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.1224\t Accuracy 0.9141\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.1228\t Accuracy 0.9136\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.1236\t Accuracy 0.9124\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.1245\t Accuracy 0.9112\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.1244\t Accuracy 0.9113\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.1248\t Accuracy 0.9107\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.1244\t Accuracy 0.9111\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.1246\t Accuracy 0.9112\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.1247\t Accuracy 0.9113\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.1253\t Accuracy 0.9104\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.1248\t Accuracy 0.9111\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.1243\t Accuracy 0.9122\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.1242\t Accuracy 0.9125\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.1238\t Accuracy 0.9131\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.1237\t Accuracy 0.9136\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.1240\t Accuracy 0.9129\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.1238\t Accuracy 0.9131\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.1234\t Accuracy 0.9136\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.1237\t Accuracy 0.9137\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.1240\t Accuracy 0.9134\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.1240\t Accuracy 0.9134\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.1240\t Accuracy 0.9134\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.1239\t Accuracy 0.9133\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.1241\t Accuracy 0.9130\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.1239\t Accuracy 0.9132\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.1238\t Accuracy 0.9135\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.1237\t Accuracy 0.9140\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.1239\t Accuracy 0.9138\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.1240\t Accuracy 0.9136\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.1239\t Accuracy 0.9138\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.1238\t Accuracy 0.9139\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.1238\t Accuracy 0.9138\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.1239\t Accuracy 0.9138\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.1242\t Accuracy 0.9132\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.1243\t Accuracy 0.9133\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.1246\t Accuracy 0.9129\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.1249\t Accuracy 0.9124\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.1251\t Accuracy 0.9121\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.1255\t Accuracy 0.9114\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.1253\t Accuracy 0.9118\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.1253\t Accuracy 0.9119\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.1249\t Accuracy 0.9124\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.1247\t Accuracy 0.9126\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.1249\t Accuracy 0.9123\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.1248\t Accuracy 0.9124\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.1248\t Accuracy 0.9126\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.1248\t Accuracy 0.9125\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.1246\t Accuracy 0.9127\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.1249\t Accuracy 0.9121\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.1250\t Accuracy 0.9119\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.1249\t Accuracy 0.9120\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.1248\t Accuracy 0.9120\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.1248\t Accuracy 0.9120\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.1249\t Accuracy 0.9119\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.1248\t Accuracy 0.9122\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.1247\t Accuracy 0.9124\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.1250\t Accuracy 0.9122\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.1250\t Accuracy 0.9122\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.1253\t Accuracy 0.9118\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.1253\t Accuracy 0.9117\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.1252\t Accuracy 0.9118\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.1252\t Accuracy 0.9118\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.1254\t Accuracy 0.9116\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.1252\t Accuracy 0.9119\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.1253\t Accuracy 0.9116\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.1254\t Accuracy 0.9115\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.1257\t Accuracy 0.9112\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.1255\t Accuracy 0.9115\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.1255\t Accuracy 0.9114\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.1255\t Accuracy 0.9113\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.1254\t Accuracy 0.9115\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.1253\t Accuracy 0.9113\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.1254\t Accuracy 0.9111\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.1257\t Accuracy 0.9109\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.1258\t Accuracy 0.9107\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.1257\t Accuracy 0.9107\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.1256\t Accuracy 0.9108\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.1255\t Accuracy 0.9110\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.1255\t Accuracy 0.9107\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.1256\t Accuracy 0.9105\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.1255\t Accuracy 0.9108\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.1255\t Accuracy 0.9107\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.1254\t Accuracy 0.9109\n",
      "\n",
      "Epoch [29]\t Average training loss 0.1254\t Average training accuracy 0.9108\n",
      "Epoch [29]\t Average validation loss 0.1099\t Average validation accuracy 0.9294\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9161.\n",
      "\n",
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 3.4146\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 1.3944\t Accuracy 0.2255\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 0.9326\t Accuracy 0.3089\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 0.7512\t Accuracy 0.3603\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 0.6449\t Accuracy 0.4254\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 0.5738\t Accuracy 0.4813\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 0.5196\t Accuracy 0.5236\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.4769\t Accuracy 0.5604\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.4419\t Accuracy 0.5900\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.4143\t Accuracy 0.6142\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.3901\t Accuracy 0.6369\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.3691\t Accuracy 0.6568\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.3512\t Accuracy 0.6737\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.3348\t Accuracy 0.6905\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.3207\t Accuracy 0.7039\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.3091\t Accuracy 0.7146\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.2985\t Accuracy 0.7246\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.2892\t Accuracy 0.7325\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.2817\t Accuracy 0.7390\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.2730\t Accuracy 0.7475\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.2646\t Accuracy 0.7560\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.2582\t Accuracy 0.7615\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.2515\t Accuracy 0.7685\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.2454\t Accuracy 0.7746\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.2408\t Accuracy 0.7789\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.2365\t Accuracy 0.7826\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.2327\t Accuracy 0.7856\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.2291\t Accuracy 0.7893\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.2254\t Accuracy 0.7926\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.2225\t Accuracy 0.7950\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.2199\t Accuracy 0.7977\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.2163\t Accuracy 0.8012\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.2135\t Accuracy 0.8037\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.2101\t Accuracy 0.8070\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.2073\t Accuracy 0.8093\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.2045\t Accuracy 0.8117\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.2024\t Accuracy 0.8135\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.1993\t Accuracy 0.8167\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.1965\t Accuracy 0.8199\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.1942\t Accuracy 0.8223\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.1918\t Accuracy 0.8247\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.1897\t Accuracy 0.8268\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.1881\t Accuracy 0.8282\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.1862\t Accuracy 0.8302\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.1841\t Accuracy 0.8325\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.1825\t Accuracy 0.8340\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.1812\t Accuracy 0.8352\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.1796\t Accuracy 0.8368\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.1783\t Accuracy 0.8378\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.1767\t Accuracy 0.8394\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.1755\t Accuracy 0.8405\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.1738\t Accuracy 0.8423\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.1724\t Accuracy 0.8439\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.1709\t Accuracy 0.8455\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.1698\t Accuracy 0.8467\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.1688\t Accuracy 0.8475\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.1675\t Accuracy 0.8486\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.1663\t Accuracy 0.8499\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.1652\t Accuracy 0.8509\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.1644\t Accuracy 0.8517\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.1635\t Accuracy 0.8525\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.1627\t Accuracy 0.8534\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.1620\t Accuracy 0.8540\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.1613\t Accuracy 0.8546\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.1606\t Accuracy 0.8551\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.1602\t Accuracy 0.8552\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.1592\t Accuracy 0.8563\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.1582\t Accuracy 0.8572\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.1570\t Accuracy 0.8588\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.1560\t Accuracy 0.8597\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.1554\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.1546\t Accuracy 0.8609\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.1537\t Accuracy 0.8618\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.1530\t Accuracy 0.8626\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.1521\t Accuracy 0.8636\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.1517\t Accuracy 0.8639\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.1511\t Accuracy 0.8642\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.1505\t Accuracy 0.8649\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.1497\t Accuracy 0.8657\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.1491\t Accuracy 0.8661\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.1486\t Accuracy 0.8668\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.1479\t Accuracy 0.8676\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.1472\t Accuracy 0.8684\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.1468\t Accuracy 0.8686\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.1461\t Accuracy 0.8693\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.1458\t Accuracy 0.8695\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.1453\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.1446\t Accuracy 0.8707\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.1440\t Accuracy 0.8713\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.1435\t Accuracy 0.8718\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.1430\t Accuracy 0.8723\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.1425\t Accuracy 0.8728\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.1420\t Accuracy 0.8733\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.1417\t Accuracy 0.8734\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.1411\t Accuracy 0.8742\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.1407\t Accuracy 0.8745\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.1403\t Accuracy 0.8748\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.1398\t Accuracy 0.8754\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.1394\t Accuracy 0.8757\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.1389\t Accuracy 0.8761\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.1387\t Accuracy 0.8761\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.1384\t Accuracy 0.8764\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.1379\t Accuracy 0.8770\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.1374\t Accuracy 0.8775\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.1369\t Accuracy 0.8779\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.1366\t Accuracy 0.8782\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.1363\t Accuracy 0.8783\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.1359\t Accuracy 0.8789\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.1354\t Accuracy 0.8793\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.1349\t Accuracy 0.8798\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1346\t Average training accuracy 0.8799\n",
      "Epoch [0]\t Average validation loss 0.0785\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.0450\t Accuracy 1.0000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.0847\t Accuracy 0.9353\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.0862\t Accuracy 0.9317\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.0940\t Accuracy 0.9232\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.0910\t Accuracy 0.9244\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.0888\t Accuracy 0.9271\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.0880\t Accuracy 0.9276\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.0868\t Accuracy 0.9299\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.0859\t Accuracy 0.9314\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.0858\t Accuracy 0.9319\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.0854\t Accuracy 0.9319\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.0854\t Accuracy 0.9312\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.0855\t Accuracy 0.9308\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.0847\t Accuracy 0.9329\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.0850\t Accuracy 0.9330\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.0852\t Accuracy 0.9330\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.0853\t Accuracy 0.9331\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.0859\t Accuracy 0.9313\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.0873\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.0871\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.0864\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.0866\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.0863\t Accuracy 0.9312\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.0860\t Accuracy 0.9319\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.0867\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.0869\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.0879\t Accuracy 0.9290\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.0885\t Accuracy 0.9286\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.0887\t Accuracy 0.9286\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.0893\t Accuracy 0.9279\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.0901\t Accuracy 0.9270\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.0900\t Accuracy 0.9270\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.0903\t Accuracy 0.9265\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.0899\t Accuracy 0.9268\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.0900\t Accuracy 0.9267\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.0900\t Accuracy 0.9267\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.0902\t Accuracy 0.9264\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.0896\t Accuracy 0.9271\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.0891\t Accuracy 0.9279\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.0890\t Accuracy 0.9282\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.0887\t Accuracy 0.9285\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.0884\t Accuracy 0.9290\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.0886\t Accuracy 0.9285\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.0883\t Accuracy 0.9288\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.0881\t Accuracy 0.9289\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.0880\t Accuracy 0.9289\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.0882\t Accuracy 0.9288\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.0881\t Accuracy 0.9290\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.0882\t Accuracy 0.9287\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.0880\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.0880\t Accuracy 0.9291\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.0877\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.0876\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.0874\t Accuracy 0.9298\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.0875\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.0876\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.0874\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.0873\t Accuracy 0.9297\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.0872\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.0874\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.0874\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.0875\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.0876\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.0877\t Accuracy 0.9290\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.0879\t Accuracy 0.9286\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.0882\t Accuracy 0.9281\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.0880\t Accuracy 0.9284\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.0879\t Accuracy 0.9286\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.0875\t Accuracy 0.9292\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.0873\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.0873\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.0872\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.0871\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.0870\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.0868\t Accuracy 0.9299\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.0871\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.0872\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.0871\t Accuracy 0.9294\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.0870\t Accuracy 0.9295\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.0870\t Accuracy 0.9292\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.0870\t Accuracy 0.9293\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.0868\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.0867\t Accuracy 0.9298\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.0868\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.0867\t Accuracy 0.9299\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.0868\t Accuracy 0.9297\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.0868\t Accuracy 0.9296\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.0867\t Accuracy 0.9299\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.0866\t Accuracy 0.9301\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.0866\t Accuracy 0.9301\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.0865\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.0864\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.0864\t Accuracy 0.9304\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.0866\t Accuracy 0.9302\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.0864\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.0865\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.0865\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.0864\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.0864\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.0864\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.0865\t Accuracy 0.9302\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.0866\t Accuracy 0.9302\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.0865\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.0864\t Accuracy 0.9305\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.0862\t Accuracy 0.9306\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.0862\t Accuracy 0.9306\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.0863\t Accuracy 0.9303\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.0862\t Accuracy 0.9304\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.0861\t Accuracy 0.9304\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.0859\t Accuracy 0.9307\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0859\t Average training accuracy 0.9307\n",
      "Epoch [1]\t Average validation loss 0.0697\t Average validation accuracy 0.9526\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.0401\t Accuracy 1.0000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.0746\t Accuracy 0.9412\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.0743\t Accuracy 0.9416\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.0820\t Accuracy 0.9331\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.0795\t Accuracy 0.9353\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.0778\t Accuracy 0.9382\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.0770\t Accuracy 0.9385\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.0760\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.0755\t Accuracy 0.9416\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.0756\t Accuracy 0.9417\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.0752\t Accuracy 0.9425\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.0750\t Accuracy 0.9425\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.0751\t Accuracy 0.9423\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.0743\t Accuracy 0.9438\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.0747\t Accuracy 0.9434\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.0748\t Accuracy 0.9438\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.0747\t Accuracy 0.9439\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.0753\t Accuracy 0.9428\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.0765\t Accuracy 0.9412\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.0764\t Accuracy 0.9409\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.0758\t Accuracy 0.9417\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.0761\t Accuracy 0.9418\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.0759\t Accuracy 0.9424\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.0757\t Accuracy 0.9429\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.0765\t Accuracy 0.9417\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.0766\t Accuracy 0.9416\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.0776\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.0781\t Accuracy 0.9398\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.0783\t Accuracy 0.9397\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.0788\t Accuracy 0.9392\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.0795\t Accuracy 0.9387\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.0794\t Accuracy 0.9384\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.0797\t Accuracy 0.9379\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.0794\t Accuracy 0.9382\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.0795\t Accuracy 0.9380\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.0796\t Accuracy 0.9379\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.0797\t Accuracy 0.9378\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.0792\t Accuracy 0.9384\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.0788\t Accuracy 0.9392\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.0786\t Accuracy 0.9395\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.0783\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.0780\t Accuracy 0.9403\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.0782\t Accuracy 0.9398\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.0780\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.0778\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.0778\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.0780\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.0780\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.0781\t Accuracy 0.9398\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.0779\t Accuracy 0.9403\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.0779\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.0778\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.0777\t Accuracy 0.9402\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.0776\t Accuracy 0.9404\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.0777\t Accuracy 0.9402\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.0778\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.0777\t Accuracy 0.9402\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.0776\t Accuracy 0.9403\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.0775\t Accuracy 0.9402\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.0777\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.0778\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.0778\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.0780\t Accuracy 0.9398\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.0781\t Accuracy 0.9395\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.0783\t Accuracy 0.9393\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.0786\t Accuracy 0.9388\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.0784\t Accuracy 0.9390\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.0784\t Accuracy 0.9392\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.0780\t Accuracy 0.9397\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.0778\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.0779\t Accuracy 0.9398\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.0778\t Accuracy 0.9397\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.0777\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.0776\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.0774\t Accuracy 0.9403\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.0776\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.0778\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.0777\t Accuracy 0.9399\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.0776\t Accuracy 0.9401\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.0776\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.0776\t Accuracy 0.9402\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.0775\t Accuracy 0.9404\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.0773\t Accuracy 0.9406\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.0775\t Accuracy 0.9404\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.0774\t Accuracy 0.9406\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.0775\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.0775\t Accuracy 0.9403\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.0774\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.0773\t Accuracy 0.9406\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.0773\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.0772\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.0772\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.0772\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.0774\t Accuracy 0.9406\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.0772\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.0773\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.0774\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.0773\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.0773\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.0773\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.0775\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.0775\t Accuracy 0.9404\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.0775\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.0774\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.0773\t Accuracy 0.9408\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.0773\t Accuracy 0.9407\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.0774\t Accuracy 0.9404\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.0773\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.0773\t Accuracy 0.9405\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.0771\t Accuracy 0.9408\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0771\t Average training accuracy 0.9407\n",
      "Epoch [2]\t Average validation loss 0.0662\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.0401\t Accuracy 1.0000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.0685\t Accuracy 0.9392\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.0683\t Accuracy 0.9455\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.0761\t Accuracy 0.9351\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.0733\t Accuracy 0.9403\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.0716\t Accuracy 0.9446\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.0712\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.0700\t Accuracy 0.9487\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.0694\t Accuracy 0.9494\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.0697\t Accuracy 0.9494\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.0691\t Accuracy 0.9501\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.0689\t Accuracy 0.9501\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.0692\t Accuracy 0.9499\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.0685\t Accuracy 0.9507\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.0688\t Accuracy 0.9506\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.0688\t Accuracy 0.9513\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.0688\t Accuracy 0.9512\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.0691\t Accuracy 0.9504\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.0704\t Accuracy 0.9484\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.0704\t Accuracy 0.9477\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.0698\t Accuracy 0.9482\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.0701\t Accuracy 0.9480\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.0700\t Accuracy 0.9485\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.0699\t Accuracy 0.9492\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.0707\t Accuracy 0.9478\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.0707\t Accuracy 0.9476\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.0717\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.0720\t Accuracy 0.9463\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.0722\t Accuracy 0.9460\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.0726\t Accuracy 0.9459\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.0732\t Accuracy 0.9456\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.0732\t Accuracy 0.9453\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.0735\t Accuracy 0.9449\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.0731\t Accuracy 0.9452\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.0732\t Accuracy 0.9451\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.0733\t Accuracy 0.9449\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.0735\t Accuracy 0.9448\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.0731\t Accuracy 0.9454\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.0726\t Accuracy 0.9459\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.0725\t Accuracy 0.9461\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.0721\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.0719\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.0721\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.0719\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.0718\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.0718\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.0720\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.0719\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.0720\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.0718\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.0718\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.0717\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.0717\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.0716\t Accuracy 0.9469\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.0717\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.0718\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.0717\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.0717\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.0716\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.0718\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.0719\t Accuracy 0.9463\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.0720\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.0721\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.0722\t Accuracy 0.9459\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.0724\t Accuracy 0.9457\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.0726\t Accuracy 0.9453\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.0725\t Accuracy 0.9455\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.0724\t Accuracy 0.9457\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.0721\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.0719\t Accuracy 0.9463\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.0719\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.0719\t Accuracy 0.9460\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.0718\t Accuracy 0.9463\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.0717\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.0716\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.0717\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.0718\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.0718\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.0717\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.0717\t Accuracy 0.9462\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.0716\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.0715\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.0714\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.0715\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.0714\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.0715\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.0716\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.0715\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.0713\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.0714\t Accuracy 0.9469\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.0713\t Accuracy 0.9470\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.0713\t Accuracy 0.9469\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.0713\t Accuracy 0.9469\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.0715\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.0714\t Accuracy 0.9469\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.0714\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.0715\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.0715\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.0715\t Accuracy 0.9468\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.0715\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.0717\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.0718\t Accuracy 0.9464\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.0717\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.0717\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.0716\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.0716\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.0717\t Accuracy 0.9465\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.0716\t Accuracy 0.9467\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.0716\t Accuracy 0.9466\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.0714\t Accuracy 0.9469\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0714\t Average training accuracy 0.9468\n",
      "Epoch [3]\t Average validation loss 0.0628\t Average validation accuracy 0.9604\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.0382\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.0637\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.0638\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.0708\t Accuracy 0.9424\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.0676\t Accuracy 0.9463\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.0660\t Accuracy 0.9498\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.0659\t Accuracy 0.9518\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.0648\t Accuracy 0.9541\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.0642\t Accuracy 0.9541\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.0645\t Accuracy 0.9537\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.0640\t Accuracy 0.9548\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.0644\t Accuracy 0.9546\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.0637\t Accuracy 0.9555\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.0640\t Accuracy 0.9553\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.0640\t Accuracy 0.9559\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.0641\t Accuracy 0.9559\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.0645\t Accuracy 0.9555\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.0658\t Accuracy 0.9533\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.0659\t Accuracy 0.9528\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.0653\t Accuracy 0.9528\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.0656\t Accuracy 0.9526\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.0655\t Accuracy 0.9530\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.0653\t Accuracy 0.9532\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.0661\t Accuracy 0.9518\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.0662\t Accuracy 0.9518\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.0670\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.0674\t Accuracy 0.9509\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.0675\t Accuracy 0.9507\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.0678\t Accuracy 0.9507\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.0683\t Accuracy 0.9505\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.0684\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.0687\t Accuracy 0.9499\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.0684\t Accuracy 0.9502\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.0684\t Accuracy 0.9502\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.0685\t Accuracy 0.9499\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.0687\t Accuracy 0.9498\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.0683\t Accuracy 0.9502\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.0679\t Accuracy 0.9508\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.0678\t Accuracy 0.9508\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.0675\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.0673\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.0675\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.0673\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.0672\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.0672\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.0674\t Accuracy 0.9508\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.0673\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.0674\t Accuracy 0.9508\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.0672\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.0672\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.0671\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.0671\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.0670\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.0672\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.0674\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.0672\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.0672\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.0672\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.0673\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.0674\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.0675\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.0676\t Accuracy 0.9509\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.0678\t Accuracy 0.9507\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.0679\t Accuracy 0.9505\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.0681\t Accuracy 0.9503\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.0680\t Accuracy 0.9505\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.0679\t Accuracy 0.9507\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.0676\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.0675\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.0675\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.0675\t Accuracy 0.9510\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.0674\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.0673\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.0672\t Accuracy 0.9514\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.0673\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.0674\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.0673\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.0672\t Accuracy 0.9514\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.0673\t Accuracy 0.9514\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.0672\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.0671\t Accuracy 0.9517\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.0670\t Accuracy 0.9519\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.0671\t Accuracy 0.9516\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.0671\t Accuracy 0.9517\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.0672\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.0672\t Accuracy 0.9514\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.0671\t Accuracy 0.9516\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.0670\t Accuracy 0.9518\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.0670\t Accuracy 0.9519\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.0670\t Accuracy 0.9520\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.0670\t Accuracy 0.9520\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.0670\t Accuracy 0.9519\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.0671\t Accuracy 0.9517\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.0671\t Accuracy 0.9518\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.0671\t Accuracy 0.9517\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.0672\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.0672\t Accuracy 0.9515\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.0672\t Accuracy 0.9516\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.0673\t Accuracy 0.9514\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.0674\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.0675\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.0675\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.0675\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.0674\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.0674\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.0675\t Accuracy 0.9511\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.0674\t Accuracy 0.9513\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.0674\t Accuracy 0.9512\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.0672\t Accuracy 0.9514\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0673\t Average training accuracy 0.9513\n",
      "Epoch [4]\t Average validation loss 0.0604\t Average validation accuracy 0.9620\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.0387\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.0600\t Accuracy 0.9549\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.0604\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.0676\t Accuracy 0.9444\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.0643\t Accuracy 0.9488\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.0627\t Accuracy 0.9522\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.0626\t Accuracy 0.9538\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.0615\t Accuracy 0.9567\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.0611\t Accuracy 0.9564\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.0612\t Accuracy 0.9561\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.0607\t Accuracy 0.9569\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.0607\t Accuracy 0.9572\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.0611\t Accuracy 0.9566\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.0604\t Accuracy 0.9576\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.0606\t Accuracy 0.9578\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.0607\t Accuracy 0.9585\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.0608\t Accuracy 0.9586\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.0612\t Accuracy 0.9582\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.0627\t Accuracy 0.9562\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.0627\t Accuracy 0.9560\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.0621\t Accuracy 0.9561\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.0625\t Accuracy 0.9560\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.0623\t Accuracy 0.9563\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.0622\t Accuracy 0.9566\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.0629\t Accuracy 0.9554\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.0630\t Accuracy 0.9556\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.0638\t Accuracy 0.9550\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.0642\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.0643\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.0646\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.0650\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.0651\t Accuracy 0.9538\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.0653\t Accuracy 0.9534\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.0650\t Accuracy 0.9537\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.0650\t Accuracy 0.9539\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.0651\t Accuracy 0.9536\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.0653\t Accuracy 0.9535\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.0649\t Accuracy 0.9538\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.0646\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.0645\t Accuracy 0.9543\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.0642\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.0640\t Accuracy 0.9550\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.0641\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.0640\t Accuracy 0.9550\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.0639\t Accuracy 0.9549\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.0639\t Accuracy 0.9550\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.0641\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.0640\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.0641\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.0639\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.0639\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.0639\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.0638\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.0638\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.0640\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.0641\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.0640\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.0640\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.0639\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.0641\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.0641\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.0642\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.0643\t Accuracy 0.9543\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.0645\t Accuracy 0.9540\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.0646\t Accuracy 0.9538\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.0648\t Accuracy 0.9537\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.0646\t Accuracy 0.9538\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.0646\t Accuracy 0.9540\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.0643\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.0642\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.0642\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.0642\t Accuracy 0.9543\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.0641\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.0640\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.0639\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.0641\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.0640\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.0640\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.0639\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.0638\t Accuracy 0.9549\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.0639\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.0639\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.0640\t Accuracy 0.9543\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.0639\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.0638\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.0639\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.0638\t Accuracy 0.9549\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.0638\t Accuracy 0.9549\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.0638\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.0640\t Accuracy 0.9546\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.0639\t Accuracy 0.9548\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.0640\t Accuracy 0.9547\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.0641\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.0640\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.0641\t Accuracy 0.9545\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.0642\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.0644\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.0645\t Accuracy 0.9541\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.0644\t Accuracy 0.9541\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.0644\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.0643\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.0643\t Accuracy 0.9544\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.0644\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.0643\t Accuracy 0.9543\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.0643\t Accuracy 0.9542\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.0642\t Accuracy 0.9544\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0642\t Average training accuracy 0.9544\n",
      "Epoch [5]\t Average validation loss 0.0589\t Average validation accuracy 0.9626\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.0410\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.0573\t Accuracy 0.9588\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.0580\t Accuracy 0.9584\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.0651\t Accuracy 0.9490\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.0620\t Accuracy 0.9527\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.0602\t Accuracy 0.9558\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.0602\t Accuracy 0.9568\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.0591\t Accuracy 0.9595\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.0586\t Accuracy 0.9589\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.0588\t Accuracy 0.9588\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.0583\t Accuracy 0.9595\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.0583\t Accuracy 0.9593\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.0587\t Accuracy 0.9592\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.0582\t Accuracy 0.9599\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.0584\t Accuracy 0.9603\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.0584\t Accuracy 0.9609\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.0590\t Accuracy 0.9608\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.0604\t Accuracy 0.9590\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.0604\t Accuracy 0.9590\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.0599\t Accuracy 0.9592\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.0602\t Accuracy 0.9591\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.0600\t Accuracy 0.9595\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.0598\t Accuracy 0.9597\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.0605\t Accuracy 0.9586\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.0606\t Accuracy 0.9587\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.0613\t Accuracy 0.9583\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.0617\t Accuracy 0.9582\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.0618\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.0621\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.0625\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.0626\t Accuracy 0.9571\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.0628\t Accuracy 0.9569\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.0625\t Accuracy 0.9571\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.0625\t Accuracy 0.9574\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.0626\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.0627\t Accuracy 0.9572\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.0624\t Accuracy 0.9574\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.0621\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.0620\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.0617\t Accuracy 0.9583\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.0615\t Accuracy 0.9585\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.0617\t Accuracy 0.9584\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.0616\t Accuracy 0.9585\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.0615\t Accuracy 0.9585\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.0615\t Accuracy 0.9585\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.0617\t Accuracy 0.9581\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.0616\t Accuracy 0.9583\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.0617\t Accuracy 0.9581\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.0615\t Accuracy 0.9581\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.0615\t Accuracy 0.9582\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.0615\t Accuracy 0.9581\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.0615\t Accuracy 0.9582\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.0614\t Accuracy 0.9583\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.0616\t Accuracy 0.9581\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.0617\t Accuracy 0.9582\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.0616\t Accuracy 0.9582\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.0616\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.0615\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.0617\t Accuracy 0.9579\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.0618\t Accuracy 0.9579\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.0619\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.0619\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.0621\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.0622\t Accuracy 0.9571\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.0624\t Accuracy 0.9572\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.0623\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.0622\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.0620\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.0618\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.0618\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.0618\t Accuracy 0.9575\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.0617\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.0615\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.0617\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.0617\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.0616\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.0615\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.0616\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.0616\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.0617\t Accuracy 0.9575\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.0617\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.0616\t Accuracy 0.9579\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.0616\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.0615\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.0615\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.0615\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.0617\t Accuracy 0.9579\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.0616\t Accuracy 0.9580\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.0617\t Accuracy 0.9578\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.0618\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.0618\t Accuracy 0.9577\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.0619\t Accuracy 0.9576\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.0620\t Accuracy 0.9575\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.0621\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.0622\t Accuracy 0.9572\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.0622\t Accuracy 0.9572\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.0622\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.0621\t Accuracy 0.9574\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.0621\t Accuracy 0.9575\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.0622\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.0621\t Accuracy 0.9574\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.0621\t Accuracy 0.9573\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.0620\t Accuracy 0.9575\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0620\t Average training accuracy 0.9574\n",
      "Epoch [6]\t Average validation loss 0.0583\t Average validation accuracy 0.9638\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.0387\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.0556\t Accuracy 0.9608\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.0570\t Accuracy 0.9594\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.0635\t Accuracy 0.9497\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.0603\t Accuracy 0.9532\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.0583\t Accuracy 0.9566\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.0582\t Accuracy 0.9581\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.0571\t Accuracy 0.9613\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.0567\t Accuracy 0.9608\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.0568\t Accuracy 0.9612\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.0564\t Accuracy 0.9617\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.0565\t Accuracy 0.9613\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.0569\t Accuracy 0.9607\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.0565\t Accuracy 0.9614\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.0566\t Accuracy 0.9619\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.0567\t Accuracy 0.9622\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.0568\t Accuracy 0.9623\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.0572\t Accuracy 0.9613\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.0587\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.0587\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.0581\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.0584\t Accuracy 0.9598\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.0582\t Accuracy 0.9604\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.0582\t Accuracy 0.9609\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.0588\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.0589\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.0596\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.0600\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.0601\t Accuracy 0.9594\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.0603\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.0606\t Accuracy 0.9589\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.0607\t Accuracy 0.9585\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.0609\t Accuracy 0.9584\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.0607\t Accuracy 0.9586\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.0607\t Accuracy 0.9588\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.0607\t Accuracy 0.9588\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.0609\t Accuracy 0.9589\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.0605\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.0602\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.0602\t Accuracy 0.9598\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.0599\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.0597\t Accuracy 0.9602\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.0599\t Accuracy 0.9602\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.0598\t Accuracy 0.9603\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.0597\t Accuracy 0.9602\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.0597\t Accuracy 0.9602\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.0599\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.0598\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.0599\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.0598\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.0597\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.0597\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.0597\t Accuracy 0.9602\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.0597\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.0599\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.0599\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.0598\t Accuracy 0.9601\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.0598\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.0597\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.0599\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.0600\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.0601\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.0601\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.0603\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.0604\t Accuracy 0.9590\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.0606\t Accuracy 0.9590\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.0604\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.0604\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.0601\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.0600\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.0600\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.0600\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.0599\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.0597\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.0599\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.0598\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.0599\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.0598\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.0597\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.0598\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.0598\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.0599\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.0600\t Accuracy 0.9594\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.0598\t Accuracy 0.9598\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.0598\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.0598\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.0598\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.0598\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.0599\t Accuracy 0.9598\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.0599\t Accuracy 0.9599\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.0600\t Accuracy 0.9598\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.0601\t Accuracy 0.9596\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.0601\t Accuracy 0.9597\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.0602\t Accuracy 0.9595\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.0602\t Accuracy 0.9594\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.0604\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.0605\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.0605\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.0605\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.0604\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.0604\t Accuracy 0.9593\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.0605\t Accuracy 0.9591\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.0604\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.0604\t Accuracy 0.9592\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.0603\t Accuracy 0.9593\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0603\t Average training accuracy 0.9593\n",
      "Epoch [7]\t Average validation loss 0.0573\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.0370\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.0541\t Accuracy 0.9627\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.0555\t Accuracy 0.9624\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.0619\t Accuracy 0.9530\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.0589\t Accuracy 0.9557\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.0568\t Accuracy 0.9594\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.0567\t Accuracy 0.9608\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.0557\t Accuracy 0.9635\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.0553\t Accuracy 0.9631\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.0554\t Accuracy 0.9634\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.0551\t Accuracy 0.9639\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.0551\t Accuracy 0.9637\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.0556\t Accuracy 0.9632\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.0551\t Accuracy 0.9636\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.0552\t Accuracy 0.9642\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.0553\t Accuracy 0.9644\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.0554\t Accuracy 0.9645\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.0558\t Accuracy 0.9633\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.0572\t Accuracy 0.9617\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.0572\t Accuracy 0.9616\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.0567\t Accuracy 0.9620\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.0570\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.0568\t Accuracy 0.9625\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.0568\t Accuracy 0.9629\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.0574\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.0575\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.0582\t Accuracy 0.9610\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.0586\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.0587\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.0589\t Accuracy 0.9609\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.0592\t Accuracy 0.9609\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.0593\t Accuracy 0.9605\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.0595\t Accuracy 0.9603\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.0593\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.0592\t Accuracy 0.9607\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.0593\t Accuracy 0.9607\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.0594\t Accuracy 0.9606\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.0591\t Accuracy 0.9607\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.0588\t Accuracy 0.9613\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.0588\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.0584\t Accuracy 0.9617\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.0583\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.0585\t Accuracy 0.9618\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.0584\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.0583\t Accuracy 0.9619\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.0583\t Accuracy 0.9618\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.0585\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.0584\t Accuracy 0.9616\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.0585\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.0584\t Accuracy 0.9616\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.0583\t Accuracy 0.9616\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.0585\t Accuracy 0.9613\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.0585\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.0585\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.0584\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.0587\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.0587\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.0587\t Accuracy 0.9610\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.0589\t Accuracy 0.9607\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.0590\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.0592\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.0591\t Accuracy 0.9606\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.0590\t Accuracy 0.9607\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.0588\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.0586\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.0587\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.0586\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.0585\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.0584\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.0586\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.0586\t Accuracy 0.9610\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.0585\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.0584\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.0583\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.0585\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.0586\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.0586\t Accuracy 0.9610\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.0585\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.0585\t Accuracy 0.9613\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.0585\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.0584\t Accuracy 0.9615\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.0585\t Accuracy 0.9614\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.0586\t Accuracy 0.9613\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.0586\t Accuracy 0.9613\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.0587\t Accuracy 0.9612\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.0588\t Accuracy 0.9610\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.0587\t Accuracy 0.9611\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.0588\t Accuracy 0.9609\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.0589\t Accuracy 0.9608\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.0591\t Accuracy 0.9605\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.0592\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.0592\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.0592\t Accuracy 0.9605\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.0591\t Accuracy 0.9606\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.0591\t Accuracy 0.9606\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.0592\t Accuracy 0.9604\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.0591\t Accuracy 0.9605\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.0591\t Accuracy 0.9605\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.0590\t Accuracy 0.9607\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0590\t Average training accuracy 0.9607\n",
      "Epoch [8]\t Average validation loss 0.0565\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.0345\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.0530\t Accuracy 0.9647\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.0544\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.0606\t Accuracy 0.9556\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.0578\t Accuracy 0.9582\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.0557\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.0556\t Accuracy 0.9635\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.0545\t Accuracy 0.9658\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.0542\t Accuracy 0.9651\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.0542\t Accuracy 0.9652\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.0539\t Accuracy 0.9653\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.0540\t Accuracy 0.9650\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.0544\t Accuracy 0.9644\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.0539\t Accuracy 0.9648\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.0540\t Accuracy 0.9653\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.0542\t Accuracy 0.9654\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.0543\t Accuracy 0.9657\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.0547\t Accuracy 0.9646\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.0561\t Accuracy 0.9630\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.0561\t Accuracy 0.9629\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.0556\t Accuracy 0.9632\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.0559\t Accuracy 0.9632\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.0558\t Accuracy 0.9638\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.0557\t Accuracy 0.9640\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.0564\t Accuracy 0.9629\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.0564\t Accuracy 0.9630\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.0572\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.0576\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.0577\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.0579\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.0581\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.0582\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.0584\t Accuracy 0.9614\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.0582\t Accuracy 0.9615\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.0581\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.0581\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.0582\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.0580\t Accuracy 0.9619\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.0576\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.0576\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.0573\t Accuracy 0.9627\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.0572\t Accuracy 0.9629\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.0573\t Accuracy 0.9628\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.0572\t Accuracy 0.9629\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.0571\t Accuracy 0.9628\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.0571\t Accuracy 0.9628\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.0574\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.0573\t Accuracy 0.9626\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.0574\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.0573\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.0572\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.0572\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.0572\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.0571\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.0573\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.0572\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.0573\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.0572\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.0573\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.0575\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.0575\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.0575\t Accuracy 0.9620\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.0577\t Accuracy 0.9618\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.0578\t Accuracy 0.9615\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.0580\t Accuracy 0.9616\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.0579\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.0578\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.0575\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.0574\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.0574\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.0574\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.0573\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.0571\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.0574\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.0572\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.0571\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.0572\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.0572\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.0573\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.0574\t Accuracy 0.9620\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.0573\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.0573\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.0573\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.0572\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.0572\t Accuracy 0.9626\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.0573\t Accuracy 0.9625\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.0574\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.0574\t Accuracy 0.9624\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.0575\t Accuracy 0.9623\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.0576\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.0575\t Accuracy 0.9622\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.0577\t Accuracy 0.9621\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.0577\t Accuracy 0.9619\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.0579\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.0580\t Accuracy 0.9616\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.0580\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.0580\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.0579\t Accuracy 0.9618\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.0579\t Accuracy 0.9619\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.0580\t Accuracy 0.9617\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.0579\t Accuracy 0.9618\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.0579\t Accuracy 0.9618\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.0578\t Accuracy 0.9620\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0578\t Average training accuracy 0.9619\n",
      "Epoch [9]\t Average validation loss 0.0562\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.0357\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.0524\t Accuracy 0.9667\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.0539\t Accuracy 0.9644\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.0600\t Accuracy 0.9570\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.0570\t Accuracy 0.9587\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.0548\t Accuracy 0.9641\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.0547\t Accuracy 0.9654\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.0536\t Accuracy 0.9672\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.0532\t Accuracy 0.9671\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.0532\t Accuracy 0.9670\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.0528\t Accuracy 0.9671\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.0530\t Accuracy 0.9668\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.0533\t Accuracy 0.9661\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.0529\t Accuracy 0.9664\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.0530\t Accuracy 0.9669\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.0531\t Accuracy 0.9670\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.0531\t Accuracy 0.9673\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.0536\t Accuracy 0.9663\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.0550\t Accuracy 0.9646\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.0550\t Accuracy 0.9646\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.0545\t Accuracy 0.9649\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.0549\t Accuracy 0.9651\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.0547\t Accuracy 0.9655\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.0546\t Accuracy 0.9659\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.0553\t Accuracy 0.9646\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.0554\t Accuracy 0.9644\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.0562\t Accuracy 0.9638\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.0565\t Accuracy 0.9640\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.0567\t Accuracy 0.9638\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.0569\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.0571\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.0572\t Accuracy 0.9631\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.0573\t Accuracy 0.9627\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.0571\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.0570\t Accuracy 0.9631\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.0571\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.0572\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.0569\t Accuracy 0.9632\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.0565\t Accuracy 0.9637\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.0565\t Accuracy 0.9637\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.0563\t Accuracy 0.9639\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.0562\t Accuracy 0.9641\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.0563\t Accuracy 0.9639\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.0562\t Accuracy 0.9640\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.0561\t Accuracy 0.9640\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.0561\t Accuracy 0.9639\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.0564\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.0563\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.0564\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.0563\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.0562\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.0562\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.0562\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.0561\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.0563\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.0563\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.0562\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.0563\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.0562\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.0565\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.0565\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.0565\t Accuracy 0.9632\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.0567\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.0568\t Accuracy 0.9628\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.0570\t Accuracy 0.9628\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.0569\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.0568\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.0565\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.0564\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.0564\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.0564\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.0563\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.0562\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.0564\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.0563\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.0563\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.0564\t Accuracy 0.9632\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.0564\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.0562\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.0561\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.0564\t Accuracy 0.9631\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.0563\t Accuracy 0.9632\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.0563\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.0563\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.0562\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.0562\t Accuracy 0.9637\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.0562\t Accuracy 0.9636\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.0564\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.0563\t Accuracy 0.9635\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.0565\t Accuracy 0.9634\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.0565\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.0565\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.0566\t Accuracy 0.9633\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.0567\t Accuracy 0.9631\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.0569\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.0570\t Accuracy 0.9628\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.0570\t Accuracy 0.9628\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.0569\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.0569\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.0568\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.0569\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.0569\t Accuracy 0.9630\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.0569\t Accuracy 0.9629\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.0568\t Accuracy 0.9631\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0568\t Average training accuracy 0.9631\n",
      "Epoch [10]\t Average validation loss 0.0557\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.0314\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.0513\t Accuracy 0.9667\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.0531\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.0592\t Accuracy 0.9563\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.0561\t Accuracy 0.9597\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.0539\t Accuracy 0.9649\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.0538\t Accuracy 0.9658\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.0527\t Accuracy 0.9675\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.0523\t Accuracy 0.9671\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.0523\t Accuracy 0.9676\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.0519\t Accuracy 0.9679\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.0521\t Accuracy 0.9675\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.0524\t Accuracy 0.9669\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.0520\t Accuracy 0.9673\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.0521\t Accuracy 0.9679\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.0522\t Accuracy 0.9676\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.0523\t Accuracy 0.9680\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.0527\t Accuracy 0.9672\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.0537\t Accuracy 0.9662\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.0540\t Accuracy 0.9663\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.0538\t Accuracy 0.9666\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.0537\t Accuracy 0.9669\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.0544\t Accuracy 0.9657\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.0545\t Accuracy 0.9655\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.0552\t Accuracy 0.9649\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.0556\t Accuracy 0.9651\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.0557\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.0558\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.0561\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.0561\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.0563\t Accuracy 0.9636\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.0561\t Accuracy 0.9637\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.0560\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.0560\t Accuracy 0.9639\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.0561\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.0558\t Accuracy 0.9642\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.0554\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.0555\t Accuracy 0.9647\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.0552\t Accuracy 0.9650\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.0551\t Accuracy 0.9651\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.0552\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.0552\t Accuracy 0.9649\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.0550\t Accuracy 0.9649\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.0550\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.0553\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.0553\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.0552\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.0551\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.0551\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.0551\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.0550\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.0552\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.0551\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.0551\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.0551\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.0552\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.0553\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.0554\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.0554\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.0556\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.0556\t Accuracy 0.9639\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.0558\t Accuracy 0.9639\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.0557\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.0556\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.0554\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.0553\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.0553\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.0553\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.0552\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.0550\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.0552\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.0552\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.0553\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.0552\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.0551\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.0550\t Accuracy 0.9647\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.0552\t Accuracy 0.9644\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.0553\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.0553\t Accuracy 0.9642\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.0552\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.0552\t Accuracy 0.9646\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.0552\t Accuracy 0.9647\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.0551\t Accuracy 0.9647\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.0551\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.0552\t Accuracy 0.9649\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.0553\t Accuracy 0.9647\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.0553\t Accuracy 0.9648\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.0554\t Accuracy 0.9646\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.0554\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.0554\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.0555\t Accuracy 0.9645\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.0556\t Accuracy 0.9643\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.0558\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.0559\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.0559\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.0559\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.0558\t Accuracy 0.9642\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.0558\t Accuracy 0.9642\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.0559\t Accuracy 0.9640\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.0558\t Accuracy 0.9642\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.0558\t Accuracy 0.9641\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.0557\t Accuracy 0.9643\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0557\t Average training accuracy 0.9643\n",
      "Epoch [11]\t Average validation loss 0.0551\t Average validation accuracy 0.9668\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.0303\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.0503\t Accuracy 0.9686\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.0524\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.0586\t Accuracy 0.9570\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.0554\t Accuracy 0.9597\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.0531\t Accuracy 0.9649\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.0529\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.0517\t Accuracy 0.9672\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.0513\t Accuracy 0.9671\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.0512\t Accuracy 0.9676\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.0508\t Accuracy 0.9681\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.0510\t Accuracy 0.9679\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.0513\t Accuracy 0.9674\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.0509\t Accuracy 0.9677\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.0511\t Accuracy 0.9685\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.0512\t Accuracy 0.9682\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.0512\t Accuracy 0.9685\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.0517\t Accuracy 0.9677\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.0530\t Accuracy 0.9664\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.0530\t Accuracy 0.9665\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.0526\t Accuracy 0.9668\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.0529\t Accuracy 0.9669\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.0527\t Accuracy 0.9672\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.0525\t Accuracy 0.9676\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.0532\t Accuracy 0.9664\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.0533\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.0540\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.0544\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.0545\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.0547\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.0549\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.0549\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.0550\t Accuracy 0.9652\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.0548\t Accuracy 0.9654\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.0547\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.0547\t Accuracy 0.9654\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.0548\t Accuracy 0.9654\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.0546\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.0542\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.0543\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.0540\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.0539\t Accuracy 0.9665\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.0541\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.0539\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.0539\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.0542\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.0541\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.0540\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.0540\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.0539\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.0539\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.0539\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.0541\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.0540\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.0540\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.0540\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.0539\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.0541\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.0542\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.0542\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.0542\t Accuracy 0.9658\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.0544\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.0545\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.0546\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.0545\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.0544\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.0542\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.0541\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.0541\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.0539\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.0538\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.0540\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.0540\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.0541\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.0541\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.0539\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.0539\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.0540\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.0540\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.0541\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.0541\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.0540\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.0540\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.0540\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.0539\t Accuracy 0.9664\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.0539\t Accuracy 0.9665\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.0539\t Accuracy 0.9665\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.0541\t Accuracy 0.9662\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.0541\t Accuracy 0.9663\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.0541\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.0542\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.0542\t Accuracy 0.9661\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.0543\t Accuracy 0.9660\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.0544\t Accuracy 0.9659\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.0546\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.0546\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.0547\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.0546\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.0546\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.0545\t Accuracy 0.9657\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.0546\t Accuracy 0.9655\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.0546\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.0546\t Accuracy 0.9656\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.0545\t Accuracy 0.9657\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0545\t Average training accuracy 0.9657\n",
      "Epoch [12]\t Average validation loss 0.0541\t Average validation accuracy 0.9686\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.0295\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.0493\t Accuracy 0.9725\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.0511\t Accuracy 0.9703\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.0573\t Accuracy 0.9603\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.0541\t Accuracy 0.9632\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.0519\t Accuracy 0.9677\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.0516\t Accuracy 0.9688\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.0500\t Accuracy 0.9701\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.0499\t Accuracy 0.9703\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.0495\t Accuracy 0.9707\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.0497\t Accuracy 0.9701\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.0499\t Accuracy 0.9694\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.0496\t Accuracy 0.9697\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.0498\t Accuracy 0.9705\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.0499\t Accuracy 0.9700\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.0500\t Accuracy 0.9703\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.0504\t Accuracy 0.9693\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.0517\t Accuracy 0.9680\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.0517\t Accuracy 0.9678\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.0513\t Accuracy 0.9681\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.0516\t Accuracy 0.9682\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.0513\t Accuracy 0.9685\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.0512\t Accuracy 0.9688\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.0518\t Accuracy 0.9678\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.0520\t Accuracy 0.9678\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.0527\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.0531\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.0533\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.0534\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.0536\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.0536\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.0537\t Accuracy 0.9665\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.0535\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.0535\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.0535\t Accuracy 0.9665\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.0536\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.0534\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.0530\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.0531\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.0528\t Accuracy 0.9674\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.0528\t Accuracy 0.9675\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.0529\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.0528\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.0527\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.0527\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.0530\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.0529\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.0529\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.0529\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.0528\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.0528\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.0528\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.0527\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.0529\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.0529\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.0528\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.0528\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.0528\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.0529\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.0530\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.0530\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.0530\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.0532\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.0533\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.0534\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.0533\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.0532\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.0530\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.0529\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.0529\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.0529\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.0528\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.0528\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.0527\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.0528\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.0529\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.0529\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.0528\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.0529\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.0529\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.0528\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.0527\t Accuracy 0.9674\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.0528\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.0528\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.0529\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.0530\t Accuracy 0.9669\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.0529\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.0528\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.0528\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.0528\t Accuracy 0.9674\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.0527\t Accuracy 0.9675\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.0528\t Accuracy 0.9674\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.0529\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.0529\t Accuracy 0.9673\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.0530\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.0531\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.0530\t Accuracy 0.9672\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.0531\t Accuracy 0.9671\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.0532\t Accuracy 0.9670\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.0534\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.0535\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.0535\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.0534\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.0534\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.0534\t Accuracy 0.9668\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.0535\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.0534\t Accuracy 0.9667\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.0534\t Accuracy 0.9666\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.0533\t Accuracy 0.9668\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0533\t Average training accuracy 0.9668\n",
      "Epoch [13]\t Average validation loss 0.0536\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0276\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.0486\t Accuracy 0.9725\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.0562\t Accuracy 0.9609\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.0531\t Accuracy 0.9637\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.0510\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.0507\t Accuracy 0.9694\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.0494\t Accuracy 0.9704\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.0490\t Accuracy 0.9701\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.0489\t Accuracy 0.9703\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.0485\t Accuracy 0.9707\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.0487\t Accuracy 0.9699\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.0490\t Accuracy 0.9694\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.0487\t Accuracy 0.9696\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.0489\t Accuracy 0.9703\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.0490\t Accuracy 0.9702\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.0490\t Accuracy 0.9705\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.0494\t Accuracy 0.9696\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.0506\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.0506\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.0502\t Accuracy 0.9688\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.0505\t Accuracy 0.9690\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.0503\t Accuracy 0.9694\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.0501\t Accuracy 0.9697\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.0508\t Accuracy 0.9688\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.0509\t Accuracy 0.9689\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.0516\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.0520\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.0521\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.0523\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.0526\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.0525\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.0527\t Accuracy 0.9677\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.0525\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.0524\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.0524\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.0525\t Accuracy 0.9679\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.0523\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.0521\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.0518\t Accuracy 0.9687\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.0520\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.0519\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.0520\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.0519\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.0519\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.0518\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.0519\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.0518\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.0518\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.0519\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.0521\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.0521\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.0521\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.0522\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.0523\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.0525\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.0523\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.0523\t Accuracy 0.9682\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.0519\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.0520\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.0519\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.0519\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.0518\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.0520\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.0519\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.0518\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.0517\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.0519\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.0520\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.0519\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.0520\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.0519\t Accuracy 0.9685\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.0520\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.0521\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.0521\t Accuracy 0.9684\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.0522\t Accuracy 0.9683\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.0522\t Accuracy 0.9681\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.0524\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.0525\t Accuracy 0.9677\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.0525\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.0525\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.0524\t Accuracy 0.9679\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.0524\t Accuracy 0.9680\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.0525\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.0525\t Accuracy 0.9679\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.0524\t Accuracy 0.9678\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.0524\t Accuracy 0.9679\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0524\t Average training accuracy 0.9679\n",
      "Epoch [14]\t Average validation loss 0.0534\t Average validation accuracy 0.9706\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0277\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.0481\t Accuracy 0.9745\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.0497\t Accuracy 0.9703\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.0559\t Accuracy 0.9603\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.0528\t Accuracy 0.9632\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.0507\t Accuracy 0.9677\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.0502\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.0490\t Accuracy 0.9704\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.0485\t Accuracy 0.9706\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.0482\t Accuracy 0.9707\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.0478\t Accuracy 0.9711\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.0481\t Accuracy 0.9704\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.0483\t Accuracy 0.9702\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.0480\t Accuracy 0.9704\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.0482\t Accuracy 0.9710\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.0483\t Accuracy 0.9708\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.0483\t Accuracy 0.9713\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.0487\t Accuracy 0.9703\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.0498\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.0498\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.0494\t Accuracy 0.9696\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.0497\t Accuracy 0.9697\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.0494\t Accuracy 0.9701\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.0493\t Accuracy 0.9704\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.0499\t Accuracy 0.9694\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.0501\t Accuracy 0.9695\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.0508\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.0511\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.0513\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.0515\t Accuracy 0.9688\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.0517\t Accuracy 0.9688\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.0517\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.0518\t Accuracy 0.9684\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.0516\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.0515\t Accuracy 0.9688\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.0515\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.0516\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.0514\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.0512\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.0510\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.0509\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.0510\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.0512\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.0511\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.0512\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.0511\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.0511\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.0510\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.0510\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.0509\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.0511\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.0510\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.0510\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.0511\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.0512\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.0512\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.0513\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.0514\t Accuracy 0.9688\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.0515\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.0516\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.0515\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.0514\t Accuracy 0.9689\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.0510\t Accuracy 0.9694\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.0510\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.0509\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.0511\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.0510\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.0509\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.0510\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.0511\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.0512\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.0511\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.0511\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.0511\t Accuracy 0.9694\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.0510\t Accuracy 0.9694\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.0510\t Accuracy 0.9695\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.0510\t Accuracy 0.9695\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.0512\t Accuracy 0.9693\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.0512\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.0513\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.0513\t Accuracy 0.9692\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.0514\t Accuracy 0.9691\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.0515\t Accuracy 0.9690\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.0516\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.0517\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.0517\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.0517\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.0517\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.0517\t Accuracy 0.9688\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.0518\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.0517\t Accuracy 0.9687\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.0517\t Accuracy 0.9686\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.0516\t Accuracy 0.9687\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0516\t Average training accuracy 0.9687\n",
      "Epoch [15]\t Average validation loss 0.0524\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0280\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.0476\t Accuracy 0.9745\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.0492\t Accuracy 0.9713\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.0552\t Accuracy 0.9616\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.0522\t Accuracy 0.9647\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.0501\t Accuracy 0.9693\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.0496\t Accuracy 0.9708\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.0483\t Accuracy 0.9721\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.0478\t Accuracy 0.9726\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.0476\t Accuracy 0.9725\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.0472\t Accuracy 0.9729\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.0474\t Accuracy 0.9722\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.0476\t Accuracy 0.9720\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.0473\t Accuracy 0.9722\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.0476\t Accuracy 0.9724\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.0475\t Accuracy 0.9728\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.0479\t Accuracy 0.9717\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.0490\t Accuracy 0.9707\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.0490\t Accuracy 0.9707\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.0486\t Accuracy 0.9713\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.0489\t Accuracy 0.9714\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.0486\t Accuracy 0.9718\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.0485\t Accuracy 0.9720\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.0493\t Accuracy 0.9712\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.0499\t Accuracy 0.9706\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.0503\t Accuracy 0.9707\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.0505\t Accuracy 0.9707\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.0506\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.0509\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.0509\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.0510\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.0508\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.0506\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.0506\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.0508\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.0506\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.0503\t Accuracy 0.9705\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.0504\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.0502\t Accuracy 0.9705\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.0502\t Accuracy 0.9706\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.0503\t Accuracy 0.9705\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.0502\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.0502\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.0504\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.0504\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.0503\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.0503\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.0502\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.0503\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.0503\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.0506\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.0506\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.0507\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.0508\t Accuracy 0.9698\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.0509\t Accuracy 0.9698\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.0508\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.0507\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.0505\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.0504\t Accuracy 0.9704\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.0505\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.0505\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.0504\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.0505\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.0505\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.0504\t Accuracy 0.9702\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.0503\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.0504\t Accuracy 0.9703\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.0505\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.0505\t Accuracy 0.9701\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.0506\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.0507\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.0506\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.0507\t Accuracy 0.9699\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.0508\t Accuracy 0.9697\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.0510\t Accuracy 0.9694\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.0511\t Accuracy 0.9694\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.0511\t Accuracy 0.9694\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.0511\t Accuracy 0.9694\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.0510\t Accuracy 0.9695\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.0510\t Accuracy 0.9696\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.0511\t Accuracy 0.9695\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.0510\t Accuracy 0.9696\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.0510\t Accuracy 0.9695\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.0510\t Accuracy 0.9696\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0510\t Average training accuracy 0.9697\n",
      "Epoch [16]\t Average validation loss 0.0524\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0276\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.0476\t Accuracy 0.9765\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.0490\t Accuracy 0.9723\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.0549\t Accuracy 0.9629\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.0518\t Accuracy 0.9652\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.0497\t Accuracy 0.9697\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.0492\t Accuracy 0.9708\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.0478\t Accuracy 0.9718\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.0473\t Accuracy 0.9723\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.0470\t Accuracy 0.9723\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.0466\t Accuracy 0.9727\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.0469\t Accuracy 0.9719\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.0470\t Accuracy 0.9717\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.0467\t Accuracy 0.9717\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.0468\t Accuracy 0.9726\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.0470\t Accuracy 0.9723\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.0469\t Accuracy 0.9727\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.0473\t Accuracy 0.9717\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.0484\t Accuracy 0.9707\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.0484\t Accuracy 0.9706\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.0480\t Accuracy 0.9713\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.0482\t Accuracy 0.9713\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.0480\t Accuracy 0.9716\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.0479\t Accuracy 0.9719\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.0484\t Accuracy 0.9709\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.0486\t Accuracy 0.9710\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.0492\t Accuracy 0.9706\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.0495\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.0497\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.0499\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.0501\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.0501\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.0502\t Accuracy 0.9697\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.0500\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.0499\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.0499\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.0500\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.0499\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.0495\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.0496\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.0494\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.0494\t Accuracy 0.9706\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.0495\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.0494\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.0495\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.0497\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.0496\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.0496\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.0496\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.0496\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.0495\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.0495\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.0494\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.0496\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.0496\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.0495\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.0495\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.0495\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.0496\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.0498\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.0498\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.0498\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.0499\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.0500\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.0501\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.0500\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.0499\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.0497\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.0496\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.0496\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.0496\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.0495\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.0495\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.0495\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.0497\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.0497\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.0495\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.0496\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.0496\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.0497\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.0497\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.0496\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.0496\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.0496\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.0495\t Accuracy 0.9704\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.0495\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.0496\t Accuracy 0.9705\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.0497\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.0497\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.0498\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.0499\t Accuracy 0.9702\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.0498\t Accuracy 0.9703\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.0499\t Accuracy 0.9701\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.0500\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.0502\t Accuracy 0.9697\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.0503\t Accuracy 0.9696\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.0503\t Accuracy 0.9696\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.0503\t Accuracy 0.9697\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.0502\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.0502\t Accuracy 0.9699\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.0503\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.0502\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.0502\t Accuracy 0.9698\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.0501\t Accuracy 0.9699\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0502\t Average training accuracy 0.9700\n",
      "Epoch [17]\t Average validation loss 0.0524\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0261\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.0464\t Accuracy 0.9745\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.0481\t Accuracy 0.9723\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.0541\t Accuracy 0.9623\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.0512\t Accuracy 0.9652\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.0493\t Accuracy 0.9697\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.0488\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.0473\t Accuracy 0.9721\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.0469\t Accuracy 0.9726\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.0466\t Accuracy 0.9725\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.0462\t Accuracy 0.9731\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.0466\t Accuracy 0.9726\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.0467\t Accuracy 0.9725\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.0463\t Accuracy 0.9725\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.0465\t Accuracy 0.9733\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.0466\t Accuracy 0.9734\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.0465\t Accuracy 0.9737\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.0469\t Accuracy 0.9727\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.0480\t Accuracy 0.9717\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.0480\t Accuracy 0.9716\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.0475\t Accuracy 0.9722\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.0478\t Accuracy 0.9721\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.0475\t Accuracy 0.9725\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.0474\t Accuracy 0.9727\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.0480\t Accuracy 0.9718\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.0481\t Accuracy 0.9720\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.0487\t Accuracy 0.9716\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.0490\t Accuracy 0.9717\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.0492\t Accuracy 0.9715\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.0493\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.0496\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.0495\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.0497\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.0495\t Accuracy 0.9709\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.0494\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.0493\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.0495\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.0493\t Accuracy 0.9709\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.0489\t Accuracy 0.9715\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.0489\t Accuracy 0.9716\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.0489\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.0492\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.0491\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.0491\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.0491\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.0490\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.0489\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.0490\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.0489\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.0489\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.0489\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.0491\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.0492\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.0492\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.0492\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.0494\t Accuracy 0.9709\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.0494\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.0496\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.0495\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.0494\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.0492\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.0491\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.0491\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.0490\t Accuracy 0.9715\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.0489\t Accuracy 0.9714\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.0491\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.0490\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.0491\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.0491\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.0490\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.0489\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.0490\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.0490\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.0491\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.0491\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.0491\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.0490\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.0492\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.0491\t Accuracy 0.9712\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.0492\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.0493\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.0492\t Accuracy 0.9711\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.0494\t Accuracy 0.9710\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.0494\t Accuracy 0.9709\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.0496\t Accuracy 0.9706\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.0497\t Accuracy 0.9705\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.0497\t Accuracy 0.9706\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.0496\t Accuracy 0.9706\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.0496\t Accuracy 0.9707\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.0496\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.0497\t Accuracy 0.9707\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.0496\t Accuracy 0.9708\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.0496\t Accuracy 0.9707\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.0495\t Accuracy 0.9709\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0495\t Average training accuracy 0.9709\n",
      "Epoch [18]\t Average validation loss 0.0525\t Average validation accuracy 0.9720\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0265\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.0453\t Accuracy 0.9784\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.0473\t Accuracy 0.9743\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.0534\t Accuracy 0.9642\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.0506\t Accuracy 0.9662\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.0487\t Accuracy 0.9697\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.0481\t Accuracy 0.9704\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.0467\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.0462\t Accuracy 0.9726\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.0460\t Accuracy 0.9725\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.0457\t Accuracy 0.9731\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.0461\t Accuracy 0.9728\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.0462\t Accuracy 0.9729\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.0458\t Accuracy 0.9730\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.0460\t Accuracy 0.9738\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.0461\t Accuracy 0.9736\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.0464\t Accuracy 0.9732\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.0475\t Accuracy 0.9724\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.0475\t Accuracy 0.9723\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.0470\t Accuracy 0.9730\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.0473\t Accuracy 0.9731\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.0470\t Accuracy 0.9736\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.0469\t Accuracy 0.9738\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.0475\t Accuracy 0.9729\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.0476\t Accuracy 0.9730\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.0481\t Accuracy 0.9727\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.0484\t Accuracy 0.9726\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.0486\t Accuracy 0.9722\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.0487\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.0490\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.0490\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.0491\t Accuracy 0.9715\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.0489\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.0488\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.0488\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.0490\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.0488\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.0485\t Accuracy 0.9722\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.0486\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.0484\t Accuracy 0.9723\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.0483\t Accuracy 0.9724\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.0485\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.0484\t Accuracy 0.9722\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.0483\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.0486\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.0485\t Accuracy 0.9716\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.0485\t Accuracy 0.9716\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.0484\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.0484\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.0483\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.0484\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.0483\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.0483\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.0483\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.0485\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.0486\t Accuracy 0.9716\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.0486\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.0486\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.0488\t Accuracy 0.9715\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.0488\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.0488\t Accuracy 0.9716\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.0487\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.0485\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.0485\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.0484\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.0483\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.0484\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.0485\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.0484\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.0485\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.0485\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.0484\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.0483\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.0484\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.0484\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.0485\t Accuracy 0.9717\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.0485\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.0484\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.0484\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.0484\t Accuracy 0.9721\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.0484\t Accuracy 0.9720\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.0486\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.0485\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.0486\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.0487\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.0486\t Accuracy 0.9719\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.0487\t Accuracy 0.9718\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.0488\t Accuracy 0.9716\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.0490\t Accuracy 0.9713\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.0490\t Accuracy 0.9712\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.0491\t Accuracy 0.9713\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.0491\t Accuracy 0.9713\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.0490\t Accuracy 0.9715\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.0491\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.0490\t Accuracy 0.9714\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.0489\t Accuracy 0.9715\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0489\t Average training accuracy 0.9716\n",
      "Epoch [19]\t Average validation loss 0.0527\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0257\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.0446\t Accuracy 0.9784\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.0467\t Accuracy 0.9733\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.0529\t Accuracy 0.9649\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.0502\t Accuracy 0.9667\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.0483\t Accuracy 0.9705\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.0477\t Accuracy 0.9714\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.0462\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.0458\t Accuracy 0.9731\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.0455\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.0453\t Accuracy 0.9733\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.0456\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.0457\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.0454\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.0455\t Accuracy 0.9739\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.0456\t Accuracy 0.9739\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.0456\t Accuracy 0.9742\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.0460\t Accuracy 0.9736\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.0470\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.0470\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.0466\t Accuracy 0.9733\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.0468\t Accuracy 0.9736\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.0465\t Accuracy 0.9742\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.0464\t Accuracy 0.9745\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.0469\t Accuracy 0.9736\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.0470\t Accuracy 0.9739\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.0475\t Accuracy 0.9736\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.0478\t Accuracy 0.9735\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.0480\t Accuracy 0.9732\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.0482\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.0484\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.0484\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.0485\t Accuracy 0.9722\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.0483\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.0482\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.0482\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.0484\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.0482\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.0479\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.0479\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.0477\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.0477\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.0478\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.0478\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.0477\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.0477\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.0480\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.0479\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.0479\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.0479\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.0478\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.0477\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.0477\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.0477\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.0479\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.0478\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.0477\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.0477\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.0477\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.0479\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.0480\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.0480\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.0480\t Accuracy 0.9726\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.0482\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.0482\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.0484\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.0482\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.0481\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.0479\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.0478\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.0479\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.0477\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.0478\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.0479\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.0479\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.0478\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.0479\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.0479\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.0478\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.0477\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.0478\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.0478\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.0479\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.0480\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.0479\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.0478\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.0478\t Accuracy 0.9729\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.0480\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.0480\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.0480\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.0481\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.0481\t Accuracy 0.9728\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.0482\t Accuracy 0.9727\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.0482\t Accuracy 0.9725\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.0484\t Accuracy 0.9722\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.0485\t Accuracy 0.9721\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.0485\t Accuracy 0.9721\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.0485\t Accuracy 0.9722\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.0484\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.0484\t Accuracy 0.9724\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.0485\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.0484\t Accuracy 0.9723\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.0484\t Accuracy 0.9722\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.0483\t Accuracy 0.9724\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0484\t Average training accuracy 0.9724\n",
      "Epoch [20]\t Average validation loss 0.0530\t Average validation accuracy 0.9726\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0264\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.0440\t Accuracy 0.9784\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.0466\t Accuracy 0.9752\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.0529\t Accuracy 0.9662\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.0502\t Accuracy 0.9677\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.0482\t Accuracy 0.9709\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.0476\t Accuracy 0.9724\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.0456\t Accuracy 0.9741\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.0453\t Accuracy 0.9738\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.0454\t Accuracy 0.9735\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.0454\t Accuracy 0.9737\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.0450\t Accuracy 0.9736\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.0451\t Accuracy 0.9745\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.0452\t Accuracy 0.9744\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.0452\t Accuracy 0.9748\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.0455\t Accuracy 0.9741\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.0466\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.0466\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.0461\t Accuracy 0.9736\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.0463\t Accuracy 0.9738\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.0460\t Accuracy 0.9744\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.0459\t Accuracy 0.9746\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.0464\t Accuracy 0.9737\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.0464\t Accuracy 0.9741\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.0469\t Accuracy 0.9739\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.0472\t Accuracy 0.9739\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.0474\t Accuracy 0.9736\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.0476\t Accuracy 0.9733\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.0478\t Accuracy 0.9733\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.0478\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.0480\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.0478\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.0476\t Accuracy 0.9733\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.0476\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.0478\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.0476\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.0474\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.0472\t Accuracy 0.9733\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.0472\t Accuracy 0.9734\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.0472\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.0471\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.0472\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.0474\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.0473\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.0473\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.0473\t Accuracy 0.9727\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.0473\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.0472\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.0472\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.0471\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.0473\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.0472\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.0472\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.0472\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.0472\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.0473\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.0474\t Accuracy 0.9727\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.0474\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.0475\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.0476\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.0477\t Accuracy 0.9727\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.0478\t Accuracy 0.9727\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.0477\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.0476\t Accuracy 0.9728\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.0474\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.0472\t Accuracy 0.9733\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.0472\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.0471\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.0473\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.0474\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.0472\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.0473\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.0473\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.0473\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.0473\t Accuracy 0.9732\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.0473\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.0475\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.0474\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.0475\t Accuracy 0.9730\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.0476\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.0476\t Accuracy 0.9731\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.0477\t Accuracy 0.9729\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.0477\t Accuracy 0.9727\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.0479\t Accuracy 0.9724\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.0480\t Accuracy 0.9723\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.0480\t Accuracy 0.9723\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.0480\t Accuracy 0.9724\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.0479\t Accuracy 0.9725\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.0479\t Accuracy 0.9726\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.0480\t Accuracy 0.9725\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.0479\t Accuracy 0.9725\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.0479\t Accuracy 0.9724\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.0478\t Accuracy 0.9726\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0479\t Average training accuracy 0.9726\n",
      "Epoch [21]\t Average validation loss 0.0521\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0281\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.0426\t Accuracy 0.9765\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.0453\t Accuracy 0.9743\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.0514\t Accuracy 0.9662\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.0490\t Accuracy 0.9677\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.0471\t Accuracy 0.9709\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.0466\t Accuracy 0.9721\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.0453\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.0449\t Accuracy 0.9741\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.0447\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.0444\t Accuracy 0.9745\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.0448\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.0448\t Accuracy 0.9739\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.0444\t Accuracy 0.9740\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.0446\t Accuracy 0.9749\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.0446\t Accuracy 0.9752\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.0450\t Accuracy 0.9745\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.0460\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.0460\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.0456\t Accuracy 0.9740\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.0458\t Accuracy 0.9743\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.0455\t Accuracy 0.9748\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.0454\t Accuracy 0.9749\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.0459\t Accuracy 0.9740\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.0460\t Accuracy 0.9745\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.0465\t Accuracy 0.9743\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.0468\t Accuracy 0.9742\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.0470\t Accuracy 0.9739\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.0472\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.0474\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.0474\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.0476\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.0473\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.0472\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.0472\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.0473\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.0472\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.0468\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.0469\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.0467\t Accuracy 0.9739\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.0467\t Accuracy 0.9740\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.0468\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.0467\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.0467\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.0469\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.0468\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.0468\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.0468\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.0467\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.0466\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.0466\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.0466\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.0467\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.0467\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.0468\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.0469\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.0469\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.0469\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.0470\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.0471\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.0472\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.0471\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.0470\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.0468\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.0467\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.0467\t Accuracy 0.9739\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.0467\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.0466\t Accuracy 0.9739\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.0466\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.0466\t Accuracy 0.9738\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.0467\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.0468\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.0468\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.0467\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.0468\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.0468\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.0467\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.0466\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.0467\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.0467\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.0468\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.0469\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.0468\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.0467\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.0467\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.0467\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.0467\t Accuracy 0.9737\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.0467\t Accuracy 0.9736\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.0469\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.0469\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.0469\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.0470\t Accuracy 0.9734\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.0470\t Accuracy 0.9735\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.0471\t Accuracy 0.9733\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.0471\t Accuracy 0.9732\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.0473\t Accuracy 0.9729\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.0474\t Accuracy 0.9728\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.0474\t Accuracy 0.9728\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.0474\t Accuracy 0.9729\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.0473\t Accuracy 0.9730\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0473\t Average training accuracy 0.9730\n",
      "Epoch [22]\t Average validation loss 0.0523\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0308\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.0429\t Accuracy 0.9765\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.0452\t Accuracy 0.9752\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.0510\t Accuracy 0.9675\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.0486\t Accuracy 0.9687\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.0468\t Accuracy 0.9717\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.0463\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.0449\t Accuracy 0.9746\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.0445\t Accuracy 0.9753\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.0442\t Accuracy 0.9752\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.0439\t Accuracy 0.9754\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.0443\t Accuracy 0.9746\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.0443\t Accuracy 0.9749\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.0439\t Accuracy 0.9750\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.0440\t Accuracy 0.9756\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.0441\t Accuracy 0.9755\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.0441\t Accuracy 0.9757\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.0445\t Accuracy 0.9749\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.0455\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.0455\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.0453\t Accuracy 0.9746\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.0450\t Accuracy 0.9750\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.0449\t Accuracy 0.9752\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.0454\t Accuracy 0.9744\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.0455\t Accuracy 0.9747\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.0459\t Accuracy 0.9746\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.0463\t Accuracy 0.9745\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.0464\t Accuracy 0.9743\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.0466\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.0468\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.0469\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.0470\t Accuracy 0.9736\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.0468\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.0467\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.0466\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.0467\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.0463\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.0464\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.0461\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.0461\t Accuracy 0.9743\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.0462\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.0461\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.0460\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.0463\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.0462\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.0460\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.0460\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.0460\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.0460\t Accuracy 0.9741\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.0460\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.0460\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.0462\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.0463\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.0464\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.0465\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.0466\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.0465\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.0464\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.0462\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.0461\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.0461\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.0461\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.0460\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.0460\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.0459\t Accuracy 0.9742\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.0462\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.0460\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.0462\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.0462\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.0461\t Accuracy 0.9740\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.0461\t Accuracy 0.9739\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.0463\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.0462\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.0463\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.0464\t Accuracy 0.9737\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.0464\t Accuracy 0.9738\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.0465\t Accuracy 0.9736\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.0465\t Accuracy 0.9734\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.0467\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.0467\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.0468\t Accuracy 0.9730\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.0467\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.0467\t Accuracy 0.9732\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.0467\t Accuracy 0.9732\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.0468\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.0467\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.0467\t Accuracy 0.9731\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.0466\t Accuracy 0.9732\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0466\t Average training accuracy 0.9733\n",
      "Epoch [23]\t Average validation loss 0.0506\t Average validation accuracy 0.9728\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0293\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.0413\t Accuracy 0.9784\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.0437\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.0497\t Accuracy 0.9675\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.0474\t Accuracy 0.9697\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.0456\t Accuracy 0.9729\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.0452\t Accuracy 0.9741\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.0440\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.0435\t Accuracy 0.9756\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.0433\t Accuracy 0.9754\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.0430\t Accuracy 0.9758\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.0435\t Accuracy 0.9750\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.0435\t Accuracy 0.9750\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.0432\t Accuracy 0.9751\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.0434\t Accuracy 0.9756\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.0435\t Accuracy 0.9756\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.0435\t Accuracy 0.9759\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.0438\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.0448\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.0448\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.0444\t Accuracy 0.9749\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.0446\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.0443\t Accuracy 0.9757\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.0442\t Accuracy 0.9759\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.0447\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.0447\t Accuracy 0.9755\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.0452\t Accuracy 0.9753\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.0455\t Accuracy 0.9752\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.0457\t Accuracy 0.9748\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.0460\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.0462\t Accuracy 0.9747\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.0462\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.0464\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.0462\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.0461\t Accuracy 0.9750\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.0460\t Accuracy 0.9749\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.0461\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.0460\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.0457\t Accuracy 0.9749\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.0458\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.0456\t Accuracy 0.9748\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.0455\t Accuracy 0.9749\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.0456\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.0455\t Accuracy 0.9747\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.0456\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.0455\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.0453\t Accuracy 0.9747\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.0454\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.0454\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.0456\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.0456\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.0456\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.0458\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.0458\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.0459\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.0458\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.0457\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.0455\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.0455\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.0455\t Accuracy 0.9747\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.0455\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.0454\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.0453\t Accuracy 0.9746\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.0456\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.0456\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.0455\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.0454\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.0455\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.0455\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.0456\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.0456\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.0455\t Accuracy 0.9743\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.0455\t Accuracy 0.9745\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.0455\t Accuracy 0.9744\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.0457\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.0456\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.0457\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.0458\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.0457\t Accuracy 0.9742\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.0458\t Accuracy 0.9741\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.0459\t Accuracy 0.9740\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.0460\t Accuracy 0.9737\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.0461\t Accuracy 0.9736\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.0462\t Accuracy 0.9736\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.0461\t Accuracy 0.9737\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.0461\t Accuracy 0.9738\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.0461\t Accuracy 0.9737\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.0461\t Accuracy 0.9737\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.0461\t Accuracy 0.9737\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.0460\t Accuracy 0.9738\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0460\t Average training accuracy 0.9738\n",
      "Epoch [24]\t Average validation loss 0.0509\t Average validation accuracy 0.9736\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0303\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.0410\t Accuracy 0.9784\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.0433\t Accuracy 0.9762\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.0492\t Accuracy 0.9689\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.0469\t Accuracy 0.9711\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.0452\t Accuracy 0.9741\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.0447\t Accuracy 0.9751\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.0434\t Accuracy 0.9761\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.0429\t Accuracy 0.9763\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.0428\t Accuracy 0.9763\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.0425\t Accuracy 0.9766\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.0429\t Accuracy 0.9759\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.0430\t Accuracy 0.9759\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.0426\t Accuracy 0.9759\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.0428\t Accuracy 0.9763\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.0429\t Accuracy 0.9760\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.0429\t Accuracy 0.9762\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.0433\t Accuracy 0.9756\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.0442\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.0443\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.0439\t Accuracy 0.9752\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.0441\t Accuracy 0.9754\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.0438\t Accuracy 0.9759\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.0437\t Accuracy 0.9761\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.0442\t Accuracy 0.9754\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.0446\t Accuracy 0.9755\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.0450\t Accuracy 0.9754\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.0452\t Accuracy 0.9750\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.0454\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.0456\t Accuracy 0.9750\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.0456\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.0458\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.0456\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.0455\t Accuracy 0.9752\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.0455\t Accuracy 0.9750\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.0456\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.0455\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.0452\t Accuracy 0.9751\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.0452\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.0450\t Accuracy 0.9751\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.0450\t Accuracy 0.9752\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.0450\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.0450\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.0449\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.0451\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.0450\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.0450\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.0450\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.0448\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.0448\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.0448\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.0449\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.0450\t Accuracy 0.9747\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.0451\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.0450\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.0451\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.0452\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.0453\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.0454\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.0453\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.0452\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.0450\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.0449\t Accuracy 0.9749\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.0449\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.0450\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.0450\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.0449\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.0450\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.0450\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.0450\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.0449\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.0450\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.0450\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.0450\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.0450\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.0449\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.0449\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.0449\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.0449\t Accuracy 0.9746\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.0450\t Accuracy 0.9745\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.0451\t Accuracy 0.9742\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.0452\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.0452\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.0452\t Accuracy 0.9744\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.0453\t Accuracy 0.9743\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.0453\t Accuracy 0.9741\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.0455\t Accuracy 0.9739\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.0456\t Accuracy 0.9738\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.0456\t Accuracy 0.9738\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.0456\t Accuracy 0.9739\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.0455\t Accuracy 0.9740\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.0455\t Accuracy 0.9741\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.0456\t Accuracy 0.9739\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.0455\t Accuracy 0.9740\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.0455\t Accuracy 0.9739\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.0455\t Accuracy 0.9741\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0455\t Average training accuracy 0.9741\n",
      "Epoch [25]\t Average validation loss 0.0499\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0316\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.0406\t Accuracy 0.9784\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.0429\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.0488\t Accuracy 0.9689\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.0466\t Accuracy 0.9716\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.0450\t Accuracy 0.9741\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.0444\t Accuracy 0.9757\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.0432\t Accuracy 0.9764\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.0427\t Accuracy 0.9768\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.0425\t Accuracy 0.9772\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.0422\t Accuracy 0.9774\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.0427\t Accuracy 0.9766\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.0428\t Accuracy 0.9765\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.0424\t Accuracy 0.9767\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.0426\t Accuracy 0.9770\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.0427\t Accuracy 0.9768\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.0427\t Accuracy 0.9768\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.0430\t Accuracy 0.9761\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.0440\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.0440\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.0437\t Accuracy 0.9759\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.0439\t Accuracy 0.9760\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.0435\t Accuracy 0.9767\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.0439\t Accuracy 0.9760\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.0439\t Accuracy 0.9763\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.0444\t Accuracy 0.9761\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.0447\t Accuracy 0.9759\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.0449\t Accuracy 0.9756\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.0451\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.0453\t Accuracy 0.9756\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.0453\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.0455\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.0453\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.0452\t Accuracy 0.9757\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.0452\t Accuracy 0.9756\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.0453\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.0452\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.0449\t Accuracy 0.9756\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.0449\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.0447\t Accuracy 0.9757\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.0447\t Accuracy 0.9758\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.0447\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.0446\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.0446\t Accuracy 0.9756\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.0446\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.0448\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.0446\t Accuracy 0.9755\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.0446\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.0446\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.0446\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.0445\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.0445\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.0444\t Accuracy 0.9754\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.0446\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.0445\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.0445\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.0445\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.0445\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.0446\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.0447\t Accuracy 0.9749\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.0447\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.0447\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.0449\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.0450\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.0449\t Accuracy 0.9749\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.0448\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.0446\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.0445\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.0445\t Accuracy 0.9753\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.0445\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.0444\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.0444\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.0444\t Accuracy 0.9752\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.0445\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.0447\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.0447\t Accuracy 0.9747\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.0446\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.0445\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.0446\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.0446\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.0447\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.0447\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.0446\t Accuracy 0.9749\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.0446\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.0445\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.0445\t Accuracy 0.9751\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.0446\t Accuracy 0.9750\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.0447\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.0447\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.0448\t Accuracy 0.9748\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.0449\t Accuracy 0.9747\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.0449\t Accuracy 0.9746\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.0451\t Accuracy 0.9744\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.0451\t Accuracy 0.9743\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.0452\t Accuracy 0.9743\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.0451\t Accuracy 0.9744\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.0451\t Accuracy 0.9745\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.0451\t Accuracy 0.9745\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.0452\t Accuracy 0.9744\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.0451\t Accuracy 0.9745\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.0451\t Accuracy 0.9744\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.0451\t Accuracy 0.9746\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0451\t Average training accuracy 0.9746\n",
      "Epoch [26]\t Average validation loss 0.0498\t Average validation accuracy 0.9736\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0295\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.0405\t Accuracy 0.9784\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.0426\t Accuracy 0.9752\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.0484\t Accuracy 0.9689\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.0463\t Accuracy 0.9716\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.0446\t Accuracy 0.9741\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.0441\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.0429\t Accuracy 0.9766\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.0423\t Accuracy 0.9771\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.0421\t Accuracy 0.9772\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.0418\t Accuracy 0.9776\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.0423\t Accuracy 0.9770\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.0424\t Accuracy 0.9769\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.0421\t Accuracy 0.9771\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.0423\t Accuracy 0.9777\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.0424\t Accuracy 0.9775\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.0424\t Accuracy 0.9775\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.0427\t Accuracy 0.9770\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.0436\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.0436\t Accuracy 0.9762\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.0433\t Accuracy 0.9767\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.0435\t Accuracy 0.9768\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.0432\t Accuracy 0.9772\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.0431\t Accuracy 0.9774\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.0436\t Accuracy 0.9767\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.0436\t Accuracy 0.9769\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.0440\t Accuracy 0.9766\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.0443\t Accuracy 0.9764\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.0445\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.0447\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.0449\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.0450\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.0452\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.0450\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.0449\t Accuracy 0.9762\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.0449\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.0450\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.0448\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.0445\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.0446\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.0444\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.0444\t Accuracy 0.9762\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.0444\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.0443\t Accuracy 0.9762\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.0442\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.0442\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.0444\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.0442\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.0441\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.0441\t Accuracy 0.9761\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.0441\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.0441\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.0441\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.0441\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.0441\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.0443\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.0443\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.0443\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.0445\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.0445\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.0446\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.0445\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.0444\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.0442\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.0442\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.0441\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.0441\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.0441\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.0440\t Accuracy 0.9759\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.0441\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.0442\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.0443\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.0443\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.0442\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.0441\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.0442\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.0442\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.0443\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.0443\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.0442\t Accuracy 0.9756\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.0441\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.0441\t Accuracy 0.9758\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.0443\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.0443\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.0444\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.0444\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.0444\t Accuracy 0.9755\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.0445\t Accuracy 0.9754\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.0445\t Accuracy 0.9753\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.0446\t Accuracy 0.9751\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.0447\t Accuracy 0.9750\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.0447\t Accuracy 0.9749\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.0447\t Accuracy 0.9750\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.0447\t Accuracy 0.9751\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.0446\t Accuracy 0.9751\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.0447\t Accuracy 0.9750\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.0447\t Accuracy 0.9751\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.0447\t Accuracy 0.9751\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.0446\t Accuracy 0.9753\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0446\t Average training accuracy 0.9753\n",
      "Epoch [27]\t Average validation loss 0.0493\t Average validation accuracy 0.9732\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0291\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.0399\t Accuracy 0.9784\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.0420\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.0478\t Accuracy 0.9709\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.0457\t Accuracy 0.9731\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.0441\t Accuracy 0.9757\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.0436\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.0425\t Accuracy 0.9781\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.0419\t Accuracy 0.9783\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.0417\t Accuracy 0.9783\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.0414\t Accuracy 0.9786\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.0419\t Accuracy 0.9777\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.0421\t Accuracy 0.9775\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.0417\t Accuracy 0.9777\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.0419\t Accuracy 0.9782\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.0419\t Accuracy 0.9779\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.0419\t Accuracy 0.9780\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.0423\t Accuracy 0.9773\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.0432\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.0432\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.0429\t Accuracy 0.9773\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.0431\t Accuracy 0.9774\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.0429\t Accuracy 0.9777\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.0427\t Accuracy 0.9779\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.0432\t Accuracy 0.9772\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.0432\t Accuracy 0.9775\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.0436\t Accuracy 0.9772\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.0439\t Accuracy 0.9772\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.0441\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.0443\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.0445\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.0446\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.0447\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.0445\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.0444\t Accuracy 0.9771\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.0444\t Accuracy 0.9770\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.0445\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.0444\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.0441\t Accuracy 0.9771\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.0441\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.0439\t Accuracy 0.9770\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.0439\t Accuracy 0.9770\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.0439\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.0438\t Accuracy 0.9769\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.0438\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.0438\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.0439\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.0438\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.0438\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.0438\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.0437\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.0436\t Accuracy 0.9768\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.0436\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.0436\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.0437\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.0437\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.0436\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.0436\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.0437\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.0438\t Accuracy 0.9765\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.0439\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.0439\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.0439\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.0440\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.0441\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.0442\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.0441\t Accuracy 0.9763\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.0440\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.0438\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.0437\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.0437\t Accuracy 0.9767\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.0437\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.0438\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.0437\t Accuracy 0.9763\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.0438\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.0437\t Accuracy 0.9763\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.0438\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.0439\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.0438\t Accuracy 0.9763\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.0437\t Accuracy 0.9763\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.0439\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.0439\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.0440\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.0439\t Accuracy 0.9762\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.0440\t Accuracy 0.9761\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.0441\t Accuracy 0.9760\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.0442\t Accuracy 0.9757\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.0443\t Accuracy 0.9756\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.0443\t Accuracy 0.9756\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.0443\t Accuracy 0.9757\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.0443\t Accuracy 0.9757\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.0442\t Accuracy 0.9758\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.0442\t Accuracy 0.9760\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0442\t Average training accuracy 0.9760\n",
      "Epoch [28]\t Average validation loss 0.0488\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0267\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.0392\t Accuracy 0.9784\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.0414\t Accuracy 0.9762\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.0473\t Accuracy 0.9715\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.0451\t Accuracy 0.9746\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.0435\t Accuracy 0.9773\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.0431\t Accuracy 0.9781\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.0420\t Accuracy 0.9792\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.0415\t Accuracy 0.9796\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.0413\t Accuracy 0.9796\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.0410\t Accuracy 0.9798\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.0415\t Accuracy 0.9788\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.0417\t Accuracy 0.9785\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.0414\t Accuracy 0.9786\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.0416\t Accuracy 0.9793\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.0416\t Accuracy 0.9791\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.0416\t Accuracy 0.9792\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.0420\t Accuracy 0.9787\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.0428\t Accuracy 0.9778\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.0429\t Accuracy 0.9778\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.0425\t Accuracy 0.9784\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.0428\t Accuracy 0.9785\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.0425\t Accuracy 0.9788\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.0424\t Accuracy 0.9790\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.0428\t Accuracy 0.9782\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.0428\t Accuracy 0.9784\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.0432\t Accuracy 0.9782\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.0435\t Accuracy 0.9779\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.0437\t Accuracy 0.9775\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.0439\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.0441\t Accuracy 0.9773\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.0442\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.0443\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.0441\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.0440\t Accuracy 0.9775\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.0440\t Accuracy 0.9774\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.0441\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.0440\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.0437\t Accuracy 0.9774\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.0438\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.0436\t Accuracy 0.9774\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.0435\t Accuracy 0.9775\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.0436\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.0435\t Accuracy 0.9774\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.0434\t Accuracy 0.9773\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.0434\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.0435\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.0434\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.0434\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.0434\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.0433\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.0432\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.0433\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.0432\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.0434\t Accuracy 0.9769\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.0433\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.0433\t Accuracy 0.9769\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.0433\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.0433\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.0434\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.0435\t Accuracy 0.9769\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.0435\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.0435\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.0436\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.0437\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.0438\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.0437\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.0436\t Accuracy 0.9769\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.0434\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.0433\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.0433\t Accuracy 0.9772\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.0433\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.0432\t Accuracy 0.9771\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.0433\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.0432\t Accuracy 0.9770\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.0433\t Accuracy 0.9769\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.0434\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.0434\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.0434\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.0435\t Accuracy 0.9766\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.0435\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.0434\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.0433\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.0434\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.0434\t Accuracy 0.9766\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.0435\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.0435\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.0434\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.0434\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.0434\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.0433\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.0433\t Accuracy 0.9768\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.0434\t Accuracy 0.9767\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.0435\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.0435\t Accuracy 0.9766\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.0436\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.0436\t Accuracy 0.9766\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.0437\t Accuracy 0.9765\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.0437\t Accuracy 0.9764\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.0438\t Accuracy 0.9762\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.0439\t Accuracy 0.9761\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.0439\t Accuracy 0.9761\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.0439\t Accuracy 0.9762\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.0438\t Accuracy 0.9763\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.0438\t Accuracy 0.9763\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.0439\t Accuracy 0.9762\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.0439\t Accuracy 0.9763\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.0439\t Accuracy 0.9763\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.0438\t Accuracy 0.9764\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0438\t Average training accuracy 0.9764\n",
      "Epoch [29]\t Average validation loss 0.0485\t Average validation accuracy 0.9728\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9661.\n",
      "\n",
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 3.3327\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 2.7593\t Accuracy 0.1059\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 2.6364\t Accuracy 0.1198\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 2.5526\t Accuracy 0.1358\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 2.4945\t Accuracy 0.1522\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 2.4490\t Accuracy 0.1709\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 2.4010\t Accuracy 0.1973\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 2.3654\t Accuracy 0.2140\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 2.3345\t Accuracy 0.2289\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 2.3084\t Accuracy 0.2426\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 2.2853\t Accuracy 0.2569\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 2.2623\t Accuracy 0.2793\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 2.2424\t Accuracy 0.2963\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 2.2211\t Accuracy 0.3126\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 2.2029\t Accuracy 0.3268\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 2.1896\t Accuracy 0.3375\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 2.1759\t Accuracy 0.3489\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 2.1620\t Accuracy 0.3592\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 2.1478\t Accuracy 0.3700\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 2.1329\t Accuracy 0.3817\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 2.1185\t Accuracy 0.3934\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 2.1045\t Accuracy 0.4032\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 2.0908\t Accuracy 0.4133\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 2.0769\t Accuracy 0.4229\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 2.0659\t Accuracy 0.4290\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 2.0544\t Accuracy 0.4382\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 2.0440\t Accuracy 0.4445\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 2.0336\t Accuracy 0.4525\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 2.0231\t Accuracy 0.4595\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 2.0141\t Accuracy 0.4640\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 2.0063\t Accuracy 0.4674\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 1.9960\t Accuracy 0.4745\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 1.9871\t Accuracy 0.4793\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 1.9769\t Accuracy 0.4841\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 1.9683\t Accuracy 0.4891\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 1.9583\t Accuracy 0.4941\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 1.9515\t Accuracy 0.4973\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 1.9418\t Accuracy 0.5023\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 1.9322\t Accuracy 0.5082\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 1.9227\t Accuracy 0.5132\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 1.9126\t Accuracy 0.5177\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 1.9034\t Accuracy 0.5226\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 1.8941\t Accuracy 0.5270\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 1.8849\t Accuracy 0.5315\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 1.8744\t Accuracy 0.5365\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 1.8672\t Accuracy 0.5395\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 1.8584\t Accuracy 0.5438\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 1.8494\t Accuracy 0.5478\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 1.8418\t Accuracy 0.5514\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 1.8333\t Accuracy 0.5550\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 1.8264\t Accuracy 0.5574\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 1.8172\t Accuracy 0.5610\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 1.8085\t Accuracy 0.5650\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 1.8008\t Accuracy 0.5678\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 1.7939\t Accuracy 0.5705\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 1.7873\t Accuracy 0.5730\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 1.7792\t Accuracy 0.5765\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 1.7705\t Accuracy 0.5798\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 1.7629\t Accuracy 0.5828\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 1.7553\t Accuracy 0.5855\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 1.7495\t Accuracy 0.5877\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 1.7435\t Accuracy 0.5899\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 1.7378\t Accuracy 0.5917\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 1.7318\t Accuracy 0.5940\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 1.7259\t Accuracy 0.5961\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 1.7208\t Accuracy 0.5978\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 1.7139\t Accuracy 0.6004\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 1.7076\t Accuracy 0.6029\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 1.6997\t Accuracy 0.6062\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 1.6929\t Accuracy 0.6086\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 1.6869\t Accuracy 0.6104\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 1.6807\t Accuracy 0.6128\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 1.6745\t Accuracy 0.6152\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 1.6679\t Accuracy 0.6174\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 1.6607\t Accuracy 0.6200\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 1.6546\t Accuracy 0.6219\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 1.6486\t Accuracy 0.6242\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 1.6425\t Accuracy 0.6263\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 1.6359\t Accuracy 0.6286\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 1.6295\t Accuracy 0.6306\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 1.6241\t Accuracy 0.6321\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 1.6180\t Accuracy 0.6343\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 1.6118\t Accuracy 0.6362\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 1.6072\t Accuracy 0.6377\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 1.6021\t Accuracy 0.6393\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 1.5971\t Accuracy 0.6404\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 1.5914\t Accuracy 0.6421\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 1.5854\t Accuracy 0.6440\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 1.5799\t Accuracy 0.6459\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 1.5751\t Accuracy 0.6475\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 1.5694\t Accuracy 0.6493\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 1.5641\t Accuracy 0.6509\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 1.5590\t Accuracy 0.6524\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 1.5542\t Accuracy 0.6539\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 1.5481\t Accuracy 0.6558\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 1.5432\t Accuracy 0.6571\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 1.5378\t Accuracy 0.6588\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 1.5313\t Accuracy 0.6607\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 1.5261\t Accuracy 0.6620\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 1.5215\t Accuracy 0.6630\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 1.5180\t Accuracy 0.6641\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 1.5134\t Accuracy 0.6653\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 1.5086\t Accuracy 0.6663\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 1.5033\t Accuracy 0.6678\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 1.4980\t Accuracy 0.6696\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 1.4936\t Accuracy 0.6707\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 1.4889\t Accuracy 0.6719\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 1.4840\t Accuracy 0.6733\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 1.4792\t Accuracy 0.6746\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 1.4744\t Accuracy 0.6759\n",
      "\n",
      "Epoch [0]\t Average training loss 1.4698\t Average training accuracy 0.6770\n",
      "Epoch [0]\t Average validation loss 0.8945\t Average validation accuracy 0.8598\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.8390\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.9289\t Accuracy 0.8098\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.9756\t Accuracy 0.7990\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.9814\t Accuracy 0.8007\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.9596\t Accuracy 0.8104\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.9456\t Accuracy 0.8151\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.9338\t Accuracy 0.8156\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.9365\t Accuracy 0.8171\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.9314\t Accuracy 0.8185\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.9300\t Accuracy 0.8188\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.9240\t Accuracy 0.8200\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.9191\t Accuracy 0.8196\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.9167\t Accuracy 0.8220\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.9077\t Accuracy 0.8247\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.9042\t Accuracy 0.8243\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.9106\t Accuracy 0.8222\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.9122\t Accuracy 0.8208\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.9132\t Accuracy 0.8208\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.9142\t Accuracy 0.8189\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.9104\t Accuracy 0.8200\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.9053\t Accuracy 0.8216\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.9002\t Accuracy 0.8227\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.8957\t Accuracy 0.8243\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.8911\t Accuracy 0.8255\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.8926\t Accuracy 0.8240\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.8917\t Accuracy 0.8240\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.8922\t Accuracy 0.8231\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.8919\t Accuracy 0.8232\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.8914\t Accuracy 0.8226\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.8923\t Accuracy 0.8212\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.8951\t Accuracy 0.8191\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.8926\t Accuracy 0.8201\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.8934\t Accuracy 0.8190\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.8910\t Accuracy 0.8194\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.8908\t Accuracy 0.8193\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.8898\t Accuracy 0.8187\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.8916\t Accuracy 0.8177\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.8886\t Accuracy 0.8189\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.8853\t Accuracy 0.8207\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.8835\t Accuracy 0.8209\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.8798\t Accuracy 0.8215\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.8780\t Accuracy 0.8221\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.8768\t Accuracy 0.8216\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.8740\t Accuracy 0.8223\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.8699\t Accuracy 0.8236\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.8694\t Accuracy 0.8237\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.8677\t Accuracy 0.8239\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.8651\t Accuracy 0.8242\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.8641\t Accuracy 0.8244\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.8622\t Accuracy 0.8246\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.8623\t Accuracy 0.8238\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.8592\t Accuracy 0.8246\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.8566\t Accuracy 0.8255\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.8550\t Accuracy 0.8261\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.8544\t Accuracy 0.8258\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.8536\t Accuracy 0.8261\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.8513\t Accuracy 0.8270\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.8486\t Accuracy 0.8277\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.8470\t Accuracy 0.8278\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.8456\t Accuracy 0.8277\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.8458\t Accuracy 0.8277\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.8458\t Accuracy 0.8272\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.8459\t Accuracy 0.8271\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.8459\t Accuracy 0.8268\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.8454\t Accuracy 0.8266\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.8462\t Accuracy 0.8260\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.8445\t Accuracy 0.8265\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.8438\t Accuracy 0.8264\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.8407\t Accuracy 0.8274\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.8388\t Accuracy 0.8279\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.8381\t Accuracy 0.8277\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.8366\t Accuracy 0.8280\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.8352\t Accuracy 0.8281\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.8336\t Accuracy 0.8285\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.8309\t Accuracy 0.8292\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.8302\t Accuracy 0.8292\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.8289\t Accuracy 0.8293\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.8275\t Accuracy 0.8295\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.8256\t Accuracy 0.8300\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.8241\t Accuracy 0.8302\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.8235\t Accuracy 0.8303\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.8218\t Accuracy 0.8307\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.8200\t Accuracy 0.8312\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.8199\t Accuracy 0.8311\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.8190\t Accuracy 0.8314\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.8188\t Accuracy 0.8309\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.8175\t Accuracy 0.8309\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.8158\t Accuracy 0.8313\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.8143\t Accuracy 0.8314\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.8138\t Accuracy 0.8313\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.8120\t Accuracy 0.8317\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.8110\t Accuracy 0.8316\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.8099\t Accuracy 0.8318\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.8094\t Accuracy 0.8318\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.8072\t Accuracy 0.8321\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.8063\t Accuracy 0.8322\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.8049\t Accuracy 0.8324\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.8024\t Accuracy 0.8330\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.8010\t Accuracy 0.8332\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.8004\t Accuracy 0.8329\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.8006\t Accuracy 0.8328\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.8001\t Accuracy 0.8329\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.7988\t Accuracy 0.8330\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.7972\t Accuracy 0.8333\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.7954\t Accuracy 0.8338\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.7947\t Accuracy 0.8338\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.7938\t Accuracy 0.8338\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.7923\t Accuracy 0.8342\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.7911\t Accuracy 0.8343\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.7896\t Accuracy 0.8347\n",
      "\n",
      "Epoch [1]\t Average training loss 0.7885\t Average training accuracy 0.8348\n",
      "Epoch [1]\t Average validation loss 0.5787\t Average validation accuracy 0.8900\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.4625\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.6324\t Accuracy 0.8627\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.6773\t Accuracy 0.8475\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.6867\t Accuracy 0.8477\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.6627\t Accuracy 0.8547\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.6496\t Accuracy 0.8582\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.6402\t Accuracy 0.8588\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.6424\t Accuracy 0.8598\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.6388\t Accuracy 0.8621\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.6383\t Accuracy 0.8630\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.6345\t Accuracy 0.8633\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.6324\t Accuracy 0.8637\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.6305\t Accuracy 0.8644\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.6236\t Accuracy 0.8662\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.6215\t Accuracy 0.8649\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.6280\t Accuracy 0.8631\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.6313\t Accuracy 0.8615\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.6340\t Accuracy 0.8612\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.6385\t Accuracy 0.8585\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.6363\t Accuracy 0.8595\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.6325\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.6287\t Accuracy 0.8614\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.6253\t Accuracy 0.8631\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.6222\t Accuracy 0.8638\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.6254\t Accuracy 0.8621\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.6258\t Accuracy 0.8620\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.6278\t Accuracy 0.8609\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.6290\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.6299\t Accuracy 0.8591\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.6324\t Accuracy 0.8582\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.6365\t Accuracy 0.8561\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.6348\t Accuracy 0.8568\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.6367\t Accuracy 0.8559\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.6354\t Accuracy 0.8561\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.6362\t Accuracy 0.8556\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.6364\t Accuracy 0.8551\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.6389\t Accuracy 0.8541\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.6365\t Accuracy 0.8548\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.6340\t Accuracy 0.8562\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.6335\t Accuracy 0.8564\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.6309\t Accuracy 0.8569\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.6301\t Accuracy 0.8576\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.6306\t Accuracy 0.8566\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.6287\t Accuracy 0.8576\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.6259\t Accuracy 0.8584\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.6260\t Accuracy 0.8586\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.6255\t Accuracy 0.8584\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.6240\t Accuracy 0.8586\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.6240\t Accuracy 0.8586\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.6231\t Accuracy 0.8587\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.6241\t Accuracy 0.8578\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.6222\t Accuracy 0.8584\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.6206\t Accuracy 0.8591\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.6199\t Accuracy 0.8595\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.6204\t Accuracy 0.8593\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.6204\t Accuracy 0.8592\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.6190\t Accuracy 0.8597\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.6175\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.6169\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.6167\t Accuracy 0.8598\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.6177\t Accuracy 0.8594\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.6185\t Accuracy 0.8590\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.6194\t Accuracy 0.8588\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.6204\t Accuracy 0.8587\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.6208\t Accuracy 0.8582\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.6224\t Accuracy 0.8573\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.6215\t Accuracy 0.8578\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.6217\t Accuracy 0.8575\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.6193\t Accuracy 0.8584\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.6182\t Accuracy 0.8588\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.6184\t Accuracy 0.8585\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.6176\t Accuracy 0.8586\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.6169\t Accuracy 0.8587\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.6161\t Accuracy 0.8588\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.6142\t Accuracy 0.8596\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.6147\t Accuracy 0.8593\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.6143\t Accuracy 0.8595\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.6135\t Accuracy 0.8596\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.6125\t Accuracy 0.8599\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.6120\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.6122\t Accuracy 0.8599\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.6112\t Accuracy 0.8601\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.6101\t Accuracy 0.8605\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.6107\t Accuracy 0.8604\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.6105\t Accuracy 0.8605\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.6112\t Accuracy 0.8601\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.6107\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.6097\t Accuracy 0.8601\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.6090\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.6092\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.6080\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.6079\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.6075\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.6077\t Accuracy 0.8601\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.6063\t Accuracy 0.8604\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.6061\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.6056\t Accuracy 0.8604\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.6039\t Accuracy 0.8609\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.6032\t Accuracy 0.8610\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.6034\t Accuracy 0.8605\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.6042\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.6044\t Accuracy 0.8602\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.6038\t Accuracy 0.8603\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.6029\t Accuracy 0.8604\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.6018\t Accuracy 0.8607\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.6018\t Accuracy 0.8606\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.6017\t Accuracy 0.8605\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.6009\t Accuracy 0.8607\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.6003\t Accuracy 0.8607\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.5995\t Accuracy 0.8610\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5991\t Average training accuracy 0.8609\n",
      "Epoch [2]\t Average validation loss 0.4592\t Average validation accuracy 0.9020\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.3279\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.5180\t Accuracy 0.8784\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.5602\t Accuracy 0.8644\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.5704\t Accuracy 0.8629\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.5455\t Accuracy 0.8701\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.5325\t Accuracy 0.8765\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.5241\t Accuracy 0.8767\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.5247\t Accuracy 0.8786\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.5218\t Accuracy 0.8810\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.5212\t Accuracy 0.8823\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.5183\t Accuracy 0.8814\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.5174\t Accuracy 0.8826\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.5155\t Accuracy 0.8834\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.5096\t Accuracy 0.8848\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.5080\t Accuracy 0.8836\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.5139\t Accuracy 0.8820\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.5174\t Accuracy 0.8808\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.5205\t Accuracy 0.8798\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.5263\t Accuracy 0.8770\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.5249\t Accuracy 0.8779\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.5217\t Accuracy 0.8784\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.5183\t Accuracy 0.8794\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.5152\t Accuracy 0.8809\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.5126\t Accuracy 0.8813\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.5163\t Accuracy 0.8794\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.5170\t Accuracy 0.8787\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.5195\t Accuracy 0.8778\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.5212\t Accuracy 0.8772\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.5224\t Accuracy 0.8759\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.5254\t Accuracy 0.8750\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.5298\t Accuracy 0.8730\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.5284\t Accuracy 0.8738\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.5304\t Accuracy 0.8726\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.5293\t Accuracy 0.8729\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.5303\t Accuracy 0.8722\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.5307\t Accuracy 0.8714\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.5332\t Accuracy 0.8705\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.5310\t Accuracy 0.8715\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.5287\t Accuracy 0.8728\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.5285\t Accuracy 0.8728\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.5264\t Accuracy 0.8732\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.5257\t Accuracy 0.8737\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.5268\t Accuracy 0.8729\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.5253\t Accuracy 0.8737\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.5229\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.5230\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.5228\t Accuracy 0.8741\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.5217\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.5220\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.5215\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.5227\t Accuracy 0.8739\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.5211\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.5198\t Accuracy 0.8749\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.5194\t Accuracy 0.8754\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.5203\t Accuracy 0.8752\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.5204\t Accuracy 0.8751\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.5194\t Accuracy 0.8753\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.5183\t Accuracy 0.8758\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.5180\t Accuracy 0.8758\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.5181\t Accuracy 0.8754\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.5194\t Accuracy 0.8750\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.5203\t Accuracy 0.8747\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.5214\t Accuracy 0.8746\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.5227\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.5233\t Accuracy 0.8739\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.5251\t Accuracy 0.8730\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.5245\t Accuracy 0.8734\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.5248\t Accuracy 0.8733\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.5227\t Accuracy 0.8741\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.5218\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.5223\t Accuracy 0.8740\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.5216\t Accuracy 0.8740\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.5211\t Accuracy 0.8740\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.5205\t Accuracy 0.8741\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.5189\t Accuracy 0.8748\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.5199\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.5197\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.5191\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.5184\t Accuracy 0.8746\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.5182\t Accuracy 0.8747\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.5186\t Accuracy 0.8747\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.5178\t Accuracy 0.8748\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.5170\t Accuracy 0.8752\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.5177\t Accuracy 0.8750\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.5176\t Accuracy 0.8750\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.5186\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.5185\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.5176\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.5172\t Accuracy 0.8746\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.5175\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.5167\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.5168\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.5167\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.5172\t Accuracy 0.8742\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.5160\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.5160\t Accuracy 0.8742\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.5157\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.5144\t Accuracy 0.8747\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.5139\t Accuracy 0.8748\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.5143\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.5153\t Accuracy 0.8741\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.5158\t Accuracy 0.8740\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.5155\t Accuracy 0.8741\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.5148\t Accuracy 0.8742\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.5139\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.5142\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.5144\t Accuracy 0.8743\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.5137\t Accuracy 0.8744\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.5134\t Accuracy 0.8745\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.5128\t Accuracy 0.8747\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5126\t Average training accuracy 0.8746\n",
      "Epoch [3]\t Average validation loss 0.3974\t Average validation accuracy 0.9084\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.2635\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.4568\t Accuracy 0.8922\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.4972\t Accuracy 0.8762\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.5078\t Accuracy 0.8742\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.4825\t Accuracy 0.8811\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.4696\t Accuracy 0.8865\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.4614\t Accuracy 0.8880\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.4609\t Accuracy 0.8895\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.4583\t Accuracy 0.8913\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.4577\t Accuracy 0.8925\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.4552\t Accuracy 0.8906\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.4549\t Accuracy 0.8915\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.4531\t Accuracy 0.8920\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.4478\t Accuracy 0.8935\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.4466\t Accuracy 0.8926\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.4519\t Accuracy 0.8912\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.4553\t Accuracy 0.8899\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.4585\t Accuracy 0.8886\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.4650\t Accuracy 0.8858\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.4642\t Accuracy 0.8864\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.4612\t Accuracy 0.8870\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.4582\t Accuracy 0.8882\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.4551\t Accuracy 0.8896\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.4527\t Accuracy 0.8901\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.4566\t Accuracy 0.8884\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.4575\t Accuracy 0.8878\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.4603\t Accuracy 0.8869\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.4621\t Accuracy 0.8862\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.4635\t Accuracy 0.8851\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.4668\t Accuracy 0.8841\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.4711\t Accuracy 0.8823\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.4699\t Accuracy 0.8830\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.4718\t Accuracy 0.8821\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.4708\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.4719\t Accuracy 0.8819\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.4722\t Accuracy 0.8810\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.4747\t Accuracy 0.8801\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.4725\t Accuracy 0.8812\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.4704\t Accuracy 0.8822\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.4703\t Accuracy 0.8822\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.4684\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.4678\t Accuracy 0.8831\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.4693\t Accuracy 0.8822\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.4678\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.4657\t Accuracy 0.8839\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.4658\t Accuracy 0.8838\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.4657\t Accuracy 0.8834\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.4648\t Accuracy 0.8837\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.4652\t Accuracy 0.8836\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.4647\t Accuracy 0.8836\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.4661\t Accuracy 0.8831\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.4646\t Accuracy 0.8834\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.4634\t Accuracy 0.8839\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.4632\t Accuracy 0.8843\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.4642\t Accuracy 0.8841\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.4645\t Accuracy 0.8839\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.4636\t Accuracy 0.8840\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.4627\t Accuracy 0.8844\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.4625\t Accuracy 0.8845\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.4629\t Accuracy 0.8842\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.4641\t Accuracy 0.8837\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.4650\t Accuracy 0.8834\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.4662\t Accuracy 0.8833\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.4676\t Accuracy 0.8830\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.4684\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.4702\t Accuracy 0.8818\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.4697\t Accuracy 0.8820\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.4700\t Accuracy 0.8818\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.4681\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.4672\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.4678\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.4672\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.4667\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.4662\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.4648\t Accuracy 0.8832\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.4660\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.4660\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.4654\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.4648\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.4648\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.4652\t Accuracy 0.8828\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.4646\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.4638\t Accuracy 0.8833\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.4646\t Accuracy 0.8831\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.4646\t Accuracy 0.8831\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.4657\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.4657\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.4649\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.4646\t Accuracy 0.8828\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.4650\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.4643\t Accuracy 0.8828\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.4645\t Accuracy 0.8827\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.4645\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.4651\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.4641\t Accuracy 0.8826\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.4642\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.4640\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.4629\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.4625\t Accuracy 0.8829\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.4629\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.4640\t Accuracy 0.8822\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.4647\t Accuracy 0.8820\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.4644\t Accuracy 0.8821\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.4639\t Accuracy 0.8822\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.4631\t Accuracy 0.8825\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.4635\t Accuracy 0.8823\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.4639\t Accuracy 0.8821\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.4633\t Accuracy 0.8823\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.4631\t Accuracy 0.8824\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.4626\t Accuracy 0.8825\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4625\t Average training accuracy 0.8824\n",
      "Epoch [4]\t Average validation loss 0.3597\t Average validation accuracy 0.9142\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.2256\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.4184\t Accuracy 0.8902\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.4575\t Accuracy 0.8792\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.4684\t Accuracy 0.8795\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.4430\t Accuracy 0.8856\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.4302\t Accuracy 0.8904\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.4220\t Accuracy 0.8927\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.4205\t Accuracy 0.8949\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.4182\t Accuracy 0.8970\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.4176\t Accuracy 0.8976\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.4152\t Accuracy 0.8962\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.4154\t Accuracy 0.8966\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.4137\t Accuracy 0.8973\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.4088\t Accuracy 0.8988\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.4077\t Accuracy 0.8977\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.4126\t Accuracy 0.8964\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.4159\t Accuracy 0.8953\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.4192\t Accuracy 0.8941\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.4262\t Accuracy 0.8911\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.4257\t Accuracy 0.8915\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.4229\t Accuracy 0.8921\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.4201\t Accuracy 0.8928\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.4170\t Accuracy 0.8938\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.4148\t Accuracy 0.8944\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.4188\t Accuracy 0.8932\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.4198\t Accuracy 0.8924\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.4228\t Accuracy 0.8915\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.4246\t Accuracy 0.8908\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.4261\t Accuracy 0.8899\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.4294\t Accuracy 0.8890\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.4338\t Accuracy 0.8874\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.4326\t Accuracy 0.8881\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.4344\t Accuracy 0.8875\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.4334\t Accuracy 0.8881\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.4345\t Accuracy 0.8875\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.4348\t Accuracy 0.8868\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.4372\t Accuracy 0.8858\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.4350\t Accuracy 0.8869\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.4330\t Accuracy 0.8880\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.4330\t Accuracy 0.8880\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.4312\t Accuracy 0.8885\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.4306\t Accuracy 0.8889\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.4323\t Accuracy 0.8880\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.4309\t Accuracy 0.8888\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.4290\t Accuracy 0.8897\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.4290\t Accuracy 0.8896\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.4290\t Accuracy 0.8892\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.4282\t Accuracy 0.8894\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.4286\t Accuracy 0.8894\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.4282\t Accuracy 0.8893\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.4296\t Accuracy 0.8889\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.4283\t Accuracy 0.8891\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.4272\t Accuracy 0.8896\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.4270\t Accuracy 0.8899\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.4282\t Accuracy 0.8896\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.4284\t Accuracy 0.8894\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.4277\t Accuracy 0.8895\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.4268\t Accuracy 0.8898\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.4267\t Accuracy 0.8899\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.4272\t Accuracy 0.8895\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.4285\t Accuracy 0.8890\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.4293\t Accuracy 0.8886\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.4305\t Accuracy 0.8885\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.4320\t Accuracy 0.8881\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.4328\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.4346\t Accuracy 0.8869\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.4341\t Accuracy 0.8872\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.4345\t Accuracy 0.8870\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.4326\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.4318\t Accuracy 0.8881\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.4324\t Accuracy 0.8876\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.4319\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.4313\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.4309\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.4295\t Accuracy 0.8883\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.4310\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.4310\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.4304\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.4300\t Accuracy 0.8880\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.4301\t Accuracy 0.8880\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.4304\t Accuracy 0.8879\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.4298\t Accuracy 0.8881\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.4291\t Accuracy 0.8885\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.4299\t Accuracy 0.8883\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.4299\t Accuracy 0.8882\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.4310\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.4312\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.4304\t Accuracy 0.8879\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.4302\t Accuracy 0.8879\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.4306\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.4299\t Accuracy 0.8879\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.4302\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.4304\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.4311\t Accuracy 0.8876\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.4301\t Accuracy 0.8877\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.4302\t Accuracy 0.8874\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.4301\t Accuracy 0.8875\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.4291\t Accuracy 0.8879\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.4287\t Accuracy 0.8878\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.4292\t Accuracy 0.8874\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.4303\t Accuracy 0.8873\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.4311\t Accuracy 0.8871\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.4309\t Accuracy 0.8872\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.4304\t Accuracy 0.8872\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.4297\t Accuracy 0.8875\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.4302\t Accuracy 0.8874\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.4306\t Accuracy 0.8872\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.4300\t Accuracy 0.8873\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.4299\t Accuracy 0.8874\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.4295\t Accuracy 0.8875\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4294\t Average training accuracy 0.8874\n",
      "Epoch [5]\t Average validation loss 0.3344\t Average validation accuracy 0.9184\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.1997\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.3917\t Accuracy 0.8902\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.4298\t Accuracy 0.8851\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.4413\t Accuracy 0.8841\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.4158\t Accuracy 0.8910\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.4029\t Accuracy 0.8956\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.3948\t Accuracy 0.8987\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.3925\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.3904\t Accuracy 0.9017\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.3898\t Accuracy 0.9024\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.3875\t Accuracy 0.9016\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.3880\t Accuracy 0.9015\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.3864\t Accuracy 0.9020\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.3817\t Accuracy 0.9035\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.3808\t Accuracy 0.9026\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.3854\t Accuracy 0.9013\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.3885\t Accuracy 0.9002\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.3919\t Accuracy 0.8991\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.3991\t Accuracy 0.8962\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.3989\t Accuracy 0.8963\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.3963\t Accuracy 0.8970\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.3936\t Accuracy 0.8977\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.3905\t Accuracy 0.8989\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.3884\t Accuracy 0.8997\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.3925\t Accuracy 0.8982\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.3936\t Accuracy 0.8974\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.3967\t Accuracy 0.8964\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.3986\t Accuracy 0.8958\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.4001\t Accuracy 0.8948\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.4034\t Accuracy 0.8939\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.4077\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.4067\t Accuracy 0.8929\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.4083\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.4073\t Accuracy 0.8927\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.4084\t Accuracy 0.8925\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.4087\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.4109\t Accuracy 0.8911\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.4087\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.4068\t Accuracy 0.8931\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.4068\t Accuracy 0.8932\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.4051\t Accuracy 0.8937\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.4045\t Accuracy 0.8941\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.4064\t Accuracy 0.8932\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.4051\t Accuracy 0.8940\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.4032\t Accuracy 0.8948\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.4032\t Accuracy 0.8948\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.4032\t Accuracy 0.8943\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.4025\t Accuracy 0.8944\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.4030\t Accuracy 0.8943\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.4026\t Accuracy 0.8941\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.4040\t Accuracy 0.8936\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.4027\t Accuracy 0.8938\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.4016\t Accuracy 0.8944\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.4015\t Accuracy 0.8946\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.4027\t Accuracy 0.8944\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.4030\t Accuracy 0.8941\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.4023\t Accuracy 0.8942\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.4016\t Accuracy 0.8945\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.4015\t Accuracy 0.8945\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.4020\t Accuracy 0.8941\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.4033\t Accuracy 0.8936\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.4040\t Accuracy 0.8932\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.4052\t Accuracy 0.8931\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.4068\t Accuracy 0.8927\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.4076\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.4094\t Accuracy 0.8915\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.4090\t Accuracy 0.8918\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.4093\t Accuracy 0.8917\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.4075\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.4067\t Accuracy 0.8926\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.4073\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.4068\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.4062\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.4058\t Accuracy 0.8925\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.4045\t Accuracy 0.8929\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.4062\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.4062\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.4056\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.4052\t Accuracy 0.8925\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.4054\t Accuracy 0.8925\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.4057\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.4051\t Accuracy 0.8926\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.4044\t Accuracy 0.8930\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.4052\t Accuracy 0.8927\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.4052\t Accuracy 0.8927\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.4064\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.4066\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.4059\t Accuracy 0.8923\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.4057\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.4061\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.4055\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.4059\t Accuracy 0.8922\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.4060\t Accuracy 0.8921\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.4068\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.4058\t Accuracy 0.8921\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.4060\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.4059\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.4050\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.4046\t Accuracy 0.8924\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.4051\t Accuracy 0.8920\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.4062\t Accuracy 0.8918\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.4071\t Accuracy 0.8917\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.4069\t Accuracy 0.8917\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.4064\t Accuracy 0.8917\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.4058\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.4063\t Accuracy 0.8918\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.4068\t Accuracy 0.8917\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.4063\t Accuracy 0.8918\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.4062\t Accuracy 0.8919\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.4058\t Accuracy 0.8920\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4058\t Average training accuracy 0.8919\n",
      "Epoch [6]\t Average validation loss 0.3160\t Average validation accuracy 0.9200\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.1800\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.3719\t Accuracy 0.8961\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.4092\t Accuracy 0.8911\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.4213\t Accuracy 0.8887\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.3957\t Accuracy 0.8970\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.3828\t Accuracy 0.9008\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.3747\t Accuracy 0.9037\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.3718\t Accuracy 0.9048\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.3698\t Accuracy 0.9062\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.3693\t Accuracy 0.9067\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.3669\t Accuracy 0.9062\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.3677\t Accuracy 0.9062\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.3662\t Accuracy 0.9067\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.3617\t Accuracy 0.9081\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.3609\t Accuracy 0.9074\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.3652\t Accuracy 0.9063\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.3683\t Accuracy 0.9050\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.3717\t Accuracy 0.9039\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.3791\t Accuracy 0.9010\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.3791\t Accuracy 0.9008\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.3765\t Accuracy 0.9015\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.3740\t Accuracy 0.9020\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.3709\t Accuracy 0.9031\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.3688\t Accuracy 0.9039\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.3730\t Accuracy 0.9022\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.3742\t Accuracy 0.9014\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.3773\t Accuracy 0.9004\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.3792\t Accuracy 0.8999\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.3808\t Accuracy 0.8988\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.3841\t Accuracy 0.8979\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.3883\t Accuracy 0.8965\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.3873\t Accuracy 0.8970\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.3889\t Accuracy 0.8964\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.3879\t Accuracy 0.8968\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.3889\t Accuracy 0.8964\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.3891\t Accuracy 0.8959\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.3913\t Accuracy 0.8949\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.3891\t Accuracy 0.8959\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.3873\t Accuracy 0.8968\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.3872\t Accuracy 0.8970\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.3856\t Accuracy 0.8974\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.3850\t Accuracy 0.8977\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.3870\t Accuracy 0.8968\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.3857\t Accuracy 0.8975\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.3840\t Accuracy 0.8984\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.3840\t Accuracy 0.8984\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.3840\t Accuracy 0.8978\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.3833\t Accuracy 0.8980\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.3838\t Accuracy 0.8979\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.3834\t Accuracy 0.8978\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.3848\t Accuracy 0.8972\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.3836\t Accuracy 0.8975\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.3826\t Accuracy 0.8980\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.3824\t Accuracy 0.8982\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.3837\t Accuracy 0.8979\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.3841\t Accuracy 0.8976\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.3834\t Accuracy 0.8978\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.3827\t Accuracy 0.8980\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.3826\t Accuracy 0.8980\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.3831\t Accuracy 0.8977\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.3844\t Accuracy 0.8971\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.3851\t Accuracy 0.8967\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.3863\t Accuracy 0.8965\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.3879\t Accuracy 0.8960\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.3887\t Accuracy 0.8957\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.3904\t Accuracy 0.8949\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.3901\t Accuracy 0.8952\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.3904\t Accuracy 0.8951\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.3886\t Accuracy 0.8956\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.3878\t Accuracy 0.8959\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.3884\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.3879\t Accuracy 0.8956\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.3873\t Accuracy 0.8958\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.3870\t Accuracy 0.8958\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.3857\t Accuracy 0.8963\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.3874\t Accuracy 0.8957\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.3875\t Accuracy 0.8957\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.3869\t Accuracy 0.8957\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.3866\t Accuracy 0.8958\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.3868\t Accuracy 0.8957\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.3871\t Accuracy 0.8956\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.3865\t Accuracy 0.8958\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.3858\t Accuracy 0.8962\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.3866\t Accuracy 0.8959\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.3866\t Accuracy 0.8959\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.3879\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.3881\t Accuracy 0.8954\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.3874\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.3873\t Accuracy 0.8956\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.3877\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.3871\t Accuracy 0.8956\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.3874\t Accuracy 0.8954\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.3876\t Accuracy 0.8953\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.3884\t Accuracy 0.8952\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.3875\t Accuracy 0.8954\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.3877\t Accuracy 0.8952\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.3876\t Accuracy 0.8951\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.3867\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.3864\t Accuracy 0.8955\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.3869\t Accuracy 0.8952\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.3880\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.3889\t Accuracy 0.8949\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.3887\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.3883\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.3877\t Accuracy 0.8952\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.3882\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.3888\t Accuracy 0.8949\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.3882\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.3882\t Accuracy 0.8951\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.3878\t Accuracy 0.8952\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3879\t Average training accuracy 0.8951\n",
      "Epoch [7]\t Average validation loss 0.3020\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.1640\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.3565\t Accuracy 0.8961\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.3931\t Accuracy 0.8911\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.4058\t Accuracy 0.8907\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.3802\t Accuracy 0.8990\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.3673\t Accuracy 0.9028\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.3591\t Accuracy 0.9066\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.3558\t Accuracy 0.9077\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.3539\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.3534\t Accuracy 0.9102\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.3510\t Accuracy 0.9096\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.3520\t Accuracy 0.9093\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.3505\t Accuracy 0.9095\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.3461\t Accuracy 0.9108\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.3455\t Accuracy 0.9103\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.3495\t Accuracy 0.9089\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.3526\t Accuracy 0.9075\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.3560\t Accuracy 0.9062\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.3635\t Accuracy 0.9034\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.3637\t Accuracy 0.9033\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.3612\t Accuracy 0.9038\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.3588\t Accuracy 0.9043\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.3557\t Accuracy 0.9054\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.3537\t Accuracy 0.9063\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.3579\t Accuracy 0.9047\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.3591\t Accuracy 0.9040\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.3623\t Accuracy 0.9030\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.3641\t Accuracy 0.9024\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.3657\t Accuracy 0.9016\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.3690\t Accuracy 0.9010\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.3732\t Accuracy 0.8996\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.3723\t Accuracy 0.9001\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.3737\t Accuracy 0.8994\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.3727\t Accuracy 0.8996\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.3737\t Accuracy 0.8991\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.3739\t Accuracy 0.8988\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.3760\t Accuracy 0.8979\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.3738\t Accuracy 0.8988\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.3720\t Accuracy 0.8996\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.3719\t Accuracy 0.8998\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.3704\t Accuracy 0.9002\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.3697\t Accuracy 0.9004\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.3719\t Accuracy 0.8998\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.3706\t Accuracy 0.9005\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.3689\t Accuracy 0.9012\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.3689\t Accuracy 0.9012\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.3689\t Accuracy 0.9007\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.3683\t Accuracy 0.9008\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.3688\t Accuracy 0.9007\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.3684\t Accuracy 0.9007\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.3698\t Accuracy 0.9001\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.3686\t Accuracy 0.9003\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.3676\t Accuracy 0.9008\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.3675\t Accuracy 0.9010\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.3688\t Accuracy 0.9008\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.3692\t Accuracy 0.9005\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.3685\t Accuracy 0.9006\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.3679\t Accuracy 0.9008\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.3678\t Accuracy 0.9008\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.3684\t Accuracy 0.9005\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.3696\t Accuracy 0.8999\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.3702\t Accuracy 0.8994\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.3714\t Accuracy 0.8992\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.3731\t Accuracy 0.8987\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.3738\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.3755\t Accuracy 0.8976\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.3752\t Accuracy 0.8979\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.3755\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.3737\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.3730\t Accuracy 0.8986\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.3736\t Accuracy 0.8982\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.3731\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.3725\t Accuracy 0.8985\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.3721\t Accuracy 0.8986\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.3709\t Accuracy 0.8990\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.3727\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.3728\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.3722\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.3719\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.3722\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.3724\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.3719\t Accuracy 0.8986\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.3712\t Accuracy 0.8990\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.3720\t Accuracy 0.8987\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.3720\t Accuracy 0.8986\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.3732\t Accuracy 0.8982\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.3735\t Accuracy 0.8981\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.3728\t Accuracy 0.8983\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.3727\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.3731\t Accuracy 0.8982\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.3725\t Accuracy 0.8984\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.3729\t Accuracy 0.8981\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.3731\t Accuracy 0.8980\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.3740\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.3731\t Accuracy 0.8980\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.3732\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.3732\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.3723\t Accuracy 0.8982\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.3720\t Accuracy 0.8981\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.3725\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.3736\t Accuracy 0.8977\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.3745\t Accuracy 0.8975\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.3743\t Accuracy 0.8975\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.3740\t Accuracy 0.8976\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.3734\t Accuracy 0.8978\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.3739\t Accuracy 0.8976\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.3745\t Accuracy 0.8975\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.3740\t Accuracy 0.8976\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.3740\t Accuracy 0.8976\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.3736\t Accuracy 0.8977\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3737\t Average training accuracy 0.8976\n",
      "Epoch [8]\t Average validation loss 0.2910\t Average validation accuracy 0.9244\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.1505\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.3440\t Accuracy 0.8961\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.3800\t Accuracy 0.8911\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.3934\t Accuracy 0.8907\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.3677\t Accuracy 0.8995\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.3548\t Accuracy 0.9036\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.3466\t Accuracy 0.9073\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.3429\t Accuracy 0.9085\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.3411\t Accuracy 0.9112\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.3407\t Accuracy 0.9115\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.3381\t Accuracy 0.9110\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.3393\t Accuracy 0.9107\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.3379\t Accuracy 0.9106\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.3337\t Accuracy 0.9120\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.3331\t Accuracy 0.9116\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.3370\t Accuracy 0.9109\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.3399\t Accuracy 0.9095\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.3434\t Accuracy 0.9082\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.3510\t Accuracy 0.9055\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.3513\t Accuracy 0.9056\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.3488\t Accuracy 0.9060\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.3465\t Accuracy 0.9064\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.3435\t Accuracy 0.9076\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.3414\t Accuracy 0.9083\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.3457\t Accuracy 0.9067\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.3469\t Accuracy 0.9060\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.3502\t Accuracy 0.9051\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.3520\t Accuracy 0.9046\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.3536\t Accuracy 0.9038\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.3569\t Accuracy 0.9032\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.3610\t Accuracy 0.9019\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.3601\t Accuracy 0.9025\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.3614\t Accuracy 0.9019\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.3604\t Accuracy 0.9022\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.3614\t Accuracy 0.9016\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.3616\t Accuracy 0.9014\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.3636\t Accuracy 0.9006\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.3615\t Accuracy 0.9014\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.3597\t Accuracy 0.9021\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.3595\t Accuracy 0.9023\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.3580\t Accuracy 0.9027\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.3573\t Accuracy 0.9030\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.3596\t Accuracy 0.9024\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.3584\t Accuracy 0.9031\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.3568\t Accuracy 0.9037\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.3567\t Accuracy 0.9037\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.3567\t Accuracy 0.9033\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.3561\t Accuracy 0.9034\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.3566\t Accuracy 0.9033\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.3562\t Accuracy 0.9032\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.3576\t Accuracy 0.9026\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.3565\t Accuracy 0.9028\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.3555\t Accuracy 0.9033\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.3554\t Accuracy 0.9035\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.3568\t Accuracy 0.9032\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.3571\t Accuracy 0.9029\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.3565\t Accuracy 0.9030\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.3559\t Accuracy 0.9032\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.3558\t Accuracy 0.9031\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.3564\t Accuracy 0.9028\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.3576\t Accuracy 0.9022\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.3582\t Accuracy 0.9017\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.3593\t Accuracy 0.9014\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.3610\t Accuracy 0.9009\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.3617\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.3634\t Accuracy 0.8998\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.3631\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.3634\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.3616\t Accuracy 0.9008\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.3609\t Accuracy 0.9010\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.3615\t Accuracy 0.9005\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.3610\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.3604\t Accuracy 0.9009\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.3600\t Accuracy 0.9009\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.3589\t Accuracy 0.9014\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.3607\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.3609\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.3603\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.3599\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.3602\t Accuracy 0.9007\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.3604\t Accuracy 0.9006\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.3599\t Accuracy 0.9008\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.3593\t Accuracy 0.9012\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.3600\t Accuracy 0.9009\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.3601\t Accuracy 0.9009\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.3613\t Accuracy 0.9004\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.3616\t Accuracy 0.9004\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.3609\t Accuracy 0.9005\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.3608\t Accuracy 0.9006\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.3612\t Accuracy 0.9004\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.3606\t Accuracy 0.9006\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.3611\t Accuracy 0.9003\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.3613\t Accuracy 0.9003\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.3622\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.3613\t Accuracy 0.9003\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.3614\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.3614\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.3606\t Accuracy 0.9004\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.3603\t Accuracy 0.9003\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.3607\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.3619\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.3628\t Accuracy 0.8998\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.3626\t Accuracy 0.8998\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.3622\t Accuracy 0.8998\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.3617\t Accuracy 0.9001\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.3622\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.3628\t Accuracy 0.8998\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.3623\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.3623\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.3620\t Accuracy 0.9000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3620\t Average training accuracy 0.8999\n",
      "Epoch [9]\t Average validation loss 0.2820\t Average validation accuracy 0.9252\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.1389\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.3336\t Accuracy 0.8980\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.3691\t Accuracy 0.8950\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.3832\t Accuracy 0.8934\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.3574\t Accuracy 0.9020\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.3444\t Accuracy 0.9056\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.3362\t Accuracy 0.9096\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.3323\t Accuracy 0.9108\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.3304\t Accuracy 0.9137\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.3301\t Accuracy 0.9140\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.3275\t Accuracy 0.9132\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.3288\t Accuracy 0.9131\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.3275\t Accuracy 0.9128\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.3233\t Accuracy 0.9143\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.3229\t Accuracy 0.9140\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.3266\t Accuracy 0.9134\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.3294\t Accuracy 0.9121\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.3329\t Accuracy 0.9108\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.3406\t Accuracy 0.9081\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.3410\t Accuracy 0.9080\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.3385\t Accuracy 0.9085\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.3364\t Accuracy 0.9090\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.3333\t Accuracy 0.9104\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.3313\t Accuracy 0.9109\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.3355\t Accuracy 0.9093\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.3368\t Accuracy 0.9086\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.3401\t Accuracy 0.9075\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.3419\t Accuracy 0.9070\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.3435\t Accuracy 0.9064\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.3467\t Accuracy 0.9057\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.3508\t Accuracy 0.9043\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.3500\t Accuracy 0.9047\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.3512\t Accuracy 0.9042\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.3502\t Accuracy 0.9045\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.3512\t Accuracy 0.9039\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.3513\t Accuracy 0.9037\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.3532\t Accuracy 0.9029\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.3512\t Accuracy 0.9038\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.3494\t Accuracy 0.9045\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.3492\t Accuracy 0.9047\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.3477\t Accuracy 0.9051\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.3470\t Accuracy 0.9055\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.3494\t Accuracy 0.9050\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.3482\t Accuracy 0.9057\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.3466\t Accuracy 0.9063\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.3465\t Accuracy 0.9063\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.3465\t Accuracy 0.9059\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.3460\t Accuracy 0.9060\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.3465\t Accuracy 0.9059\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.3461\t Accuracy 0.9058\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.3475\t Accuracy 0.9052\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.3463\t Accuracy 0.9054\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.3454\t Accuracy 0.9058\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.3453\t Accuracy 0.9060\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.3467\t Accuracy 0.9056\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.3471\t Accuracy 0.9053\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.3465\t Accuracy 0.9054\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.3459\t Accuracy 0.9056\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.3458\t Accuracy 0.9056\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.3463\t Accuracy 0.9052\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.3476\t Accuracy 0.9046\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.3481\t Accuracy 0.9041\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.3492\t Accuracy 0.9039\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.3509\t Accuracy 0.9034\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.3516\t Accuracy 0.9031\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.3533\t Accuracy 0.9023\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.3530\t Accuracy 0.9025\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.3532\t Accuracy 0.9024\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.3515\t Accuracy 0.9031\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.3508\t Accuracy 0.9033\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.3514\t Accuracy 0.9029\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.3509\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.3503\t Accuracy 0.9032\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.3499\t Accuracy 0.9032\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.3488\t Accuracy 0.9037\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.3507\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.3509\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.3503\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.3500\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.3503\t Accuracy 0.9029\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.3505\t Accuracy 0.9029\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.3499\t Accuracy 0.9031\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.3493\t Accuracy 0.9034\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.3501\t Accuracy 0.9032\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.3501\t Accuracy 0.9031\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.3513\t Accuracy 0.9027\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.3516\t Accuracy 0.9027\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.3509\t Accuracy 0.9028\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.3509\t Accuracy 0.9028\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.3512\t Accuracy 0.9028\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.3507\t Accuracy 0.9030\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.3511\t Accuracy 0.9027\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.3514\t Accuracy 0.9026\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.3523\t Accuracy 0.9024\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.3514\t Accuracy 0.9026\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.3516\t Accuracy 0.9024\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.3516\t Accuracy 0.9024\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.3507\t Accuracy 0.9027\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.3504\t Accuracy 0.9026\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.3509\t Accuracy 0.9025\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.3520\t Accuracy 0.9023\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.3529\t Accuracy 0.9021\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.3528\t Accuracy 0.9022\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.3524\t Accuracy 0.9021\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.3519\t Accuracy 0.9024\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.3524\t Accuracy 0.9022\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.3530\t Accuracy 0.9020\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.3525\t Accuracy 0.9022\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.3526\t Accuracy 0.9022\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.3522\t Accuracy 0.9022\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3523\t Average training accuracy 0.9021\n",
      "Epoch [10]\t Average validation loss 0.2745\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.1288\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.3247\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.3598\t Accuracy 0.8970\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.3746\t Accuracy 0.8940\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.3487\t Accuracy 0.9035\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.3356\t Accuracy 0.9080\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.3274\t Accuracy 0.9120\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.3233\t Accuracy 0.9131\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.3214\t Accuracy 0.9160\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.3212\t Accuracy 0.9160\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.3185\t Accuracy 0.9154\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.3199\t Accuracy 0.9151\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.3187\t Accuracy 0.9153\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.3146\t Accuracy 0.9167\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.3142\t Accuracy 0.9163\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.3178\t Accuracy 0.9156\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.3206\t Accuracy 0.9141\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.3241\t Accuracy 0.9129\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.3318\t Accuracy 0.9102\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.3323\t Accuracy 0.9101\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.3298\t Accuracy 0.9105\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.3278\t Accuracy 0.9110\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.3247\t Accuracy 0.9124\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.3227\t Accuracy 0.9129\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.3270\t Accuracy 0.9112\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.3282\t Accuracy 0.9105\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.3316\t Accuracy 0.9093\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.3334\t Accuracy 0.9088\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.3349\t Accuracy 0.9084\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.3381\t Accuracy 0.9076\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.3422\t Accuracy 0.9065\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.3414\t Accuracy 0.9069\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.3426\t Accuracy 0.9064\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.3416\t Accuracy 0.9067\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.3424\t Accuracy 0.9061\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.3426\t Accuracy 0.9058\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.3444\t Accuracy 0.9052\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.3424\t Accuracy 0.9059\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.3407\t Accuracy 0.9067\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.3405\t Accuracy 0.9070\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.3390\t Accuracy 0.9074\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.3383\t Accuracy 0.9077\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.3407\t Accuracy 0.9072\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.3395\t Accuracy 0.9079\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.3380\t Accuracy 0.9085\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.3379\t Accuracy 0.9084\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.3379\t Accuracy 0.9080\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.3373\t Accuracy 0.9080\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.3378\t Accuracy 0.9079\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.3375\t Accuracy 0.9078\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.3389\t Accuracy 0.9072\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.3377\t Accuracy 0.9073\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.3368\t Accuracy 0.9077\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.3368\t Accuracy 0.9079\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.3381\t Accuracy 0.9075\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.3385\t Accuracy 0.9072\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.3379\t Accuracy 0.9073\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.3374\t Accuracy 0.9075\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.3373\t Accuracy 0.9075\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.3378\t Accuracy 0.9072\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.3391\t Accuracy 0.9065\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.3395\t Accuracy 0.9061\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.3405\t Accuracy 0.9059\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.3423\t Accuracy 0.9054\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.3430\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.3446\t Accuracy 0.9043\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.3444\t Accuracy 0.9045\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.3446\t Accuracy 0.9045\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.3429\t Accuracy 0.9051\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.3422\t Accuracy 0.9054\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.3427\t Accuracy 0.9049\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.3423\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.3416\t Accuracy 0.9052\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.3413\t Accuracy 0.9052\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.3402\t Accuracy 0.9057\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.3422\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.3423\t Accuracy 0.9049\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.3417\t Accuracy 0.9049\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.3414\t Accuracy 0.9049\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.3418\t Accuracy 0.9048\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.3419\t Accuracy 0.9048\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.3414\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.3408\t Accuracy 0.9053\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.3415\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.3416\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.3428\t Accuracy 0.9046\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.3431\t Accuracy 0.9045\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.3424\t Accuracy 0.9046\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.3424\t Accuracy 0.9047\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.3427\t Accuracy 0.9045\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.3422\t Accuracy 0.9047\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.3427\t Accuracy 0.9044\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.3429\t Accuracy 0.9043\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.3438\t Accuracy 0.9041\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.3429\t Accuracy 0.9043\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.3431\t Accuracy 0.9041\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.3431\t Accuracy 0.9041\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.3423\t Accuracy 0.9045\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.3420\t Accuracy 0.9044\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.3425\t Accuracy 0.9043\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.3436\t Accuracy 0.9041\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.3445\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.3443\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.3440\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.3435\t Accuracy 0.9041\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.3439\t Accuracy 0.9040\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.3446\t Accuracy 0.9037\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.3441\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.3442\t Accuracy 0.9039\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.3438\t Accuracy 0.9039\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3439\t Average training accuracy 0.9037\n",
      "Epoch [11]\t Average validation loss 0.2681\t Average validation accuracy 0.9272\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.1200\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.3171\t Accuracy 0.9078\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.3516\t Accuracy 0.8980\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.3672\t Accuracy 0.8954\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.3412\t Accuracy 0.9055\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.3281\t Accuracy 0.9096\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.3199\t Accuracy 0.9133\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.3155\t Accuracy 0.9148\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.3137\t Accuracy 0.9175\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.3135\t Accuracy 0.9175\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.3108\t Accuracy 0.9170\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.3122\t Accuracy 0.9167\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.3110\t Accuracy 0.9168\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.3071\t Accuracy 0.9183\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.3067\t Accuracy 0.9178\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.3102\t Accuracy 0.9173\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.3130\t Accuracy 0.9159\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.3165\t Accuracy 0.9146\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.3242\t Accuracy 0.9119\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.3247\t Accuracy 0.9118\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.3223\t Accuracy 0.9123\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.3203\t Accuracy 0.9127\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.3173\t Accuracy 0.9140\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.3152\t Accuracy 0.9145\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.3195\t Accuracy 0.9128\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.3208\t Accuracy 0.9121\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.3242\t Accuracy 0.9110\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.3260\t Accuracy 0.9104\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.3275\t Accuracy 0.9099\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.3307\t Accuracy 0.9091\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.3347\t Accuracy 0.9080\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.3340\t Accuracy 0.9084\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.3351\t Accuracy 0.9079\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.3340\t Accuracy 0.9081\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.3349\t Accuracy 0.9075\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.3350\t Accuracy 0.9073\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.3368\t Accuracy 0.9067\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.3348\t Accuracy 0.9074\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.3331\t Accuracy 0.9082\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.3329\t Accuracy 0.9084\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.3314\t Accuracy 0.9089\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.3307\t Accuracy 0.9091\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.3332\t Accuracy 0.9086\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.3320\t Accuracy 0.9093\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.3305\t Accuracy 0.9099\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.3304\t Accuracy 0.9099\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.3304\t Accuracy 0.9095\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.3299\t Accuracy 0.9095\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.3304\t Accuracy 0.9094\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.3300\t Accuracy 0.9092\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.3314\t Accuracy 0.9088\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.3303\t Accuracy 0.9088\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.3294\t Accuracy 0.9092\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.3293\t Accuracy 0.9094\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.3307\t Accuracy 0.9090\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.3311\t Accuracy 0.9087\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.3305\t Accuracy 0.9089\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.3300\t Accuracy 0.9090\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.3299\t Accuracy 0.9090\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.3304\t Accuracy 0.9087\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.3316\t Accuracy 0.9081\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.3320\t Accuracy 0.9078\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.3331\t Accuracy 0.9076\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.3348\t Accuracy 0.9071\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.3355\t Accuracy 0.9067\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.3371\t Accuracy 0.9061\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.3369\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.3370\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.3353\t Accuracy 0.9069\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.3347\t Accuracy 0.9071\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.3352\t Accuracy 0.9067\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.3348\t Accuracy 0.9068\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.3341\t Accuracy 0.9070\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.3338\t Accuracy 0.9070\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.3327\t Accuracy 0.9074\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.3347\t Accuracy 0.9067\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.3349\t Accuracy 0.9066\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.3343\t Accuracy 0.9066\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.3340\t Accuracy 0.9066\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.3344\t Accuracy 0.9066\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.3345\t Accuracy 0.9065\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.3340\t Accuracy 0.9067\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.3334\t Accuracy 0.9071\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.3341\t Accuracy 0.9068\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.3341\t Accuracy 0.9068\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.3354\t Accuracy 0.9063\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.3357\t Accuracy 0.9063\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.3350\t Accuracy 0.9064\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.3350\t Accuracy 0.9063\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.3353\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.3348\t Accuracy 0.9064\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.3353\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.3355\t Accuracy 0.9061\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.3365\t Accuracy 0.9060\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.3356\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.3357\t Accuracy 0.9060\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.3358\t Accuracy 0.9060\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.3349\t Accuracy 0.9063\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.3347\t Accuracy 0.9062\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.3351\t Accuracy 0.9061\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.3362\t Accuracy 0.9059\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.3371\t Accuracy 0.9057\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.3370\t Accuracy 0.9058\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.3366\t Accuracy 0.9057\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.3361\t Accuracy 0.9060\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.3366\t Accuracy 0.9058\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.3373\t Accuracy 0.9056\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.3368\t Accuracy 0.9058\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.3369\t Accuracy 0.9057\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.3365\t Accuracy 0.9057\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3366\t Average training accuracy 0.9056\n",
      "Epoch [12]\t Average validation loss 0.2625\t Average validation accuracy 0.9298\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.1122\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.3103\t Accuracy 0.9078\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.3445\t Accuracy 0.8970\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.3607\t Accuracy 0.8934\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.3346\t Accuracy 0.9040\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.3214\t Accuracy 0.9088\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.3132\t Accuracy 0.9126\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.3087\t Accuracy 0.9145\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.3069\t Accuracy 0.9172\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.3068\t Accuracy 0.9175\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.3040\t Accuracy 0.9172\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.3055\t Accuracy 0.9171\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.3044\t Accuracy 0.9171\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.3005\t Accuracy 0.9187\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.3002\t Accuracy 0.9184\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.3035\t Accuracy 0.9181\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.3063\t Accuracy 0.9166\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.3098\t Accuracy 0.9154\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.3175\t Accuracy 0.9131\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.3181\t Accuracy 0.9129\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.3157\t Accuracy 0.9135\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.3138\t Accuracy 0.9139\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.3108\t Accuracy 0.9152\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.3087\t Accuracy 0.9156\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.3130\t Accuracy 0.9140\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.3143\t Accuracy 0.9132\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.3177\t Accuracy 0.9122\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.3195\t Accuracy 0.9116\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.3209\t Accuracy 0.9111\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.3241\t Accuracy 0.9102\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.3281\t Accuracy 0.9092\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.3274\t Accuracy 0.9096\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.3284\t Accuracy 0.9091\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.3274\t Accuracy 0.9093\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.3282\t Accuracy 0.9088\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.3283\t Accuracy 0.9087\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.3301\t Accuracy 0.9082\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.3281\t Accuracy 0.9089\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.3264\t Accuracy 0.9096\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.3262\t Accuracy 0.9098\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.3247\t Accuracy 0.9103\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.3240\t Accuracy 0.9106\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.3265\t Accuracy 0.9102\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.3254\t Accuracy 0.9109\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.3239\t Accuracy 0.9114\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.3238\t Accuracy 0.9114\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.3238\t Accuracy 0.9110\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.3233\t Accuracy 0.9109\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.3238\t Accuracy 0.9108\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.3234\t Accuracy 0.9106\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.3248\t Accuracy 0.9102\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.3237\t Accuracy 0.9103\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.3228\t Accuracy 0.9106\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.3227\t Accuracy 0.9108\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.3242\t Accuracy 0.9104\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.3246\t Accuracy 0.9101\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.3240\t Accuracy 0.9102\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.3235\t Accuracy 0.9104\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.3234\t Accuracy 0.9105\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.3239\t Accuracy 0.9102\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.3251\t Accuracy 0.9097\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.3254\t Accuracy 0.9094\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.3264\t Accuracy 0.9093\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.3282\t Accuracy 0.9087\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.3289\t Accuracy 0.9083\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.3304\t Accuracy 0.9077\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.3303\t Accuracy 0.9078\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.3304\t Accuracy 0.9078\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.3287\t Accuracy 0.9084\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.3280\t Accuracy 0.9086\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.3286\t Accuracy 0.9081\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.3282\t Accuracy 0.9082\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.3274\t Accuracy 0.9084\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.3271\t Accuracy 0.9084\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.3261\t Accuracy 0.9088\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.3282\t Accuracy 0.9080\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.3283\t Accuracy 0.9079\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.3277\t Accuracy 0.9079\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.3274\t Accuracy 0.9080\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.3278\t Accuracy 0.9079\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.3279\t Accuracy 0.9079\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.3274\t Accuracy 0.9080\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.3268\t Accuracy 0.9084\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.3276\t Accuracy 0.9081\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.3276\t Accuracy 0.9080\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.3288\t Accuracy 0.9076\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.3291\t Accuracy 0.9076\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.3284\t Accuracy 0.9077\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.3284\t Accuracy 0.9076\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.3288\t Accuracy 0.9075\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.3283\t Accuracy 0.9077\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.3287\t Accuracy 0.9075\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.3290\t Accuracy 0.9074\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.3300\t Accuracy 0.9072\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.3291\t Accuracy 0.9075\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.3292\t Accuracy 0.9073\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.3293\t Accuracy 0.9073\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.3284\t Accuracy 0.9076\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.3282\t Accuracy 0.9076\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.3286\t Accuracy 0.9075\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.3297\t Accuracy 0.9073\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.3306\t Accuracy 0.9070\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.3305\t Accuracy 0.9071\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.3301\t Accuracy 0.9070\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.3296\t Accuracy 0.9073\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.3301\t Accuracy 0.9072\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.3308\t Accuracy 0.9069\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.3303\t Accuracy 0.9071\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.3304\t Accuracy 0.9071\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.3301\t Accuracy 0.9071\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3302\t Average training accuracy 0.9070\n",
      "Epoch [13]\t Average validation loss 0.2575\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.1053\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.3043\t Accuracy 0.9078\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.3381\t Accuracy 0.8980\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.3549\t Accuracy 0.8947\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.3288\t Accuracy 0.9055\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.3155\t Accuracy 0.9104\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.3073\t Accuracy 0.9140\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.3026\t Accuracy 0.9157\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.3008\t Accuracy 0.9182\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.3008\t Accuracy 0.9184\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.2980\t Accuracy 0.9180\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.2995\t Accuracy 0.9180\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.2985\t Accuracy 0.9181\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.2946\t Accuracy 0.9197\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.2943\t Accuracy 0.9193\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.2976\t Accuracy 0.9190\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.3003\t Accuracy 0.9175\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.3039\t Accuracy 0.9165\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.3116\t Accuracy 0.9144\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.3122\t Accuracy 0.9142\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.3098\t Accuracy 0.9147\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.3079\t Accuracy 0.9150\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.3050\t Accuracy 0.9163\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.3029\t Accuracy 0.9167\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.3072\t Accuracy 0.9151\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.3085\t Accuracy 0.9145\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.3119\t Accuracy 0.9135\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.3136\t Accuracy 0.9129\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.3151\t Accuracy 0.9123\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.3182\t Accuracy 0.9114\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.3222\t Accuracy 0.9104\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.3216\t Accuracy 0.9110\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.3225\t Accuracy 0.9105\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.3215\t Accuracy 0.9107\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.3223\t Accuracy 0.9102\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.3224\t Accuracy 0.9103\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.3241\t Accuracy 0.9097\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.3221\t Accuracy 0.9105\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.3205\t Accuracy 0.9113\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.3202\t Accuracy 0.9114\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.3187\t Accuracy 0.9118\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.3180\t Accuracy 0.9120\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.3206\t Accuracy 0.9116\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.3195\t Accuracy 0.9124\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.3181\t Accuracy 0.9129\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.3179\t Accuracy 0.9128\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.3179\t Accuracy 0.9124\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.3174\t Accuracy 0.9124\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.3179\t Accuracy 0.9122\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.3175\t Accuracy 0.9121\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.3189\t Accuracy 0.9116\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.3178\t Accuracy 0.9118\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.3169\t Accuracy 0.9122\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.3169\t Accuracy 0.9124\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.3183\t Accuracy 0.9120\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.3187\t Accuracy 0.9117\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.3182\t Accuracy 0.9117\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.3176\t Accuracy 0.9120\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.3176\t Accuracy 0.9120\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.3180\t Accuracy 0.9118\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.3192\t Accuracy 0.9112\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.3195\t Accuracy 0.9110\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.3205\t Accuracy 0.9109\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.3223\t Accuracy 0.9104\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.3230\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.3245\t Accuracy 0.9094\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.3243\t Accuracy 0.9095\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.3244\t Accuracy 0.9094\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.3228\t Accuracy 0.9099\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.3221\t Accuracy 0.9102\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.3226\t Accuracy 0.9097\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.3222\t Accuracy 0.9098\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.3215\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.3212\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.3202\t Accuracy 0.9103\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.3223\t Accuracy 0.9096\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.3225\t Accuracy 0.9096\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.3218\t Accuracy 0.9095\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.3216\t Accuracy 0.9096\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.3220\t Accuracy 0.9094\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.3220\t Accuracy 0.9094\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.3215\t Accuracy 0.9096\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.3209\t Accuracy 0.9099\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.3217\t Accuracy 0.9096\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.3217\t Accuracy 0.9095\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.3229\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.3233\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.3226\t Accuracy 0.9092\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.3226\t Accuracy 0.9092\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.3229\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.3224\t Accuracy 0.9093\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.3229\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.3232\t Accuracy 0.9090\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.3241\t Accuracy 0.9088\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.3233\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.3234\t Accuracy 0.9088\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.3234\t Accuracy 0.9088\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.3226\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.3224\t Accuracy 0.9091\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.3228\t Accuracy 0.9090\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.3239\t Accuracy 0.9088\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.3248\t Accuracy 0.9086\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.3247\t Accuracy 0.9086\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.3243\t Accuracy 0.9086\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.3238\t Accuracy 0.9088\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.3243\t Accuracy 0.9087\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.3250\t Accuracy 0.9084\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.3245\t Accuracy 0.9086\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.3246\t Accuracy 0.9086\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.3243\t Accuracy 0.9085\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3244\t Average training accuracy 0.9085\n",
      "Epoch [14]\t Average validation loss 0.2531\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0992\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.2989\t Accuracy 0.9059\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.3323\t Accuracy 0.8970\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.3497\t Accuracy 0.8940\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.3235\t Accuracy 0.9045\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.3102\t Accuracy 0.9096\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.3021\t Accuracy 0.9133\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.2972\t Accuracy 0.9151\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.2954\t Accuracy 0.9177\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.2954\t Accuracy 0.9182\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.2926\t Accuracy 0.9182\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.2942\t Accuracy 0.9180\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.2931\t Accuracy 0.9183\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.2894\t Accuracy 0.9198\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.2891\t Accuracy 0.9195\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.2923\t Accuracy 0.9193\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.2950\t Accuracy 0.9180\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.2985\t Accuracy 0.9169\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.3062\t Accuracy 0.9150\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.3069\t Accuracy 0.9147\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.3045\t Accuracy 0.9151\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.3027\t Accuracy 0.9154\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.2997\t Accuracy 0.9167\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.2977\t Accuracy 0.9172\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.3019\t Accuracy 0.9156\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.3032\t Accuracy 0.9151\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.3067\t Accuracy 0.9141\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.3084\t Accuracy 0.9135\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.3098\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.3129\t Accuracy 0.9121\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.3168\t Accuracy 0.9110\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.3163\t Accuracy 0.9115\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.3172\t Accuracy 0.9112\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.3161\t Accuracy 0.9113\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.3169\t Accuracy 0.9108\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.3170\t Accuracy 0.9109\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.3186\t Accuracy 0.9103\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.3167\t Accuracy 0.9110\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.3151\t Accuracy 0.9118\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.3148\t Accuracy 0.9119\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.3133\t Accuracy 0.9123\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.3126\t Accuracy 0.9125\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.3152\t Accuracy 0.9122\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.3141\t Accuracy 0.9129\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.3128\t Accuracy 0.9134\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.3126\t Accuracy 0.9134\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.3126\t Accuracy 0.9131\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.3121\t Accuracy 0.9131\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.3126\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.3122\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.3136\t Accuracy 0.9125\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.3125\t Accuracy 0.9127\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.3117\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.3116\t Accuracy 0.9132\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.3130\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.3135\t Accuracy 0.9126\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.3129\t Accuracy 0.9127\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.3124\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.3123\t Accuracy 0.9130\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.3127\t Accuracy 0.9127\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.3139\t Accuracy 0.9122\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.3142\t Accuracy 0.9119\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.3152\t Accuracy 0.9118\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.3170\t Accuracy 0.9113\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.3176\t Accuracy 0.9109\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.3191\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.3189\t Accuracy 0.9103\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.3191\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.3174\t Accuracy 0.9108\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.3167\t Accuracy 0.9111\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.3172\t Accuracy 0.9107\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.3169\t Accuracy 0.9108\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.3161\t Accuracy 0.9110\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.3158\t Accuracy 0.9109\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.3148\t Accuracy 0.9113\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.3169\t Accuracy 0.9106\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.3171\t Accuracy 0.9106\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.3165\t Accuracy 0.9105\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.3163\t Accuracy 0.9106\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.3167\t Accuracy 0.9104\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.3167\t Accuracy 0.9104\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.3162\t Accuracy 0.9106\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.3156\t Accuracy 0.9109\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.3164\t Accuracy 0.9105\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.3164\t Accuracy 0.9105\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.3176\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.3180\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.3173\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.3173\t Accuracy 0.9101\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.3176\t Accuracy 0.9101\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.3171\t Accuracy 0.9103\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.3176\t Accuracy 0.9101\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.3179\t Accuracy 0.9101\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.3189\t Accuracy 0.9099\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.3180\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.3181\t Accuracy 0.9099\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.3182\t Accuracy 0.9099\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.3174\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.3171\t Accuracy 0.9102\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.3175\t Accuracy 0.9101\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.3186\t Accuracy 0.9099\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.3195\t Accuracy 0.9096\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.3194\t Accuracy 0.9097\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.3190\t Accuracy 0.9096\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.3186\t Accuracy 0.9099\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.3190\t Accuracy 0.9098\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.3198\t Accuracy 0.9095\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.3192\t Accuracy 0.9097\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.3194\t Accuracy 0.9096\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.3190\t Accuracy 0.9096\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3191\t Average training accuracy 0.9095\n",
      "Epoch [15]\t Average validation loss 0.2492\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0938\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.2941\t Accuracy 0.9059\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.3270\t Accuracy 0.8970\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.3450\t Accuracy 0.8947\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.3187\t Accuracy 0.9055\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.3053\t Accuracy 0.9104\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.2972\t Accuracy 0.9143\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.2923\t Accuracy 0.9162\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.2904\t Accuracy 0.9185\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.2905\t Accuracy 0.9191\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.2877\t Accuracy 0.9192\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.2893\t Accuracy 0.9189\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.2883\t Accuracy 0.9191\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.2846\t Accuracy 0.9206\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.2844\t Accuracy 0.9201\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.2875\t Accuracy 0.9198\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.2901\t Accuracy 0.9185\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.2937\t Accuracy 0.9176\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.3013\t Accuracy 0.9156\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.3021\t Accuracy 0.9155\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.2997\t Accuracy 0.9158\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.2979\t Accuracy 0.9161\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.2950\t Accuracy 0.9174\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.2929\t Accuracy 0.9181\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.2972\t Accuracy 0.9164\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.2984\t Accuracy 0.9160\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.3019\t Accuracy 0.9150\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.3036\t Accuracy 0.9144\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.3050\t Accuracy 0.9138\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.3081\t Accuracy 0.9128\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.3120\t Accuracy 0.9118\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.3114\t Accuracy 0.9123\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.3123\t Accuracy 0.9119\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.3112\t Accuracy 0.9121\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.3120\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.3121\t Accuracy 0.9117\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.3137\t Accuracy 0.9110\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.3118\t Accuracy 0.9118\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.3102\t Accuracy 0.9126\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.3098\t Accuracy 0.9128\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.3084\t Accuracy 0.9131\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.3076\t Accuracy 0.9134\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.3103\t Accuracy 0.9130\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.3092\t Accuracy 0.9138\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.3079\t Accuracy 0.9143\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.3077\t Accuracy 0.9143\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.3077\t Accuracy 0.9140\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.3072\t Accuracy 0.9140\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.3077\t Accuracy 0.9139\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.3073\t Accuracy 0.9138\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.3087\t Accuracy 0.9134\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.3076\t Accuracy 0.9136\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.3068\t Accuracy 0.9140\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.3068\t Accuracy 0.9142\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.3082\t Accuracy 0.9139\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.3087\t Accuracy 0.9136\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.3081\t Accuracy 0.9136\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.3076\t Accuracy 0.9139\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.3075\t Accuracy 0.9139\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.3079\t Accuracy 0.9137\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.3091\t Accuracy 0.9133\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.3093\t Accuracy 0.9130\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.3103\t Accuracy 0.9129\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.3121\t Accuracy 0.9124\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.3127\t Accuracy 0.9120\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.3142\t Accuracy 0.9114\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.3140\t Accuracy 0.9114\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.3141\t Accuracy 0.9113\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.3125\t Accuracy 0.9118\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.3118\t Accuracy 0.9122\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.3123\t Accuracy 0.9117\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.3120\t Accuracy 0.9118\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.3112\t Accuracy 0.9120\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.3109\t Accuracy 0.9120\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.3099\t Accuracy 0.9124\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.3121\t Accuracy 0.9117\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.3123\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.3117\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.3114\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.3118\t Accuracy 0.9115\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.3119\t Accuracy 0.9114\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.3113\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.3108\t Accuracy 0.9119\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.3115\t Accuracy 0.9116\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.3115\t Accuracy 0.9115\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.3128\t Accuracy 0.9111\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.3131\t Accuracy 0.9110\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.3124\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.3124\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.3127\t Accuracy 0.9111\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.3123\t Accuracy 0.9114\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.3127\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.3130\t Accuracy 0.9111\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.3140\t Accuracy 0.9109\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.3131\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.3133\t Accuracy 0.9109\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.3133\t Accuracy 0.9109\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.3125\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.3123\t Accuracy 0.9112\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.3127\t Accuracy 0.9111\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.3138\t Accuracy 0.9109\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.3147\t Accuracy 0.9107\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.3146\t Accuracy 0.9108\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.3142\t Accuracy 0.9108\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.3137\t Accuracy 0.9110\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.3142\t Accuracy 0.9109\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.3149\t Accuracy 0.9106\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.3144\t Accuracy 0.9108\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.3145\t Accuracy 0.9107\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.3142\t Accuracy 0.9107\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3143\t Average training accuracy 0.9106\n",
      "Epoch [16]\t Average validation loss 0.2455\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0890\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.2896\t Accuracy 0.9078\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.3222\t Accuracy 0.8980\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.3406\t Accuracy 0.8967\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.3143\t Accuracy 0.9075\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.3009\t Accuracy 0.9120\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.2928\t Accuracy 0.9163\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.2878\t Accuracy 0.9182\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.2859\t Accuracy 0.9202\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.2861\t Accuracy 0.9211\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.2832\t Accuracy 0.9214\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.2848\t Accuracy 0.9209\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.2839\t Accuracy 0.9213\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.2802\t Accuracy 0.9226\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.2800\t Accuracy 0.9221\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.2831\t Accuracy 0.9216\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.2857\t Accuracy 0.9202\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.2892\t Accuracy 0.9193\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.2968\t Accuracy 0.9173\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.2976\t Accuracy 0.9171\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.2952\t Accuracy 0.9173\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.2935\t Accuracy 0.9175\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.2906\t Accuracy 0.9189\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.2885\t Accuracy 0.9195\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.2928\t Accuracy 0.9177\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.2940\t Accuracy 0.9173\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.2975\t Accuracy 0.9162\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.2992\t Accuracy 0.9157\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.3005\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.3036\t Accuracy 0.9141\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.3075\t Accuracy 0.9130\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.3070\t Accuracy 0.9134\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.3078\t Accuracy 0.9130\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.3067\t Accuracy 0.9132\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.3074\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.3075\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.3091\t Accuracy 0.9123\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.3072\t Accuracy 0.9131\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.3057\t Accuracy 0.9138\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.3053\t Accuracy 0.9139\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.3038\t Accuracy 0.9142\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.3031\t Accuracy 0.9145\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.3058\t Accuracy 0.9143\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.3047\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.3034\t Accuracy 0.9155\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.3032\t Accuracy 0.9155\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.3032\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.3028\t Accuracy 0.9152\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.3033\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.3029\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.3043\t Accuracy 0.9146\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.3031\t Accuracy 0.9149\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.3024\t Accuracy 0.9152\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.3023\t Accuracy 0.9154\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.3037\t Accuracy 0.9152\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.3042\t Accuracy 0.9148\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.3037\t Accuracy 0.9149\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.3031\t Accuracy 0.9152\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.3031\t Accuracy 0.9151\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.3035\t Accuracy 0.9150\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.3046\t Accuracy 0.9146\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.3049\t Accuracy 0.9144\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.3058\t Accuracy 0.9142\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.3076\t Accuracy 0.9137\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.3082\t Accuracy 0.9134\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.3096\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.3095\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.3096\t Accuracy 0.9127\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.3079\t Accuracy 0.9132\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.3073\t Accuracy 0.9136\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.3078\t Accuracy 0.9131\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.3074\t Accuracy 0.9132\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.3067\t Accuracy 0.9134\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.3064\t Accuracy 0.9133\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.3054\t Accuracy 0.9137\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.3076\t Accuracy 0.9129\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.3078\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.3072\t Accuracy 0.9129\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.3069\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.3074\t Accuracy 0.9127\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.3074\t Accuracy 0.9127\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.3069\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.3063\t Accuracy 0.9130\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.3070\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.3070\t Accuracy 0.9128\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.3083\t Accuracy 0.9123\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.3086\t Accuracy 0.9123\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.3079\t Accuracy 0.9125\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.3079\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.3082\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.3078\t Accuracy 0.9126\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.3083\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.3086\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.3096\t Accuracy 0.9122\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.3087\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.3088\t Accuracy 0.9122\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.3089\t Accuracy 0.9122\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.3081\t Accuracy 0.9125\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.3079\t Accuracy 0.9125\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.3083\t Accuracy 0.9124\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.3093\t Accuracy 0.9122\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.3102\t Accuracy 0.9119\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.3101\t Accuracy 0.9120\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.3097\t Accuracy 0.9120\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.3093\t Accuracy 0.9122\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.3097\t Accuracy 0.9121\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.3105\t Accuracy 0.9117\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.3100\t Accuracy 0.9119\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.3101\t Accuracy 0.9118\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.3098\t Accuracy 0.9119\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3099\t Average training accuracy 0.9118\n",
      "Epoch [17]\t Average validation loss 0.2422\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0847\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.2855\t Accuracy 0.9098\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.3176\t Accuracy 0.8990\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.3366\t Accuracy 0.8974\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.3102\t Accuracy 0.9080\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.2968\t Accuracy 0.9124\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.2888\t Accuracy 0.9173\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.2836\t Accuracy 0.9194\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.2817\t Accuracy 0.9212\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.2820\t Accuracy 0.9226\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.2791\t Accuracy 0.9228\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.2807\t Accuracy 0.9223\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.2798\t Accuracy 0.9226\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.2761\t Accuracy 0.9238\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.2760\t Accuracy 0.9233\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.2790\t Accuracy 0.9226\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.2816\t Accuracy 0.9212\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.2851\t Accuracy 0.9202\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.2927\t Accuracy 0.9184\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.2935\t Accuracy 0.9182\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.2911\t Accuracy 0.9184\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.2894\t Accuracy 0.9186\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.2866\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.2844\t Accuracy 0.9207\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.2887\t Accuracy 0.9190\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.2899\t Accuracy 0.9185\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.2934\t Accuracy 0.9174\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.2951\t Accuracy 0.9169\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.2964\t Accuracy 0.9162\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.2995\t Accuracy 0.9152\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.3033\t Accuracy 0.9143\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.3028\t Accuracy 0.9147\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.3036\t Accuracy 0.9142\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.3025\t Accuracy 0.9144\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.3032\t Accuracy 0.9141\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.3033\t Accuracy 0.9141\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.3048\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.3030\t Accuracy 0.9144\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.3014\t Accuracy 0.9151\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.3011\t Accuracy 0.9152\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.2996\t Accuracy 0.9156\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.2989\t Accuracy 0.9158\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.3016\t Accuracy 0.9156\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.3006\t Accuracy 0.9163\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.2993\t Accuracy 0.9168\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.2991\t Accuracy 0.9167\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.2991\t Accuracy 0.9163\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.2986\t Accuracy 0.9165\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.2991\t Accuracy 0.9163\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.2987\t Accuracy 0.9163\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.3001\t Accuracy 0.9159\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.2990\t Accuracy 0.9161\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.2982\t Accuracy 0.9165\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.2982\t Accuracy 0.9168\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.2996\t Accuracy 0.9166\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.3001\t Accuracy 0.9162\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.2996\t Accuracy 0.9162\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.2990\t Accuracy 0.9165\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.2990\t Accuracy 0.9164\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.2993\t Accuracy 0.9163\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.3005\t Accuracy 0.9159\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.3007\t Accuracy 0.9156\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.3016\t Accuracy 0.9154\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.3034\t Accuracy 0.9150\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.3040\t Accuracy 0.9147\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.3054\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.3053\t Accuracy 0.9141\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.3054\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.3037\t Accuracy 0.9144\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.3031\t Accuracy 0.9148\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.3036\t Accuracy 0.9143\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.3032\t Accuracy 0.9144\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.3024\t Accuracy 0.9146\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.3022\t Accuracy 0.9145\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.3012\t Accuracy 0.9149\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.3034\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.3036\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.3030\t Accuracy 0.9141\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.3027\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.3032\t Accuracy 0.9139\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.3032\t Accuracy 0.9138\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.3027\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.3021\t Accuracy 0.9142\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.3029\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.3029\t Accuracy 0.9140\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.3041\t Accuracy 0.9135\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.3044\t Accuracy 0.9135\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.3038\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.3038\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.3041\t Accuracy 0.9135\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.3036\t Accuracy 0.9138\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.3041\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.3044\t Accuracy 0.9135\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.3054\t Accuracy 0.9133\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.3045\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.3047\t Accuracy 0.9133\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.3047\t Accuracy 0.9133\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.3039\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.3037\t Accuracy 0.9136\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.3041\t Accuracy 0.9135\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.3051\t Accuracy 0.9133\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.3061\t Accuracy 0.9130\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.3059\t Accuracy 0.9131\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.3055\t Accuracy 0.9131\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.3051\t Accuracy 0.9133\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.3056\t Accuracy 0.9132\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.3063\t Accuracy 0.9129\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.3058\t Accuracy 0.9130\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.3060\t Accuracy 0.9129\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.3057\t Accuracy 0.9130\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3058\t Average training accuracy 0.9129\n",
      "Epoch [18]\t Average validation loss 0.2391\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0808\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.2817\t Accuracy 0.9118\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.3134\t Accuracy 0.9020\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.3329\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.3064\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.2930\t Accuracy 0.9139\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.2850\t Accuracy 0.9186\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.2798\t Accuracy 0.9208\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.2778\t Accuracy 0.9222\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.2781\t Accuracy 0.9233\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.2752\t Accuracy 0.9238\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.2769\t Accuracy 0.9234\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.2760\t Accuracy 0.9238\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.2724\t Accuracy 0.9249\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.2722\t Accuracy 0.9244\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.2752\t Accuracy 0.9237\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.2778\t Accuracy 0.9225\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.2812\t Accuracy 0.9214\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.2888\t Accuracy 0.9195\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.2897\t Accuracy 0.9193\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.2872\t Accuracy 0.9195\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.2856\t Accuracy 0.9197\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.2828\t Accuracy 0.9210\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.2806\t Accuracy 0.9216\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.2849\t Accuracy 0.9198\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.2861\t Accuracy 0.9193\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.2896\t Accuracy 0.9182\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.2912\t Accuracy 0.9177\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.2926\t Accuracy 0.9170\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.2956\t Accuracy 0.9159\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.2994\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.2989\t Accuracy 0.9155\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.2997\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.2986\t Accuracy 0.9154\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.2993\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.2994\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.3009\t Accuracy 0.9146\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.2991\t Accuracy 0.9153\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.2975\t Accuracy 0.9160\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.2971\t Accuracy 0.9161\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.2957\t Accuracy 0.9166\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.2949\t Accuracy 0.9169\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.2977\t Accuracy 0.9166\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.2967\t Accuracy 0.9173\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.2954\t Accuracy 0.9177\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.2952\t Accuracy 0.9176\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.2952\t Accuracy 0.9173\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.2947\t Accuracy 0.9174\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.2952\t Accuracy 0.9173\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.2948\t Accuracy 0.9173\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.2962\t Accuracy 0.9169\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.2951\t Accuracy 0.9171\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.2944\t Accuracy 0.9175\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.2943\t Accuracy 0.9178\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.2958\t Accuracy 0.9175\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.2963\t Accuracy 0.9172\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.2957\t Accuracy 0.9172\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.2952\t Accuracy 0.9175\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.2951\t Accuracy 0.9174\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.2955\t Accuracy 0.9173\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.2966\t Accuracy 0.9169\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.2968\t Accuracy 0.9167\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.2977\t Accuracy 0.9165\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.2995\t Accuracy 0.9160\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.3001\t Accuracy 0.9157\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.3014\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.3013\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.3014\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.2998\t Accuracy 0.9154\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.2991\t Accuracy 0.9158\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.2996\t Accuracy 0.9153\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.2993\t Accuracy 0.9154\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.2985\t Accuracy 0.9156\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.2982\t Accuracy 0.9155\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.2973\t Accuracy 0.9158\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.2995\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.2997\t Accuracy 0.9149\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.2991\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.2988\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.2993\t Accuracy 0.9148\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.2993\t Accuracy 0.9147\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.2988\t Accuracy 0.9149\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.2982\t Accuracy 0.9151\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.2990\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.2989\t Accuracy 0.9149\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.3002\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.3005\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.2999\t Accuracy 0.9145\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.2999\t Accuracy 0.9145\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.3001\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.2997\t Accuracy 0.9146\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.3002\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.3005\t Accuracy 0.9143\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.3015\t Accuracy 0.9142\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.3007\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.3008\t Accuracy 0.9142\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.3009\t Accuracy 0.9142\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.3001\t Accuracy 0.9145\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.2998\t Accuracy 0.9145\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.3002\t Accuracy 0.9144\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.3013\t Accuracy 0.9142\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.3022\t Accuracy 0.9140\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.3021\t Accuracy 0.9140\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.3016\t Accuracy 0.9140\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.3012\t Accuracy 0.9142\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.3017\t Accuracy 0.9141\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.3025\t Accuracy 0.9138\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.3019\t Accuracy 0.9139\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.3021\t Accuracy 0.9138\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.3018\t Accuracy 0.9139\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3019\t Average training accuracy 0.9137\n",
      "Epoch [19]\t Average validation loss 0.2362\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0774\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.2782\t Accuracy 0.9118\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.3095\t Accuracy 0.9030\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.3293\t Accuracy 0.9007\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.3029\t Accuracy 0.9109\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.2894\t Accuracy 0.9147\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.2814\t Accuracy 0.9193\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.2761\t Accuracy 0.9217\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.2741\t Accuracy 0.9232\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.2745\t Accuracy 0.9242\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.2716\t Accuracy 0.9246\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.2733\t Accuracy 0.9245\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.2724\t Accuracy 0.9248\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.2689\t Accuracy 0.9260\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.2687\t Accuracy 0.9254\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.2717\t Accuracy 0.9248\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.2742\t Accuracy 0.9235\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.2776\t Accuracy 0.9223\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.2852\t Accuracy 0.9205\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.2860\t Accuracy 0.9203\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.2836\t Accuracy 0.9205\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.2820\t Accuracy 0.9206\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.2792\t Accuracy 0.9220\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.2770\t Accuracy 0.9226\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.2813\t Accuracy 0.9207\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.2825\t Accuracy 0.9201\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.2860\t Accuracy 0.9191\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.2876\t Accuracy 0.9186\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.2889\t Accuracy 0.9179\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.2919\t Accuracy 0.9168\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.2957\t Accuracy 0.9159\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.2953\t Accuracy 0.9164\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.2960\t Accuracy 0.9159\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.2949\t Accuracy 0.9164\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.2956\t Accuracy 0.9162\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.2956\t Accuracy 0.9162\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.2971\t Accuracy 0.9157\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.2954\t Accuracy 0.9164\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.2938\t Accuracy 0.9170\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.2934\t Accuracy 0.9172\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.2919\t Accuracy 0.9177\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.2912\t Accuracy 0.9180\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.2940\t Accuracy 0.9176\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.2930\t Accuracy 0.9184\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.2918\t Accuracy 0.9188\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.2916\t Accuracy 0.9187\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.2915\t Accuracy 0.9183\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.2911\t Accuracy 0.9185\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.2916\t Accuracy 0.9183\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.2912\t Accuracy 0.9183\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.2925\t Accuracy 0.9179\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.2914\t Accuracy 0.9181\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.2907\t Accuracy 0.9185\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.2907\t Accuracy 0.9188\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.2921\t Accuracy 0.9185\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.2926\t Accuracy 0.9181\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.2921\t Accuracy 0.9182\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.2916\t Accuracy 0.9184\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.2915\t Accuracy 0.9183\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.2919\t Accuracy 0.9182\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.2930\t Accuracy 0.9178\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.2931\t Accuracy 0.9177\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.2940\t Accuracy 0.9175\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.2958\t Accuracy 0.9170\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.2964\t Accuracy 0.9167\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.2977\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.2976\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.2977\t Accuracy 0.9160\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.2960\t Accuracy 0.9164\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.2954\t Accuracy 0.9167\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.2959\t Accuracy 0.9163\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.2956\t Accuracy 0.9164\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.2948\t Accuracy 0.9166\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.2945\t Accuracy 0.9165\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.2936\t Accuracy 0.9169\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.2958\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.2960\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.2954\t Accuracy 0.9162\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.2952\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.2956\t Accuracy 0.9160\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.2956\t Accuracy 0.9159\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.2951\t Accuracy 0.9160\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.2946\t Accuracy 0.9163\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.2953\t Accuracy 0.9161\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.2953\t Accuracy 0.9160\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.2965\t Accuracy 0.9155\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.2968\t Accuracy 0.9156\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.2962\t Accuracy 0.9157\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.2962\t Accuracy 0.9156\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.2965\t Accuracy 0.9155\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.2961\t Accuracy 0.9157\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.2965\t Accuracy 0.9155\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.2969\t Accuracy 0.9154\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.2979\t Accuracy 0.9152\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.2970\t Accuracy 0.9155\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.2971\t Accuracy 0.9152\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.2972\t Accuracy 0.9152\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.2964\t Accuracy 0.9156\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.2962\t Accuracy 0.9156\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.2966\t Accuracy 0.9155\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.2976\t Accuracy 0.9153\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.2985\t Accuracy 0.9150\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.2984\t Accuracy 0.9151\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.2980\t Accuracy 0.9151\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.2976\t Accuracy 0.9152\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.2980\t Accuracy 0.9151\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.2988\t Accuracy 0.9148\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.2983\t Accuracy 0.9150\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.2984\t Accuracy 0.9149\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.2981\t Accuracy 0.9150\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2982\t Average training accuracy 0.9149\n",
      "Epoch [20]\t Average validation loss 0.2334\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0743\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.2749\t Accuracy 0.9118\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.3058\t Accuracy 0.9040\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.3260\t Accuracy 0.8987\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.2995\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.2860\t Accuracy 0.9139\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.2781\t Accuracy 0.9186\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.2727\t Accuracy 0.9214\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.2707\t Accuracy 0.9232\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.2711\t Accuracy 0.9242\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.2683\t Accuracy 0.9246\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.2699\t Accuracy 0.9249\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.2690\t Accuracy 0.9251\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.2655\t Accuracy 0.9264\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.2654\t Accuracy 0.9258\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.2683\t Accuracy 0.9252\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.2708\t Accuracy 0.9238\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.2742\t Accuracy 0.9227\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.2818\t Accuracy 0.9210\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.2826\t Accuracy 0.9207\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.2802\t Accuracy 0.9209\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.2786\t Accuracy 0.9211\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.2758\t Accuracy 0.9224\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.2737\t Accuracy 0.9230\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.2779\t Accuracy 0.9214\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.2791\t Accuracy 0.9208\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.2826\t Accuracy 0.9198\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.2842\t Accuracy 0.9192\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.2855\t Accuracy 0.9187\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.2884\t Accuracy 0.9176\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.2922\t Accuracy 0.9167\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.2918\t Accuracy 0.9171\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.2925\t Accuracy 0.9166\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.2914\t Accuracy 0.9171\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.2921\t Accuracy 0.9169\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.2921\t Accuracy 0.9170\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.2936\t Accuracy 0.9165\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.2918\t Accuracy 0.9171\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.2903\t Accuracy 0.9178\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.2899\t Accuracy 0.9179\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.2884\t Accuracy 0.9185\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.2877\t Accuracy 0.9187\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.2905\t Accuracy 0.9183\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.2895\t Accuracy 0.9191\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.2883\t Accuracy 0.9194\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.2881\t Accuracy 0.9194\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.2881\t Accuracy 0.9190\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.2876\t Accuracy 0.9191\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.2881\t Accuracy 0.9189\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.2877\t Accuracy 0.9189\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.2891\t Accuracy 0.9185\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.2880\t Accuracy 0.9187\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.2873\t Accuracy 0.9191\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.2872\t Accuracy 0.9194\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.2887\t Accuracy 0.9191\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.2892\t Accuracy 0.9187\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.2886\t Accuracy 0.9188\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.2882\t Accuracy 0.9190\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.2881\t Accuracy 0.9189\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.2884\t Accuracy 0.9188\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.2895\t Accuracy 0.9184\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.2897\t Accuracy 0.9183\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.2905\t Accuracy 0.9181\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.2923\t Accuracy 0.9176\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.2929\t Accuracy 0.9174\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.2942\t Accuracy 0.9168\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.2941\t Accuracy 0.9168\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.2941\t Accuracy 0.9167\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.2925\t Accuracy 0.9171\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.2919\t Accuracy 0.9175\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.2924\t Accuracy 0.9170\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.2921\t Accuracy 0.9171\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.2913\t Accuracy 0.9173\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.2910\t Accuracy 0.9172\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.2901\t Accuracy 0.9176\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.2923\t Accuracy 0.9168\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.2925\t Accuracy 0.9168\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.2919\t Accuracy 0.9169\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.2917\t Accuracy 0.9168\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.2921\t Accuracy 0.9166\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.2921\t Accuracy 0.9166\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.2916\t Accuracy 0.9167\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.2911\t Accuracy 0.9169\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.2918\t Accuracy 0.9167\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.2918\t Accuracy 0.9167\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.2930\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.2934\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.2927\t Accuracy 0.9163\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.2927\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.2930\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.2926\t Accuracy 0.9164\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.2931\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.2934\t Accuracy 0.9161\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.2944\t Accuracy 0.9159\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.2935\t Accuracy 0.9161\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.2937\t Accuracy 0.9159\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.2937\t Accuracy 0.9159\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.2930\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.2927\t Accuracy 0.9163\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.2931\t Accuracy 0.9162\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.2941\t Accuracy 0.9160\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.2950\t Accuracy 0.9157\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.2949\t Accuracy 0.9158\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.2945\t Accuracy 0.9158\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.2941\t Accuracy 0.9160\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.2945\t Accuracy 0.9159\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.2953\t Accuracy 0.9156\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.2948\t Accuracy 0.9158\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.2950\t Accuracy 0.9157\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.2947\t Accuracy 0.9158\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2948\t Average training accuracy 0.9156\n",
      "Epoch [21]\t Average validation loss 0.2308\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0715\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.2717\t Accuracy 0.9118\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.3022\t Accuracy 0.9050\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.3228\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.2963\t Accuracy 0.9124\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.2828\t Accuracy 0.9159\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.2749\t Accuracy 0.9203\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.2695\t Accuracy 0.9228\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.2674\t Accuracy 0.9247\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.2679\t Accuracy 0.9255\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.2650\t Accuracy 0.9257\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.2667\t Accuracy 0.9260\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.2659\t Accuracy 0.9265\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.2624\t Accuracy 0.9278\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.2623\t Accuracy 0.9272\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.2652\t Accuracy 0.9265\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.2676\t Accuracy 0.9251\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.2710\t Accuracy 0.9239\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.2785\t Accuracy 0.9222\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.2794\t Accuracy 0.9220\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.2769\t Accuracy 0.9221\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.2754\t Accuracy 0.9223\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.2726\t Accuracy 0.9235\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.2705\t Accuracy 0.9241\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.2747\t Accuracy 0.9224\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.2758\t Accuracy 0.9217\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.2794\t Accuracy 0.9207\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.2809\t Accuracy 0.9202\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.2822\t Accuracy 0.9196\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.2851\t Accuracy 0.9186\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.2889\t Accuracy 0.9177\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.2885\t Accuracy 0.9180\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.2892\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.2881\t Accuracy 0.9181\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.2887\t Accuracy 0.9179\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.2888\t Accuracy 0.9179\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.2902\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.2885\t Accuracy 0.9181\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.2870\t Accuracy 0.9188\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.2865\t Accuracy 0.9190\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.2851\t Accuracy 0.9194\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.2843\t Accuracy 0.9196\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.2871\t Accuracy 0.9193\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.2862\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.2850\t Accuracy 0.9204\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.2848\t Accuracy 0.9203\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.2848\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.2844\t Accuracy 0.9201\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.2849\t Accuracy 0.9198\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.2844\t Accuracy 0.9199\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.2858\t Accuracy 0.9194\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.2847\t Accuracy 0.9196\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.2840\t Accuracy 0.9201\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.2840\t Accuracy 0.9203\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.2854\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.2859\t Accuracy 0.9197\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.2854\t Accuracy 0.9197\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.2849\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.2848\t Accuracy 0.9198\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.2851\t Accuracy 0.9197\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.2862\t Accuracy 0.9193\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.2864\t Accuracy 0.9191\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.2872\t Accuracy 0.9190\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.2890\t Accuracy 0.9185\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.2895\t Accuracy 0.9182\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.2908\t Accuracy 0.9176\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.2907\t Accuracy 0.9177\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.2908\t Accuracy 0.9176\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.2892\t Accuracy 0.9180\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.2886\t Accuracy 0.9183\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.2890\t Accuracy 0.9178\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.2887\t Accuracy 0.9179\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.2879\t Accuracy 0.9181\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.2877\t Accuracy 0.9180\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.2867\t Accuracy 0.9184\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.2890\t Accuracy 0.9176\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.2892\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.2886\t Accuracy 0.9176\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.2884\t Accuracy 0.9176\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.2888\t Accuracy 0.9174\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.2888\t Accuracy 0.9174\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.2883\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.2878\t Accuracy 0.9177\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.2885\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.2885\t Accuracy 0.9175\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.2897\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.2900\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.2894\t Accuracy 0.9171\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.2894\t Accuracy 0.9171\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.2897\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.2893\t Accuracy 0.9172\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.2897\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.2901\t Accuracy 0.9169\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.2911\t Accuracy 0.9167\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.2902\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.2904\t Accuracy 0.9168\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.2904\t Accuracy 0.9167\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.2897\t Accuracy 0.9171\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.2895\t Accuracy 0.9172\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.2898\t Accuracy 0.9171\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.2908\t Accuracy 0.9169\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.2917\t Accuracy 0.9167\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.2916\t Accuracy 0.9167\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.2912\t Accuracy 0.9168\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.2908\t Accuracy 0.9170\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.2912\t Accuracy 0.9169\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.2920\t Accuracy 0.9166\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.2915\t Accuracy 0.9168\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.2917\t Accuracy 0.9167\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.2914\t Accuracy 0.9167\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2915\t Average training accuracy 0.9166\n",
      "Epoch [22]\t Average validation loss 0.2283\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0690\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.2688\t Accuracy 0.9118\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.2989\t Accuracy 0.9050\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.3198\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.2933\t Accuracy 0.9124\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.2797\t Accuracy 0.9159\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.2719\t Accuracy 0.9203\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.2665\t Accuracy 0.9228\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.2643\t Accuracy 0.9247\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.2649\t Accuracy 0.9255\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.2620\t Accuracy 0.9259\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.2636\t Accuracy 0.9261\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.2628\t Accuracy 0.9268\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.2594\t Accuracy 0.9281\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.2593\t Accuracy 0.9275\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.2621\t Accuracy 0.9268\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.2645\t Accuracy 0.9253\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.2679\t Accuracy 0.9241\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.2754\t Accuracy 0.9224\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.2763\t Accuracy 0.9222\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.2738\t Accuracy 0.9223\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.2723\t Accuracy 0.9225\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.2696\t Accuracy 0.9237\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.2674\t Accuracy 0.9242\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.2716\t Accuracy 0.9227\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.2727\t Accuracy 0.9221\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.2763\t Accuracy 0.9212\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.2778\t Accuracy 0.9208\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.2791\t Accuracy 0.9203\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.2819\t Accuracy 0.9193\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.2857\t Accuracy 0.9184\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.2853\t Accuracy 0.9188\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.2860\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.2849\t Accuracy 0.9188\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.2855\t Accuracy 0.9186\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.2856\t Accuracy 0.9186\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.2869\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.2853\t Accuracy 0.9187\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.2838\t Accuracy 0.9194\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.2833\t Accuracy 0.9195\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.2819\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.2811\t Accuracy 0.9202\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.2839\t Accuracy 0.9198\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.2830\t Accuracy 0.9205\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.2819\t Accuracy 0.9209\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.2817\t Accuracy 0.9209\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.2816\t Accuracy 0.9206\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.2812\t Accuracy 0.9207\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.2817\t Accuracy 0.9204\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.2813\t Accuracy 0.9205\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.2826\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.2815\t Accuracy 0.9203\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.2809\t Accuracy 0.9206\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.2808\t Accuracy 0.9209\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.2823\t Accuracy 0.9206\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.2828\t Accuracy 0.9202\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.2823\t Accuracy 0.9202\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.2818\t Accuracy 0.9205\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.2817\t Accuracy 0.9204\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.2820\t Accuracy 0.9203\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.2831\t Accuracy 0.9199\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.2832\t Accuracy 0.9197\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.2840\t Accuracy 0.9195\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.2858\t Accuracy 0.9190\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.2864\t Accuracy 0.9188\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.2876\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.2875\t Accuracy 0.9183\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.2876\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.2860\t Accuracy 0.9186\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.2854\t Accuracy 0.9189\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.2858\t Accuracy 0.9184\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.2855\t Accuracy 0.9186\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.2847\t Accuracy 0.9187\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.2845\t Accuracy 0.9187\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.2835\t Accuracy 0.9190\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.2858\t Accuracy 0.9183\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.2860\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.2854\t Accuracy 0.9183\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.2852\t Accuracy 0.9183\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.2857\t Accuracy 0.9181\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.2856\t Accuracy 0.9180\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.2851\t Accuracy 0.9181\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.2846\t Accuracy 0.9184\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.2853\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.2853\t Accuracy 0.9182\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.2865\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.2869\t Accuracy 0.9178\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.2862\t Accuracy 0.9178\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.2862\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.2865\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.2861\t Accuracy 0.9179\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.2866\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.2869\t Accuracy 0.9176\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.2879\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.2871\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.2872\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.2873\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.2865\t Accuracy 0.9178\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.2863\t Accuracy 0.9179\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.2867\t Accuracy 0.9179\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.2876\t Accuracy 0.9176\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.2886\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.2884\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.2880\t Accuracy 0.9175\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.2876\t Accuracy 0.9177\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.2880\t Accuracy 0.9176\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.2888\t Accuracy 0.9173\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.2883\t Accuracy 0.9175\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.2885\t Accuracy 0.9174\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.2882\t Accuracy 0.9175\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2883\t Average training accuracy 0.9173\n",
      "Epoch [23]\t Average validation loss 0.2260\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0668\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.2660\t Accuracy 0.9137\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.2956\t Accuracy 0.9059\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.3169\t Accuracy 0.9007\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.2903\t Accuracy 0.9129\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.2768\t Accuracy 0.9163\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.2690\t Accuracy 0.9209\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.2635\t Accuracy 0.9234\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.2613\t Accuracy 0.9252\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.2620\t Accuracy 0.9262\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.2591\t Accuracy 0.9265\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.2607\t Accuracy 0.9270\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.2599\t Accuracy 0.9278\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.2565\t Accuracy 0.9290\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.2564\t Accuracy 0.9284\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.2593\t Accuracy 0.9278\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.2616\t Accuracy 0.9263\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.2650\t Accuracy 0.9250\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.2724\t Accuracy 0.9233\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.2733\t Accuracy 0.9230\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.2708\t Accuracy 0.9231\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.2693\t Accuracy 0.9233\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.2667\t Accuracy 0.9245\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.2644\t Accuracy 0.9250\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.2686\t Accuracy 0.9236\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.2697\t Accuracy 0.9230\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.2733\t Accuracy 0.9221\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.2748\t Accuracy 0.9217\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.2760\t Accuracy 0.9213\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.2789\t Accuracy 0.9202\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.2826\t Accuracy 0.9193\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.2823\t Accuracy 0.9196\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.2829\t Accuracy 0.9191\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.2818\t Accuracy 0.9196\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.2824\t Accuracy 0.9194\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.2825\t Accuracy 0.9194\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.2838\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.2822\t Accuracy 0.9194\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.2807\t Accuracy 0.9201\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.2802\t Accuracy 0.9202\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.2788\t Accuracy 0.9207\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.2780\t Accuracy 0.9209\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.2809\t Accuracy 0.9205\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.2800\t Accuracy 0.9212\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.2788\t Accuracy 0.9215\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.2786\t Accuracy 0.9215\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.2786\t Accuracy 0.9212\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.2782\t Accuracy 0.9214\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.2787\t Accuracy 0.9211\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.2782\t Accuracy 0.9211\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.2796\t Accuracy 0.9207\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.2785\t Accuracy 0.9209\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.2779\t Accuracy 0.9213\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.2778\t Accuracy 0.9215\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.2793\t Accuracy 0.9212\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.2798\t Accuracy 0.9208\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.2793\t Accuracy 0.9208\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.2788\t Accuracy 0.9212\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.2787\t Accuracy 0.9211\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.2790\t Accuracy 0.9209\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.2801\t Accuracy 0.9205\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.2802\t Accuracy 0.9203\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.2809\t Accuracy 0.9202\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.2828\t Accuracy 0.9197\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.2833\t Accuracy 0.9194\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.2845\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.2844\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.2845\t Accuracy 0.9188\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.2829\t Accuracy 0.9192\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.2823\t Accuracy 0.9195\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.2827\t Accuracy 0.9191\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.2824\t Accuracy 0.9192\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.2816\t Accuracy 0.9193\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.2814\t Accuracy 0.9193\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.2805\t Accuracy 0.9197\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.2827\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.2830\t Accuracy 0.9188\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.2824\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.2821\t Accuracy 0.9189\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.2826\t Accuracy 0.9187\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.2825\t Accuracy 0.9186\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.2820\t Accuracy 0.9188\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.2815\t Accuracy 0.9190\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.2823\t Accuracy 0.9188\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.2822\t Accuracy 0.9188\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.2834\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.2838\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.2831\t Accuracy 0.9184\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.2832\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.2834\t Accuracy 0.9182\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.2830\t Accuracy 0.9184\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.2835\t Accuracy 0.9182\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.2839\t Accuracy 0.9182\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.2849\t Accuracy 0.9180\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.2840\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.2842\t Accuracy 0.9181\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.2842\t Accuracy 0.9181\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.2835\t Accuracy 0.9184\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.2833\t Accuracy 0.9185\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.2837\t Accuracy 0.9185\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.2846\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.2855\t Accuracy 0.9181\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.2854\t Accuracy 0.9181\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.2850\t Accuracy 0.9182\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.2846\t Accuracy 0.9183\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.2850\t Accuracy 0.9182\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.2858\t Accuracy 0.9179\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.2853\t Accuracy 0.9181\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.2855\t Accuracy 0.9180\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.2852\t Accuracy 0.9181\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2853\t Average training accuracy 0.9180\n",
      "Epoch [24]\t Average validation loss 0.2237\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0647\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.2633\t Accuracy 0.9176\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.2925\t Accuracy 0.9079\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.3141\t Accuracy 0.9026\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.2875\t Accuracy 0.9149\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.2740\t Accuracy 0.9183\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.2663\t Accuracy 0.9223\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.2607\t Accuracy 0.9248\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.2585\t Accuracy 0.9264\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.2592\t Accuracy 0.9273\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.2563\t Accuracy 0.9275\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.2579\t Accuracy 0.9279\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.2571\t Accuracy 0.9286\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.2537\t Accuracy 0.9298\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.2537\t Accuracy 0.9291\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.2565\t Accuracy 0.9285\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.2588\t Accuracy 0.9270\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.2621\t Accuracy 0.9259\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.2695\t Accuracy 0.9242\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.2704\t Accuracy 0.9239\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.2680\t Accuracy 0.9239\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.2665\t Accuracy 0.9241\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.2638\t Accuracy 0.9252\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.2616\t Accuracy 0.9257\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.2658\t Accuracy 0.9243\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.2669\t Accuracy 0.9237\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.2704\t Accuracy 0.9228\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.2719\t Accuracy 0.9224\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.2731\t Accuracy 0.9220\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.2760\t Accuracy 0.9209\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.2796\t Accuracy 0.9199\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.2793\t Accuracy 0.9202\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.2799\t Accuracy 0.9197\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.2789\t Accuracy 0.9202\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.2794\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.2795\t Accuracy 0.9201\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.2808\t Accuracy 0.9197\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.2792\t Accuracy 0.9202\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.2777\t Accuracy 0.9209\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.2773\t Accuracy 0.9209\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.2758\t Accuracy 0.9213\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.2751\t Accuracy 0.9215\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.2779\t Accuracy 0.9211\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.2770\t Accuracy 0.9218\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.2759\t Accuracy 0.9221\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.2757\t Accuracy 0.9221\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.2757\t Accuracy 0.9219\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.2753\t Accuracy 0.9220\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.2758\t Accuracy 0.9217\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.2753\t Accuracy 0.9217\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.2767\t Accuracy 0.9214\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.2756\t Accuracy 0.9216\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.2750\t Accuracy 0.9219\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.2749\t Accuracy 0.9223\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.2764\t Accuracy 0.9220\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.2769\t Accuracy 0.9216\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.2764\t Accuracy 0.9216\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.2759\t Accuracy 0.9220\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.2758\t Accuracy 0.9218\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.2761\t Accuracy 0.9216\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.2772\t Accuracy 0.9212\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.2772\t Accuracy 0.9211\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.2780\t Accuracy 0.9210\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.2798\t Accuracy 0.9205\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.2803\t Accuracy 0.9203\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.2815\t Accuracy 0.9198\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.2815\t Accuracy 0.9198\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.2815\t Accuracy 0.9197\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.2799\t Accuracy 0.9201\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.2793\t Accuracy 0.9205\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.2797\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.2795\t Accuracy 0.9201\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.2786\t Accuracy 0.9203\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.2784\t Accuracy 0.9203\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.2775\t Accuracy 0.9207\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.2798\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.2800\t Accuracy 0.9199\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.2794\t Accuracy 0.9199\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.2792\t Accuracy 0.9199\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.2797\t Accuracy 0.9197\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.2796\t Accuracy 0.9196\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.2791\t Accuracy 0.9197\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.2786\t Accuracy 0.9199\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.2793\t Accuracy 0.9198\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.2793\t Accuracy 0.9198\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.2805\t Accuracy 0.9193\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.2809\t Accuracy 0.9193\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.2802\t Accuracy 0.9194\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.2802\t Accuracy 0.9193\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.2805\t Accuracy 0.9192\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.2801\t Accuracy 0.9194\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.2806\t Accuracy 0.9192\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.2809\t Accuracy 0.9191\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.2820\t Accuracy 0.9189\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.2811\t Accuracy 0.9192\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.2812\t Accuracy 0.9190\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.2813\t Accuracy 0.9190\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.2805\t Accuracy 0.9194\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.2804\t Accuracy 0.9195\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.2807\t Accuracy 0.9195\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.2817\t Accuracy 0.9193\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.2826\t Accuracy 0.9190\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.2825\t Accuracy 0.9191\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.2820\t Accuracy 0.9191\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.2816\t Accuracy 0.9193\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.2821\t Accuracy 0.9192\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.2829\t Accuracy 0.9189\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.2824\t Accuracy 0.9191\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.2825\t Accuracy 0.9190\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.2822\t Accuracy 0.9191\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2824\t Average training accuracy 0.9190\n",
      "Epoch [25]\t Average validation loss 0.2215\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0628\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.2608\t Accuracy 0.9176\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.2895\t Accuracy 0.9079\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.3114\t Accuracy 0.9040\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.2848\t Accuracy 0.9159\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.2713\t Accuracy 0.9191\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.2636\t Accuracy 0.9229\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.2580\t Accuracy 0.9256\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.2557\t Accuracy 0.9274\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.2565\t Accuracy 0.9282\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.2536\t Accuracy 0.9285\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.2552\t Accuracy 0.9292\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.2544\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.2511\t Accuracy 0.9310\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.2511\t Accuracy 0.9304\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.2538\t Accuracy 0.9297\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.2561\t Accuracy 0.9281\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.2594\t Accuracy 0.9268\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.2667\t Accuracy 0.9251\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.2676\t Accuracy 0.9247\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.2652\t Accuracy 0.9248\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.2637\t Accuracy 0.9250\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.2611\t Accuracy 0.9262\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.2589\t Accuracy 0.9266\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.2630\t Accuracy 0.9252\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.2641\t Accuracy 0.9245\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.2677\t Accuracy 0.9237\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.2691\t Accuracy 0.9232\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.2703\t Accuracy 0.9228\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.2731\t Accuracy 0.9217\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.2768\t Accuracy 0.9207\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.2765\t Accuracy 0.9211\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.2771\t Accuracy 0.9206\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.2760\t Accuracy 0.9212\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.2766\t Accuracy 0.9210\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.2766\t Accuracy 0.9210\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.2779\t Accuracy 0.9207\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.2764\t Accuracy 0.9211\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.2749\t Accuracy 0.9218\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.2744\t Accuracy 0.9218\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.2730\t Accuracy 0.9222\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.2722\t Accuracy 0.9224\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.2751\t Accuracy 0.9220\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.2742\t Accuracy 0.9226\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.2731\t Accuracy 0.9230\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.2729\t Accuracy 0.9230\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.2729\t Accuracy 0.9228\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.2725\t Accuracy 0.9229\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.2730\t Accuracy 0.9227\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.2725\t Accuracy 0.9226\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.2738\t Accuracy 0.9222\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.2728\t Accuracy 0.9225\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.2722\t Accuracy 0.9228\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.2721\t Accuracy 0.9231\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.2735\t Accuracy 0.9229\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.2741\t Accuracy 0.9226\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.2736\t Accuracy 0.9225\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.2731\t Accuracy 0.9228\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.2730\t Accuracy 0.9227\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.2733\t Accuracy 0.9225\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.2743\t Accuracy 0.9222\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.2744\t Accuracy 0.9221\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.2751\t Accuracy 0.9220\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.2770\t Accuracy 0.9215\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.2775\t Accuracy 0.9212\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.2787\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.2786\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.2786\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.2770\t Accuracy 0.9212\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.2764\t Accuracy 0.9215\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.2769\t Accuracy 0.9211\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.2766\t Accuracy 0.9212\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.2758\t Accuracy 0.9214\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.2756\t Accuracy 0.9214\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.2747\t Accuracy 0.9218\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.2769\t Accuracy 0.9210\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.2772\t Accuracy 0.9209\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.2766\t Accuracy 0.9210\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.2763\t Accuracy 0.9209\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.2768\t Accuracy 0.9207\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.2767\t Accuracy 0.9206\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.2762\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.2757\t Accuracy 0.9210\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.2765\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.2764\t Accuracy 0.9208\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.2776\t Accuracy 0.9203\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.2780\t Accuracy 0.9204\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.2774\t Accuracy 0.9204\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.2774\t Accuracy 0.9203\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.2776\t Accuracy 0.9202\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.2773\t Accuracy 0.9204\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.2777\t Accuracy 0.9202\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.2781\t Accuracy 0.9202\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.2791\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.2783\t Accuracy 0.9203\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.2784\t Accuracy 0.9201\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.2785\t Accuracy 0.9201\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.2777\t Accuracy 0.9204\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.2775\t Accuracy 0.9205\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.2779\t Accuracy 0.9205\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.2788\t Accuracy 0.9203\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.2798\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.2796\t Accuracy 0.9201\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.2792\t Accuracy 0.9201\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.2788\t Accuracy 0.9203\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.2792\t Accuracy 0.9202\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.2800\t Accuracy 0.9199\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.2795\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.2797\t Accuracy 0.9199\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.2794\t Accuracy 0.9201\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2795\t Average training accuracy 0.9200\n",
      "Epoch [26]\t Average validation loss 0.2194\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0611\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.2584\t Accuracy 0.9196\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.2866\t Accuracy 0.9089\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.3087\t Accuracy 0.9053\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.2822\t Accuracy 0.9169\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.2687\t Accuracy 0.9199\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.2611\t Accuracy 0.9239\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.2554\t Accuracy 0.9265\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.2531\t Accuracy 0.9282\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.2539\t Accuracy 0.9288\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.2510\t Accuracy 0.9291\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.2526\t Accuracy 0.9296\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.2518\t Accuracy 0.9303\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.2485\t Accuracy 0.9313\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.2485\t Accuracy 0.9307\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.2512\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.2534\t Accuracy 0.9283\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.2567\t Accuracy 0.9270\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.2640\t Accuracy 0.9253\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.2649\t Accuracy 0.9249\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.2625\t Accuracy 0.9251\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.2610\t Accuracy 0.9252\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.2585\t Accuracy 0.9263\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.2562\t Accuracy 0.9268\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.2604\t Accuracy 0.9254\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.2614\t Accuracy 0.9248\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.2650\t Accuracy 0.9240\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.2664\t Accuracy 0.9236\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.2676\t Accuracy 0.9232\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.2703\t Accuracy 0.9221\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.2740\t Accuracy 0.9211\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.2737\t Accuracy 0.9215\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.2743\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.2732\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.2738\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.2739\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.2751\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.2736\t Accuracy 0.9215\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.2721\t Accuracy 0.9221\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.2716\t Accuracy 0.9223\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.2702\t Accuracy 0.9227\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.2694\t Accuracy 0.9228\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.2723\t Accuracy 0.9224\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.2714\t Accuracy 0.9231\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.2704\t Accuracy 0.9234\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.2702\t Accuracy 0.9235\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.2701\t Accuracy 0.9233\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.2697\t Accuracy 0.9234\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.2702\t Accuracy 0.9231\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.2698\t Accuracy 0.9231\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.2711\t Accuracy 0.9228\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.2700\t Accuracy 0.9230\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.2694\t Accuracy 0.9233\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.2694\t Accuracy 0.9236\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.2708\t Accuracy 0.9234\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.2714\t Accuracy 0.9230\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.2709\t Accuracy 0.9230\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.2704\t Accuracy 0.9233\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.2703\t Accuracy 0.9232\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.2706\t Accuracy 0.9230\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.2716\t Accuracy 0.9228\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.2717\t Accuracy 0.9227\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.2724\t Accuracy 0.9225\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.2742\t Accuracy 0.9221\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.2747\t Accuracy 0.9218\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.2759\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.2758\t Accuracy 0.9215\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.2758\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.2742\t Accuracy 0.9218\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.2737\t Accuracy 0.9221\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.2741\t Accuracy 0.9217\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.2738\t Accuracy 0.9218\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.2730\t Accuracy 0.9220\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.2728\t Accuracy 0.9220\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.2719\t Accuracy 0.9224\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.2742\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.2744\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.2738\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.2736\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.2741\t Accuracy 0.9213\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.2740\t Accuracy 0.9213\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.2735\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.2730\t Accuracy 0.9216\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.2737\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.2737\t Accuracy 0.9214\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.2749\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.2752\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.2746\t Accuracy 0.9211\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.2746\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.2749\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.2745\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.2750\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.2753\t Accuracy 0.9208\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.2764\t Accuracy 0.9206\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.2755\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.2756\t Accuracy 0.9207\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.2757\t Accuracy 0.9207\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.2750\t Accuracy 0.9210\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.2748\t Accuracy 0.9211\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.2752\t Accuracy 0.9211\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.2761\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.2770\t Accuracy 0.9206\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.2769\t Accuracy 0.9207\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.2764\t Accuracy 0.9207\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.2761\t Accuracy 0.9209\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.2765\t Accuracy 0.9208\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.2773\t Accuracy 0.9205\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.2768\t Accuracy 0.9207\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.2770\t Accuracy 0.9205\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.2767\t Accuracy 0.9207\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2768\t Average training accuracy 0.9206\n",
      "Epoch [27]\t Average validation loss 0.2173\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0596\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.2560\t Accuracy 0.9196\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.2838\t Accuracy 0.9099\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.3061\t Accuracy 0.9060\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.2796\t Accuracy 0.9179\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.2662\t Accuracy 0.9207\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.2586\t Accuracy 0.9246\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.2529\t Accuracy 0.9271\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.2505\t Accuracy 0.9289\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.2513\t Accuracy 0.9297\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.2485\t Accuracy 0.9299\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.2500\t Accuracy 0.9305\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.2493\t Accuracy 0.9309\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.2460\t Accuracy 0.9320\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.2460\t Accuracy 0.9314\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.2487\t Accuracy 0.9306\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.2509\t Accuracy 0.9291\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.2542\t Accuracy 0.9277\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.2614\t Accuracy 0.9260\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.2623\t Accuracy 0.9256\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.2599\t Accuracy 0.9258\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.2584\t Accuracy 0.9259\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.2559\t Accuracy 0.9270\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.2537\t Accuracy 0.9274\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.2578\t Accuracy 0.9262\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.2588\t Accuracy 0.9257\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.2623\t Accuracy 0.9249\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.2638\t Accuracy 0.9246\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.2649\t Accuracy 0.9243\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.2677\t Accuracy 0.9232\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.2713\t Accuracy 0.9223\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.2710\t Accuracy 0.9226\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.2716\t Accuracy 0.9222\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.2705\t Accuracy 0.9227\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.2711\t Accuracy 0.9225\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.2711\t Accuracy 0.9224\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.2724\t Accuracy 0.9220\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.2709\t Accuracy 0.9224\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.2694\t Accuracy 0.9230\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.2689\t Accuracy 0.9232\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.2675\t Accuracy 0.9235\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.2667\t Accuracy 0.9237\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.2696\t Accuracy 0.9233\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.2688\t Accuracy 0.9240\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.2677\t Accuracy 0.9243\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.2675\t Accuracy 0.9243\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.2675\t Accuracy 0.9241\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.2671\t Accuracy 0.9242\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.2676\t Accuracy 0.9239\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.2671\t Accuracy 0.9239\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.2684\t Accuracy 0.9237\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.2674\t Accuracy 0.9239\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.2668\t Accuracy 0.9241\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.2668\t Accuracy 0.9244\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.2682\t Accuracy 0.9241\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.2687\t Accuracy 0.9238\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.2682\t Accuracy 0.9238\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.2678\t Accuracy 0.9241\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.2677\t Accuracy 0.9240\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.2679\t Accuracy 0.9239\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.2690\t Accuracy 0.9236\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.2690\t Accuracy 0.9236\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.2697\t Accuracy 0.9234\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.2715\t Accuracy 0.9229\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.2720\t Accuracy 0.9227\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.2732\t Accuracy 0.9223\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.2731\t Accuracy 0.9224\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.2731\t Accuracy 0.9223\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.2715\t Accuracy 0.9227\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.2710\t Accuracy 0.9230\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.2714\t Accuracy 0.9226\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.2711\t Accuracy 0.9227\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.2703\t Accuracy 0.9229\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.2701\t Accuracy 0.9229\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.2692\t Accuracy 0.9233\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.2715\t Accuracy 0.9225\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.2717\t Accuracy 0.9225\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.2711\t Accuracy 0.9226\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.2709\t Accuracy 0.9225\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.2714\t Accuracy 0.9222\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.2713\t Accuracy 0.9222\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.2708\t Accuracy 0.9223\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.2703\t Accuracy 0.9225\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.2711\t Accuracy 0.9224\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.2710\t Accuracy 0.9223\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.2722\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.2725\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.2719\t Accuracy 0.9220\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.2720\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.2722\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.2718\t Accuracy 0.9220\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.2723\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.2726\t Accuracy 0.9218\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.2737\t Accuracy 0.9216\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.2728\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.2730\t Accuracy 0.9217\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.2731\t Accuracy 0.9217\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.2723\t Accuracy 0.9220\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.2721\t Accuracy 0.9221\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.2725\t Accuracy 0.9220\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.2734\t Accuracy 0.9219\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.2743\t Accuracy 0.9216\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.2742\t Accuracy 0.9216\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.2737\t Accuracy 0.9216\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.2734\t Accuracy 0.9218\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.2738\t Accuracy 0.9217\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.2746\t Accuracy 0.9214\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.2741\t Accuracy 0.9216\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.2743\t Accuracy 0.9214\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.2740\t Accuracy 0.9216\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2741\t Average training accuracy 0.9216\n",
      "Epoch [28]\t Average validation loss 0.2153\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0582\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.2537\t Accuracy 0.9235\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.2811\t Accuracy 0.9119\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.3036\t Accuracy 0.9073\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.2772\t Accuracy 0.9189\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.2638\t Accuracy 0.9215\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.2562\t Accuracy 0.9252\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.2505\t Accuracy 0.9276\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.2480\t Accuracy 0.9294\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.2489\t Accuracy 0.9302\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.2460\t Accuracy 0.9303\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.2476\t Accuracy 0.9309\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.2469\t Accuracy 0.9313\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.2436\t Accuracy 0.9323\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.2436\t Accuracy 0.9317\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.2463\t Accuracy 0.9310\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.2484\t Accuracy 0.9296\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.2516\t Accuracy 0.9283\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.2589\t Accuracy 0.9266\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.2598\t Accuracy 0.9262\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.2574\t Accuracy 0.9264\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.2559\t Accuracy 0.9265\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.2534\t Accuracy 0.9276\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.2511\t Accuracy 0.9281\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.2553\t Accuracy 0.9270\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.2562\t Accuracy 0.9265\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.2598\t Accuracy 0.9257\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.2612\t Accuracy 0.9254\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.2624\t Accuracy 0.9251\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.2650\t Accuracy 0.9239\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.2686\t Accuracy 0.9230\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.2684\t Accuracy 0.9234\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.2690\t Accuracy 0.9229\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.2679\t Accuracy 0.9234\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.2684\t Accuracy 0.9232\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.2685\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.2697\t Accuracy 0.9227\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.2682\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.2668\t Accuracy 0.9238\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.2663\t Accuracy 0.9239\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.2649\t Accuracy 0.9242\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.2641\t Accuracy 0.9244\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.2670\t Accuracy 0.9241\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.2661\t Accuracy 0.9247\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.2651\t Accuracy 0.9251\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.2649\t Accuracy 0.9251\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.2649\t Accuracy 0.9249\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.2645\t Accuracy 0.9249\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.2650\t Accuracy 0.9247\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.2645\t Accuracy 0.9247\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.2658\t Accuracy 0.9245\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.2648\t Accuracy 0.9247\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.2642\t Accuracy 0.9249\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.2642\t Accuracy 0.9251\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.2656\t Accuracy 0.9249\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.2662\t Accuracy 0.9245\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.2656\t Accuracy 0.9245\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.2652\t Accuracy 0.9249\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.2651\t Accuracy 0.9248\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.2653\t Accuracy 0.9246\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.2664\t Accuracy 0.9244\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.2664\t Accuracy 0.9243\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.2671\t Accuracy 0.9242\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.2689\t Accuracy 0.9237\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.2694\t Accuracy 0.9235\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.2705\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.2705\t Accuracy 0.9232\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.2705\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.2689\t Accuracy 0.9234\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.2683\t Accuracy 0.9238\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.2687\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.2685\t Accuracy 0.9234\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.2677\t Accuracy 0.9236\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.2675\t Accuracy 0.9236\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.2666\t Accuracy 0.9240\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.2689\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.2691\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.2685\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.2683\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.2688\t Accuracy 0.9230\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.2687\t Accuracy 0.9230\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.2682\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.2677\t Accuracy 0.9233\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.2684\t Accuracy 0.9231\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.2684\t Accuracy 0.9230\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.2695\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.2699\t Accuracy 0.9227\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.2693\t Accuracy 0.9228\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.2693\t Accuracy 0.9227\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.2695\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.2692\t Accuracy 0.9228\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.2697\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.2700\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.2711\t Accuracy 0.9223\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.2702\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.2703\t Accuracy 0.9225\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.2704\t Accuracy 0.9225\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.2697\t Accuracy 0.9228\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.2695\t Accuracy 0.9229\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.2699\t Accuracy 0.9228\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.2708\t Accuracy 0.9227\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.2717\t Accuracy 0.9224\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.2716\t Accuracy 0.9224\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.2711\t Accuracy 0.9224\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.2708\t Accuracy 0.9226\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.2712\t Accuracy 0.9225\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.2720\t Accuracy 0.9222\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.2715\t Accuracy 0.9223\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.2717\t Accuracy 0.9222\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.2714\t Accuracy 0.9224\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2715\t Average training accuracy 0.9223\n",
      "Epoch [29]\t Average validation loss 0.2134\t Average validation accuracy 0.9404\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9267.\n",
      "\n",
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 2.6201\t Accuracy 0.0000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 1.9902\t Accuracy 0.3333\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 1.5287\t Accuracy 0.4990\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 1.2514\t Accuracy 0.5934\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 1.0699\t Accuracy 0.6547\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 0.9490\t Accuracy 0.6988\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 0.8600\t Accuracy 0.7276\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.7998\t Accuracy 0.7501\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.7541\t Accuracy 0.7658\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.7170\t Accuracy 0.7785\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.6833\t Accuracy 0.7898\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.6574\t Accuracy 0.7987\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.6326\t Accuracy 0.8072\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.6083\t Accuracy 0.8143\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.5894\t Accuracy 0.8194\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.5769\t Accuracy 0.8236\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.5624\t Accuracy 0.8273\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.5497\t Accuracy 0.8318\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.5417\t Accuracy 0.8343\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.5308\t Accuracy 0.8377\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.5168\t Accuracy 0.8423\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.5094\t Accuracy 0.8445\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.5043\t Accuracy 0.8460\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.4926\t Accuracy 0.8492\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.4868\t Accuracy 0.8505\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.4813\t Accuracy 0.8523\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.4770\t Accuracy 0.8541\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.4735\t Accuracy 0.8553\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.4668\t Accuracy 0.8572\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.4607\t Accuracy 0.8589\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.4566\t Accuracy 0.8598\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.4508\t Accuracy 0.8616\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.4460\t Accuracy 0.8630\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.4393\t Accuracy 0.8654\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.4341\t Accuracy 0.8669\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.4277\t Accuracy 0.8690\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.4232\t Accuracy 0.8704\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.4168\t Accuracy 0.8725\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.4105\t Accuracy 0.8746\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.4051\t Accuracy 0.8763\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.3994\t Accuracy 0.8783\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.3947\t Accuracy 0.8797\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.3925\t Accuracy 0.8806\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.3884\t Accuracy 0.8819\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.3835\t Accuracy 0.8834\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.3797\t Accuracy 0.8845\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.3761\t Accuracy 0.8854\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.3729\t Accuracy 0.8864\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.3702\t Accuracy 0.8872\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.3666\t Accuracy 0.8883\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.3639\t Accuracy 0.8890\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.3594\t Accuracy 0.8903\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.3564\t Accuracy 0.8915\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.3532\t Accuracy 0.8926\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.3523\t Accuracy 0.8931\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.3496\t Accuracy 0.8939\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.3466\t Accuracy 0.8948\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.3438\t Accuracy 0.8957\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.3419\t Accuracy 0.8965\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.3395\t Accuracy 0.8973\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.3374\t Accuracy 0.8979\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.3349\t Accuracy 0.8987\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.3330\t Accuracy 0.8993\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.3319\t Accuracy 0.8998\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.3303\t Accuracy 0.9004\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.3287\t Accuracy 0.9007\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.3263\t Accuracy 0.9015\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.3237\t Accuracy 0.9022\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.3202\t Accuracy 0.9034\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.3180\t Accuracy 0.9041\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.3166\t Accuracy 0.9045\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.3145\t Accuracy 0.9050\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.3120\t Accuracy 0.9057\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.3102\t Accuracy 0.9062\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.3076\t Accuracy 0.9070\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.3077\t Accuracy 0.9073\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.3062\t Accuracy 0.9077\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.3041\t Accuracy 0.9083\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.3020\t Accuracy 0.9089\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.3009\t Accuracy 0.9093\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.2999\t Accuracy 0.9096\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.2980\t Accuracy 0.9102\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.2964\t Accuracy 0.9107\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.2953\t Accuracy 0.9110\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.2938\t Accuracy 0.9114\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.2931\t Accuracy 0.9116\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.2922\t Accuracy 0.9119\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.2902\t Accuracy 0.9125\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.2889\t Accuracy 0.9129\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.2877\t Accuracy 0.9133\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.2860\t Accuracy 0.9138\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.2849\t Accuracy 0.9141\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.2838\t Accuracy 0.9143\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.2831\t Accuracy 0.9145\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.2814\t Accuracy 0.9151\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.2806\t Accuracy 0.9153\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.2798\t Accuracy 0.9155\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.2784\t Accuracy 0.9159\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.2774\t Accuracy 0.9162\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.2762\t Accuracy 0.9165\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.2759\t Accuracy 0.9167\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.2753\t Accuracy 0.9170\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.2744\t Accuracy 0.9173\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.2729\t Accuracy 0.9178\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.2716\t Accuracy 0.9182\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.2704\t Accuracy 0.9186\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.2697\t Accuracy 0.9188\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.2683\t Accuracy 0.9191\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.2675\t Accuracy 0.9193\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.2660\t Accuracy 0.9198\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2649\t Average training accuracy 0.9201\n",
      "Epoch [0]\t Average validation loss 0.1199\t Average validation accuracy 0.9638\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.0217\t Accuracy 1.0000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.1467\t Accuracy 0.9569\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.1356\t Accuracy 0.9604\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.1531\t Accuracy 0.9570\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.1401\t Accuracy 0.9607\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.1323\t Accuracy 0.9625\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.1282\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.1203\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.1196\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.1212\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.1207\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.1194\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.1197\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.1171\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.1172\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.1193\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.1197\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.1221\t Accuracy 0.9632\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.1262\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.1273\t Accuracy 0.9616\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.1253\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.1268\t Accuracy 0.9618\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.1261\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.1243\t Accuracy 0.9628\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.1262\t Accuracy 0.9625\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.1251\t Accuracy 0.9631\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.1287\t Accuracy 0.9626\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.1306\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.1303\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.1306\t Accuracy 0.9619\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.1311\t Accuracy 0.9616\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.1299\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.1308\t Accuracy 0.9618\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.1294\t Accuracy 0.9622\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.1293\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.1298\t Accuracy 0.9620\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.1298\t Accuracy 0.9621\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.1287\t Accuracy 0.9623\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.1269\t Accuracy 0.9629\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.1266\t Accuracy 0.9627\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.1251\t Accuracy 0.9632\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.1245\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.1247\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.1238\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.1233\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.1228\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.1222\t Accuracy 0.9636\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.1218\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.1220\t Accuracy 0.9637\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.1212\t Accuracy 0.9639\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.1208\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.1197\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.1196\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.1193\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.1201\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.1202\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.1202\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.1199\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.1199\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.1196\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.1189\t Accuracy 0.9649\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.1184\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.1180\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.1185\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.1191\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.1191\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.1187\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.1181\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.1174\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.1173\t Accuracy 0.9659\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.1173\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.1176\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.1169\t Accuracy 0.9659\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.1168\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.1162\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.1174\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.1173\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.1165\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.1163\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.1166\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.1164\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.1161\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.1159\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.1163\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.1161\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.1165\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.1166\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.1161\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.1159\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.1159\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.1155\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.1153\t Accuracy 0.9666\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.1152\t Accuracy 0.9666\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.1158\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.1154\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.1158\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.1159\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.1154\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.1156\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.1154\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.1157\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.1158\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.1155\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.1152\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.1150\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.1148\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.1150\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.1146\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.1145\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.1141\t Accuracy 0.9665\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1138\t Average training accuracy 0.9666\n",
      "Epoch [1]\t Average validation loss 0.1060\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.0202\t Accuracy 1.0000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.1090\t Accuracy 0.9627\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.1003\t Accuracy 0.9683\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.1089\t Accuracy 0.9649\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.1013\t Accuracy 0.9692\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.0927\t Accuracy 0.9717\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.0906\t Accuracy 0.9738\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.0842\t Accuracy 0.9755\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.0843\t Accuracy 0.9746\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.0851\t Accuracy 0.9741\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.0831\t Accuracy 0.9745\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.0834\t Accuracy 0.9748\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.0827\t Accuracy 0.9749\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.0808\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.0802\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.0797\t Accuracy 0.9760\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.0802\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.0819\t Accuracy 0.9749\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.0850\t Accuracy 0.9738\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.0842\t Accuracy 0.9740\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.0828\t Accuracy 0.9742\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.0832\t Accuracy 0.9744\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.0820\t Accuracy 0.9749\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.0808\t Accuracy 0.9755\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.0816\t Accuracy 0.9753\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.0803\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.0816\t Accuracy 0.9754\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.0816\t Accuracy 0.9754\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.0813\t Accuracy 0.9754\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.0807\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.0806\t Accuracy 0.9759\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.0803\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.0808\t Accuracy 0.9754\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.0803\t Accuracy 0.9756\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.0805\t Accuracy 0.9752\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.0811\t Accuracy 0.9750\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.0816\t Accuracy 0.9748\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.0810\t Accuracy 0.9749\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.0800\t Accuracy 0.9753\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.0799\t Accuracy 0.9752\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.0787\t Accuracy 0.9755\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.0783\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.0785\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.0777\t Accuracy 0.9760\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.0774\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.0770\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.0766\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.0764\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.0765\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.0759\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.0756\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.0752\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.0753\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.0754\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.0765\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.0766\t Accuracy 0.9759\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.0767\t Accuracy 0.9760\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.0766\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.0766\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.0767\t Accuracy 0.9760\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.0761\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.0759\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.0756\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.0763\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.0769\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.0772\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.0772\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.0769\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.0765\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.0766\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.0770\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.0774\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.0772\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.0773\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.0768\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.0775\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.0776\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.0771\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.0770\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.0773\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.0773\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.0771\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.0769\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.0772\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.0772\t Accuracy 0.9763\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.0773\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.0773\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.0771\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.0769\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.0768\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.0765\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.0764\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.0762\t Accuracy 0.9766\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.0767\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.0765\t Accuracy 0.9766\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.0769\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.0770\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.0767\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.0769\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.0767\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.0769\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.0769\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.0766\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.0764\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.0762\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.0763\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.0764\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.0762\t Accuracy 0.9764\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.0761\t Accuracy 0.9765\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.0758\t Accuracy 0.9766\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0757\t Average training accuracy 0.9766\n",
      "Epoch [2]\t Average validation loss 0.0980\t Average validation accuracy 0.9728\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.0214\t Accuracy 1.0000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.0876\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.0765\t Accuracy 0.9792\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.0804\t Accuracy 0.9768\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.0719\t Accuracy 0.9801\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.0642\t Accuracy 0.9821\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.0622\t Accuracy 0.9821\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.0575\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.0555\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.0575\t Accuracy 0.9829\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.0570\t Accuracy 0.9824\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.0584\t Accuracy 0.9822\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.0589\t Accuracy 0.9820\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.0578\t Accuracy 0.9825\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.0580\t Accuracy 0.9827\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.0576\t Accuracy 0.9832\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.0578\t Accuracy 0.9830\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.0596\t Accuracy 0.9820\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.0616\t Accuracy 0.9817\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.0613\t Accuracy 0.9818\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.0599\t Accuracy 0.9822\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.0606\t Accuracy 0.9821\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.0597\t Accuracy 0.9823\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.0585\t Accuracy 0.9826\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.0594\t Accuracy 0.9823\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.0584\t Accuracy 0.9827\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.0585\t Accuracy 0.9827\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.0587\t Accuracy 0.9827\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.0583\t Accuracy 0.9828\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.0579\t Accuracy 0.9828\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.0577\t Accuracy 0.9829\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.0573\t Accuracy 0.9828\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.0571\t Accuracy 0.9829\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.0569\t Accuracy 0.9830\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.0567\t Accuracy 0.9831\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.0567\t Accuracy 0.9831\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.0572\t Accuracy 0.9828\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.0566\t Accuracy 0.9829\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.0557\t Accuracy 0.9831\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.0556\t Accuracy 0.9830\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.0548\t Accuracy 0.9833\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.0545\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.0550\t Accuracy 0.9833\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.0545\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.0545\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.0542\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.0538\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.0535\t Accuracy 0.9839\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.0537\t Accuracy 0.9838\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.0533\t Accuracy 0.9840\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.0530\t Accuracy 0.9841\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.0529\t Accuracy 0.9840\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.0531\t Accuracy 0.9840\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.0534\t Accuracy 0.9839\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.0543\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.0546\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.0548\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.0548\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.0546\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.0548\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.0541\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.0539\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.0543\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.0546\t Accuracy 0.9833\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.0547\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.0546\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.0544\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.0542\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.0543\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.0551\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.0548\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.0549\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.0545\t Accuracy 0.9837\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.0548\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.0550\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.0546\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.0544\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.0546\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.0548\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.0546\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.0547\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.0543\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.0542\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.0542\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.0545\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.0545\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.0548\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.0549\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.0547\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.0548\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.0547\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.0545\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.0545\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.0544\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.0544\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.0545\t Accuracy 0.9835\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.0544\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.0543\t Accuracy 0.9836\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.0541\t Accuracy 0.9836\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0541\t Average training accuracy 0.9836\n",
      "Epoch [3]\t Average validation loss 0.0960\t Average validation accuracy 0.9750\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.0127\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.0633\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.0575\t Accuracy 0.9851\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.0617\t Accuracy 0.9834\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.0557\t Accuracy 0.9851\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.0494\t Accuracy 0.9869\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.0478\t Accuracy 0.9864\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.0449\t Accuracy 0.9866\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.0428\t Accuracy 0.9868\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.0444\t Accuracy 0.9865\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.0444\t Accuracy 0.9860\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.0458\t Accuracy 0.9853\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.0461\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.0452\t Accuracy 0.9849\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.0453\t Accuracy 0.9852\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.0451\t Accuracy 0.9854\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.0451\t Accuracy 0.9858\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.0459\t Accuracy 0.9853\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.0473\t Accuracy 0.9852\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.0472\t Accuracy 0.9851\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.0462\t Accuracy 0.9852\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.0467\t Accuracy 0.9851\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.0461\t Accuracy 0.9852\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.0450\t Accuracy 0.9858\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.0457\t Accuracy 0.9857\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.0448\t Accuracy 0.9861\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.0448\t Accuracy 0.9862\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.0450\t Accuracy 0.9863\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.0446\t Accuracy 0.9864\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.0445\t Accuracy 0.9863\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.0443\t Accuracy 0.9864\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.0437\t Accuracy 0.9867\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.0435\t Accuracy 0.9868\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.0433\t Accuracy 0.9868\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.0430\t Accuracy 0.9869\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.0428\t Accuracy 0.9870\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.0431\t Accuracy 0.9869\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.0427\t Accuracy 0.9870\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.0420\t Accuracy 0.9873\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.0420\t Accuracy 0.9872\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.0416\t Accuracy 0.9874\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.0412\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.0416\t Accuracy 0.9873\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.0413\t Accuracy 0.9874\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.0413\t Accuracy 0.9874\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.0410\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.0407\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.0404\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.0404\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.0400\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.0398\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.0396\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.0399\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.0402\t Accuracy 0.9877\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.0409\t Accuracy 0.9874\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.0409\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.0411\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.0411\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.0408\t Accuracy 0.9877\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.0410\t Accuracy 0.9875\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.0406\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.0403\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.0401\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.0407\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.0406\t Accuracy 0.9877\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.0403\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.0402\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.0403\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.0409\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.0408\t Accuracy 0.9876\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.0408\t Accuracy 0.9877\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.0406\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.0408\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.0406\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.0407\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.0405\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.0405\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.0404\t Accuracy 0.9878\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.0402\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.0403\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.0402\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.0404\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.0405\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.0407\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.0408\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.0406\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.0406\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.0407\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.0408\t Accuracy 0.9879\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.0407\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.0406\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.0405\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.0405\t Accuracy 0.9881\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.0404\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.0404\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.0403\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.0403\t Accuracy 0.9880\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.0401\t Accuracy 0.9881\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0400\t Average training accuracy 0.9881\n",
      "Epoch [4]\t Average validation loss 0.0973\t Average validation accuracy 0.9752\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.0077\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.0515\t Accuracy 0.9863\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.0428\t Accuracy 0.9881\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.0510\t Accuracy 0.9868\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.0444\t Accuracy 0.9891\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.0393\t Accuracy 0.9904\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.0390\t Accuracy 0.9904\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.0361\t Accuracy 0.9912\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.0338\t Accuracy 0.9918\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.0351\t Accuracy 0.9918\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.0346\t Accuracy 0.9916\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.0345\t Accuracy 0.9913\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.0343\t Accuracy 0.9912\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.0332\t Accuracy 0.9916\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.0330\t Accuracy 0.9916\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.0325\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.0326\t Accuracy 0.9918\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.0325\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.0331\t Accuracy 0.9916\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.0329\t Accuracy 0.9913\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.0321\t Accuracy 0.9914\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.0322\t Accuracy 0.9913\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.0317\t Accuracy 0.9916\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.0312\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.0311\t Accuracy 0.9918\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.0310\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.0310\t Accuracy 0.9918\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.0310\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.0306\t Accuracy 0.9919\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.0309\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.0307\t Accuracy 0.9917\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.0302\t Accuracy 0.9920\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.0300\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.0297\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.0296\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.0296\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.0296\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.0293\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.0290\t Accuracy 0.9925\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.0289\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.0286\t Accuracy 0.9925\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.0286\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.0288\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.0285\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.0285\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.0284\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.0282\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.0281\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.0282\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.0281\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.0279\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.0278\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.0281\t Accuracy 0.9924\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.0284\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.0288\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.0288\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.0290\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.0291\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.0290\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.0292\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.0290\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.0287\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.0286\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.0289\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.0291\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.0290\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.0289\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.0289\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.0288\t Accuracy 0.9923\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.0290\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.0297\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.0292\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.0292\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.0295\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.0294\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.0294\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.0295\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.0294\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.0293\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.0293\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.0292\t Accuracy 0.9921\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.0291\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.0290\t Accuracy 0.9922\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0290\t Average training accuracy 0.9922\n",
      "Epoch [5]\t Average validation loss 0.0915\t Average validation accuracy 0.9772\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.0105\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.0358\t Accuracy 0.9941\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.0305\t Accuracy 0.9941\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.0359\t Accuracy 0.9914\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.0316\t Accuracy 0.9920\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.0276\t Accuracy 0.9932\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.0264\t Accuracy 0.9934\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.0249\t Accuracy 0.9934\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.0230\t Accuracy 0.9943\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.0238\t Accuracy 0.9940\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.0242\t Accuracy 0.9930\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.0243\t Accuracy 0.9931\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.0243\t Accuracy 0.9928\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.0236\t Accuracy 0.9931\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.0239\t Accuracy 0.9930\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.0239\t Accuracy 0.9928\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.0241\t Accuracy 0.9929\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.0238\t Accuracy 0.9928\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.0239\t Accuracy 0.9930\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.0239\t Accuracy 0.9931\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.0232\t Accuracy 0.9933\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.0231\t Accuracy 0.9933\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.0228\t Accuracy 0.9935\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.0223\t Accuracy 0.9937\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.0223\t Accuracy 0.9937\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.0222\t Accuracy 0.9938\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.0221\t Accuracy 0.9939\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.0221\t Accuracy 0.9939\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.0218\t Accuracy 0.9939\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.0217\t Accuracy 0.9941\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.0214\t Accuracy 0.9943\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.0213\t Accuracy 0.9943\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.0212\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.0209\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.0213\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.0212\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.0211\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.0210\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.0207\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.0207\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.0205\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.0205\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.0208\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.0206\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.0207\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.0205\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.0205\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.0205\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.0205\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.0205\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.0204\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.0207\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.0207\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.0213\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.0213\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.0215\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.0217\t Accuracy 0.9943\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.0216\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.0218\t Accuracy 0.9943\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.0217\t Accuracy 0.9944\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.0215\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.0214\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.0215\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.0216\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.0216\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.0215\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.0214\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.0212\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.0215\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.0217\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.0220\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.0218\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.0219\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.0218\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.0218\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.0218\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.0218\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.0218\t Accuracy 0.9946\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.0218\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.0217\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.0218\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.0216\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.0216\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.0215\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.0215\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.0215\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.0215\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.0217\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.0217\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.0219\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.0218\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.0217\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.0218\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.0219\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.0218\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.0217\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.0217\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.0216\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.0215\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.0215\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.0214\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.0214\t Accuracy 0.9948\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.0213\t Accuracy 0.9948\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0212\t Average training accuracy 0.9949\n",
      "Epoch [6]\t Average validation loss 0.0938\t Average validation accuracy 0.9772\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.0133\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.0302\t Accuracy 0.9902\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.0233\t Accuracy 0.9921\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.0291\t Accuracy 0.9914\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.0251\t Accuracy 0.9925\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.0217\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.0212\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.0198\t Accuracy 0.9940\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.0185\t Accuracy 0.9945\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.0191\t Accuracy 0.9947\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.0183\t Accuracy 0.9948\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.0182\t Accuracy 0.9949\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.0180\t Accuracy 0.9952\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.0175\t Accuracy 0.9954\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.0175\t Accuracy 0.9954\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.0174\t Accuracy 0.9955\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.0175\t Accuracy 0.9955\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.0174\t Accuracy 0.9957\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.0174\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.0175\t Accuracy 0.9955\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.0170\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.0169\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.0166\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.0164\t Accuracy 0.9957\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.0164\t Accuracy 0.9957\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.0165\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.0166\t Accuracy 0.9955\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.0166\t Accuracy 0.9954\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.0164\t Accuracy 0.9955\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.0163\t Accuracy 0.9956\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.0161\t Accuracy 0.9957\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.0160\t Accuracy 0.9957\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.0159\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.0158\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.0159\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.0159\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.0159\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.0159\t Accuracy 0.9958\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.0157\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.0157\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.0155\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.0154\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.0155\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.0153\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.0153\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.0152\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.0151\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.0149\t Accuracy 0.9963\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.0149\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.0147\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.0147\t Accuracy 0.9963\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.0146\t Accuracy 0.9963\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.0149\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.0149\t Accuracy 0.9962\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.0152\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.0154\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.0155\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.0156\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.0154\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.0155\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.0154\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.0153\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.0152\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.0152\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.0153\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.0153\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.0152\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.0152\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.0151\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.0153\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.0155\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.0156\t Accuracy 0.9959\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.0155\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.0156\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.0156\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.0156\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.0156\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.0156\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.0156\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.0158\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.0158\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.0158\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.0158\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.0157\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.0156\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.0157\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.0157\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.0157\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.0159\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.0160\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.0160\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.0160\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.0160\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.0160\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.0160\t Accuracy 0.9960\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.0159\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.0158\t Accuracy 0.9961\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.0157\t Accuracy 0.9961\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0158\t Average training accuracy 0.9961\n",
      "Epoch [7]\t Average validation loss 0.0935\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.0044\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.0209\t Accuracy 0.9941\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.0161\t Accuracy 0.9960\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.0192\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.0167\t Accuracy 0.9960\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.0148\t Accuracy 0.9960\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.0141\t Accuracy 0.9963\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.0133\t Accuracy 0.9969\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.0128\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.0132\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.0129\t Accuracy 0.9972\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.0128\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.0130\t Accuracy 0.9972\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.0130\t Accuracy 0.9971\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.0128\t Accuracy 0.9971\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.0127\t Accuracy 0.9972\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.0128\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.0128\t Accuracy 0.9972\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.0129\t Accuracy 0.9971\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.0128\t Accuracy 0.9972\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.0125\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.0128\t Accuracy 0.9973\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.0126\t Accuracy 0.9974\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.0122\t Accuracy 0.9975\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.0123\t Accuracy 0.9975\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.0122\t Accuracy 0.9974\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.0122\t Accuracy 0.9975\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.0122\t Accuracy 0.9975\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.0120\t Accuracy 0.9976\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.0120\t Accuracy 0.9976\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.0120\t Accuracy 0.9976\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.0116\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.0115\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.0113\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.0112\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.0113\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.0111\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.0111\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.0109\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.0109\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.0109\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.0110\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.0110\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.0112\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.0114\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.0114\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.0114\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.0113\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.0113\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.0113\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.0112\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.0112\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.0112\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.0111\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.0111\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.0112\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.0112\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.0113\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.0113\t Accuracy 0.9979\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.0113\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.0114\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.0115\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.0115\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.0115\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.0115\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.0115\t Accuracy 0.9978\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.0116\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.0117\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.0119\t Accuracy 0.9976\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.0120\t Accuracy 0.9976\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.0119\t Accuracy 0.9977\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0119\t Average training accuracy 0.9977\n",
      "Epoch [8]\t Average validation loss 0.0926\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.0075\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.0122\t Accuracy 0.9941\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.0134\t Accuracy 0.9950\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.0164\t Accuracy 0.9947\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.0149\t Accuracy 0.9950\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.0131\t Accuracy 0.9956\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.0127\t Accuracy 0.9960\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.0123\t Accuracy 0.9960\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.0115\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.0127\t Accuracy 0.9962\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.0125\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.0125\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.0126\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.0124\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.0121\t Accuracy 0.9969\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.0122\t Accuracy 0.9969\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.0121\t Accuracy 0.9970\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.0118\t Accuracy 0.9972\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.0117\t Accuracy 0.9973\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.0117\t Accuracy 0.9974\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.0114\t Accuracy 0.9975\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.0112\t Accuracy 0.9976\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.0111\t Accuracy 0.9976\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.0109\t Accuracy 0.9977\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.0108\t Accuracy 0.9978\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.0107\t Accuracy 0.9977\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.0107\t Accuracy 0.9977\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.0107\t Accuracy 0.9978\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.0107\t Accuracy 0.9977\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.0107\t Accuracy 0.9977\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.0105\t Accuracy 0.9978\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.0105\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.0103\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.0102\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.0101\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.0100\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.0100\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.0100\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.0099\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.0098\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.0098\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.0098\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.0097\t Accuracy 0.9979\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.0097\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.0094\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.0094\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.0093\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.0093\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.0093\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.0092\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.0094\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.0094\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.0095\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.0095\t Accuracy 0.9981\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.0095\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.0095\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.0094\t Accuracy 0.9982\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0094\t Average training accuracy 0.9982\n",
      "Epoch [9]\t Average validation loss 0.0959\t Average validation accuracy 0.9778\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.0148\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.0091\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.0082\t Accuracy 0.9990\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.0106\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.0095\t Accuracy 0.9990\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.0094\t Accuracy 0.9984\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.0091\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.0086\t Accuracy 0.9986\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.0079\t Accuracy 0.9988\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.0087\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.0082\t Accuracy 0.9988\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.0083\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.0084\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.0083\t Accuracy 0.9988\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.0080\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.0080\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.0079\t Accuracy 0.9986\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.0079\t Accuracy 0.9987\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.0079\t Accuracy 0.9988\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.0078\t Accuracy 0.9988\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.0076\t Accuracy 0.9989\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.0072\t Accuracy 0.9990\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.0072\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.0071\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.0071\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.0070\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.0070\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.0070\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.0070\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.0070\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.0069\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.0069\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.0069\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.0069\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.0068\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.0068\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.0067\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.0067\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.0067\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.0067\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.0066\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.0066\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.0067\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.0067\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.0066\t Accuracy 0.9995\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.0064\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.0065\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.0065\t Accuracy 0.9994\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.0065\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.0066\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.0066\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.0065\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.0066\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.0066\t Accuracy 0.9993\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.0066\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.0067\t Accuracy 0.9991\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.0067\t Accuracy 0.9992\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0067\t Average training accuracy 0.9992\n",
      "Epoch [10]\t Average validation loss 0.0955\t Average validation accuracy 0.9784\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.0141\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.0069\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.0068\t Accuracy 0.9990\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.0088\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.0081\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.0086\t Accuracy 0.9977\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.0077\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.0071\t Accuracy 0.9983\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.0078\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.0073\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.0073\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.0073\t Accuracy 0.9983\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.0071\t Accuracy 0.9985\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.0069\t Accuracy 0.9986\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.0068\t Accuracy 0.9987\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.0066\t Accuracy 0.9988\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.0066\t Accuracy 0.9987\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.0066\t Accuracy 0.9988\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.0066\t Accuracy 0.9988\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.0064\t Accuracy 0.9989\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.0063\t Accuracy 0.9990\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.0062\t Accuracy 0.9990\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.0060\t Accuracy 0.9990\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.0060\t Accuracy 0.9991\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.0059\t Accuracy 0.9991\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.0059\t Accuracy 0.9992\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.0055\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.0055\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.0054\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.0053\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.0052\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.0051\t Accuracy 0.9995\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.0051\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.0052\t Accuracy 0.9994\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0052\t Average training accuracy 0.9994\n",
      "Epoch [11]\t Average validation loss 0.0982\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.0119\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.0069\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.0064\t Accuracy 0.9990\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.0079\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.0070\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.0062\t Accuracy 0.9984\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.0059\t Accuracy 0.9987\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.0054\t Accuracy 0.9989\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.0052\t Accuracy 0.9990\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.0054\t Accuracy 0.9989\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.0051\t Accuracy 0.9990\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.0051\t Accuracy 0.9991\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.0054\t Accuracy 0.9992\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.0053\t Accuracy 0.9992\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.0053\t Accuracy 0.9991\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.0053\t Accuracy 0.9992\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.0053\t Accuracy 0.9993\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.0052\t Accuracy 0.9992\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.0051\t Accuracy 0.9992\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.0051\t Accuracy 0.9993\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.0049\t Accuracy 0.9993\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.0049\t Accuracy 0.9993\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.0048\t Accuracy 0.9994\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.0047\t Accuracy 0.9994\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.0047\t Accuracy 0.9994\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.0047\t Accuracy 0.9994\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.0047\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.0046\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.0045\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.0045\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.0045\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.0045\t Accuracy 0.9995\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.0041\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.0041\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.0041\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.0041\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0042\t Average training accuracy 0.9996\n",
      "Epoch [12]\t Average validation loss 0.0941\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.0053\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.0049\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.0061\t Accuracy 0.9993\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.0053\t Accuracy 0.9995\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.0048\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.0054\t Accuracy 0.9993\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.0049\t Accuracy 0.9994\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.0047\t Accuracy 0.9995\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.0046\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.0044\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.0043\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.0043\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.0042\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.0042\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.0041\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.0040\t Accuracy 0.9996\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.0041\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.0040\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.0040\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.0040\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.0039\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.0038\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.0038\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.0037\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.0037\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.0037\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.0036\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.0037\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.0036\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.0036\t Accuracy 0.9997\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0032\t Average training accuracy 0.9999\n",
      "Epoch [13]\t Average validation loss 0.0879\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.0037\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.0033\t Accuracy 0.9995\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.0029\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.0028\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.0029\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.0029\t Accuracy 0.9996\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.0029\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.0029\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.0028\t Accuracy 0.9997\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.0027\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.0027\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.0027\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.0026\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.0025\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.0025\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.0025\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.0025\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0024\t Average training accuracy 0.9999\n",
      "Epoch [14]\t Average validation loss 0.0884\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0020\t Average training accuracy 1.0000\n",
      "Epoch [15]\t Average validation loss 0.0884\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0017\t Average training accuracy 1.0000\n",
      "Epoch [16]\t Average validation loss 0.0882\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0016\t Average training accuracy 1.0000\n",
      "Epoch [17]\t Average validation loss 0.0878\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0014\t Average training accuracy 1.0000\n",
      "Epoch [18]\t Average validation loss 0.0878\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0014\t Average training accuracy 1.0000\n",
      "Epoch [19]\t Average validation loss 0.0875\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0013\t Average training accuracy 1.0000\n",
      "Epoch [20]\t Average validation loss 0.0872\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0012\t Average training accuracy 1.0000\n",
      "Epoch [21]\t Average validation loss 0.0870\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0012\t Average training accuracy 1.0000\n",
      "Epoch [22]\t Average validation loss 0.0869\t Average validation accuracy 0.9816\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0011\t Average training accuracy 1.0000\n",
      "Epoch [23]\t Average validation loss 0.0865\t Average validation accuracy 0.9816\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0011\t Average training accuracy 1.0000\n",
      "Epoch [24]\t Average validation loss 0.0865\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [25]\t Average validation loss 0.0861\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [26]\t Average validation loss 0.0860\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [27]\t Average validation loss 0.0857\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [28]\t Average validation loss 0.0855\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0009\t Average training accuracy 1.0000\n",
      "Epoch [29]\t Average validation loss 0.0854\t Average validation accuracy 0.9810\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9802.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from criterion import EuclideanLossLayer,SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "exec_result = pd.DataFrame(columns=['mode','batch_size','learning_rate_SGD', 'momentum','weight_decay','time','loss_validate','acc_validate','acc_test'])\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "batch_size =10\n",
    "max_epoch = 30\n",
    "disp_freq = 50\n",
    "init_std = 0.01\n",
    "\n",
    "#Euclidean+Sigmoid\n",
    "momentum = 0.55\n",
    "weight_decay= 0.0001\n",
    "criterion = EuclideanLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "t1=time.time()\n",
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))\n",
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "sigmoid_acc_test =test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result.loc[exec_result.shape[0]] = ['Euclidean_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc, sigmoid_acc_test]   \n",
    "\n",
    "#Euclidean+ReLU\n",
    "momentum = 0.99\n",
    "weight_decay= 0.0001\n",
    "criterion = EuclideanLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "t1=time.time()\n",
    "reluMLP = Network()\n",
    "# 使用FCLayer和ReLULayer构建多层感知机\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result.loc[exec_result.shape[0]] = ['Euclidean_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc,relu_acc_test]     \n",
    "\n",
    "#CrossEntropy+Sigmoid\n",
    "momentum = 0.55\n",
    "weight_decay= 0.00001\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "t1=time.time()\n",
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))\n",
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "sigmoid_acc_test =test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc, sigmoid_acc_test]         \n",
    "\n",
    "#CrossEntropy+ReLU\n",
    "momentum = 0.99\n",
    "weight_decay= 0.00001\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc, relu_acc_test]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Euclidean+Sigmoid': [exec_result.loc[0].loss_validate, exec_result.loc[0].acc_validate],\n",
    "                   'Euclidean+ReLU': [exec_result.loc[1].loss_validate, exec_result.loc[1].acc_validate],\n",
    "                   'CrossEntropy+Sigmoid':  [exec_result.loc[2].loss_validate, exec_result.loc[2].acc_validate],\n",
    "                   'CrossEntropy+ReLU': [exec_result.loc[3].loss_validate, exec_result.loc[3].acc_validate]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result\n",
    "exec_result['acc_validate_float'] = exec_result['acc_validate'].map(lambda x: np.average(x))\n",
    "exec_result.to_csv('./result/result_bestaccs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>349.379198</td>\n",
       "      <td>[0.23836118365678977, 0.21362315250851602, 0.2...</td>\n",
       "      <td>[0.838, 0.8686, 0.8796000000000002, 0.88420000...</td>\n",
       "      <td>0.9161</td>\n",
       "      <td>0.904627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>391.413110</td>\n",
       "      <td>[0.07845318137437149, 0.06966425223821666, 0.0...</td>\n",
       "      <td>[0.9464000000000001, 0.9526000000000001, 0.956...</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.967840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>429.770796</td>\n",
       "      <td>[0.8944735146564363, 0.5787039352339894, 0.459...</td>\n",
       "      <td>[0.8598000000000001, 0.89, 0.902, 0.9084000000...</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.925740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>393.383177</td>\n",
       "      <td>[0.11989968974397595, 0.10602467090037623, 0.0...</td>\n",
       "      <td>[0.9638, 0.9698000000000001, 0.972800000000000...</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.978580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0     Euclidean_Sigmoid          10              0.001      0.55   \n",
       "1        Euclidean_ReLU          10              0.001      0.99   \n",
       "2  CrossEntropy_Sigmoid          10              0.001      0.55   \n",
       "3     CrossEntropy_ReLU          10              0.001      0.99   \n",
       "\n",
       "   weight_decay        time  \\\n",
       "0       0.00010  349.379198   \n",
       "1       0.00010  391.413110   \n",
       "2       0.00001  429.770796   \n",
       "3       0.00001  393.383177   \n",
       "\n",
       "                                       loss_validate  \\\n",
       "0  [0.23836118365678977, 0.21362315250851602, 0.2...   \n",
       "1  [0.07845318137437149, 0.06966425223821666, 0.0...   \n",
       "2  [0.8944735146564363, 0.5787039352339894, 0.459...   \n",
       "3  [0.11989968974397595, 0.10602467090037623, 0.0...   \n",
       "\n",
       "                                        acc_validate  acc_test  \\\n",
       "0  [0.838, 0.8686, 0.8796000000000002, 0.88420000...    0.9161   \n",
       "1  [0.9464000000000001, 0.9526000000000001, 0.956...    0.9661   \n",
       "2  [0.8598000000000001, 0.89, 0.902, 0.9084000000...    0.9267   \n",
       "3  [0.9638, 0.9698000000000001, 0.972800000000000...    0.9802   \n",
       "\n",
       "   acc_validate_float  \n",
       "0            0.904627  \n",
       "1            0.967840  \n",
       "2            0.925740  \n",
       "3            0.978580  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.具有多层隐含层的多层感知机\n",
    "\n",
    "接下来，根据案例要求，还需要完成**构造具有两个隐含层的多层感知机，自行选取合适的激活函数和损失函数，与只有一个隐含层的结果相比较**.\n",
    "\n",
    "注意: 请在下方插入新的代码块，不要直接修改上面的代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 2.3711\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 2.0152\t Accuracy 0.3235\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 1.6825\t Accuracy 0.4792\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 1.4421\t Accuracy 0.5709\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 1.2568\t Accuracy 0.6323\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 1.1206\t Accuracy 0.6741\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 1.0228\t Accuracy 0.7043\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.9464\t Accuracy 0.7313\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.8849\t Accuracy 0.7494\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.8360\t Accuracy 0.7630\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.7935\t Accuracy 0.7745\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.7575\t Accuracy 0.7853\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.7254\t Accuracy 0.7945\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.6939\t Accuracy 0.8041\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.6703\t Accuracy 0.8097\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.6531\t Accuracy 0.8150\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.6361\t Accuracy 0.8203\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.6214\t Accuracy 0.8254\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.6140\t Accuracy 0.8275\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.6004\t Accuracy 0.8310\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.5846\t Accuracy 0.8360\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.5709\t Accuracy 0.8401\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.5562\t Accuracy 0.8443\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.5433\t Accuracy 0.8480\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.5356\t Accuracy 0.8501\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.5263\t Accuracy 0.8524\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.5212\t Accuracy 0.8533\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.5148\t Accuracy 0.8553\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.5088\t Accuracy 0.8567\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.5043\t Accuracy 0.8575\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.5001\t Accuracy 0.8584\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.4934\t Accuracy 0.8607\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.4885\t Accuracy 0.8617\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.4816\t Accuracy 0.8637\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.4767\t Accuracy 0.8646\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.4715\t Accuracy 0.8660\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.4685\t Accuracy 0.8667\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.4613\t Accuracy 0.8688\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.4552\t Accuracy 0.8705\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.4501\t Accuracy 0.8719\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.4439\t Accuracy 0.8736\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.4392\t Accuracy 0.8751\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.4373\t Accuracy 0.8759\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.4326\t Accuracy 0.8774\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.4278\t Accuracy 0.8787\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.4241\t Accuracy 0.8797\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.4205\t Accuracy 0.8806\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.4162\t Accuracy 0.8816\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.4133\t Accuracy 0.8824\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.4095\t Accuracy 0.8834\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.4077\t Accuracy 0.8838\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.4032\t Accuracy 0.8851\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.3994\t Accuracy 0.8862\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.3965\t Accuracy 0.8871\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.3952\t Accuracy 0.8876\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.3927\t Accuracy 0.8881\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.3893\t Accuracy 0.8891\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.3866\t Accuracy 0.8901\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.3840\t Accuracy 0.8909\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.3819\t Accuracy 0.8915\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.3798\t Accuracy 0.8920\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.3773\t Accuracy 0.8924\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.3754\t Accuracy 0.8928\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.3749\t Accuracy 0.8929\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.3735\t Accuracy 0.8934\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.3717\t Accuracy 0.8937\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.3692\t Accuracy 0.8944\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.3666\t Accuracy 0.8952\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.3633\t Accuracy 0.8962\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.3609\t Accuracy 0.8969\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.3596\t Accuracy 0.8973\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.3574\t Accuracy 0.8980\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.3548\t Accuracy 0.8987\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.3527\t Accuracy 0.8992\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.3499\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.3497\t Accuracy 0.8999\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.3483\t Accuracy 0.9004\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.3459\t Accuracy 0.9010\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.3439\t Accuracy 0.9014\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.3425\t Accuracy 0.9019\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.3409\t Accuracy 0.9024\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.3388\t Accuracy 0.9030\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.3368\t Accuracy 0.9035\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.3357\t Accuracy 0.9037\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.3342\t Accuracy 0.9041\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.3336\t Accuracy 0.9043\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.3323\t Accuracy 0.9046\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.3303\t Accuracy 0.9052\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.3288\t Accuracy 0.9057\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.3275\t Accuracy 0.9060\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.3260\t Accuracy 0.9064\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.3247\t Accuracy 0.9067\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.3238\t Accuracy 0.9070\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.3234\t Accuracy 0.9069\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.3216\t Accuracy 0.9076\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.3205\t Accuracy 0.9077\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.3195\t Accuracy 0.9080\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.3177\t Accuracy 0.9084\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.3167\t Accuracy 0.9087\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.3156\t Accuracy 0.9089\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.3152\t Accuracy 0.9090\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.3148\t Accuracy 0.9091\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.3136\t Accuracy 0.9096\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.3120\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.3106\t Accuracy 0.9104\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.3096\t Accuracy 0.9107\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.3092\t Accuracy 0.9107\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.3077\t Accuracy 0.9112\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.3069\t Accuracy 0.9113\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.3055\t Accuracy 0.9118\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3044\t Average training accuracy 0.9121\n",
      "Epoch [0]\t Average validation loss 0.1485\t Average validation accuracy 0.9620\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.0761\t Accuracy 1.0000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.1770\t Accuracy 0.9549\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.1812\t Accuracy 0.9564\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.1965\t Accuracy 0.9477\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.1781\t Accuracy 0.9527\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.1644\t Accuracy 0.9558\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.1583\t Accuracy 0.9581\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.1524\t Accuracy 0.9604\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.1494\t Accuracy 0.9601\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.1497\t Accuracy 0.9596\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.1478\t Accuracy 0.9609\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.1466\t Accuracy 0.9610\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.1462\t Accuracy 0.9606\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.1449\t Accuracy 0.9611\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.1461\t Accuracy 0.9606\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.1481\t Accuracy 0.9598\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.1490\t Accuracy 0.9589\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.1517\t Accuracy 0.9577\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.1564\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.1570\t Accuracy 0.9557\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.1555\t Accuracy 0.9557\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.1563\t Accuracy 0.9557\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.1549\t Accuracy 0.9560\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.1528\t Accuracy 0.9566\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.1540\t Accuracy 0.9565\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.1528\t Accuracy 0.9568\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.1548\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.1549\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.1551\t Accuracy 0.9567\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.1560\t Accuracy 0.9561\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.1564\t Accuracy 0.9560\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.1565\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.1566\t Accuracy 0.9562\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.1558\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.1554\t Accuracy 0.9564\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.1552\t Accuracy 0.9563\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.1559\t Accuracy 0.9559\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.1546\t Accuracy 0.9562\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.1536\t Accuracy 0.9565\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.1533\t Accuracy 0.9567\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.1523\t Accuracy 0.9571\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.1517\t Accuracy 0.9573\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.1533\t Accuracy 0.9569\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.1528\t Accuracy 0.9571\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.1522\t Accuracy 0.9571\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.1520\t Accuracy 0.9572\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.1519\t Accuracy 0.9571\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.1512\t Accuracy 0.9573\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.1513\t Accuracy 0.9573\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.1503\t Accuracy 0.9575\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.1505\t Accuracy 0.9575\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.1495\t Accuracy 0.9578\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.1492\t Accuracy 0.9579\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.1490\t Accuracy 0.9580\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.1500\t Accuracy 0.9578\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.1501\t Accuracy 0.9578\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.1496\t Accuracy 0.9579\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.1494\t Accuracy 0.9580\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.1493\t Accuracy 0.9580\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.1491\t Accuracy 0.9579\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.1488\t Accuracy 0.9578\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.1484\t Accuracy 0.9578\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.1480\t Accuracy 0.9579\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.1489\t Accuracy 0.9577\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.1494\t Accuracy 0.9576\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.1493\t Accuracy 0.9575\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.1490\t Accuracy 0.9576\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.1485\t Accuracy 0.9577\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.1476\t Accuracy 0.9581\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.1473\t Accuracy 0.9583\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.1476\t Accuracy 0.9582\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.1475\t Accuracy 0.9582\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.1468\t Accuracy 0.9584\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.1467\t Accuracy 0.9584\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.1458\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.1466\t Accuracy 0.9584\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.1467\t Accuracy 0.9583\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.1461\t Accuracy 0.9585\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.1456\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.1457\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.1456\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.1451\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.1446\t Accuracy 0.9589\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.1449\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.1447\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.1451\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.1451\t Accuracy 0.9585\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.1445\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.1443\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.1442\t Accuracy 0.9589\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.1439\t Accuracy 0.9590\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.1438\t Accuracy 0.9589\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.1440\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.1444\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.1440\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.1441\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.1442\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.1437\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.1437\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.1438\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.1442\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.1444\t Accuracy 0.9586\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.1443\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.1438\t Accuracy 0.9588\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.1435\t Accuracy 0.9589\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.1433\t Accuracy 0.9590\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.1437\t Accuracy 0.9590\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.1433\t Accuracy 0.9591\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.1434\t Accuracy 0.9590\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.1430\t Accuracy 0.9591\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1427\t Average training accuracy 0.9591\n",
      "Epoch [1]\t Average validation loss 0.1145\t Average validation accuracy 0.9702\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.0452\t Accuracy 1.0000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.1288\t Accuracy 0.9647\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.1268\t Accuracy 0.9644\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.1387\t Accuracy 0.9589\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.1246\t Accuracy 0.9632\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.1134\t Accuracy 0.9665\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.1100\t Accuracy 0.9684\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.1045\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.1019\t Accuracy 0.9713\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.1014\t Accuracy 0.9716\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.1001\t Accuracy 0.9729\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.0985\t Accuracy 0.9728\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.0985\t Accuracy 0.9724\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.0979\t Accuracy 0.9720\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.0996\t Accuracy 0.9719\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.1012\t Accuracy 0.9718\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.1020\t Accuracy 0.9713\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.1040\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.1071\t Accuracy 0.9700\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.1073\t Accuracy 0.9697\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.1059\t Accuracy 0.9700\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.1069\t Accuracy 0.9696\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.1060\t Accuracy 0.9701\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.1044\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.1052\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.1040\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.1053\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.1051\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.1054\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.1055\t Accuracy 0.9703\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.1054\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.1057\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.1059\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.1053\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.1051\t Accuracy 0.9703\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.1052\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.1057\t Accuracy 0.9702\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.1050\t Accuracy 0.9702\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.1043\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.1043\t Accuracy 0.9703\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.1036\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.1032\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.1044\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.1042\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.1039\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.1037\t Accuracy 0.9703\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.1037\t Accuracy 0.9702\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.1032\t Accuracy 0.9703\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.1034\t Accuracy 0.9702\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.1027\t Accuracy 0.9704\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.1027\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.1020\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.1021\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.1022\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.1030\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.1032\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.1030\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.1029\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.1028\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.1027\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.1024\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.1020\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.1017\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.1024\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.1028\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.1028\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.1027\t Accuracy 0.9705\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.1025\t Accuracy 0.9706\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.1020\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.1018\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.1022\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.1022\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.1017\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.1018\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.1012\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.1019\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.1021\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.1016\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.1014\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.1015\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.1015\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.1012\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.1010\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.1013\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.1012\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.1015\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.1015\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.1011\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.1011\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.1011\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.1009\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.1009\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.1011\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.1014\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.1011\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.1013\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.1015\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.1012\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.1013\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.1014\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.1018\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.1019\t Accuracy 0.9707\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.1018\t Accuracy 0.9708\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.1016\t Accuracy 0.9709\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.1014\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.1013\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.1016\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.1015\t Accuracy 0.9711\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.1015\t Accuracy 0.9710\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.1012\t Accuracy 0.9711\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1010\t Average training accuracy 0.9712\n",
      "Epoch [2]\t Average validation loss 0.0992\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.0342\t Accuracy 1.0000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.1011\t Accuracy 0.9706\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.0973\t Accuracy 0.9723\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.1069\t Accuracy 0.9669\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.0958\t Accuracy 0.9716\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.0864\t Accuracy 0.9741\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.0842\t Accuracy 0.9754\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.0794\t Accuracy 0.9772\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.0772\t Accuracy 0.9778\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.0764\t Accuracy 0.9778\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.0754\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.0742\t Accuracy 0.9791\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.0744\t Accuracy 0.9794\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.0737\t Accuracy 0.9791\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.0754\t Accuracy 0.9790\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.0765\t Accuracy 0.9791\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.0773\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.0786\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.0808\t Accuracy 0.9774\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.0808\t Accuracy 0.9774\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.0795\t Accuracy 0.9777\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.0803\t Accuracy 0.9774\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.0795\t Accuracy 0.9776\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.0783\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.0789\t Accuracy 0.9776\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.0778\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.0786\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.0782\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.0784\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.0782\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.0779\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.0783\t Accuracy 0.9781\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.0784\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.0779\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.0777\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.0780\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.0785\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.0780\t Accuracy 0.9778\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.0774\t Accuracy 0.9781\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.0776\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.0770\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.0767\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.0777\t Accuracy 0.9781\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.0775\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.0772\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.0772\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.0770\t Accuracy 0.9779\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.0767\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.0769\t Accuracy 0.9780\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.0764\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.0763\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.0758\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.0760\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.0769\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.0771\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.0770\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.0769\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.0768\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.0767\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.0764\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.0760\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.0757\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.0763\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.0767\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.0766\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.0766\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.0765\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.0761\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.0761\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.0763\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.0765\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.0762\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.0763\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.0759\t Accuracy 0.9789\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.0765\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.0766\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.0763\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.0763\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.0763\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.0761\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.0760\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.0762\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.0764\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.0765\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.0761\t Accuracy 0.9788\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.0762\t Accuracy 0.9787\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.0763\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.0765\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.0764\t Accuracy 0.9786\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.0766\t Accuracy 0.9785\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.0768\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.0766\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.0766\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.0767\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.0770\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.0771\t Accuracy 0.9782\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.0770\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.0769\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.0767\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.0767\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.0770\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.0769\t Accuracy 0.9784\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.0769\t Accuracy 0.9783\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.0767\t Accuracy 0.9784\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0765\t Average training accuracy 0.9785\n",
      "Epoch [3]\t Average validation loss 0.0903\t Average validation accuracy 0.9762\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.0281\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.0827\t Accuracy 0.9804\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.0761\t Accuracy 0.9802\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.0840\t Accuracy 0.9762\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.0753\t Accuracy 0.9796\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.0674\t Accuracy 0.9817\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.0662\t Accuracy 0.9827\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.0622\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.0603\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.0595\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.0588\t Accuracy 0.9850\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.0580\t Accuracy 0.9855\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.0584\t Accuracy 0.9859\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.0577\t Accuracy 0.9857\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.0594\t Accuracy 0.9853\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.0601\t Accuracy 0.9852\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.0607\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.0618\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.0634\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.0633\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.0621\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.0628\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.0623\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.0612\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.0617\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.0608\t Accuracy 0.9849\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.0612\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.0608\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.0609\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.0605\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.0601\t Accuracy 0.9850\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.0604\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.0606\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.0601\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.0600\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.0602\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.0607\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.0603\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.0598\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.0600\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.0595\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.0593\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.0601\t Accuracy 0.9842\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.0599\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.0598\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.0598\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.0595\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.0592\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.0595\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.0590\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.0589\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.0586\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.0589\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.0591\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.0597\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.0600\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.0600\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.0598\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.0597\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.0595\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.0592\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.0589\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.0587\t Accuracy 0.9849\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.0591\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.0594\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.0593\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.0593\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.0592\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.0590\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.0589\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.0592\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.0594\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.0592\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.0594\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.0590\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.0595\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.0596\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.0594\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.0593\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.0595\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.0594\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.0593\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.0592\t Accuracy 0.9848\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.0594\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.0594\t Accuracy 0.9847\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.0595\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.0596\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.0594\t Accuracy 0.9846\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.0595\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.0596\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.0594\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.0595\t Accuracy 0.9845\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.0596\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.0597\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.0596\t Accuracy 0.9844\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.0599\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.0601\t Accuracy 0.9842\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.0599\t Accuracy 0.9843\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.0599\t Accuracy 0.9842\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.0599\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.0601\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.0602\t Accuracy 0.9840\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.0601\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.0600\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.0599\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.0599\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.0601\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.0600\t Accuracy 0.9841\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.0600\t Accuracy 0.9840\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.0599\t Accuracy 0.9840\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0598\t Average training accuracy 0.9841\n",
      "Epoch [4]\t Average validation loss 0.0859\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.0238\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.0703\t Accuracy 0.9843\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.0608\t Accuracy 0.9861\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.0673\t Accuracy 0.9828\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.0605\t Accuracy 0.9846\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.0540\t Accuracy 0.9865\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.0534\t Accuracy 0.9870\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.0499\t Accuracy 0.9883\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.0483\t Accuracy 0.9885\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.0477\t Accuracy 0.9882\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.0472\t Accuracy 0.9884\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.0466\t Accuracy 0.9887\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.0472\t Accuracy 0.9889\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.0465\t Accuracy 0.9889\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.0480\t Accuracy 0.9884\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.0485\t Accuracy 0.9883\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.0490\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.0500\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.0512\t Accuracy 0.9871\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.0511\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.0500\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.0506\t Accuracy 0.9873\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.0503\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.0494\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.0498\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.0490\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.0492\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.0488\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.0487\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.0483\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.0479\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.0481\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.0482\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.0477\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.0476\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.0479\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.0482\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.0479\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.0475\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.0477\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.0472\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.0471\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.0477\t Accuracy 0.9873\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.0476\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.0475\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.0474\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.0472\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.0469\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.0472\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.0468\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.0467\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.0465\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.0468\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.0470\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.0475\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.0476\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.0474\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.0473\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.0470\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.0468\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.0466\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.0469\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.0471\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.0471\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.0471\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.0469\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.0467\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.0468\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.0469\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.0471\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.0470\t Accuracy 0.9880\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.0472\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.0474\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.0474\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.0473\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.0472\t Accuracy 0.9879\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.0473\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.0473\t Accuracy 0.9878\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.0474\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.0476\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.0474\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.0475\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.0476\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.0474\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.0475\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.0475\t Accuracy 0.9877\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.0476\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.0476\t Accuracy 0.9876\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.0480\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.0480\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.0480\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.0479\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.0479\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.0478\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.0477\t Accuracy 0.9875\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.0479\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.0479\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.0478\t Accuracy 0.9874\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.0477\t Accuracy 0.9874\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0476\t Average training accuracy 0.9875\n",
      "Epoch [5]\t Average validation loss 0.0838\t Average validation accuracy 0.9766\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.0197\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.0605\t Accuracy 0.9882\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.0487\t Accuracy 0.9901\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.0545\t Accuracy 0.9868\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.0489\t Accuracy 0.9876\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.0436\t Accuracy 0.9892\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.0436\t Accuracy 0.9897\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.0405\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.0390\t Accuracy 0.9915\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.0386\t Accuracy 0.9918\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.0381\t Accuracy 0.9918\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.0378\t Accuracy 0.9918\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.0385\t Accuracy 0.9917\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.0378\t Accuracy 0.9917\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.0392\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.0396\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.0401\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.0410\t Accuracy 0.9904\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.0418\t Accuracy 0.9900\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.0417\t Accuracy 0.9903\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.0408\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.0412\t Accuracy 0.9904\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.0410\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.0403\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.0407\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.0400\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.0401\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.0396\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.0395\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.0391\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.0387\t Accuracy 0.9913\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.0388\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.0388\t Accuracy 0.9913\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.0384\t Accuracy 0.9913\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.0383\t Accuracy 0.9914\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.0385\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.0388\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.0386\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.0382\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.0384\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.0380\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.0378\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.0384\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.0382\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.0382\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.0381\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.0379\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.0376\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.0379\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.0376\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.0375\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.0373\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.0376\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.0377\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.0383\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.0385\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.0385\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.0383\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.0382\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.0380\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.0378\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.0376\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.0374\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.0377\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.0378\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.0378\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.0378\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.0376\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.0375\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.0375\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.0377\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.0379\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.0378\t Accuracy 0.9913\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.0380\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.0378\t Accuracy 0.9912\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.0379\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.0381\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.0380\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.0380\t Accuracy 0.9911\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.0381\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.0381\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.0380\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.0380\t Accuracy 0.9910\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.0381\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.0381\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.0382\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.0382\t Accuracy 0.9909\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.0384\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.0384\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.0383\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.0386\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.0387\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.0386\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.0385\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.0385\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.0387\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.0387\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.0386\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.0386\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.0385\t Accuracy 0.9908\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.0384\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.0386\t Accuracy 0.9907\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.0385\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.0385\t Accuracy 0.9906\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.0384\t Accuracy 0.9906\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0384\t Average training accuracy 0.9907\n",
      "Epoch [6]\t Average validation loss 0.0826\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.0167\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.0531\t Accuracy 0.9882\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.0401\t Accuracy 0.9911\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.0447\t Accuracy 0.9894\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.0398\t Accuracy 0.9905\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.0356\t Accuracy 0.9916\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.0359\t Accuracy 0.9920\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.0331\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.0319\t Accuracy 0.9938\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.0316\t Accuracy 0.9938\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.0311\t Accuracy 0.9938\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.0310\t Accuracy 0.9938\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.0317\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.0311\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.0324\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.0327\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.0331\t Accuracy 0.9929\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.0340\t Accuracy 0.9926\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.0346\t Accuracy 0.9925\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.0344\t Accuracy 0.9926\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.0336\t Accuracy 0.9929\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.0338\t Accuracy 0.9926\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.0338\t Accuracy 0.9926\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.0332\t Accuracy 0.9929\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.0336\t Accuracy 0.9928\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.0329\t Accuracy 0.9930\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.0330\t Accuracy 0.9929\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.0326\t Accuracy 0.9930\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.0323\t Accuracy 0.9931\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.0320\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.0316\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.0316\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.0316\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.0313\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.0312\t Accuracy 0.9937\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.0314\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.0315\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.0313\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.0310\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.0311\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.0308\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.0307\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.0311\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.0310\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.0310\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.0309\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.0307\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.0305\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.0307\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.0304\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.0303\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.0302\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.0305\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.0306\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.0311\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.0312\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.0313\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.0311\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.0309\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.0308\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.0307\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.0305\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.0303\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.0306\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.0306\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.0306\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.0306\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.0304\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.0303\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.0304\t Accuracy 0.9937\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.0305\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.0308\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.0307\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.0308\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.0307\t Accuracy 0.9937\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.0308\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.0309\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.0308\t Accuracy 0.9936\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.0309\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.0310\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.0310\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.0309\t Accuracy 0.9935\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.0309\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.0309\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.0310\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.0310\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.0311\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.0311\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.0312\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.0312\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.0311\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.0312\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.0312\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.0312\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.0312\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.0314\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.0315\t Accuracy 0.9932\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.0314\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.0313\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.0313\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.0314\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.0314\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.0313\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.0314\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.0313\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.0312\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.0313\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.0313\t Accuracy 0.9933\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.0312\t Accuracy 0.9934\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.0312\t Accuracy 0.9934\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0311\t Average training accuracy 0.9934\n",
      "Epoch [7]\t Average validation loss 0.0820\t Average validation accuracy 0.9770\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.0145\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.0462\t Accuracy 0.9882\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.0333\t Accuracy 0.9941\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.0365\t Accuracy 0.9921\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.0323\t Accuracy 0.9930\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.0290\t Accuracy 0.9936\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.0295\t Accuracy 0.9940\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.0272\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.0261\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.0260\t Accuracy 0.9951\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.0256\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.0256\t Accuracy 0.9955\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.0263\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.0257\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.0267\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.0271\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.0275\t Accuracy 0.9946\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.0283\t Accuracy 0.9944\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.0286\t Accuracy 0.9942\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.0284\t Accuracy 0.9944\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.0277\t Accuracy 0.9947\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.0278\t Accuracy 0.9946\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.0278\t Accuracy 0.9946\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.0272\t Accuracy 0.9947\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.0276\t Accuracy 0.9947\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.0271\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.0271\t Accuracy 0.9948\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.0267\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.0264\t Accuracy 0.9949\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.0261\t Accuracy 0.9951\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.0258\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.0258\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.0258\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.0254\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.0256\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.0255\t Accuracy 0.9951\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.0252\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.0253\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.0250\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.0249\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.0253\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.0250\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.0248\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.0250\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.0247\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.0246\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.0246\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.0248\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.0249\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.0253\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.0254\t Accuracy 0.9951\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.0255\t Accuracy 0.9951\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.0253\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.0252\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.0251\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.0250\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.0248\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.0247\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.0249\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.0250\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.0249\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.0249\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.0247\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.0246\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.0247\t Accuracy 0.9955\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.0248\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.0251\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.0250\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.0251\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.0250\t Accuracy 0.9955\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.0250\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.0252\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.0252\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.0252\t Accuracy 0.9954\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.0253\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.0253\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.0252\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.0253\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.0253\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.0254\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.0254\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.0254\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.0255\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.0254\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.0256\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.0257\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.0256\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.0256\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.0256\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.0257\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.0256\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.0256\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.0256\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.0255\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.0255\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.0255\t Accuracy 0.9952\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.0254\t Accuracy 0.9953\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.0254\t Accuracy 0.9953\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0253\t Average training accuracy 0.9953\n",
      "Epoch [8]\t Average validation loss 0.0815\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.0114\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.0402\t Accuracy 0.9882\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.0281\t Accuracy 0.9941\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.0301\t Accuracy 0.9927\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.0265\t Accuracy 0.9940\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.0238\t Accuracy 0.9944\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.0245\t Accuracy 0.9947\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.0224\t Accuracy 0.9954\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.0215\t Accuracy 0.9958\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.0216\t Accuracy 0.9958\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.0212\t Accuracy 0.9960\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.0212\t Accuracy 0.9962\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.0217\t Accuracy 0.9958\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.0212\t Accuracy 0.9960\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.0221\t Accuracy 0.9957\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.0225\t Accuracy 0.9957\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.0229\t Accuracy 0.9956\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.0235\t Accuracy 0.9953\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.0237\t Accuracy 0.9952\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.0234\t Accuracy 0.9954\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.0228\t Accuracy 0.9956\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.0228\t Accuracy 0.9955\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.0228\t Accuracy 0.9955\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.0224\t Accuracy 0.9956\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.0227\t Accuracy 0.9955\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.0222\t Accuracy 0.9957\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.0222\t Accuracy 0.9957\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.0219\t Accuracy 0.9958\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.0216\t Accuracy 0.9959\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.0214\t Accuracy 0.9960\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.0211\t Accuracy 0.9961\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.0211\t Accuracy 0.9962\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.0210\t Accuracy 0.9962\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.0208\t Accuracy 0.9962\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.0208\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.0208\t Accuracy 0.9963\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.0209\t Accuracy 0.9963\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.0208\t Accuracy 0.9963\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.0205\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.0206\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.0204\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.0204\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.0206\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.0205\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.0206\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.0205\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.0204\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.0202\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.0203\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.0202\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.0201\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.0200\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.0202\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.0204\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.0207\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.0208\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.0208\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.0207\t Accuracy 0.9964\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.0205\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.0205\t Accuracy 0.9965\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.0204\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.0203\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.0202\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.0203\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.0204\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.0204\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.0203\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.0202\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.0201\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.0201\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.0203\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.0205\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.0205\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.0205\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.0205\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.0205\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.0206\t Accuracy 0.9968\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.0206\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.0206\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.0208\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.0208\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.0208\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.0208\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.0210\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.0211\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.0210\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.0210\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.0210\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.0209\t Accuracy 0.9966\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.0209\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.0208\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.0208\t Accuracy 0.9967\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.0207\t Accuracy 0.9967\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0207\t Average training accuracy 0.9967\n",
      "Epoch [9]\t Average validation loss 0.0807\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.0088\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.0342\t Accuracy 0.9902\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.0237\t Accuracy 0.9950\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.0250\t Accuracy 0.9940\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.0219\t Accuracy 0.9950\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.0196\t Accuracy 0.9956\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.0203\t Accuracy 0.9957\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.0186\t Accuracy 0.9963\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.0178\t Accuracy 0.9965\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.0180\t Accuracy 0.9965\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.0177\t Accuracy 0.9966\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.0177\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.0180\t Accuracy 0.9965\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.0177\t Accuracy 0.9968\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.0183\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.0187\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.0191\t Accuracy 0.9966\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.0195\t Accuracy 0.9965\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.0196\t Accuracy 0.9964\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.0193\t Accuracy 0.9965\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.0188\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.0187\t Accuracy 0.9968\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.0188\t Accuracy 0.9966\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.0184\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.0187\t Accuracy 0.9966\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.0183\t Accuracy 0.9967\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.0182\t Accuracy 0.9968\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.0177\t Accuracy 0.9969\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.0175\t Accuracy 0.9970\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.0173\t Accuracy 0.9971\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.0173\t Accuracy 0.9972\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.0172\t Accuracy 0.9971\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.0170\t Accuracy 0.9972\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.0170\t Accuracy 0.9973\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.0171\t Accuracy 0.9973\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.0171\t Accuracy 0.9973\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.0170\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.0168\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.0169\t Accuracy 0.9973\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.0167\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.0167\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.0169\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.0168\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.0169\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.0168\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.0167\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.0166\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.0166\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.0165\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.0164\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.0164\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.0166\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.0168\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.0170\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.0171\t Accuracy 0.9973\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.0171\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.0170\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.0169\t Accuracy 0.9974\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.0169\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.0168\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.0167\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.0166\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.0167\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.0168\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.0168\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.0167\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.0166\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.0165\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.0166\t Accuracy 0.9977\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.0167\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.0170\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.0170\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.0170\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.0170\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.0171\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.0171\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.0174\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.0174\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.0173\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.0172\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.0172\t Accuracy 0.9975\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.0171\t Accuracy 0.9976\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0170\t Average training accuracy 0.9976\n",
      "Epoch [10]\t Average validation loss 0.0798\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.0062\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.0283\t Accuracy 0.9922\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.0197\t Accuracy 0.9960\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.0206\t Accuracy 0.9960\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.0182\t Accuracy 0.9970\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.0163\t Accuracy 0.9972\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.0169\t Accuracy 0.9970\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.0155\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.0147\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.0150\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.0147\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.0147\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.0150\t Accuracy 0.9975\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.0147\t Accuracy 0.9977\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.0152\t Accuracy 0.9976\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.0156\t Accuracy 0.9975\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.0159\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.0162\t Accuracy 0.9972\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.0162\t Accuracy 0.9971\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.0159\t Accuracy 0.9973\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.0155\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.0154\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.0154\t Accuracy 0.9973\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.0151\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.0154\t Accuracy 0.9973\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.0151\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.0150\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.0148\t Accuracy 0.9974\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.0146\t Accuracy 0.9975\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.0144\t Accuracy 0.9976\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.0143\t Accuracy 0.9977\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.0142\t Accuracy 0.9977\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.0142\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.0140\t Accuracy 0.9978\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.0140\t Accuracy 0.9979\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.0140\t Accuracy 0.9979\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.0141\t Accuracy 0.9979\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.0140\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.0138\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.0139\t Accuracy 0.9979\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.0137\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.0137\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.0139\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.0138\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.0139\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.0139\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.0138\t Accuracy 0.9980\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.0137\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.0137\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.0136\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.0136\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.0136\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.0137\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.0139\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.0141\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.0141\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.0141\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.0141\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.0140\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.0140\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.0139\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.0138\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.0138\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.0139\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.0139\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.0139\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.0138\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.0138\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.0137\t Accuracy 0.9983\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.0137\t Accuracy 0.9983\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.0139\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.0140\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.0140\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.0141\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.0140\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.0140\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.0141\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.0143\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.0145\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.0144\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.0144\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.0143\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.0142\t Accuracy 0.9981\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.0142\t Accuracy 0.9982\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0141\t Average training accuracy 0.9982\n",
      "Epoch [11]\t Average validation loss 0.0793\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.0044\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.0231\t Accuracy 0.9922\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.0163\t Accuracy 0.9960\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.0170\t Accuracy 0.9967\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.0151\t Accuracy 0.9975\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.0136\t Accuracy 0.9976\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.0141\t Accuracy 0.9973\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.0129\t Accuracy 0.9977\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.0123\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.0127\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.0124\t Accuracy 0.9982\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.0124\t Accuracy 0.9984\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.0123\t Accuracy 0.9983\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.0130\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.0132\t Accuracy 0.9979\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.0134\t Accuracy 0.9978\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.0134\t Accuracy 0.9978\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.0132\t Accuracy 0.9979\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.0128\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.0127\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.0127\t Accuracy 0.9979\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.0126\t Accuracy 0.9979\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.0124\t Accuracy 0.9981\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.0122\t Accuracy 0.9981\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.0120\t Accuracy 0.9981\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.0119\t Accuracy 0.9982\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.0118\t Accuracy 0.9983\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.0117\t Accuracy 0.9983\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.0117\t Accuracy 0.9983\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.0116\t Accuracy 0.9984\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.0116\t Accuracy 0.9984\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.0116\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.0117\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.0116\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.0115\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.0115\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.0114\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.0114\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.0115\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.0114\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.0115\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.0115\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.0114\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.0113\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.0114\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.0113\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.0113\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.0113\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.0114\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.0118\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.0118\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.0118\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.0117\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.0116\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.0117\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.0115\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.0116\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.0115\t Accuracy 0.9987\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.0114\t Accuracy 0.9987\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.0115\t Accuracy 0.9987\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.0116\t Accuracy 0.9987\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.0117\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.0117\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.0118\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.0117\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.0117\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.0118\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.0120\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.0120\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.0121\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.0121\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.0120\t Accuracy 0.9985\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.0120\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.0119\t Accuracy 0.9986\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0118\t Average training accuracy 0.9987\n",
      "Epoch [12]\t Average validation loss 0.0789\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.0188\t Accuracy 0.9961\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.0136\t Accuracy 0.9980\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.0143\t Accuracy 0.9980\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.0128\t Accuracy 0.9985\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.0115\t Accuracy 0.9984\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.0118\t Accuracy 0.9980\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.0109\t Accuracy 0.9983\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.0103\t Accuracy 0.9985\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.0107\t Accuracy 0.9984\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.0105\t Accuracy 0.9986\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.0105\t Accuracy 0.9987\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.0105\t Accuracy 0.9988\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.0104\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.0106\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.0109\t Accuracy 0.9987\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.0110\t Accuracy 0.9986\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.0112\t Accuracy 0.9986\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.0112\t Accuracy 0.9987\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.0110\t Accuracy 0.9987\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.0107\t Accuracy 0.9988\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.0106\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.0106\t Accuracy 0.9988\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.0104\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.0105\t Accuracy 0.9988\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.0104\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.0103\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.0102\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.0098\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.0098\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.0096\t Accuracy 0.9993\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.0097\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.0094\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.0098\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.0097\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.0098\t Accuracy 0.9992\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.0101\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.0101\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.0101\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.0101\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.0102\t Accuracy 0.9989\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.0102\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.0100\t Accuracy 0.9991\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0100\t Average training accuracy 0.9991\n",
      "Epoch [13]\t Average validation loss 0.0787\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.0152\t Accuracy 0.9961\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.0113\t Accuracy 0.9980\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.0121\t Accuracy 0.9980\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.0108\t Accuracy 0.9985\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.0099\t Accuracy 0.9980\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.0092\t Accuracy 0.9983\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.0087\t Accuracy 0.9985\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.0091\t Accuracy 0.9984\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.0089\t Accuracy 0.9986\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.0089\t Accuracy 0.9987\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.0089\t Accuracy 0.9988\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.0088\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.0089\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.0091\t Accuracy 0.9987\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.0094\t Accuracy 0.9986\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.0093\t Accuracy 0.9987\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.0092\t Accuracy 0.9987\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.0090\t Accuracy 0.9988\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.0089\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.0088\t Accuracy 0.9988\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.0087\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.0088\t Accuracy 0.9988\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.0087\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.0087\t Accuracy 0.9989\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.0085\t Accuracy 0.9990\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.0084\t Accuracy 0.9990\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.0084\t Accuracy 0.9990\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.0083\t Accuracy 0.9991\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.0083\t Accuracy 0.9991\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.0082\t Accuracy 0.9991\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.0082\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.0082\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.0082\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.0082\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.0082\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.0082\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.0080\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.0080\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.0080\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.0080\t Accuracy 0.9994\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.0080\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.0081\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.0082\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.0084\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.0084\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.0082\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.0082\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.0082\t Accuracy 0.9994\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.0082\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.0083\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.0085\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.0085\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.0087\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0085\t Average training accuracy 0.9993\n",
      "Epoch [14]\t Average validation loss 0.0786\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.0124\t Accuracy 0.9961\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.0096\t Accuracy 0.9980\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.0103\t Accuracy 0.9980\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.0092\t Accuracy 0.9985\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.0084\t Accuracy 0.9988\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.0084\t Accuracy 0.9987\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.0078\t Accuracy 0.9989\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.0075\t Accuracy 0.9990\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.0078\t Accuracy 0.9989\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.0077\t Accuracy 0.9990\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.0076\t Accuracy 0.9991\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.0077\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.0076\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.0076\t Accuracy 0.9991\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.0078\t Accuracy 0.9991\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.0078\t Accuracy 0.9990\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.0079\t Accuracy 0.9991\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.0079\t Accuracy 0.9991\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.0078\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.0076\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.0076\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.0075\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.0073\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.0074\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.0074\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.0073\t Accuracy 0.9992\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.0072\t Accuracy 0.9993\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.0072\t Accuracy 0.9993\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.0071\t Accuracy 0.9993\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.0071\t Accuracy 0.9993\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.0069\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.0068\t Accuracy 0.9996\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.0068\t Accuracy 0.9996\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.0068\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.0071\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.0072\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.0072\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.0072\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.0074\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.0073\t Accuracy 0.9994\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0073\t Average training accuracy 0.9994\n",
      "Epoch [15]\t Average validation loss 0.0785\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.0102\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.0082\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.0088\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.0079\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.0072\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.0073\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.0067\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.0064\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.0067\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.0066\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.0066\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.0066\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.0066\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.0066\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.0067\t Accuracy 0.9996\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.0067\t Accuracy 0.9996\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.0068\t Accuracy 0.9996\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.0068\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.0067\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.0065\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.0065\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.0064\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.0063\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.0063\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.0060\t Accuracy 0.9999\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.0059\t Accuracy 0.9999\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.0060\t Accuracy 0.9999\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.0060\t Accuracy 0.9999\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.0059\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.0059\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.0059\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.0059\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.0059\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.0060\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.0061\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.0062\t Accuracy 0.9998\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.0062\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.0064\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.0063\t Accuracy 0.9997\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0063\t Average training accuracy 0.9997\n",
      "Epoch [16]\t Average validation loss 0.0786\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.0086\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.0070\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.0076\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.0068\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.0063\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.0063\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.0059\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.0056\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.0058\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.0057\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.0057\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.0057\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.0057\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.0057\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.0058\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.0058\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.0059\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.0059\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.0058\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.0057\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.0057\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.0056\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.0055\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.0056\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.0055\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.0055\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.0054\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.0056\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0055\t Average training accuracy 0.9998\n",
      "Epoch [17]\t Average validation loss 0.0785\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.0074\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.0061\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.0066\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.0060\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.0055\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.0055\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.0051\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.0049\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.0051\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.0050\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.0050\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.0051\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.0051\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.0051\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.0051\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.0051\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.0051\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.0050\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.0050\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.0048\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.0049\t Accuracy 0.9999\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0049\t Average training accuracy 0.9999\n",
      "Epoch [18]\t Average validation loss 0.0785\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.0065\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.0054\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.0058\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.0052\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.0048\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.0049\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.0045\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.0044\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.0044\t Accuracy 0.9998\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.0044\t Accuracy 0.9998\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.0042\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.0043\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0043\t Average training accuracy 0.9999\n",
      "Epoch [19]\t Average validation loss 0.0786\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.0057\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.0048\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.0051\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.0047\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.0043\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.0043\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.0039\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.0039\t Accuracy 0.9998\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.0041\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0039\t Average training accuracy 0.9999\n",
      "Epoch [20]\t Average validation loss 0.0786\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.0051\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.0043\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.0042\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.0039\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0035\t Average training accuracy 0.9999\n",
      "Epoch [21]\t Average validation loss 0.0788\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.0046\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.0039\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.0041\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.0033\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.0032\t Accuracy 0.9999\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0032\t Average training accuracy 0.9999\n",
      "Epoch [22]\t Average validation loss 0.0790\t Average validation accuracy 0.9816\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.0042\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0030\t Average training accuracy 0.9999\n",
      "Epoch [23]\t Average validation loss 0.0792\t Average validation accuracy 0.9820\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.0027\t Accuracy 0.9999\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0027\t Average training accuracy 1.0000\n",
      "Epoch [24]\t Average validation loss 0.0794\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0025\t Average training accuracy 1.0000\n",
      "Epoch [25]\t Average validation loss 0.0795\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0024\t Average training accuracy 1.0000\n",
      "Epoch [26]\t Average validation loss 0.0797\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0022\t Average training accuracy 1.0000\n",
      "Epoch [27]\t Average validation loss 0.0799\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0021\t Average training accuracy 1.0000\n",
      "Epoch [28]\t Average validation loss 0.0800\t Average validation accuracy 0.9826\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0020\t Average training accuracy 1.0000\n",
      "Epoch [29]\t Average validation loss 0.0802\t Average validation accuracy 0.9822\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9809.\n",
      "\n",
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 2.2058\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 1.9529\t Accuracy 0.3804\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 1.5848\t Accuracy 0.5248\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 1.3564\t Accuracy 0.6007\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 1.1676\t Accuracy 0.6597\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 1.0330\t Accuracy 0.7028\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 0.9391\t Accuracy 0.7309\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.8659\t Accuracy 0.7541\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.8102\t Accuracy 0.7688\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.7627\t Accuracy 0.7823\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.7230\t Accuracy 0.7932\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.6902\t Accuracy 0.8024\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.6616\t Accuracy 0.8100\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.6329\t Accuracy 0.8186\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.6141\t Accuracy 0.8225\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.5984\t Accuracy 0.8273\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.5835\t Accuracy 0.8318\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.5711\t Accuracy 0.8342\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.5627\t Accuracy 0.8365\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.5512\t Accuracy 0.8392\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.5361\t Accuracy 0.8439\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.5229\t Accuracy 0.8472\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.5090\t Accuracy 0.8510\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.4963\t Accuracy 0.8546\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.4898\t Accuracy 0.8564\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.4815\t Accuracy 0.8588\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.4760\t Accuracy 0.8610\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.4700\t Accuracy 0.8625\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.4638\t Accuracy 0.8640\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.4595\t Accuracy 0.8648\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.4559\t Accuracy 0.8657\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.4500\t Accuracy 0.8674\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.4447\t Accuracy 0.8690\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.4382\t Accuracy 0.8710\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.4340\t Accuracy 0.8720\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.4286\t Accuracy 0.8736\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.4254\t Accuracy 0.8747\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.4187\t Accuracy 0.8770\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.4134\t Accuracy 0.8789\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.4090\t Accuracy 0.8801\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.4037\t Accuracy 0.8817\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.3995\t Accuracy 0.8832\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.3975\t Accuracy 0.8839\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.3935\t Accuracy 0.8852\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.3889\t Accuracy 0.8866\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.3857\t Accuracy 0.8877\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.3824\t Accuracy 0.8883\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.3782\t Accuracy 0.8893\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.3760\t Accuracy 0.8899\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.3725\t Accuracy 0.8907\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.3702\t Accuracy 0.8911\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.3661\t Accuracy 0.8925\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.3630\t Accuracy 0.8933\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.3602\t Accuracy 0.8943\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.3589\t Accuracy 0.8947\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.3565\t Accuracy 0.8955\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.3532\t Accuracy 0.8964\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.3505\t Accuracy 0.8972\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.3479\t Accuracy 0.8981\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.3454\t Accuracy 0.8988\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.3431\t Accuracy 0.8993\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.3409\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.3388\t Accuracy 0.9004\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.3379\t Accuracy 0.9005\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.3362\t Accuracy 0.9011\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.3350\t Accuracy 0.9014\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.3324\t Accuracy 0.9022\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.3300\t Accuracy 0.9029\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.3268\t Accuracy 0.9039\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.3245\t Accuracy 0.9045\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.3233\t Accuracy 0.9048\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.3212\t Accuracy 0.9055\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.3187\t Accuracy 0.9062\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.3169\t Accuracy 0.9068\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.3145\t Accuracy 0.9074\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.3138\t Accuracy 0.9076\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.3125\t Accuracy 0.9081\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.3105\t Accuracy 0.9086\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.3085\t Accuracy 0.9093\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.3071\t Accuracy 0.9097\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.3055\t Accuracy 0.9101\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.3034\t Accuracy 0.9108\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.3015\t Accuracy 0.9113\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.3005\t Accuracy 0.9115\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.2990\t Accuracy 0.9119\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.2981\t Accuracy 0.9123\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.2971\t Accuracy 0.9126\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.2951\t Accuracy 0.9131\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.2936\t Accuracy 0.9136\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.2923\t Accuracy 0.9139\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.2906\t Accuracy 0.9143\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.2894\t Accuracy 0.9145\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.2885\t Accuracy 0.9148\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.2881\t Accuracy 0.9148\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.2863\t Accuracy 0.9153\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.2854\t Accuracy 0.9155\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.2844\t Accuracy 0.9158\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.2829\t Accuracy 0.9162\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.2818\t Accuracy 0.9165\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.2807\t Accuracy 0.9168\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.2802\t Accuracy 0.9170\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.2795\t Accuracy 0.9173\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.2783\t Accuracy 0.9178\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.2767\t Accuracy 0.9181\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.2754\t Accuracy 0.9185\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.2746\t Accuracy 0.9187\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.2742\t Accuracy 0.9188\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.2730\t Accuracy 0.9192\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.2723\t Accuracy 0.9193\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.2709\t Accuracy 0.9198\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2698\t Average training accuracy 0.9201\n",
      "Epoch [0]\t Average validation loss 0.1325\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.0296\t Accuracy 1.0000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.1369\t Accuracy 0.9667\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.1425\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.1612\t Accuracy 0.9576\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.1457\t Accuracy 0.9587\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.1342\t Accuracy 0.9614\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.1297\t Accuracy 0.9625\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.1244\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.1236\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.1224\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.1204\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.1193\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.1201\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.1186\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.1201\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.1216\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.1230\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.1245\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.1260\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.1271\t Accuracy 0.9632\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.1258\t Accuracy 0.9637\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.1263\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.1252\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.1233\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.1247\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.1236\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.1246\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.1241\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.1237\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.1240\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.1240\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.1243\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.1243\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.1237\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.1238\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.1235\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.1242\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.1232\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.1221\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.1224\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.1214\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.1216\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.1227\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.1222\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.1218\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.1221\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.1222\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.1215\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.1217\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.1208\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.1205\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.1196\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.1197\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.1196\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.1204\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.1206\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.1200\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.1196\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.1194\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.1190\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.1186\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.1183\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.1179\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.1180\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.1183\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.1186\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.1183\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.1181\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.1174\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.1171\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.1173\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.1171\t Accuracy 0.9652\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.1165\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.1166\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.1160\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.1166\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.1167\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.1162\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.1158\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.1160\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.1159\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.1153\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.1150\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.1152\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.1151\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.1154\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.1155\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.1149\t Accuracy 0.9659\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.1147\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.1146\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.1142\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.1142\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.1143\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.1146\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.1141\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.1143\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.1144\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.1141\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.1140\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.1139\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.1144\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.1144\t Accuracy 0.9660\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.1142\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.1138\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.1135\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.1136\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.1138\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.1137\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.1139\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.1135\t Accuracy 0.9663\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1133\t Average training accuracy 0.9663\n",
      "Epoch [1]\t Average validation loss 0.1000\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.0189\t Accuracy 1.0000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.0870\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.0889\t Accuracy 0.9752\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.1052\t Accuracy 0.9695\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.0940\t Accuracy 0.9731\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.0838\t Accuracy 0.9757\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.0818\t Accuracy 0.9761\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.0773\t Accuracy 0.9769\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.0762\t Accuracy 0.9771\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.0759\t Accuracy 0.9776\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.0747\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.0743\t Accuracy 0.9789\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.0750\t Accuracy 0.9790\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.0737\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.0753\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.0764\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.0779\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.0791\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.0801\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.0807\t Accuracy 0.9783\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.0792\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.0799\t Accuracy 0.9782\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.0795\t Accuracy 0.9781\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.0781\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.0791\t Accuracy 0.9781\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.0783\t Accuracy 0.9782\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.0788\t Accuracy 0.9781\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.0783\t Accuracy 0.9782\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.0782\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.0780\t Accuracy 0.9785\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.0777\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.0776\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.0776\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.0773\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.0774\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.0774\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.0781\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.0776\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.0768\t Accuracy 0.9789\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.0775\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.0767\t Accuracy 0.9789\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.0768\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.0775\t Accuracy 0.9785\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.0772\t Accuracy 0.9785\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.0771\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.0773\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.0772\t Accuracy 0.9783\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.0768\t Accuracy 0.9785\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.0771\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.0763\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.0760\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.0756\t Accuracy 0.9789\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.0758\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.0760\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.0767\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.0770\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.0767\t Accuracy 0.9787\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.0764\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.0763\t Accuracy 0.9788\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.0761\t Accuracy 0.9789\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.0757\t Accuracy 0.9790\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.0754\t Accuracy 0.9791\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.0751\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.0752\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.0754\t Accuracy 0.9791\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.0757\t Accuracy 0.9790\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.0756\t Accuracy 0.9790\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.0755\t Accuracy 0.9790\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.0751\t Accuracy 0.9791\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.0749\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.0750\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.0752\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.0748\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.0750\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.0746\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.0751\t Accuracy 0.9791\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.0752\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.0749\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.0748\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.0750\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.0749\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.0746\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.0745\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.0747\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.0746\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.0749\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.0751\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.0747\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.0746\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.0747\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.0743\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.0744\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.0745\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.0746\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.0744\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.0746\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.0747\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.0744\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.0744\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.0744\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.0749\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.0749\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.0748\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.0746\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.0745\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.0745\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.0747\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.0747\t Accuracy 0.9792\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.0748\t Accuracy 0.9791\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.0746\t Accuracy 0.9792\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0745\t Average training accuracy 0.9792\n",
      "Epoch [2]\t Average validation loss 0.0911\t Average validation accuracy 0.9732\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.0136\t Accuracy 1.0000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.0640\t Accuracy 0.9843\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.0619\t Accuracy 0.9802\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.0749\t Accuracy 0.9762\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.0675\t Accuracy 0.9801\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.0592\t Accuracy 0.9833\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.0576\t Accuracy 0.9834\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.0538\t Accuracy 0.9846\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.0525\t Accuracy 0.9845\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.0526\t Accuracy 0.9845\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.0520\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.0518\t Accuracy 0.9851\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.0524\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.0511\t Accuracy 0.9853\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.0528\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.0538\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.0550\t Accuracy 0.9846\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.0559\t Accuracy 0.9843\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.0561\t Accuracy 0.9844\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.0564\t Accuracy 0.9845\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.0550\t Accuracy 0.9851\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.0554\t Accuracy 0.9849\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.0550\t Accuracy 0.9847\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.0540\t Accuracy 0.9851\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.0548\t Accuracy 0.9848\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.0540\t Accuracy 0.9849\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.0544\t Accuracy 0.9849\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.0540\t Accuracy 0.9849\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.0538\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.0537\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.0533\t Accuracy 0.9852\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.0530\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.0529\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.0527\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.0528\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.0528\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.0534\t Accuracy 0.9853\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.0531\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.0525\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.0533\t Accuracy 0.9853\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.0527\t Accuracy 0.9856\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.0527\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.0530\t Accuracy 0.9852\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.0529\t Accuracy 0.9853\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.0528\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.0528\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.0527\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.0524\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.0527\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.0521\t Accuracy 0.9856\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.0518\t Accuracy 0.9857\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.0515\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.0518\t Accuracy 0.9857\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.0520\t Accuracy 0.9856\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.0527\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.0529\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.0529\t Accuracy 0.9854\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.0526\t Accuracy 0.9855\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.0525\t Accuracy 0.9856\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.0523\t Accuracy 0.9856\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.0518\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.0516\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.0516\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.0521\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.0519\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.0516\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.0516\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.0517\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.0517\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.0521\t Accuracy 0.9857\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.0523\t Accuracy 0.9857\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.0521\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.0522\t Accuracy 0.9857\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.0521\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.0518\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.0519\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.0521\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.0523\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.0521\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.0520\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.0521\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.0518\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.0518\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.0520\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.0518\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.0517\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.0517\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.0521\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.0520\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.0520\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.0519\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.0518\t Accuracy 0.9860\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.0520\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.0519\t Accuracy 0.9859\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.0520\t Accuracy 0.9858\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.0519\t Accuracy 0.9858\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0518\t Average training accuracy 0.9859\n",
      "Epoch [3]\t Average validation loss 0.0860\t Average validation accuracy 0.9758\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.0102\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.0508\t Accuracy 0.9902\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.0439\t Accuracy 0.9901\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.0541\t Accuracy 0.9861\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.0494\t Accuracy 0.9886\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.0431\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.0415\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.0386\t Accuracy 0.9920\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.0375\t Accuracy 0.9915\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.0377\t Accuracy 0.9916\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.0374\t Accuracy 0.9916\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.0375\t Accuracy 0.9913\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.0382\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.0369\t Accuracy 0.9911\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.0384\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.0393\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.0400\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.0407\t Accuracy 0.9902\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.0405\t Accuracy 0.9900\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.0406\t Accuracy 0.9902\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.0395\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.0398\t Accuracy 0.9903\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.0396\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.0388\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.0394\t Accuracy 0.9902\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.0388\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.0390\t Accuracy 0.9902\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.0386\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.0383\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.0382\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.0379\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.0376\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.0374\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.0372\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.0371\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.0370\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.0375\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.0374\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.0369\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.0376\t Accuracy 0.9904\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.0371\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.0371\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.0373\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.0371\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.0370\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.0370\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.0369\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.0367\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.0370\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.0366\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.0363\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.0362\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.0364\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.0365\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.0371\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.0373\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.0374\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.0372\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.0370\t Accuracy 0.9905\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.0369\t Accuracy 0.9906\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.0367\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.0364\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.0363\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.0364\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.0366\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.0365\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.0363\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.0364\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.0365\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.0367\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.0368\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.0366\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.0368\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.0370\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.0368\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.0368\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.0369\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.0369\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.0367\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.0367\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.0369\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.0370\t Accuracy 0.9907\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.0369\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.0368\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.0369\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.0367\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.0367\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.0367\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.0367\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.0368\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.0368\t Accuracy 0.9909\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.0368\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.0367\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.0366\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.0365\t Accuracy 0.9910\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0364\t Average training accuracy 0.9910\n",
      "Epoch [4]\t Average validation loss 0.0841\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.0074\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.0409\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.0318\t Accuracy 0.9950\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.0386\t Accuracy 0.9927\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.0358\t Accuracy 0.9930\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.0312\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.0299\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.0277\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.0267\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.0271\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.0269\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.0270\t Accuracy 0.9942\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.0276\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.0266\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.0278\t Accuracy 0.9942\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.0285\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.0290\t Accuracy 0.9936\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.0297\t Accuracy 0.9932\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.0294\t Accuracy 0.9930\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.0292\t Accuracy 0.9932\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.0284\t Accuracy 0.9934\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.0284\t Accuracy 0.9933\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.0281\t Accuracy 0.9935\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.0275\t Accuracy 0.9937\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.0279\t Accuracy 0.9937\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.0275\t Accuracy 0.9939\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.0276\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.0273\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.0270\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.0268\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.0267\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.0264\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.0263\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.0260\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.0260\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.0258\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.0262\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.0260\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.0257\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.0260\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.0257\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.0257\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.0257\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.0256\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.0256\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.0256\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.0255\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.0254\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.0257\t Accuracy 0.9942\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.0254\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.0252\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.0251\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.0254\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.0254\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.0259\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.0262\t Accuracy 0.9942\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.0262\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.0260\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.0259\t Accuracy 0.9943\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.0258\t Accuracy 0.9944\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.0256\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.0254\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.0253\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.0254\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.0255\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.0255\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.0253\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.0254\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.0255\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.0257\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.0258\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.0257\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.0258\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.0259\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.0258\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.0259\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.0259\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.0259\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.0258\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.0259\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.0258\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.0259\t Accuracy 0.9945\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.0258\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.0256\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.0257\t Accuracy 0.9946\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.0256\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.0255\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.0254\t Accuracy 0.9947\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.0254\t Accuracy 0.9947\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0253\t Average training accuracy 0.9947\n",
      "Epoch [5]\t Average validation loss 0.0819\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.0059\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.0321\t Accuracy 0.9941\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.0233\t Accuracy 0.9960\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.0278\t Accuracy 0.9947\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.0257\t Accuracy 0.9945\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.0223\t Accuracy 0.9952\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.0213\t Accuracy 0.9953\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.0196\t Accuracy 0.9960\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.0187\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.0194\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.0190\t Accuracy 0.9966\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.0193\t Accuracy 0.9966\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.0199\t Accuracy 0.9963\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.0191\t Accuracy 0.9966\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.0200\t Accuracy 0.9964\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.0205\t Accuracy 0.9963\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.0207\t Accuracy 0.9961\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.0213\t Accuracy 0.9957\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.0210\t Accuracy 0.9959\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.0207\t Accuracy 0.9960\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.0202\t Accuracy 0.9962\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.0201\t Accuracy 0.9962\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.0198\t Accuracy 0.9963\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.0194\t Accuracy 0.9964\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.0198\t Accuracy 0.9963\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.0194\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.0195\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.0193\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.0190\t Accuracy 0.9966\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.0189\t Accuracy 0.9967\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.0188\t Accuracy 0.9967\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.0185\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.0184\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.0182\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.0182\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.0180\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.0183\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.0182\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.0179\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.0179\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.0179\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.0179\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.0177\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.0178\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.0178\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.0177\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.0176\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.0178\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.0176\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.0175\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.0174\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.0176\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.0177\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.0181\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.0183\t Accuracy 0.9967\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.0183\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.0182\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.0181\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.0178\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.0177\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.0177\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.0178\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.0176\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.0176\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.0178\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.0180\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.0180\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.0182\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.0181\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.0180\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.0180\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.0180\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.0180\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.0181\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.0181\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.0179\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.0179\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.0179\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.0180\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.0179\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.0179\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.0178\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.0179\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.0178\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.0177\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.0176\t Accuracy 0.9969\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.0176\t Accuracy 0.9969\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0176\t Average training accuracy 0.9969\n",
      "Epoch [6]\t Average validation loss 0.0806\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.0229\t Accuracy 0.9941\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.0167\t Accuracy 0.9970\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.0196\t Accuracy 0.9967\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.0180\t Accuracy 0.9970\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.0155\t Accuracy 0.9972\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.0147\t Accuracy 0.9973\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.0135\t Accuracy 0.9977\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.0128\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.0137\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.0133\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.0136\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.0141\t Accuracy 0.9977\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.0135\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.0140\t Accuracy 0.9976\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.0144\t Accuracy 0.9973\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.0144\t Accuracy 0.9973\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.0148\t Accuracy 0.9972\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.0146\t Accuracy 0.9973\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.0144\t Accuracy 0.9975\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.0141\t Accuracy 0.9976\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.0139\t Accuracy 0.9976\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.0138\t Accuracy 0.9976\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.0135\t Accuracy 0.9977\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.0137\t Accuracy 0.9977\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.0135\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.0135\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.0133\t Accuracy 0.9978\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.0132\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.0131\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.0130\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.0128\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.0127\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.0126\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.0127\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.0126\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.0125\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.0124\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.0123\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.0123\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.0123\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.0122\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.0122\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.0123\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.0122\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.0121\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.0121\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.0122\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.0123\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.0126\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.0128\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.0128\t Accuracy 0.9979\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.0127\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.0126\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.0126\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.0124\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.0124\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.0124\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.0124\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.0123\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.0123\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.0125\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.0128\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.0127\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.0126\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.0126\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.0126\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.0126\t Accuracy 0.9981\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.0126\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.0124\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.0125\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.0124\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.0124\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.0124\t Accuracy 0.9982\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.0123\t Accuracy 0.9983\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0123\t Average training accuracy 0.9983\n",
      "Epoch [7]\t Average validation loss 0.0800\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.0160\t Accuracy 0.9961\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.0120\t Accuracy 0.9980\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.0137\t Accuracy 0.9980\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.0122\t Accuracy 0.9985\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.0106\t Accuracy 0.9988\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.0103\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.0095\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.0090\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.0095\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.0098\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.0101\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.0097\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.0099\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.0100\t Accuracy 0.9989\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.0100\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.0103\t Accuracy 0.9989\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.0102\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.0101\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.0099\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.0098\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.0096\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.0095\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.0096\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.0094\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.0094\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.0093\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.0091\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.0091\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.0090\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.0089\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.0088\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.0087\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.0087\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.0086\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.0085\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.0084\t Accuracy 0.9993\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.0085\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.0084\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.0084\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.0086\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.0090\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.0090\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.0088\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.0087\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.0087\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.0086\t Accuracy 0.9992\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.0090\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.0090\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.0090\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.0090\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.0089\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.0088\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.0089\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.0088\t Accuracy 0.9990\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.0088\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.0087\t Accuracy 0.9991\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.0086\t Accuracy 0.9991\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0086\t Average training accuracy 0.9991\n",
      "Epoch [8]\t Average validation loss 0.0806\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.0108\t Accuracy 0.9961\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.0085\t Accuracy 0.9980\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.0096\t Accuracy 0.9987\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.0084\t Accuracy 0.9990\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.0073\t Accuracy 0.9992\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.0072\t Accuracy 0.9993\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.0066\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.0072\t Accuracy 0.9993\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.0069\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.0071\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.0072\t Accuracy 0.9993\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.0069\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.0071\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.0070\t Accuracy 0.9994\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.0070\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.0068\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.0068\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.0067\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.0066\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.0067\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.0065\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.0065\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.0064\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.0064\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.0063\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.0063\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.0060\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.0060\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.0060\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.0060\t Accuracy 0.9997\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.0059\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.0060\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.0059\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.0059\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.0059\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.0060\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.0061\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.0061\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.0062\t Accuracy 0.9996\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.0063\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.0062\t Accuracy 0.9995\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.0061\t Accuracy 0.9995\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0061\t Average training accuracy 0.9995\n",
      "Epoch [9]\t Average validation loss 0.0809\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.0072\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.0063\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.0071\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.0063\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.0055\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.0054\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.0050\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.0047\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.0055\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.0053\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.0054\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.0053\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.0053\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.0052\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.0053\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.0052\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.0052\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.0051\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.0050\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.0050\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.0050\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.0049\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.0048\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.0048\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.0047\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.0046\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.0045\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.0044\t Accuracy 0.9999\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.0045\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.0046\t Accuracy 0.9997\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.0046\t Accuracy 0.9998\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0046\t Average training accuracy 0.9998\n",
      "Epoch [10]\t Average validation loss 0.0810\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.0051\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.0049\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.0054\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.0048\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.0043\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.0042\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.0036\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.0042\t Accuracy 0.9998\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.0041\t Accuracy 0.9998\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.0041\t Accuracy 0.9998\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.0040\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.0036\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.0035\t Accuracy 0.9999\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0035\t Average training accuracy 0.9999\n",
      "Epoch [11]\t Average validation loss 0.0811\t Average validation accuracy 0.9818\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.0040\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.0043\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.0033\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.0030\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.0032\t Accuracy 0.9998\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.0030\t Accuracy 0.9998\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.0031\t Accuracy 0.9998\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.0031\t Accuracy 0.9998\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.0031\t Accuracy 0.9998\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.0031\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.0030\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.0029\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.0028\t Accuracy 0.9999\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.0029\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0028\t Average training accuracy 1.0000\n",
      "Epoch [12]\t Average validation loss 0.0815\t Average validation accuracy 0.9818\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0023\t Average training accuracy 1.0000\n",
      "Epoch [13]\t Average validation loss 0.0818\t Average validation accuracy 0.9824\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.0025\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.0022\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0020\t Average training accuracy 1.0000\n",
      "Epoch [14]\t Average validation loss 0.0823\t Average validation accuracy 0.9828\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0017\t Average training accuracy 1.0000\n",
      "Epoch [15]\t Average validation loss 0.0827\t Average validation accuracy 0.9832\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0015\t Average training accuracy 1.0000\n",
      "Epoch [16]\t Average validation loss 0.0832\t Average validation accuracy 0.9834\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0014\t Average training accuracy 1.0000\n",
      "Epoch [17]\t Average validation loss 0.0837\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0012\t Average training accuracy 1.0000\n",
      "Epoch [18]\t Average validation loss 0.0841\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0011\t Average training accuracy 1.0000\n",
      "Epoch [19]\t Average validation loss 0.0846\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [20]\t Average validation loss 0.0851\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0009\t Average training accuracy 1.0000\n",
      "Epoch [21]\t Average validation loss 0.0856\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0009\t Average training accuracy 1.0000\n",
      "Epoch [22]\t Average validation loss 0.0860\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0008\t Average training accuracy 1.0000\n",
      "Epoch [23]\t Average validation loss 0.0864\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0008\t Average training accuracy 1.0000\n",
      "Epoch [24]\t Average validation loss 0.0868\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0007\t Average training accuracy 1.0000\n",
      "Epoch [25]\t Average validation loss 0.0871\t Average validation accuracy 0.9834\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0007\t Average training accuracy 1.0000\n",
      "Epoch [26]\t Average validation loss 0.0875\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0006\t Average training accuracy 1.0000\n",
      "Epoch [27]\t Average validation loss 0.0878\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0006\t Average training accuracy 1.0000\n",
      "Epoch [28]\t Average validation loss 0.0882\t Average validation accuracy 0.9836\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0006\t Average training accuracy 1.0000\n",
      "Epoch [29]\t Average validation loss 0.0885\t Average validation accuracy 0.9838\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9802.\n",
      "\n",
      "Epoch [0][30]\t Batch [0][5500]\t Training Loss 2.6454\t Accuracy 0.2000\n",
      "Epoch [0][30]\t Batch [50][5500]\t Training Loss 1.9295\t Accuracy 0.4020\n",
      "Epoch [0][30]\t Batch [100][5500]\t Training Loss 1.5728\t Accuracy 0.5376\n",
      "Epoch [0][30]\t Batch [150][5500]\t Training Loss 1.3232\t Accuracy 0.6139\n",
      "Epoch [0][30]\t Batch [200][5500]\t Training Loss 1.1427\t Accuracy 0.6701\n",
      "Epoch [0][30]\t Batch [250][5500]\t Training Loss 1.0041\t Accuracy 0.7116\n",
      "Epoch [0][30]\t Batch [300][5500]\t Training Loss 0.9093\t Accuracy 0.7405\n",
      "Epoch [0][30]\t Batch [350][5500]\t Training Loss 0.8306\t Accuracy 0.7650\n",
      "Epoch [0][30]\t Batch [400][5500]\t Training Loss 0.7783\t Accuracy 0.7791\n",
      "Epoch [0][30]\t Batch [450][5500]\t Training Loss 0.7349\t Accuracy 0.7914\n",
      "Epoch [0][30]\t Batch [500][5500]\t Training Loss 0.6962\t Accuracy 0.8008\n",
      "Epoch [0][30]\t Batch [550][5500]\t Training Loss 0.6621\t Accuracy 0.8105\n",
      "Epoch [0][30]\t Batch [600][5500]\t Training Loss 0.6348\t Accuracy 0.8181\n",
      "Epoch [0][30]\t Batch [650][5500]\t Training Loss 0.6038\t Accuracy 0.8272\n",
      "Epoch [0][30]\t Batch [700][5500]\t Training Loss 0.5864\t Accuracy 0.8321\n",
      "Epoch [0][30]\t Batch [750][5500]\t Training Loss 0.5717\t Accuracy 0.8364\n",
      "Epoch [0][30]\t Batch [800][5500]\t Training Loss 0.5555\t Accuracy 0.8409\n",
      "Epoch [0][30]\t Batch [850][5500]\t Training Loss 0.5445\t Accuracy 0.8441\n",
      "Epoch [0][30]\t Batch [900][5500]\t Training Loss 0.5398\t Accuracy 0.8450\n",
      "Epoch [0][30]\t Batch [950][5500]\t Training Loss 0.5303\t Accuracy 0.8478\n",
      "Epoch [0][30]\t Batch [1000][5500]\t Training Loss 0.5156\t Accuracy 0.8523\n",
      "Epoch [0][30]\t Batch [1050][5500]\t Training Loss 0.5046\t Accuracy 0.8553\n",
      "Epoch [0][30]\t Batch [1100][5500]\t Training Loss 0.4911\t Accuracy 0.8591\n",
      "Epoch [0][30]\t Batch [1150][5500]\t Training Loss 0.4783\t Accuracy 0.8627\n",
      "Epoch [0][30]\t Batch [1200][5500]\t Training Loss 0.4725\t Accuracy 0.8641\n",
      "Epoch [0][30]\t Batch [1250][5500]\t Training Loss 0.4648\t Accuracy 0.8659\n",
      "Epoch [0][30]\t Batch [1300][5500]\t Training Loss 0.4608\t Accuracy 0.8664\n",
      "Epoch [0][30]\t Batch [1350][5500]\t Training Loss 0.4547\t Accuracy 0.8679\n",
      "Epoch [0][30]\t Batch [1400][5500]\t Training Loss 0.4497\t Accuracy 0.8695\n",
      "Epoch [0][30]\t Batch [1450][5500]\t Training Loss 0.4451\t Accuracy 0.8707\n",
      "Epoch [0][30]\t Batch [1500][5500]\t Training Loss 0.4409\t Accuracy 0.8714\n",
      "Epoch [0][30]\t Batch [1550][5500]\t Training Loss 0.4345\t Accuracy 0.8735\n",
      "Epoch [0][30]\t Batch [1600][5500]\t Training Loss 0.4300\t Accuracy 0.8742\n",
      "Epoch [0][30]\t Batch [1650][5500]\t Training Loss 0.4232\t Accuracy 0.8761\n",
      "Epoch [0][30]\t Batch [1700][5500]\t Training Loss 0.4197\t Accuracy 0.8770\n",
      "Epoch [0][30]\t Batch [1750][5500]\t Training Loss 0.4143\t Accuracy 0.8787\n",
      "Epoch [0][30]\t Batch [1800][5500]\t Training Loss 0.4105\t Accuracy 0.8797\n",
      "Epoch [0][30]\t Batch [1850][5500]\t Training Loss 0.4043\t Accuracy 0.8815\n",
      "Epoch [0][30]\t Batch [1900][5500]\t Training Loss 0.3986\t Accuracy 0.8830\n",
      "Epoch [0][30]\t Batch [1950][5500]\t Training Loss 0.3941\t Accuracy 0.8844\n",
      "Epoch [0][30]\t Batch [2000][5500]\t Training Loss 0.3891\t Accuracy 0.8859\n",
      "Epoch [0][30]\t Batch [2050][5500]\t Training Loss 0.3851\t Accuracy 0.8871\n",
      "Epoch [0][30]\t Batch [2100][5500]\t Training Loss 0.3827\t Accuracy 0.8880\n",
      "Epoch [0][30]\t Batch [2150][5500]\t Training Loss 0.3780\t Accuracy 0.8893\n",
      "Epoch [0][30]\t Batch [2200][5500]\t Training Loss 0.3734\t Accuracy 0.8905\n",
      "Epoch [0][30]\t Batch [2250][5500]\t Training Loss 0.3705\t Accuracy 0.8913\n",
      "Epoch [0][30]\t Batch [2300][5500]\t Training Loss 0.3679\t Accuracy 0.8920\n",
      "Epoch [0][30]\t Batch [2350][5500]\t Training Loss 0.3640\t Accuracy 0.8930\n",
      "Epoch [0][30]\t Batch [2400][5500]\t Training Loss 0.3614\t Accuracy 0.8937\n",
      "Epoch [0][30]\t Batch [2450][5500]\t Training Loss 0.3578\t Accuracy 0.8947\n",
      "Epoch [0][30]\t Batch [2500][5500]\t Training Loss 0.3556\t Accuracy 0.8954\n",
      "Epoch [0][30]\t Batch [2550][5500]\t Training Loss 0.3520\t Accuracy 0.8966\n",
      "Epoch [0][30]\t Batch [2600][5500]\t Training Loss 0.3490\t Accuracy 0.8975\n",
      "Epoch [0][30]\t Batch [2650][5500]\t Training Loss 0.3463\t Accuracy 0.8983\n",
      "Epoch [0][30]\t Batch [2700][5500]\t Training Loss 0.3447\t Accuracy 0.8986\n",
      "Epoch [0][30]\t Batch [2750][5500]\t Training Loss 0.3425\t Accuracy 0.8993\n",
      "Epoch [0][30]\t Batch [2800][5500]\t Training Loss 0.3393\t Accuracy 0.9001\n",
      "Epoch [0][30]\t Batch [2850][5500]\t Training Loss 0.3365\t Accuracy 0.9009\n",
      "Epoch [0][30]\t Batch [2900][5500]\t Training Loss 0.3340\t Accuracy 0.9016\n",
      "Epoch [0][30]\t Batch [2950][5500]\t Training Loss 0.3321\t Accuracy 0.9020\n",
      "Epoch [0][30]\t Batch [3000][5500]\t Training Loss 0.3300\t Accuracy 0.9026\n",
      "Epoch [0][30]\t Batch [3050][5500]\t Training Loss 0.3272\t Accuracy 0.9034\n",
      "Epoch [0][30]\t Batch [3100][5500]\t Training Loss 0.3244\t Accuracy 0.9042\n",
      "Epoch [0][30]\t Batch [3150][5500]\t Training Loss 0.3234\t Accuracy 0.9044\n",
      "Epoch [0][30]\t Batch [3200][5500]\t Training Loss 0.3217\t Accuracy 0.9049\n",
      "Epoch [0][30]\t Batch [3250][5500]\t Training Loss 0.3202\t Accuracy 0.9053\n",
      "Epoch [0][30]\t Batch [3300][5500]\t Training Loss 0.3178\t Accuracy 0.9061\n",
      "Epoch [0][30]\t Batch [3350][5500]\t Training Loss 0.3153\t Accuracy 0.9070\n",
      "Epoch [0][30]\t Batch [3400][5500]\t Training Loss 0.3128\t Accuracy 0.9078\n",
      "Epoch [0][30]\t Batch [3450][5500]\t Training Loss 0.3106\t Accuracy 0.9085\n",
      "Epoch [0][30]\t Batch [3500][5500]\t Training Loss 0.3093\t Accuracy 0.9089\n",
      "Epoch [0][30]\t Batch [3550][5500]\t Training Loss 0.3075\t Accuracy 0.9095\n",
      "Epoch [0][30]\t Batch [3600][5500]\t Training Loss 0.3054\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [3650][5500]\t Training Loss 0.3037\t Accuracy 0.9105\n",
      "Epoch [0][30]\t Batch [3700][5500]\t Training Loss 0.3011\t Accuracy 0.9113\n",
      "Epoch [0][30]\t Batch [3750][5500]\t Training Loss 0.3011\t Accuracy 0.9113\n",
      "Epoch [0][30]\t Batch [3800][5500]\t Training Loss 0.2999\t Accuracy 0.9116\n",
      "Epoch [0][30]\t Batch [3850][5500]\t Training Loss 0.2978\t Accuracy 0.9123\n",
      "Epoch [0][30]\t Batch [3900][5500]\t Training Loss 0.2959\t Accuracy 0.9129\n",
      "Epoch [0][30]\t Batch [3950][5500]\t Training Loss 0.2944\t Accuracy 0.9135\n",
      "Epoch [0][30]\t Batch [4000][5500]\t Training Loss 0.2927\t Accuracy 0.9140\n",
      "Epoch [0][30]\t Batch [4050][5500]\t Training Loss 0.2908\t Accuracy 0.9145\n",
      "Epoch [0][30]\t Batch [4100][5500]\t Training Loss 0.2893\t Accuracy 0.9150\n",
      "Epoch [0][30]\t Batch [4150][5500]\t Training Loss 0.2887\t Accuracy 0.9151\n",
      "Epoch [0][30]\t Batch [4200][5500]\t Training Loss 0.2872\t Accuracy 0.9155\n",
      "Epoch [0][30]\t Batch [4250][5500]\t Training Loss 0.2862\t Accuracy 0.9158\n",
      "Epoch [0][30]\t Batch [4300][5500]\t Training Loss 0.2853\t Accuracy 0.9162\n",
      "Epoch [0][30]\t Batch [4350][5500]\t Training Loss 0.2832\t Accuracy 0.9168\n",
      "Epoch [0][30]\t Batch [4400][5500]\t Training Loss 0.2817\t Accuracy 0.9173\n",
      "Epoch [0][30]\t Batch [4450][5500]\t Training Loss 0.2805\t Accuracy 0.9177\n",
      "Epoch [0][30]\t Batch [4500][5500]\t Training Loss 0.2787\t Accuracy 0.9182\n",
      "Epoch [0][30]\t Batch [4550][5500]\t Training Loss 0.2775\t Accuracy 0.9185\n",
      "Epoch [0][30]\t Batch [4600][5500]\t Training Loss 0.2770\t Accuracy 0.9186\n",
      "Epoch [0][30]\t Batch [4650][5500]\t Training Loss 0.2768\t Accuracy 0.9186\n",
      "Epoch [0][30]\t Batch [4700][5500]\t Training Loss 0.2750\t Accuracy 0.9191\n",
      "Epoch [0][30]\t Batch [4750][5500]\t Training Loss 0.2741\t Accuracy 0.9194\n",
      "Epoch [0][30]\t Batch [4800][5500]\t Training Loss 0.2734\t Accuracy 0.9195\n",
      "Epoch [0][30]\t Batch [4850][5500]\t Training Loss 0.2718\t Accuracy 0.9199\n",
      "Epoch [0][30]\t Batch [4900][5500]\t Training Loss 0.2710\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [4950][5500]\t Training Loss 0.2701\t Accuracy 0.9201\n",
      "Epoch [0][30]\t Batch [5000][5500]\t Training Loss 0.2699\t Accuracy 0.9203\n",
      "Epoch [0][30]\t Batch [5050][5500]\t Training Loss 0.2693\t Accuracy 0.9204\n",
      "Epoch [0][30]\t Batch [5100][5500]\t Training Loss 0.2681\t Accuracy 0.9209\n",
      "Epoch [0][30]\t Batch [5150][5500]\t Training Loss 0.2669\t Accuracy 0.9212\n",
      "Epoch [0][30]\t Batch [5200][5500]\t Training Loss 0.2657\t Accuracy 0.9216\n",
      "Epoch [0][30]\t Batch [5250][5500]\t Training Loss 0.2646\t Accuracy 0.9219\n",
      "Epoch [0][30]\t Batch [5300][5500]\t Training Loss 0.2640\t Accuracy 0.9221\n",
      "Epoch [0][30]\t Batch [5350][5500]\t Training Loss 0.2626\t Accuracy 0.9226\n",
      "Epoch [0][30]\t Batch [5400][5500]\t Training Loss 0.2617\t Accuracy 0.9229\n",
      "Epoch [0][30]\t Batch [5450][5500]\t Training Loss 0.2604\t Accuracy 0.9234\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2594\t Average training accuracy 0.9237\n",
      "Epoch [0]\t Average validation loss 0.1403\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [1][30]\t Batch [0][5500]\t Training Loss 0.0399\t Accuracy 1.0000\n",
      "Epoch [1][30]\t Batch [50][5500]\t Training Loss 0.1322\t Accuracy 0.9667\n",
      "Epoch [1][30]\t Batch [100][5500]\t Training Loss 0.1337\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [150][5500]\t Training Loss 0.1542\t Accuracy 0.9536\n",
      "Epoch [1][30]\t Batch [200][5500]\t Training Loss 0.1383\t Accuracy 0.9572\n",
      "Epoch [1][30]\t Batch [250][5500]\t Training Loss 0.1273\t Accuracy 0.9602\n",
      "Epoch [1][30]\t Batch [300][5500]\t Training Loss 0.1243\t Accuracy 0.9615\n",
      "Epoch [1][30]\t Batch [350][5500]\t Training Loss 0.1173\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [400][5500]\t Training Loss 0.1149\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [450][5500]\t Training Loss 0.1146\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [500][5500]\t Training Loss 0.1134\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [550][5500]\t Training Loss 0.1112\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [600][5500]\t Training Loss 0.1115\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [650][5500]\t Training Loss 0.1089\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [700][5500]\t Training Loss 0.1110\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [750][5500]\t Training Loss 0.1129\t Accuracy 0.9658\n",
      "Epoch [1][30]\t Batch [800][5500]\t Training Loss 0.1142\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [850][5500]\t Training Loss 0.1164\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [900][5500]\t Training Loss 0.1196\t Accuracy 0.9639\n",
      "Epoch [1][30]\t Batch [950][5500]\t Training Loss 0.1204\t Accuracy 0.9639\n",
      "Epoch [1][30]\t Batch [1000][5500]\t Training Loss 0.1185\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [1050][5500]\t Training Loss 0.1201\t Accuracy 0.9640\n",
      "Epoch [1][30]\t Batch [1100][5500]\t Training Loss 0.1190\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1150][5500]\t Training Loss 0.1171\t Accuracy 0.9647\n",
      "Epoch [1][30]\t Batch [1200][5500]\t Training Loss 0.1180\t Accuracy 0.9643\n",
      "Epoch [1][30]\t Batch [1250][5500]\t Training Loss 0.1169\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [1300][5500]\t Training Loss 0.1184\t Accuracy 0.9641\n",
      "Epoch [1][30]\t Batch [1350][5500]\t Training Loss 0.1180\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1400][5500]\t Training Loss 0.1181\t Accuracy 0.9642\n",
      "Epoch [1][30]\t Batch [1450][5500]\t Training Loss 0.1186\t Accuracy 0.9640\n",
      "Epoch [1][30]\t Batch [1500][5500]\t Training Loss 0.1188\t Accuracy 0.9638\n",
      "Epoch [1][30]\t Batch [1550][5500]\t Training Loss 0.1191\t Accuracy 0.9636\n",
      "Epoch [1][30]\t Batch [1600][5500]\t Training Loss 0.1192\t Accuracy 0.9636\n",
      "Epoch [1][30]\t Batch [1650][5500]\t Training Loss 0.1183\t Accuracy 0.9639\n",
      "Epoch [1][30]\t Batch [1700][5500]\t Training Loss 0.1191\t Accuracy 0.9637\n",
      "Epoch [1][30]\t Batch [1750][5500]\t Training Loss 0.1189\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [1800][5500]\t Training Loss 0.1192\t Accuracy 0.9634\n",
      "Epoch [1][30]\t Batch [1850][5500]\t Training Loss 0.1185\t Accuracy 0.9636\n",
      "Epoch [1][30]\t Batch [1900][5500]\t Training Loss 0.1171\t Accuracy 0.9640\n",
      "Epoch [1][30]\t Batch [1950][5500]\t Training Loss 0.1171\t Accuracy 0.9640\n",
      "Epoch [1][30]\t Batch [2000][5500]\t Training Loss 0.1162\t Accuracy 0.9644\n",
      "Epoch [1][30]\t Batch [2050][5500]\t Training Loss 0.1156\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2100][5500]\t Training Loss 0.1158\t Accuracy 0.9645\n",
      "Epoch [1][30]\t Batch [2150][5500]\t Training Loss 0.1151\t Accuracy 0.9646\n",
      "Epoch [1][30]\t Batch [2200][5500]\t Training Loss 0.1142\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [2250][5500]\t Training Loss 0.1143\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [2300][5500]\t Training Loss 0.1143\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [2350][5500]\t Training Loss 0.1138\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [2400][5500]\t Training Loss 0.1137\t Accuracy 0.9648\n",
      "Epoch [1][30]\t Batch [2450][5500]\t Training Loss 0.1129\t Accuracy 0.9650\n",
      "Epoch [1][30]\t Batch [2500][5500]\t Training Loss 0.1126\t Accuracy 0.9651\n",
      "Epoch [1][30]\t Batch [2550][5500]\t Training Loss 0.1117\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [2600][5500]\t Training Loss 0.1117\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [2650][5500]\t Training Loss 0.1114\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [2700][5500]\t Training Loss 0.1122\t Accuracy 0.9654\n",
      "Epoch [1][30]\t Batch [2750][5500]\t Training Loss 0.1125\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [2800][5500]\t Training Loss 0.1119\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [2850][5500]\t Training Loss 0.1116\t Accuracy 0.9653\n",
      "Epoch [1][30]\t Batch [2900][5500]\t Training Loss 0.1114\t Accuracy 0.9655\n",
      "Epoch [1][30]\t Batch [2950][5500]\t Training Loss 0.1111\t Accuracy 0.9656\n",
      "Epoch [1][30]\t Batch [3000][5500]\t Training Loss 0.1106\t Accuracy 0.9657\n",
      "Epoch [1][30]\t Batch [3050][5500]\t Training Loss 0.1101\t Accuracy 0.9659\n",
      "Epoch [1][30]\t Batch [3100][5500]\t Training Loss 0.1094\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [3150][5500]\t Training Loss 0.1095\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [3200][5500]\t Training Loss 0.1098\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [3250][5500]\t Training Loss 0.1098\t Accuracy 0.9661\n",
      "Epoch [1][30]\t Batch [3300][5500]\t Training Loss 0.1098\t Accuracy 0.9662\n",
      "Epoch [1][30]\t Batch [3350][5500]\t Training Loss 0.1094\t Accuracy 0.9663\n",
      "Epoch [1][30]\t Batch [3400][5500]\t Training Loss 0.1091\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [3450][5500]\t Training Loss 0.1090\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [3500][5500]\t Training Loss 0.1092\t Accuracy 0.9664\n",
      "Epoch [1][30]\t Batch [3550][5500]\t Training Loss 0.1092\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [3600][5500]\t Training Loss 0.1088\t Accuracy 0.9667\n",
      "Epoch [1][30]\t Batch [3650][5500]\t Training Loss 0.1088\t Accuracy 0.9667\n",
      "Epoch [1][30]\t Batch [3700][5500]\t Training Loss 0.1081\t Accuracy 0.9668\n",
      "Epoch [1][30]\t Batch [3750][5500]\t Training Loss 0.1091\t Accuracy 0.9665\n",
      "Epoch [1][30]\t Batch [3800][5500]\t Training Loss 0.1090\t Accuracy 0.9666\n",
      "Epoch [1][30]\t Batch [3850][5500]\t Training Loss 0.1084\t Accuracy 0.9668\n",
      "Epoch [1][30]\t Batch [3900][5500]\t Training Loss 0.1080\t Accuracy 0.9669\n",
      "Epoch [1][30]\t Batch [3950][5500]\t Training Loss 0.1081\t Accuracy 0.9669\n",
      "Epoch [1][30]\t Batch [4000][5500]\t Training Loss 0.1080\t Accuracy 0.9670\n",
      "Epoch [1][30]\t Batch [4050][5500]\t Training Loss 0.1076\t Accuracy 0.9671\n",
      "Epoch [1][30]\t Batch [4100][5500]\t Training Loss 0.1074\t Accuracy 0.9672\n",
      "Epoch [1][30]\t Batch [4150][5500]\t Training Loss 0.1079\t Accuracy 0.9670\n",
      "Epoch [1][30]\t Batch [4200][5500]\t Training Loss 0.1078\t Accuracy 0.9670\n",
      "Epoch [1][30]\t Batch [4250][5500]\t Training Loss 0.1079\t Accuracy 0.9670\n",
      "Epoch [1][30]\t Batch [4300][5500]\t Training Loss 0.1080\t Accuracy 0.9670\n",
      "Epoch [1][30]\t Batch [4350][5500]\t Training Loss 0.1075\t Accuracy 0.9672\n",
      "Epoch [1][30]\t Batch [4400][5500]\t Training Loss 0.1074\t Accuracy 0.9673\n",
      "Epoch [1][30]\t Batch [4450][5500]\t Training Loss 0.1073\t Accuracy 0.9674\n",
      "Epoch [1][30]\t Batch [4500][5500]\t Training Loss 0.1069\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [4550][5500]\t Training Loss 0.1067\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [4600][5500]\t Training Loss 0.1069\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [4650][5500]\t Training Loss 0.1072\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [4700][5500]\t Training Loss 0.1067\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [4750][5500]\t Training Loss 0.1068\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [4800][5500]\t Training Loss 0.1070\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [4850][5500]\t Training Loss 0.1066\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [4900][5500]\t Training Loss 0.1066\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [4950][5500]\t Training Loss 0.1067\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [5000][5500]\t Training Loss 0.1074\t Accuracy 0.9674\n",
      "Epoch [1][30]\t Batch [5050][5500]\t Training Loss 0.1073\t Accuracy 0.9674\n",
      "Epoch [1][30]\t Batch [5100][5500]\t Training Loss 0.1072\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [5150][5500]\t Training Loss 0.1069\t Accuracy 0.9675\n",
      "Epoch [1][30]\t Batch [5200][5500]\t Training Loss 0.1068\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [5250][5500]\t Training Loss 0.1066\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [5300][5500]\t Training Loss 0.1068\t Accuracy 0.9676\n",
      "Epoch [1][30]\t Batch [5350][5500]\t Training Loss 0.1065\t Accuracy 0.9677\n",
      "Epoch [1][30]\t Batch [5400][5500]\t Training Loss 0.1064\t Accuracy 0.9677\n",
      "Epoch [1][30]\t Batch [5450][5500]\t Training Loss 0.1061\t Accuracy 0.9678\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1058\t Average training accuracy 0.9679\n",
      "Epoch [1]\t Average validation loss 0.1115\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [2][30]\t Batch [0][5500]\t Training Loss 0.0328\t Accuracy 1.0000\n",
      "Epoch [2][30]\t Batch [50][5500]\t Training Loss 0.0798\t Accuracy 0.9784\n",
      "Epoch [2][30]\t Batch [100][5500]\t Training Loss 0.0833\t Accuracy 0.9762\n",
      "Epoch [2][30]\t Batch [150][5500]\t Training Loss 0.0982\t Accuracy 0.9695\n",
      "Epoch [2][30]\t Batch [200][5500]\t Training Loss 0.0892\t Accuracy 0.9726\n",
      "Epoch [2][30]\t Batch [250][5500]\t Training Loss 0.0797\t Accuracy 0.9753\n",
      "Epoch [2][30]\t Batch [300][5500]\t Training Loss 0.0772\t Accuracy 0.9767\n",
      "Epoch [2][30]\t Batch [350][5500]\t Training Loss 0.0715\t Accuracy 0.9786\n",
      "Epoch [2][30]\t Batch [400][5500]\t Training Loss 0.0684\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [450][5500]\t Training Loss 0.0680\t Accuracy 0.9803\n",
      "Epoch [2][30]\t Batch [500][5500]\t Training Loss 0.0673\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [550][5500]\t Training Loss 0.0660\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [600][5500]\t Training Loss 0.0658\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [650][5500]\t Training Loss 0.0636\t Accuracy 0.9819\n",
      "Epoch [2][30]\t Batch [700][5500]\t Training Loss 0.0658\t Accuracy 0.9819\n",
      "Epoch [2][30]\t Batch [750][5500]\t Training Loss 0.0671\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [800][5500]\t Training Loss 0.0682\t Accuracy 0.9806\n",
      "Epoch [2][30]\t Batch [850][5500]\t Training Loss 0.0698\t Accuracy 0.9801\n",
      "Epoch [2][30]\t Batch [900][5500]\t Training Loss 0.0720\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [950][5500]\t Training Loss 0.0722\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [1000][5500]\t Training Loss 0.0710\t Accuracy 0.9798\n",
      "Epoch [2][30]\t Batch [1050][5500]\t Training Loss 0.0717\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [1100][5500]\t Training Loss 0.0708\t Accuracy 0.9798\n",
      "Epoch [2][30]\t Batch [1150][5500]\t Training Loss 0.0697\t Accuracy 0.9801\n",
      "Epoch [2][30]\t Batch [1200][5500]\t Training Loss 0.0707\t Accuracy 0.9799\n",
      "Epoch [2][30]\t Batch [1250][5500]\t Training Loss 0.0698\t Accuracy 0.9800\n",
      "Epoch [2][30]\t Batch [1300][5500]\t Training Loss 0.0710\t Accuracy 0.9797\n",
      "Epoch [2][30]\t Batch [1350][5500]\t Training Loss 0.0705\t Accuracy 0.9799\n",
      "Epoch [2][30]\t Batch [1400][5500]\t Training Loss 0.0704\t Accuracy 0.9798\n",
      "Epoch [2][30]\t Batch [1450][5500]\t Training Loss 0.0703\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [1500][5500]\t Training Loss 0.0699\t Accuracy 0.9797\n",
      "Epoch [2][30]\t Batch [1550][5500]\t Training Loss 0.0702\t Accuracy 0.9797\n",
      "Epoch [2][30]\t Batch [1600][5500]\t Training Loss 0.0700\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [1650][5500]\t Training Loss 0.0695\t Accuracy 0.9798\n",
      "Epoch [2][30]\t Batch [1700][5500]\t Training Loss 0.0699\t Accuracy 0.9795\n",
      "Epoch [2][30]\t Batch [1750][5500]\t Training Loss 0.0698\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [1800][5500]\t Training Loss 0.0703\t Accuracy 0.9793\n",
      "Epoch [2][30]\t Batch [1850][5500]\t Training Loss 0.0700\t Accuracy 0.9794\n",
      "Epoch [2][30]\t Batch [1900][5500]\t Training Loss 0.0692\t Accuracy 0.9796\n",
      "Epoch [2][30]\t Batch [1950][5500]\t Training Loss 0.0693\t Accuracy 0.9797\n",
      "Epoch [2][30]\t Batch [2000][5500]\t Training Loss 0.0687\t Accuracy 0.9799\n",
      "Epoch [2][30]\t Batch [2050][5500]\t Training Loss 0.0685\t Accuracy 0.9800\n",
      "Epoch [2][30]\t Batch [2100][5500]\t Training Loss 0.0685\t Accuracy 0.9799\n",
      "Epoch [2][30]\t Batch [2150][5500]\t Training Loss 0.0681\t Accuracy 0.9800\n",
      "Epoch [2][30]\t Batch [2200][5500]\t Training Loss 0.0676\t Accuracy 0.9802\n",
      "Epoch [2][30]\t Batch [2250][5500]\t Training Loss 0.0675\t Accuracy 0.9801\n",
      "Epoch [2][30]\t Batch [2300][5500]\t Training Loss 0.0675\t Accuracy 0.9801\n",
      "Epoch [2][30]\t Batch [2350][5500]\t Training Loss 0.0672\t Accuracy 0.9803\n",
      "Epoch [2][30]\t Batch [2400][5500]\t Training Loss 0.0673\t Accuracy 0.9803\n",
      "Epoch [2][30]\t Batch [2450][5500]\t Training Loss 0.0667\t Accuracy 0.9805\n",
      "Epoch [2][30]\t Batch [2500][5500]\t Training Loss 0.0664\t Accuracy 0.9806\n",
      "Epoch [2][30]\t Batch [2550][5500]\t Training Loss 0.0658\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [2600][5500]\t Training Loss 0.0661\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [2650][5500]\t Training Loss 0.0661\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [2700][5500]\t Training Loss 0.0668\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [2750][5500]\t Training Loss 0.0671\t Accuracy 0.9807\n",
      "Epoch [2][30]\t Batch [2800][5500]\t Training Loss 0.0668\t Accuracy 0.9808\n",
      "Epoch [2][30]\t Batch [2850][5500]\t Training Loss 0.0667\t Accuracy 0.9808\n",
      "Epoch [2][30]\t Batch [2900][5500]\t Training Loss 0.0665\t Accuracy 0.9808\n",
      "Epoch [2][30]\t Batch [2950][5500]\t Training Loss 0.0663\t Accuracy 0.9808\n",
      "Epoch [2][30]\t Batch [3000][5500]\t Training Loss 0.0660\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [3050][5500]\t Training Loss 0.0656\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3100][5500]\t Training Loss 0.0652\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [3150][5500]\t Training Loss 0.0652\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3200][5500]\t Training Loss 0.0654\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3250][5500]\t Training Loss 0.0654\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3300][5500]\t Training Loss 0.0655\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [3350][5500]\t Training Loss 0.0654\t Accuracy 0.9808\n",
      "Epoch [2][30]\t Batch [3400][5500]\t Training Loss 0.0652\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3450][5500]\t Training Loss 0.0651\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3500][5500]\t Training Loss 0.0653\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3550][5500]\t Training Loss 0.0656\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3600][5500]\t Training Loss 0.0654\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3650][5500]\t Training Loss 0.0655\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3700][5500]\t Training Loss 0.0651\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3750][5500]\t Training Loss 0.0657\t Accuracy 0.9809\n",
      "Epoch [2][30]\t Batch [3800][5500]\t Training Loss 0.0656\t Accuracy 0.9810\n",
      "Epoch [2][30]\t Batch [3850][5500]\t Training Loss 0.0653\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3900][5500]\t Training Loss 0.0651\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [3950][5500]\t Training Loss 0.0652\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4000][5500]\t Training Loss 0.0651\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4050][5500]\t Training Loss 0.0649\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4100][5500]\t Training Loss 0.0649\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4150][5500]\t Training Loss 0.0652\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4200][5500]\t Training Loss 0.0651\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4250][5500]\t Training Loss 0.0652\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [4300][5500]\t Training Loss 0.0654\t Accuracy 0.9811\n",
      "Epoch [2][30]\t Batch [4350][5500]\t Training Loss 0.0651\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4400][5500]\t Training Loss 0.0653\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4450][5500]\t Training Loss 0.0653\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [4500][5500]\t Training Loss 0.0650\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4550][5500]\t Training Loss 0.0649\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [4600][5500]\t Training Loss 0.0648\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [4650][5500]\t Training Loss 0.0650\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4700][5500]\t Training Loss 0.0647\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [4750][5500]\t Training Loss 0.0649\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4800][5500]\t Training Loss 0.0651\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [4850][5500]\t Training Loss 0.0649\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [4900][5500]\t Training Loss 0.0648\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [4950][5500]\t Training Loss 0.0650\t Accuracy 0.9814\n",
      "Epoch [2][30]\t Batch [5000][5500]\t Training Loss 0.0656\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [5050][5500]\t Training Loss 0.0655\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [5100][5500]\t Training Loss 0.0654\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [5150][5500]\t Training Loss 0.0654\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [5200][5500]\t Training Loss 0.0653\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [5250][5500]\t Training Loss 0.0652\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [5300][5500]\t Training Loss 0.0653\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [5350][5500]\t Training Loss 0.0653\t Accuracy 0.9813\n",
      "Epoch [2][30]\t Batch [5400][5500]\t Training Loss 0.0652\t Accuracy 0.9812\n",
      "Epoch [2][30]\t Batch [5450][5500]\t Training Loss 0.0652\t Accuracy 0.9813\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0649\t Average training accuracy 0.9814\n",
      "Epoch [2]\t Average validation loss 0.0965\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [3][30]\t Batch [0][5500]\t Training Loss 0.0085\t Accuracy 1.0000\n",
      "Epoch [3][30]\t Batch [50][5500]\t Training Loss 0.0554\t Accuracy 0.9882\n",
      "Epoch [3][30]\t Batch [100][5500]\t Training Loss 0.0560\t Accuracy 0.9851\n",
      "Epoch [3][30]\t Batch [150][5500]\t Training Loss 0.0644\t Accuracy 0.9828\n",
      "Epoch [3][30]\t Batch [200][5500]\t Training Loss 0.0601\t Accuracy 0.9821\n",
      "Epoch [3][30]\t Batch [250][5500]\t Training Loss 0.0530\t Accuracy 0.9841\n",
      "Epoch [3][30]\t Batch [300][5500]\t Training Loss 0.0512\t Accuracy 0.9850\n",
      "Epoch [3][30]\t Batch [350][5500]\t Training Loss 0.0473\t Accuracy 0.9863\n",
      "Epoch [3][30]\t Batch [400][5500]\t Training Loss 0.0449\t Accuracy 0.9873\n",
      "Epoch [3][30]\t Batch [450][5500]\t Training Loss 0.0441\t Accuracy 0.9878\n",
      "Epoch [3][30]\t Batch [500][5500]\t Training Loss 0.0438\t Accuracy 0.9878\n",
      "Epoch [3][30]\t Batch [550][5500]\t Training Loss 0.0428\t Accuracy 0.9884\n",
      "Epoch [3][30]\t Batch [600][5500]\t Training Loss 0.0425\t Accuracy 0.9884\n",
      "Epoch [3][30]\t Batch [650][5500]\t Training Loss 0.0407\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [700][5500]\t Training Loss 0.0427\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [750][5500]\t Training Loss 0.0431\t Accuracy 0.9887\n",
      "Epoch [3][30]\t Batch [800][5500]\t Training Loss 0.0433\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [850][5500]\t Training Loss 0.0444\t Accuracy 0.9879\n",
      "Epoch [3][30]\t Batch [900][5500]\t Training Loss 0.0456\t Accuracy 0.9875\n",
      "Epoch [3][30]\t Batch [950][5500]\t Training Loss 0.0459\t Accuracy 0.9879\n",
      "Epoch [3][30]\t Batch [1000][5500]\t Training Loss 0.0451\t Accuracy 0.9879\n",
      "Epoch [3][30]\t Batch [1050][5500]\t Training Loss 0.0457\t Accuracy 0.9875\n",
      "Epoch [3][30]\t Batch [1100][5500]\t Training Loss 0.0452\t Accuracy 0.9878\n",
      "Epoch [3][30]\t Batch [1150][5500]\t Training Loss 0.0445\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1200][5500]\t Training Loss 0.0454\t Accuracy 0.9878\n",
      "Epoch [3][30]\t Batch [1250][5500]\t Training Loss 0.0445\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1300][5500]\t Training Loss 0.0450\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1350][5500]\t Training Loss 0.0447\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1400][5500]\t Training Loss 0.0446\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1450][5500]\t Training Loss 0.0445\t Accuracy 0.9879\n",
      "Epoch [3][30]\t Batch [1500][5500]\t Training Loss 0.0441\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1550][5500]\t Training Loss 0.0441\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1600][5500]\t Training Loss 0.0439\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1650][5500]\t Training Loss 0.0435\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1700][5500]\t Training Loss 0.0436\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1750][5500]\t Training Loss 0.0434\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1800][5500]\t Training Loss 0.0437\t Accuracy 0.9879\n",
      "Epoch [3][30]\t Batch [1850][5500]\t Training Loss 0.0436\t Accuracy 0.9880\n",
      "Epoch [3][30]\t Batch [1900][5500]\t Training Loss 0.0430\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [1950][5500]\t Training Loss 0.0432\t Accuracy 0.9881\n",
      "Epoch [3][30]\t Batch [2000][5500]\t Training Loss 0.0428\t Accuracy 0.9883\n",
      "Epoch [3][30]\t Batch [2050][5500]\t Training Loss 0.0427\t Accuracy 0.9884\n",
      "Epoch [3][30]\t Batch [2100][5500]\t Training Loss 0.0427\t Accuracy 0.9885\n",
      "Epoch [3][30]\t Batch [2150][5500]\t Training Loss 0.0425\t Accuracy 0.9885\n",
      "Epoch [3][30]\t Batch [2200][5500]\t Training Loss 0.0423\t Accuracy 0.9886\n",
      "Epoch [3][30]\t Batch [2250][5500]\t Training Loss 0.0420\t Accuracy 0.9886\n",
      "Epoch [3][30]\t Batch [2300][5500]\t Training Loss 0.0419\t Accuracy 0.9887\n",
      "Epoch [3][30]\t Batch [2350][5500]\t Training Loss 0.0415\t Accuracy 0.9888\n",
      "Epoch [3][30]\t Batch [2400][5500]\t Training Loss 0.0418\t Accuracy 0.9887\n",
      "Epoch [3][30]\t Batch [2450][5500]\t Training Loss 0.0413\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [2500][5500]\t Training Loss 0.0411\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [2550][5500]\t Training Loss 0.0408\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [2600][5500]\t Training Loss 0.0411\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [2650][5500]\t Training Loss 0.0412\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [2700][5500]\t Training Loss 0.0418\t Accuracy 0.9888\n",
      "Epoch [3][30]\t Batch [2750][5500]\t Training Loss 0.0420\t Accuracy 0.9888\n",
      "Epoch [3][30]\t Batch [2800][5500]\t Training Loss 0.0420\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [2850][5500]\t Training Loss 0.0418\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [2900][5500]\t Training Loss 0.0417\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [2950][5500]\t Training Loss 0.0416\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [3000][5500]\t Training Loss 0.0414\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [3050][5500]\t Training Loss 0.0411\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [3100][5500]\t Training Loss 0.0409\t Accuracy 0.9893\n",
      "Epoch [3][30]\t Batch [3150][5500]\t Training Loss 0.0409\t Accuracy 0.9893\n",
      "Epoch [3][30]\t Batch [3200][5500]\t Training Loss 0.0409\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [3250][5500]\t Training Loss 0.0410\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3300][5500]\t Training Loss 0.0411\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3350][5500]\t Training Loss 0.0411\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [3400][5500]\t Training Loss 0.0410\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3450][5500]\t Training Loss 0.0410\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3500][5500]\t Training Loss 0.0411\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3550][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3600][5500]\t Training Loss 0.0413\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3650][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3700][5500]\t Training Loss 0.0411\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [3750][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [3800][5500]\t Training Loss 0.0415\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3850][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3900][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [3950][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4000][5500]\t Training Loss 0.0413\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4050][5500]\t Training Loss 0.0412\t Accuracy 0.9893\n",
      "Epoch [3][30]\t Batch [4100][5500]\t Training Loss 0.0412\t Accuracy 0.9893\n",
      "Epoch [3][30]\t Batch [4150][5500]\t Training Loss 0.0413\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4200][5500]\t Training Loss 0.0412\t Accuracy 0.9893\n",
      "Epoch [3][30]\t Batch [4250][5500]\t Training Loss 0.0412\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4300][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4350][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4400][5500]\t Training Loss 0.0416\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4450][5500]\t Training Loss 0.0416\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4500][5500]\t Training Loss 0.0413\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4550][5500]\t Training Loss 0.0413\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4600][5500]\t Training Loss 0.0412\t Accuracy 0.9892\n",
      "Epoch [3][30]\t Batch [4650][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4700][5500]\t Training Loss 0.0412\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4750][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4800][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [4850][5500]\t Training Loss 0.0414\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4900][5500]\t Training Loss 0.0413\t Accuracy 0.9891\n",
      "Epoch [3][30]\t Batch [4950][5500]\t Training Loss 0.0414\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5000][5500]\t Training Loss 0.0416\t Accuracy 0.9889\n",
      "Epoch [3][30]\t Batch [5050][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5100][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5150][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5200][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5250][5500]\t Training Loss 0.0414\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5300][5500]\t Training Loss 0.0415\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5350][5500]\t Training Loss 0.0414\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5400][5500]\t Training Loss 0.0414\t Accuracy 0.9890\n",
      "Epoch [3][30]\t Batch [5450][5500]\t Training Loss 0.0413\t Accuracy 0.9890\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0411\t Average training accuracy 0.9891\n",
      "Epoch [3]\t Average validation loss 0.0945\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [4][30]\t Batch [0][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [50][5500]\t Training Loss 0.0400\t Accuracy 0.9922\n",
      "Epoch [4][30]\t Batch [100][5500]\t Training Loss 0.0375\t Accuracy 0.9901\n",
      "Epoch [4][30]\t Batch [150][5500]\t Training Loss 0.0414\t Accuracy 0.9901\n",
      "Epoch [4][30]\t Batch [200][5500]\t Training Loss 0.0389\t Accuracy 0.9891\n",
      "Epoch [4][30]\t Batch [250][5500]\t Training Loss 0.0339\t Accuracy 0.9908\n",
      "Epoch [4][30]\t Batch [300][5500]\t Training Loss 0.0333\t Accuracy 0.9910\n",
      "Epoch [4][30]\t Batch [350][5500]\t Training Loss 0.0303\t Accuracy 0.9923\n",
      "Epoch [4][30]\t Batch [400][5500]\t Training Loss 0.0289\t Accuracy 0.9928\n",
      "Epoch [4][30]\t Batch [450][5500]\t Training Loss 0.0285\t Accuracy 0.9929\n",
      "Epoch [4][30]\t Batch [500][5500]\t Training Loss 0.0290\t Accuracy 0.9924\n",
      "Epoch [4][30]\t Batch [550][5500]\t Training Loss 0.0284\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [600][5500]\t Training Loss 0.0279\t Accuracy 0.9930\n",
      "Epoch [4][30]\t Batch [650][5500]\t Training Loss 0.0267\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [700][5500]\t Training Loss 0.0284\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [750][5500]\t Training Loss 0.0284\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [800][5500]\t Training Loss 0.0284\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [850][5500]\t Training Loss 0.0292\t Accuracy 0.9928\n",
      "Epoch [4][30]\t Batch [900][5500]\t Training Loss 0.0296\t Accuracy 0.9927\n",
      "Epoch [4][30]\t Batch [950][5500]\t Training Loss 0.0301\t Accuracy 0.9925\n",
      "Epoch [4][30]\t Batch [1000][5500]\t Training Loss 0.0295\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1050][5500]\t Training Loss 0.0299\t Accuracy 0.9924\n",
      "Epoch [4][30]\t Batch [1100][5500]\t Training Loss 0.0299\t Accuracy 0.9925\n",
      "Epoch [4][30]\t Batch [1150][5500]\t Training Loss 0.0293\t Accuracy 0.9927\n",
      "Epoch [4][30]\t Batch [1200][5500]\t Training Loss 0.0299\t Accuracy 0.9924\n",
      "Epoch [4][30]\t Batch [1250][5500]\t Training Loss 0.0294\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1300][5500]\t Training Loss 0.0294\t Accuracy 0.9925\n",
      "Epoch [4][30]\t Batch [1350][5500]\t Training Loss 0.0292\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1400][5500]\t Training Loss 0.0290\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1450][5500]\t Training Loss 0.0289\t Accuracy 0.9925\n",
      "Epoch [4][30]\t Batch [1500][5500]\t Training Loss 0.0285\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1550][5500]\t Training Loss 0.0284\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1600][5500]\t Training Loss 0.0282\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1650][5500]\t Training Loss 0.0279\t Accuracy 0.9927\n",
      "Epoch [4][30]\t Batch [1700][5500]\t Training Loss 0.0278\t Accuracy 0.9928\n",
      "Epoch [4][30]\t Batch [1750][5500]\t Training Loss 0.0277\t Accuracy 0.9927\n",
      "Epoch [4][30]\t Batch [1800][5500]\t Training Loss 0.0280\t Accuracy 0.9926\n",
      "Epoch [4][30]\t Batch [1850][5500]\t Training Loss 0.0279\t Accuracy 0.9927\n",
      "Epoch [4][30]\t Batch [1900][5500]\t Training Loss 0.0275\t Accuracy 0.9928\n",
      "Epoch [4][30]\t Batch [1950][5500]\t Training Loss 0.0274\t Accuracy 0.9928\n",
      "Epoch [4][30]\t Batch [2000][5500]\t Training Loss 0.0270\t Accuracy 0.9929\n",
      "Epoch [4][30]\t Batch [2050][5500]\t Training Loss 0.0270\t Accuracy 0.9929\n",
      "Epoch [4][30]\t Batch [2100][5500]\t Training Loss 0.0271\t Accuracy 0.9929\n",
      "Epoch [4][30]\t Batch [2150][5500]\t Training Loss 0.0269\t Accuracy 0.9929\n",
      "Epoch [4][30]\t Batch [2200][5500]\t Training Loss 0.0268\t Accuracy 0.9930\n",
      "Epoch [4][30]\t Batch [2250][5500]\t Training Loss 0.0266\t Accuracy 0.9931\n",
      "Epoch [4][30]\t Batch [2300][5500]\t Training Loss 0.0265\t Accuracy 0.9931\n",
      "Epoch [4][30]\t Batch [2350][5500]\t Training Loss 0.0262\t Accuracy 0.9932\n",
      "Epoch [4][30]\t Batch [2400][5500]\t Training Loss 0.0265\t Accuracy 0.9932\n",
      "Epoch [4][30]\t Batch [2450][5500]\t Training Loss 0.0262\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2500][5500]\t Training Loss 0.0260\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [2550][5500]\t Training Loss 0.0258\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [2600][5500]\t Training Loss 0.0260\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2650][5500]\t Training Loss 0.0262\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [2700][5500]\t Training Loss 0.0268\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2750][5500]\t Training Loss 0.0269\t Accuracy 0.9932\n",
      "Epoch [4][30]\t Batch [2800][5500]\t Training Loss 0.0269\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2850][5500]\t Training Loss 0.0268\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2900][5500]\t Training Loss 0.0267\t Accuracy 0.9933\n",
      "Epoch [4][30]\t Batch [2950][5500]\t Training Loss 0.0266\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [3000][5500]\t Training Loss 0.0264\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [3050][5500]\t Training Loss 0.0262\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3100][5500]\t Training Loss 0.0260\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3150][5500]\t Training Loss 0.0260\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3200][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [3250][5500]\t Training Loss 0.0261\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [3300][5500]\t Training Loss 0.0261\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3350][5500]\t Training Loss 0.0262\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3400][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [3450][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [3500][5500]\t Training Loss 0.0261\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3550][5500]\t Training Loss 0.0263\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3600][5500]\t Training Loss 0.0263\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3650][5500]\t Training Loss 0.0263\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3700][5500]\t Training Loss 0.0262\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3750][5500]\t Training Loss 0.0264\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [3800][5500]\t Training Loss 0.0264\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [3850][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [3900][5500]\t Training Loss 0.0265\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [3950][5500]\t Training Loss 0.0265\t Accuracy 0.9934\n",
      "Epoch [4][30]\t Batch [4000][5500]\t Training Loss 0.0265\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4050][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4100][5500]\t Training Loss 0.0264\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4150][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4200][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4250][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4300][5500]\t Training Loss 0.0263\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4350][5500]\t Training Loss 0.0263\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4400][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4450][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4500][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4550][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4600][5500]\t Training Loss 0.0261\t Accuracy 0.9937\n",
      "Epoch [4][30]\t Batch [4650][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4700][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4750][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4800][5500]\t Training Loss 0.0264\t Accuracy 0.9935\n",
      "Epoch [4][30]\t Batch [4850][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4900][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [4950][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5000][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5050][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5100][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5150][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5200][5500]\t Training Loss 0.0263\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5250][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5300][5500]\t Training Loss 0.0262\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5350][5500]\t Training Loss 0.0261\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5400][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "Epoch [4][30]\t Batch [5450][5500]\t Training Loss 0.0260\t Accuracy 0.9936\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0258\t Average training accuracy 0.9937\n",
      "Epoch [4]\t Average validation loss 0.0989\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [5][30]\t Batch [0][5500]\t Training Loss 0.0032\t Accuracy 1.0000\n",
      "Epoch [5][30]\t Batch [50][5500]\t Training Loss 0.0332\t Accuracy 0.9922\n",
      "Epoch [5][30]\t Batch [100][5500]\t Training Loss 0.0247\t Accuracy 0.9941\n",
      "Epoch [5][30]\t Batch [150][5500]\t Training Loss 0.0271\t Accuracy 0.9934\n",
      "Epoch [5][30]\t Batch [200][5500]\t Training Loss 0.0245\t Accuracy 0.9940\n",
      "Epoch [5][30]\t Batch [250][5500]\t Training Loss 0.0214\t Accuracy 0.9948\n",
      "Epoch [5][30]\t Batch [300][5500]\t Training Loss 0.0213\t Accuracy 0.9950\n",
      "Epoch [5][30]\t Batch [350][5500]\t Training Loss 0.0192\t Accuracy 0.9957\n",
      "Epoch [5][30]\t Batch [400][5500]\t Training Loss 0.0181\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [450][5500]\t Training Loss 0.0183\t Accuracy 0.9958\n",
      "Epoch [5][30]\t Batch [500][5500]\t Training Loss 0.0183\t Accuracy 0.9958\n",
      "Epoch [5][30]\t Batch [550][5500]\t Training Loss 0.0178\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [600][5500]\t Training Loss 0.0177\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [650][5500]\t Training Loss 0.0170\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [700][5500]\t Training Loss 0.0180\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [750][5500]\t Training Loss 0.0183\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [800][5500]\t Training Loss 0.0181\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [850][5500]\t Training Loss 0.0188\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [900][5500]\t Training Loss 0.0188\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [950][5500]\t Training Loss 0.0190\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [1000][5500]\t Training Loss 0.0186\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [1050][5500]\t Training Loss 0.0187\t Accuracy 0.9959\n",
      "Epoch [5][30]\t Batch [1100][5500]\t Training Loss 0.0187\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [1150][5500]\t Training Loss 0.0183\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1200][5500]\t Training Loss 0.0185\t Accuracy 0.9959\n",
      "Epoch [5][30]\t Batch [1250][5500]\t Training Loss 0.0182\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1300][5500]\t Training Loss 0.0180\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1350][5500]\t Training Loss 0.0180\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1400][5500]\t Training Loss 0.0178\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1450][5500]\t Training Loss 0.0178\t Accuracy 0.9960\n",
      "Epoch [5][30]\t Batch [1500][5500]\t Training Loss 0.0176\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1550][5500]\t Training Loss 0.0175\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1600][5500]\t Training Loss 0.0174\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1650][5500]\t Training Loss 0.0172\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [1700][5500]\t Training Loss 0.0171\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [1750][5500]\t Training Loss 0.0170\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [1800][5500]\t Training Loss 0.0171\t Accuracy 0.9961\n",
      "Epoch [5][30]\t Batch [1850][5500]\t Training Loss 0.0170\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [1900][5500]\t Training Loss 0.0168\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [1950][5500]\t Training Loss 0.0167\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2000][5500]\t Training Loss 0.0165\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2050][5500]\t Training Loss 0.0165\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2100][5500]\t Training Loss 0.0166\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [2150][5500]\t Training Loss 0.0164\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2200][5500]\t Training Loss 0.0164\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2250][5500]\t Training Loss 0.0163\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2300][5500]\t Training Loss 0.0162\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2350][5500]\t Training Loss 0.0160\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [2400][5500]\t Training Loss 0.0163\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2450][5500]\t Training Loss 0.0161\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [2500][5500]\t Training Loss 0.0160\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [2550][5500]\t Training Loss 0.0159\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [2600][5500]\t Training Loss 0.0161\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [2650][5500]\t Training Loss 0.0162\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2700][5500]\t Training Loss 0.0166\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [2750][5500]\t Training Loss 0.0167\t Accuracy 0.9962\n",
      "Epoch [5][30]\t Batch [2800][5500]\t Training Loss 0.0167\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2850][5500]\t Training Loss 0.0167\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2900][5500]\t Training Loss 0.0166\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [2950][5500]\t Training Loss 0.0166\t Accuracy 0.9963\n",
      "Epoch [5][30]\t Batch [3000][5500]\t Training Loss 0.0164\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [3050][5500]\t Training Loss 0.0163\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [3100][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3150][5500]\t Training Loss 0.0162\t Accuracy 0.9964\n",
      "Epoch [5][30]\t Batch [3200][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3250][5500]\t Training Loss 0.0162\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3300][5500]\t Training Loss 0.0161\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3350][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3400][5500]\t Training Loss 0.0159\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3450][5500]\t Training Loss 0.0159\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3500][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3550][5500]\t Training Loss 0.0161\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3600][5500]\t Training Loss 0.0161\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3650][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3700][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [3750][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3800][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3850][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3900][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [3950][5500]\t Training Loss 0.0162\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4000][5500]\t Training Loss 0.0162\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4050][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4100][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4150][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4200][5500]\t Training Loss 0.0160\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4250][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4300][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4350][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4400][5500]\t Training Loss 0.0161\t Accuracy 0.9965\n",
      "Epoch [5][30]\t Batch [4450][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4500][5500]\t Training Loss 0.0159\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4550][5500]\t Training Loss 0.0159\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4600][5500]\t Training Loss 0.0158\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4650][5500]\t Training Loss 0.0158\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4700][5500]\t Training Loss 0.0157\t Accuracy 0.9967\n",
      "Epoch [5][30]\t Batch [4750][5500]\t Training Loss 0.0158\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4800][5500]\t Training Loss 0.0160\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4850][5500]\t Training Loss 0.0159\t Accuracy 0.9966\n",
      "Epoch [5][30]\t Batch [4900][5500]\t Training Loss 0.0159\t Accuracy 0.9967\n",
      "Epoch [5][30]\t Batch [4950][5500]\t Training Loss 0.0159\t Accuracy 0.9967\n",
      "Epoch [5][30]\t Batch [5000][5500]\t Training Loss 0.0158\t Accuracy 0.9967\n",
      "Epoch [5][30]\t Batch [5050][5500]\t Training Loss 0.0158\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5100][5500]\t Training Loss 0.0158\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5150][5500]\t Training Loss 0.0158\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5200][5500]\t Training Loss 0.0157\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5250][5500]\t Training Loss 0.0157\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5300][5500]\t Training Loss 0.0157\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5350][5500]\t Training Loss 0.0156\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5400][5500]\t Training Loss 0.0155\t Accuracy 0.9968\n",
      "Epoch [5][30]\t Batch [5450][5500]\t Training Loss 0.0155\t Accuracy 0.9968\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0154\t Average training accuracy 0.9968\n",
      "Epoch [5]\t Average validation loss 0.0973\t Average validation accuracy 0.9770\n",
      "\n",
      "Epoch [6][30]\t Batch [0][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [6][30]\t Batch [50][5500]\t Training Loss 0.0223\t Accuracy 0.9941\n",
      "Epoch [6][30]\t Batch [100][5500]\t Training Loss 0.0153\t Accuracy 0.9970\n",
      "Epoch [6][30]\t Batch [150][5500]\t Training Loss 0.0171\t Accuracy 0.9960\n",
      "Epoch [6][30]\t Batch [200][5500]\t Training Loss 0.0153\t Accuracy 0.9965\n",
      "Epoch [6][30]\t Batch [250][5500]\t Training Loss 0.0136\t Accuracy 0.9968\n",
      "Epoch [6][30]\t Batch [300][5500]\t Training Loss 0.0137\t Accuracy 0.9967\n",
      "Epoch [6][30]\t Batch [350][5500]\t Training Loss 0.0126\t Accuracy 0.9972\n",
      "Epoch [6][30]\t Batch [400][5500]\t Training Loss 0.0118\t Accuracy 0.9975\n",
      "Epoch [6][30]\t Batch [450][5500]\t Training Loss 0.0123\t Accuracy 0.9976\n",
      "Epoch [6][30]\t Batch [500][5500]\t Training Loss 0.0120\t Accuracy 0.9976\n",
      "Epoch [6][30]\t Batch [550][5500]\t Training Loss 0.0118\t Accuracy 0.9976\n",
      "Epoch [6][30]\t Batch [600][5500]\t Training Loss 0.0120\t Accuracy 0.9977\n",
      "Epoch [6][30]\t Batch [650][5500]\t Training Loss 0.0116\t Accuracy 0.9978\n",
      "Epoch [6][30]\t Batch [700][5500]\t Training Loss 0.0118\t Accuracy 0.9977\n",
      "Epoch [6][30]\t Batch [750][5500]\t Training Loss 0.0116\t Accuracy 0.9979\n",
      "Epoch [6][30]\t Batch [800][5500]\t Training Loss 0.0114\t Accuracy 0.9980\n",
      "Epoch [6][30]\t Batch [850][5500]\t Training Loss 0.0122\t Accuracy 0.9978\n",
      "Epoch [6][30]\t Batch [900][5500]\t Training Loss 0.0122\t Accuracy 0.9979\n",
      "Epoch [6][30]\t Batch [950][5500]\t Training Loss 0.0121\t Accuracy 0.9979\n",
      "Epoch [6][30]\t Batch [1000][5500]\t Training Loss 0.0118\t Accuracy 0.9980\n",
      "Epoch [6][30]\t Batch [1050][5500]\t Training Loss 0.0117\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1100][5500]\t Training Loss 0.0119\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1150][5500]\t Training Loss 0.0116\t Accuracy 0.9982\n",
      "Epoch [6][30]\t Batch [1200][5500]\t Training Loss 0.0117\t Accuracy 0.9980\n",
      "Epoch [6][30]\t Batch [1250][5500]\t Training Loss 0.0115\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1300][5500]\t Training Loss 0.0114\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1350][5500]\t Training Loss 0.0113\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1400][5500]\t Training Loss 0.0111\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1450][5500]\t Training Loss 0.0110\t Accuracy 0.9981\n",
      "Epoch [6][30]\t Batch [1500][5500]\t Training Loss 0.0108\t Accuracy 0.9982\n",
      "Epoch [6][30]\t Batch [1550][5500]\t Training Loss 0.0107\t Accuracy 0.9983\n",
      "Epoch [6][30]\t Batch [1600][5500]\t Training Loss 0.0106\t Accuracy 0.9983\n",
      "Epoch [6][30]\t Batch [1650][5500]\t Training Loss 0.0105\t Accuracy 0.9983\n",
      "Epoch [6][30]\t Batch [1700][5500]\t Training Loss 0.0104\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [1750][5500]\t Training Loss 0.0103\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [1800][5500]\t Training Loss 0.0103\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [1850][5500]\t Training Loss 0.0103\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [1900][5500]\t Training Loss 0.0101\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [1950][5500]\t Training Loss 0.0101\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2000][5500]\t Training Loss 0.0100\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2050][5500]\t Training Loss 0.0100\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2100][5500]\t Training Loss 0.0100\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2150][5500]\t Training Loss 0.0099\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2200][5500]\t Training Loss 0.0100\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2250][5500]\t Training Loss 0.0099\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2300][5500]\t Training Loss 0.0098\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2350][5500]\t Training Loss 0.0097\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [2400][5500]\t Training Loss 0.0097\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2450][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2500][5500]\t Training Loss 0.0095\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [2550][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2600][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2650][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [2700][5500]\t Training Loss 0.0099\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2750][5500]\t Training Loss 0.0100\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2800][5500]\t Training Loss 0.0099\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2850][5500]\t Training Loss 0.0099\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2900][5500]\t Training Loss 0.0099\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [2950][5500]\t Training Loss 0.0099\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3000][5500]\t Training Loss 0.0098\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3050][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3100][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3150][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3200][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3250][5500]\t Training Loss 0.0097\t Accuracy 0.9984\n",
      "Epoch [6][30]\t Batch [3300][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3350][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3400][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3450][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3500][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3550][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3600][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3650][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3700][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3750][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3800][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3850][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3900][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [3950][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4000][5500]\t Training Loss 0.0096\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4050][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4100][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4150][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4200][5500]\t Training Loss 0.0095\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4250][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4300][5500]\t Training Loss 0.0094\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4350][5500]\t Training Loss 0.0094\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4400][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4450][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4500][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4550][5500]\t Training Loss 0.0093\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4600][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4650][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4700][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4750][5500]\t Training Loss 0.0093\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4800][5500]\t Training Loss 0.0094\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4850][5500]\t Training Loss 0.0093\t Accuracy 0.9985\n",
      "Epoch [6][30]\t Batch [4900][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [4950][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5000][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5050][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5100][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5150][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5200][5500]\t Training Loss 0.0093\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5250][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5300][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5350][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5400][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "Epoch [6][30]\t Batch [5450][5500]\t Training Loss 0.0092\t Accuracy 0.9986\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0091\t Average training accuracy 0.9986\n",
      "Epoch [6]\t Average validation loss 0.0993\t Average validation accuracy 0.9770\n",
      "\n",
      "Epoch [7][30]\t Batch [0][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [7][30]\t Batch [50][5500]\t Training Loss 0.0101\t Accuracy 0.9980\n",
      "Epoch [7][30]\t Batch [100][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [150][5500]\t Training Loss 0.0088\t Accuracy 0.9987\n",
      "Epoch [7][30]\t Batch [200][5500]\t Training Loss 0.0074\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [250][5500]\t Training Loss 0.0068\t Accuracy 0.9988\n",
      "Epoch [7][30]\t Batch [300][5500]\t Training Loss 0.0072\t Accuracy 0.9983\n",
      "Epoch [7][30]\t Batch [350][5500]\t Training Loss 0.0065\t Accuracy 0.9986\n",
      "Epoch [7][30]\t Batch [400][5500]\t Training Loss 0.0061\t Accuracy 0.9988\n",
      "Epoch [7][30]\t Batch [450][5500]\t Training Loss 0.0069\t Accuracy 0.9987\n",
      "Epoch [7][30]\t Batch [500][5500]\t Training Loss 0.0067\t Accuracy 0.9988\n",
      "Epoch [7][30]\t Batch [550][5500]\t Training Loss 0.0066\t Accuracy 0.9989\n",
      "Epoch [7][30]\t Batch [600][5500]\t Training Loss 0.0066\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [650][5500]\t Training Loss 0.0065\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [700][5500]\t Training Loss 0.0066\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [750][5500]\t Training Loss 0.0065\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [800][5500]\t Training Loss 0.0062\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [850][5500]\t Training Loss 0.0067\t Accuracy 0.9989\n",
      "Epoch [7][30]\t Batch [900][5500]\t Training Loss 0.0067\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [950][5500]\t Training Loss 0.0066\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1000][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1050][5500]\t Training Loss 0.0065\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1100][5500]\t Training Loss 0.0066\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1150][5500]\t Training Loss 0.0065\t Accuracy 0.9990\n",
      "Epoch [7][30]\t Batch [1200][5500]\t Training Loss 0.0065\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1250][5500]\t Training Loss 0.0064\t Accuracy 0.9991\n",
      "Epoch [7][30]\t Batch [1300][5500]\t Training Loss 0.0063\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [1350][5500]\t Training Loss 0.0063\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [1400][5500]\t Training Loss 0.0062\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [1450][5500]\t Training Loss 0.0062\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [1500][5500]\t Training Loss 0.0061\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [1550][5500]\t Training Loss 0.0060\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [1600][5500]\t Training Loss 0.0060\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [1650][5500]\t Training Loss 0.0059\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [1700][5500]\t Training Loss 0.0059\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [1750][5500]\t Training Loss 0.0059\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [1800][5500]\t Training Loss 0.0058\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [1850][5500]\t Training Loss 0.0058\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [1900][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [1950][5500]\t Training Loss 0.0058\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2000][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2050][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2100][5500]\t Training Loss 0.0058\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2150][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2200][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2250][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2300][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2350][5500]\t Training Loss 0.0057\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2400][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2450][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2500][5500]\t Training Loss 0.0056\t Accuracy 0.9994\n",
      "Epoch [7][30]\t Batch [2550][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2600][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2650][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2700][5500]\t Training Loss 0.0058\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [2750][5500]\t Training Loss 0.0059\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [2800][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [2850][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [2900][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [2950][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3000][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3050][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3100][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3150][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3200][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3250][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3300][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3350][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3400][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3450][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3500][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3550][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3600][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3650][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3700][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3750][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3800][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3850][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3900][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [3950][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4000][5500]\t Training Loss 0.0058\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4050][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4100][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4150][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4200][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4250][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4300][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4350][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4400][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4450][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4500][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4550][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [4600][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [4650][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4700][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [4750][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [4800][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4850][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4900][5500]\t Training Loss 0.0057\t Accuracy 0.9992\n",
      "Epoch [7][30]\t Batch [4950][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5000][5500]\t Training Loss 0.0057\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5050][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5100][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5150][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5200][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5250][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5300][5500]\t Training Loss 0.0056\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5350][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5400][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "Epoch [7][30]\t Batch [5450][5500]\t Training Loss 0.0055\t Accuracy 0.9993\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0055\t Average training accuracy 0.9993\n",
      "Epoch [7]\t Average validation loss 0.0961\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [8][30]\t Batch [0][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [50][5500]\t Training Loss 0.0058\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [100][5500]\t Training Loss 0.0045\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [150][5500]\t Training Loss 0.0055\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [200][5500]\t Training Loss 0.0047\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [250][5500]\t Training Loss 0.0042\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [300][5500]\t Training Loss 0.0042\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [350][5500]\t Training Loss 0.0038\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [400][5500]\t Training Loss 0.0037\t Accuracy 1.0000\n",
      "Epoch [8][30]\t Batch [450][5500]\t Training Loss 0.0042\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [500][5500]\t Training Loss 0.0041\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [550][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [600][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [650][5500]\t Training Loss 0.0040\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [700][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [750][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [800][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [850][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [900][5500]\t Training Loss 0.0039\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [950][5500]\t Training Loss 0.0038\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [1000][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [1050][5500]\t Training Loss 0.0037\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [1100][5500]\t Training Loss 0.0037\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1150][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1200][5500]\t Training Loss 0.0038\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1250][5500]\t Training Loss 0.0037\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1300][5500]\t Training Loss 0.0037\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1350][5500]\t Training Loss 0.0037\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1400][5500]\t Training Loss 0.0037\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1450][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1500][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1550][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1600][5500]\t Training Loss 0.0036\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1650][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1700][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1750][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1800][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1850][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1900][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [1950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2000][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [2050][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [2100][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [2150][5500]\t Training Loss 0.0034\t Accuracy 0.9999\n",
      "Epoch [8][30]\t Batch [2200][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2250][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2300][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2350][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2400][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2450][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2500][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2550][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2600][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2650][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2700][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2750][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2800][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2850][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2900][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [2950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3000][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3050][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3100][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3150][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3200][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3250][5500]\t Training Loss 0.0035\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3300][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3350][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3400][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3450][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3500][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3550][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3600][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3650][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3700][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3750][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3800][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3850][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3900][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [3950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4000][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4050][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4100][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4150][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4200][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4250][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4300][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4350][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4400][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4450][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4500][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4550][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4600][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4650][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4700][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4750][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4800][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4850][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4900][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [4950][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5000][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5050][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5100][5500]\t Training Loss 0.0034\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5150][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5200][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5250][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5300][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5350][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5400][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "Epoch [8][30]\t Batch [5450][5500]\t Training Loss 0.0033\t Accuracy 0.9998\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0033\t Average training accuracy 0.9998\n",
      "Epoch [8]\t Average validation loss 0.0962\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [9][30]\t Batch [0][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [50][5500]\t Training Loss 0.0034\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [100][5500]\t Training Loss 0.0027\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [150][5500]\t Training Loss 0.0035\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [200][5500]\t Training Loss 0.0031\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [250][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [300][5500]\t Training Loss 0.0028\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [350][5500]\t Training Loss 0.0026\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [400][5500]\t Training Loss 0.0024\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [450][5500]\t Training Loss 0.0027\t Accuracy 0.9998\n",
      "Epoch [9][30]\t Batch [500][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [9][30]\t Batch [550][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [9][30]\t Batch [600][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [9][30]\t Batch [650][5500]\t Training Loss 0.0025\t Accuracy 0.9998\n",
      "Epoch [9][30]\t Batch [700][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [750][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [800][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [850][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [900][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [950][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1000][5500]\t Training Loss 0.0023\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1050][5500]\t Training Loss 0.0024\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1100][5500]\t Training Loss 0.0023\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1150][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1200][5500]\t Training Loss 0.0023\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1250][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1300][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1350][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1400][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1450][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1500][5500]\t Training Loss 0.0022\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1550][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1600][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1650][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1700][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1750][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1800][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1850][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1900][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [1950][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2150][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2250][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2300][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2500][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2600][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2650][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [2700][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2750][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2800][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2850][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2900][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [2950][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3000][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3050][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3100][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3150][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3200][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3250][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3300][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3350][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3400][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3450][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3500][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3550][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3600][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3650][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3700][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3750][5500]\t Training Loss 0.0020\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3800][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3850][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3900][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [3950][5500]\t Training Loss 0.0021\t Accuracy 0.9999\n",
      "Epoch [9][30]\t Batch [4000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4350][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4400][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4450][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4500][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4550][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4600][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4650][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4700][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4750][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4800][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4850][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4900][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [4950][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5000][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5050][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5100][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5150][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5200][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5250][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5300][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5350][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5400][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [9][30]\t Batch [5450][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0020\t Average training accuracy 1.0000\n",
      "Epoch [9]\t Average validation loss 0.0975\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [10][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [50][5500]\t Training Loss 0.0021\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [100][5500]\t Training Loss 0.0019\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [150][5500]\t Training Loss 0.0023\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [200][5500]\t Training Loss 0.0020\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [250][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [300][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [350][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [400][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [450][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [500][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [550][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [600][5500]\t Training Loss 0.0018\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [650][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [700][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [750][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [800][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [850][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [900][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [950][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1000][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1050][5500]\t Training Loss 0.0017\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1100][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1250][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1300][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1350][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1400][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1450][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1500][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1550][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1600][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1650][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1700][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1750][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1800][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [1950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [2950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [3950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4500][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4550][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4600][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4650][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4700][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4750][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4800][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4850][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4900][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [4950][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5000][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5050][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5100][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5150][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5200][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5250][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5300][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5350][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5400][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [10][30]\t Batch [5450][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0015\t Average training accuracy 1.0000\n",
      "Epoch [10]\t Average validation loss 0.0993\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [11][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [50][5500]\t Training Loss 0.0015\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [100][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [150][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [200][5500]\t Training Loss 0.0016\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [250][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [300][5500]\t Training Loss 0.0014\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [350][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [450][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [600][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [650][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [700][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [750][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [850][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [900][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [950][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1050][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [1950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2300][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2350][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2400][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2450][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2500][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2550][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2600][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [2950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [3950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4500][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4550][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4600][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4650][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4700][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4750][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4800][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4850][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4900][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [4950][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5000][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5050][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5100][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5150][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5200][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5250][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5300][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5350][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5400][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [11][30]\t Batch [5450][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0012\t Average training accuracy 1.0000\n",
      "Epoch [11]\t Average validation loss 0.1010\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [12][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [50][5500]\t Training Loss 0.0012\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [100][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [150][5500]\t Training Loss 0.0013\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [200][5500]\t Training Loss 0.0011\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [1950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2850][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2900][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [2950][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3000][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3050][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3250][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3300][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3350][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3400][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3450][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3500][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3550][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3600][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3650][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3700][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3750][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3800][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [3950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4500][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4550][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4600][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4650][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4700][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4750][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4800][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4850][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4900][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [4950][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5000][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5050][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5100][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5200][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5250][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5300][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5350][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5400][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [12][30]\t Batch [5450][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0010\t Average training accuracy 1.0000\n",
      "Epoch [12]\t Average validation loss 0.1022\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [13][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [50][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [100][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [150][5500]\t Training Loss 0.0010\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [200][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [1950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [2950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [3950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4500][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4550][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4600][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4650][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4700][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4750][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4800][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4850][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4900][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [4950][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5000][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5050][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5250][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5300][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5350][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5400][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [13][30]\t Batch [5450][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0008\t Average training accuracy 1.0000\n",
      "Epoch [13]\t Average validation loss 0.1033\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [14][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [50][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [100][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [150][5500]\t Training Loss 0.0009\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [200][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [1950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [2950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [3950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4500][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4550][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4600][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4650][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4700][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4750][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4800][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4850][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4900][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [4950][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5000][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5050][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5250][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5300][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5350][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5400][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [14][30]\t Batch [5450][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0007\t Average training accuracy 1.0000\n",
      "Epoch [14]\t Average validation loss 0.1044\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [15][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [50][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [100][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [150][5500]\t Training Loss 0.0008\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [200][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [1950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [2950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [3950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4500][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4550][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4600][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4650][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4700][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4750][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4800][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4850][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4900][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [4950][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5250][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5300][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5350][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5400][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [15][30]\t Batch [5450][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0006\t Average training accuracy 1.0000\n",
      "Epoch [15]\t Average validation loss 0.1053\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [16][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [50][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [150][5500]\t Training Loss 0.0007\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [200][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [1950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [2950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [3950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [4950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5000][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5050][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5100][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [16][30]\t Batch [5450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0005\t Average training accuracy 1.0000\n",
      "Epoch [16]\t Average validation loss 0.1062\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [17][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [50][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [1950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [2950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [3950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4500][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4550][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4600][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4650][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4700][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4750][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4800][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4850][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4900][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [4950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5250][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5300][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5350][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5400][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [5450][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0005\t Average training accuracy 1.0000\n",
      "Epoch [17]\t Average validation loss 0.1071\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [18][30]\t Batch [0][5500]\t Training Loss 0.0001\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [50][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [150][5500]\t Training Loss 0.0006\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [1950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [2950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [3950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [4950][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5000][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5050][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [18][30]\t Batch [5450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0004\t Average training accuracy 1.0000\n",
      "Epoch [18]\t Average validation loss 0.1079\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [19][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [50][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [100][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [200][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [1950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [2950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [3950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [4950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [19][30]\t Batch [5450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0004\t Average training accuracy 1.0000\n",
      "Epoch [19]\t Average validation loss 0.1086\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [20][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [50][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [150][5500]\t Training Loss 0.0005\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [1950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [2950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [3950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4500][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4550][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [4950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [20][30]\t Batch [5450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0004\t Average training accuracy 1.0000\n",
      "Epoch [20]\t Average validation loss 0.1093\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [21][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [50][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [1950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4600][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4650][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4700][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4750][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4800][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4850][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4900][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [4950][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5000][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5050][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5250][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5300][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5350][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5400][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [21][30]\t Batch [5450][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0004\t Average training accuracy 1.0000\n",
      "Epoch [21]\t Average validation loss 0.1100\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [22][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [50][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [100][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [200][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [1950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [22][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [22]\t Average validation loss 0.1105\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [23][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [50][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [1950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [23][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [23]\t Average validation loss 0.1111\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [24][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [50][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [150][5500]\t Training Loss 0.0004\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [1950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [24][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [24]\t Average validation loss 0.1116\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [25][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [50][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [1950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [25][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [25]\t Average validation loss 0.1121\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [26][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [50][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [1950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [2950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [3950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4500][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4550][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4600][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [26][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [26]\t Average validation loss 0.1125\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [27][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [50][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [1950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [2950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [3950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4650][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4700][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4750][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4800][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4850][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4900][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [4950][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5000][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5050][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5250][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5300][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5350][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5400][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [27][30]\t Batch [5450][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0003\t Average training accuracy 1.0000\n",
      "Epoch [27]\t Average validation loss 0.1129\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [28][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [50][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [1950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [2950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [3950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [4950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [28][30]\t Batch [5450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0002\t Average training accuracy 1.0000\n",
      "Epoch [28]\t Average validation loss 0.1133\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [29][30]\t Batch [0][5500]\t Training Loss 0.0000\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [50][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [100][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [150][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [200][5500]\t Training Loss 0.0003\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [1950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [2950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [3950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4500][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4550][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4600][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4650][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4700][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4750][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4800][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4850][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4900][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [4950][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5000][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5050][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5100][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5150][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5200][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5250][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5300][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5350][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5400][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "Epoch [29][30]\t Batch [5450][5500]\t Training Loss 0.0002\t Accuracy 1.0000\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0002\t Average training accuracy 1.0000\n",
      "Epoch [29]\t Average validation loss 0.1137\t Average validation accuracy 0.9794\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9814.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from criterion import EuclideanLossLayer,SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "exec_result2 = pd.DataFrame(columns=['mode','hid_layer','number_of_neurons','time','loss_validate','acc_validate','acc_test'])\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "batch_size =10\n",
    "max_epoch = 30\n",
    "disp_freq = 50\n",
    "init_std = 0.01\n",
    "momentum = 0.90\n",
    "weight_decay= 0.00001\n",
    "\n",
    "#CrossEntropy+ReLU+1隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 1, '128', t2-t1,relu_loss, relu_acc, relu_acc_test]  \n",
    "\n",
    "#CrossEntropy+ReLU+2隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 512))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(512, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 2, '512,128', t2-t1,relu_loss, relu_acc, relu_acc_test]  \n",
    "\n",
    "#CrossEntropy+ReLU+2隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 2, '256,64', t2-t1,relu_loss, relu_acc, relu_acc_test]  \n",
    "\n",
    "#CrossEntropy+ReLU+2隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 300))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(300, 100))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(100, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 2, '300,100', t2-t1,relu_loss, relu_acc, relu_acc_test]  \n",
    "\n",
    "#CrossEntropy+ReLU+3隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 512))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(512, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 3, '512,256,128', t2-t1,relu_loss, relu_acc, relu_acc_test]  \n",
    "\n",
    "#CrossEntropy+ReLU+4隐含层\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "reluMLP = Network()\n",
    "t1=time.time()\n",
    "reluMLP.add(FCLayer(784, 512))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(512, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "relu_acc_test =test(reluMLP, criterion, data_test, batch_size, disp_freq)\n",
    "t2=time.time()\n",
    "exec_result2.loc[exec_result2.shape[0]] = ['CrossEntropy_ReLU', 3, '512,256,128,64', t2-t1,relu_loss, relu_acc, relu_acc_test]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossEntropy_ReLU_2Hidden</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>896.925900</td>\n",
       "      <td>[0.11949462972625886, 0.09438919389988301, 0.0...</td>\n",
       "      <td>[0.966, 0.9728000000000001, 0.9755999999999999...</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.983507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CrossEntropy_ReLU_3Hidden</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1105.222249</td>\n",
       "      <td>[0.1115921364747895, 0.1182899032331471, 0.100...</td>\n",
       "      <td>[0.9663999999999999, 0.967, 0.9728000000000001...</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.979707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_ReLU_3Hidden</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1225.801615</td>\n",
       "      <td>[0.13315443972962399, 0.1064612811148712, 0.08...</td>\n",
       "      <td>[0.9618000000000001, 0.9698, 0.977200000000000...</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0  CrossEntropy_ReLU_2Hidden          10              0.001      0.99   \n",
       "1  CrossEntropy_ReLU_3Hidden          10              0.001      0.99   \n",
       "2  CrossEntropy_ReLU_3Hidden          10              0.001      0.99   \n",
       "\n",
       "   weight_decay         time  \\\n",
       "0       0.00001   896.925900   \n",
       "1       0.00001  1105.222249   \n",
       "2       0.00001  1225.801615   \n",
       "\n",
       "                                       loss_validate  \\\n",
       "0  [0.11949462972625886, 0.09438919389988301, 0.0...   \n",
       "1  [0.1115921364747895, 0.1182899032331471, 0.100...   \n",
       "2  [0.13315443972962399, 0.1064612811148712, 0.08...   \n",
       "\n",
       "                                        acc_validate  acc_test  \\\n",
       "0  [0.966, 0.9728000000000001, 0.9755999999999999...    0.9865   \n",
       "1  [0.9663999999999999, 0.967, 0.9728000000000001...    0.9842   \n",
       "2  [0.9618000000000001, 0.9698, 0.977200000000000...    0.9837   \n",
       "\n",
       "   acc_validate_float  \n",
       "0            0.983507  \n",
       "1            0.979707  \n",
       "2            0.979000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result2['acc_validate_float'] = exec_result2['acc_validate'].map(lambda x: np.average(x))\n",
    "exec_result2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAGxCAYAAACOSdkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqD0lEQVR4nO3deVxU9f4/8NcwM+wwDLKI7Jq5YJoaoeSWW+7brSy7P7X0qmm3VbPuzcTyKi3XyzXNtExyveXXwGtZaSmWpnnN0BDNDWRxQQhmWIdlPr8/Jo6MMAsCM+K8no/Hecg5n/P5nPcZPsx5e87nnCMTQggQEREROSAnewdAREREZC9MhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYdktESooKEBkZCQyMzOtWv/AgQPo0qUL/Pz8sGLFCqvLiIiIiEyxSyKUn5+PMWPGWJ0EXb9+HePGjcPjjz+Ow4cPY8uWLdi/f7/FMiIiIiJzFPbY6GOPPYbHHnsMR44csWr9LVu2ICgoCIsWLYJMJsPrr7+O9evX48EHHzRb1hCdTgedTifN6/V6/P7772jTpg1kMlmz7B8RERG1LCEEiouL0a5dOzg5NeG8jrCDCxcuiD/eei8yMjIsrj99+nTx9NNPS/OXL18WXbp0sVjWkMWLFwsAnDhx4sSJE6c7YMrOzr7FbMTALmeE2rdv36j1tVotunbtKs17e3sjNzfXYllDXn31Vbz44ovSvEajQVhYGLKzs+Ht7d2ouIiIiMg+tFotQkND4eXl1aR27JIINZZCoYCLi4s07+rqirKyMotlDXFxcTFav5a3tzcTISIiolamqcNaWsXt876+vrh+/bo0X1xcDGdnZ4tlREREROa0ikQoOjraaGB1amoqgoODLZYRERERmXNbJUJarRZVVVX1lo8bNw4HDx7E/v37UV1djXfffRcPPfSQxTIiIiIic26rMULdu3dHQkICJkyYYLTcz88P//znP/HQQw9BpVLBw8MD69evt1hGREREZI5MCCHsHYS1zp8/j9OnT2PgwIH1BjabKzNHq9VCpVJBo9FwsDQREVEr0VzH71aVCLUEJkJEREStT3Mdv2+rMUJEREREtsREiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIiclhMhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIiclhMhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHpbB3AERERERWKcoGygoMPxeXNEuTTISIiJpb3S/rhri3AXxCW7YNxsAYmrsNe8dQlA2s6g1U6wzzOmF+W1ZiIkREdLPm/LJuiMIFeObnlmuDMTCGO3E/ygrM171FTISI6M7SHP/rbekv62qdYT1TcTS1DcbAGJq7jdshhhbCRIiIbi/2Phtjjy9rIQB9NVBdYWi75Jp19S7/AuiK6y8vON+0+s3RBmNoXP3yQqBCAyhcAbkzIJNZV88UIYCayjp96qp19VrwsyzMPg21FU0U/28rvHy/A6oqbsRfXQFoL1sXQyPJhBDNc5GtldJqtVCpVNBoNPD29rZ3OESOramJzOVUYN1Ay9uZdQBod2/DZda2MfAVwDPgjy/p8htf1pocIG2H5frufoDQ36gv9JbrkONQuN6YAOsSGbc2gKi50RdxZx/etToBVXxxk4/fPCNERDfYezClrc7G/LIFOPW54X/g5UWGfyv++Lc037o2DsTf+vYBoMzMdpwUhjNElviE3zhQ1lVdARRduvX6zdHGHRJDVWUZlNpsi01UeYdC6ex+y/Xrqf7jbEhjlJv+2xNOzpDpKy020dT9MFUfAHQVpXApybHYRnHIIHj5hxgngwpXFBddg9cvH1qs31hMhIhuB/a+G6O2vr0HU1rr1+3Ab7vrJzLFVp7+/9+6pm0fAEJiAE9/w5e0ss4XdoUG+GWT5fqTPgTadjd8Jko3w78KV0DuAlw9ad1ZqUc3Nnxmy9qzWqbqN0cbt0EMeWd/QsDW4Rar5436EAF3xzRYdunEQdyVNNpiG5eGfIC7evS75frnJ3yBu7rdb0h+broklH36KEK/f8liG9kPrkRo1743+lJt35I748LJQzbZj9r6NXqB4ooqaMuroSmvgraiClfOHMHDx56w2MZPkXPRvvsDULkp4e2mhFJueOThtRMHmQgR3bbsPa7ldhgbY2390nzDl3TJNcPp/uI//i3JA/LOmK9f6/Aq69Yz5e6RgG97wFVlmNx8/vjZx5BM/d90y22Mett0AmBNIuR3NxDQuVFhO5q8Eh0CmrCetrzKqvrWrmfO5aJylOUUQVeth65KD111DSqq9NBm/o67rKj/5a9XUHkto4ESZzjneeM5K9r4PMsNlRV6AOV/THVaybtsVRtfnryMyqv1/w6trR/333ScSCpFcUX9M5pRskw87GK5jX99ew6n9t6o7+4sh7erEt2cMvCRFTE0FhMhoqa6E+4yEgKo0JqvX+vENiDjQP3l1g5k/GiIYRxDU3QYDKgj6ycxpfnAbsv/c8agV8yfhaAmJyFNrQ/YLpE5e60Y59OuSGcvas9gaMqr4Jx3Ae9Y0cZbX/+GU6L+pacoWRYet+Lgvyc9D6dOXWiwLEqWh+fs3Ia19QvLKlEsbiQxbko5vN0UULkpEVnjDljxDES1uzO8qhUo1hnaKausQVllDZygRIWLEq6yKsuNNAITIaKmstW4lqu/AhA3Tnkr6lxOsfaeh9P/Bc5+U/9MTMk1wx0m1vjpg1veBQA3kiB3P8CrrWHAsWdbwCsQxWUV8Dq+xmITeTGvNHwp4zZIYq5We0AtlHAx82WtE0oUVnugbQu10RwxFAoveAvzB50KoUSh8GowCWlKfSEESitrkFftiRAr2vjkFy2qzp9ERVWN4YxMteGMjFx7FeusqL90Xx4u43iD5e0AiwffCqFEtYsa7Vxc4aKUw0XhJP2r0lWiosBy/d5dOuB+dViD5TWFTqi4YN82rK0/d9T96Ny5q+GylqsSzoobb/L67Td/VGy13MaiR/uhU6euqK7Ro0T3x6W18mqkZhdh8M5/Qi0z3JVWpSsH8LLJtqzFRIjIVi7sA7IOGy69lFz74988QGt58CAA4L/PND2GH/5ptviKXI5CuelXEKpr9AhqPxRw961XVq65jqLs/RbrK8ZshH/3hwC5sl75tRMHUXJincU2Sk2cAcgr0aHGin2QmzkLcbXaAzonF5QqTN/F5VHtBBcTSUS+PACTdDe+rBtSKLywTh5gMglpahtNrS+EgMa5LQZb0ca0qy64WHmlXlnW7y74xIr67X8ohuzgT9D+cSbGcDamGjV6Q3LfDpbbuHxGAGhoIK8bBltR30kdit7ervB2VUjjUmoP5MUVVRi8z4rP8q9j0C1YVa8sLVeDwe9ZUX9IbIP1DW2EYHC6fduwun77Tujg79lgeZVnsFV9ap1nMABAIXeCj7szfNydARieKHAZfrgs/AAAelFmsp3GsEsilJaWhieffBLnz5/HzJkz8fbbb0Nm5pkJQgi88847+PDDD1FYWIjJkyfj7bffhoeHBwDgnXfewbvvvovy8nIMGzYM69atQ5s2bWy1O9TaNcdAZWt8t8RkkVUJiEcQAGH8XI06dxZZ1Ubw/YaxKZ6BgFeg4UzMHz//mvELpqf+HZVOpv8WnfUCid2n4557RtUrS/3ff/GM+M1i/VXVevg3kAQBwPXK3zE5pJ3lNiobHndxtkKH56yo/+8K04nQmSo9/hraDnAyc/lOL8d7VXqTicwVhQJXFc4mq4tq469eIYTRmYyrmgqjL3xTUrOLoCmv/7/rjPxSq+q/t+8c5E4yaMurpUtBWqNExHIby78yN67Lcv3L503fPadwAi7rLbcxOToEoWp3uNaejVHI4aJ0wjWNDsu+Om2x/hd/7m0mgdBg5T7LMZjT2P5wu7bR1PpqD2cUKAJwudr0Z+micILaw/Q2ZIoiyBSlf8yUm1yvMWyeCOl0OowdOxYPPfQQ/vOf/+DZZ59FYmIinnzySZN11q9fj5UrV+Lzzz+HSqXCn//8Z8yZMwebNm3C999/j08++QTff/895HI5nnvuObz00ktITEy03U6Rfdl6oLIQwO8XgeyjQPZPwMUGxss0pG13oE0HQ+LhGfjHZaFAXNFkYszJf1k8eH8xIAFBHYYaF9QYHsKX9uuXmPbrm5aTmPuebTCJAYDLFQfN1geASicZLpcU4Z4GyrTVJVbV//FSLi7UXIKuWl/nUkYNdFV6XMu/ikq55TY+O3MdPxb+VufAZzj4nSoosyqGk8V6+ORqGixPv3rFfBIEAE412HPmIrLyXIwGxuqqa3CxMAceHd6FzMn07e9Cr8ATiQqIKh8pAbqZ0Rd+Q21Ue+C15DST5dbU/+aUyWKr2+jqHwYPl/qHklJdNdKvZ1ms/5e+PdE5yLvemRiVmxLn84oxdtUh80EC+H99IkyejbFmHyxpShv5FVet6g/5FfcBaDgZux3aaI4Ygn3c8J+5nZGjuW6yjRCVP4J93Kzah5ryJo41/IPNE6GvvvoKGo0GK1asgLu7O5YtW4Z58+aZTYQ2btyIBQsW4P777wcALFmyBI899hgA4OjRoxg1ahQ6deoEAHj88cfx/vvvm2xLp9NBp7tx0NNqrRwgSrcnWw1UvrAPKP/9RvJzU+Jl1dmYce81OED3wvHPrDp4X9D8jqCbC+QKQO6J3KqaJiUxjXFdq8OBs9eRp61AXrEO1/+YSq9fgjWPjf3izAVknvi+wTInFw3crTj5tvNCDfSn6j/p1sk1Fx6Rluu/u/cs3q5o+MBmbRv/+V829BX1ExhDffPPAJI5VaO4SgN9Rf1LCDIZoHQpgnO45YOOT8Fr8JD71ysrrbmOojaW6w/zXoEebSOgcr+RfNQmI6mXM/DS4SkW21gw4D8Y1KFTvbKUC7/hme+ftVg/tvN/MKhDSIPlMpnM7klIlex3q9qoksU23Ia8zGxdwNAfIDdzmed2aKMZYrhScgUz9z2MSjPjEZ3lzvhiwhcI8qz3bWdVDLfC5onQiRMn0KdPH7i7Gx641L17d6Snp5utk5+fj7CwG4O35HI55HI5AKBbt2545plnMHv2bHh5eWH9+vUYNmyYybaWL1+OJUtMX6IgG2stD+Db9azxvNzFkNSE3o8rzu4Yk7HV8hmd8vz6iQwM/3O2Rn5ZMTS6hs9ilNdYd4r48LWzuHh0N8oqK1BeXYHyqgpU1OhQUa1DnuY3q9p4O/MTVGduMXwhyQyTzKkKMh/rBlvnh34DT3xj1bqmeEeug0LmAicoIYMSMqEEhBJVNVWwJgrPwBTIRcPjGGpkJVY9jzc05CzauJTCRe4KF7kzXBUucJW74HqFDieteC/kguGdMLxjb+lslqvS8K9SLsMXZ/6Hvx21fNBZODoUY7vcX69s1+mjVtUf3NUDY7s0nPVV5hZbdeArrmy4TxZXappUH2iGRKYZDt4uLuVWteHi0vDfYICXFbdaAXB3qTL59+3uYt1dUi3ZhrX1ze1voa7QbBIEAJU1lSjUFTaYCFn7WTaWzRMhrVaLyMgbf3gymQxyuRyFhYVQqxv+7+S9996L5ORkTJw4EQCwYcMGDB9ueEjWiBEj0LFjR9x1l2HEQHR0NF555RWT23/11Vfx4osvGsUTGmq7l7tRHbZ8AN/ZbwwDlW9+AJ8m17qzOc4+QMQDQGiMYQr640F4sP6Mztmi65CVXkVBeQGul1/H9fLryC/Px/HrP1u1C4suJAAXEqxa15Qdmh2AxorXP5ghXAsgb0J9pZMLFE4Nf97Vej2q9JaziBroUCNuWk8Gq7/RhPuvaOr/K39XfoPf9QD0AG7hbt6NGX/HziuecJW7wlnuDBe5i2FSuKCg1PQZkLpSC47A/VKxVN9Z7gxXhSvyyusPXm6VmpjIWHvgbI4D7Pc53+OXvF+grdSiuLIYWp3h3yul1v0uZuyZ0eQYboc25uydAw+lh9SXa/uli9wFOivfHL/0yFIonZTQ1eigq9GhsqYSuhodSqus+7toLJu/a2zhwoWoqqrCihUrpGWhoaE4cuQIgoODG6yTmZmJkSNHws/PD1qtFidPnsT333+P/v3747PPPkNcXBySkpLg7++P+fPnQ6PRYMcO677s+a6xJmrKGZ1bfS9UeZHhsfmFlwzJzRHTl0ItuSKXY4wVg2u/GPAvBHVo+EzjV8e/wMu/vnrLMdiSrFoNBTwglykhlzlDLlNCIXOGwskZlfoKFCHVYhtT734W/SPuuXHg/uOL7kDGabz1i+Vn+Cy7f32DZzGA2jMZlr+IX+y+HP0iOklflLVflj9mncK285Zv7x8aPAGd/Ru+HHPmeg6+zU222EaUujfaeHgYtl99I4bfKzQo1Fn5mg5CW7d2ULupjJPBP/pUeVU59ufst9hGbFAsXBWu0OkNv4vaA6e2Umt1IkKtT015DU4/fbr1vWvM19cXaWnGA/yKi4vh7Gx6lHhERATS09Nx5swZvPzyywgMDET//v0BANu2bcPTTz8tjRFKSEiASqVCUVERfHx8Wmw/CM1yRseqszGHVxteSll4yZAAVRiftrWmDV//e+HSJrzeA/gKi86hMneX2d2sdJLhuk6D6uJs5JbkIrc4F7klucgpyUFuSS4uFmaarW9EOEFf7QlR7Q1R7Ql9tTcAAWf1/yxWHdNmOe5rdw9cFE5wVRgGCTv/cTnlp9wTWPWb5ee+/iP23SYnIZ19eqBPUP02VMqmH3C83Bq+m+xmkepgdFR3rLe8uMQd22A5ERocPNbs52BNIvREx7kNtpGS+Qv+emCqxfoLe7+JewIjjZKo2qQuLe88Pju/0WIbd/t0hYezCyqqK6T6lTWVKKkqRVl1y/zvubldLb+Mq+VNe6v4j1d+bKZobt09fvcgyCMI3i7e8HL2grezN7ydvaHVafHvX/5tsf6WUVvQpU2XBstOF5zGE7stv5qiJduwtn58/3gEewbXO5ujq9HhkvYSPk772GIb8+6dh/aq9vXOKuVoczD/+/kW6zeWzROh6OhofPTRjYdkZ2ZmQqfTwde3/nNJ6pLJZPD29sa3336LQ4du3EVQXV2Na9euSfNXrhi+jGtqmmc0OZnR2PE51bobD+8rvoorl7637mxM+g4E3fz7dPcD1OE4B2c8psixfLt1t+fRN3pcvbLfj38GWEiEAODP/1sCYTlXMavs0gzUlHWAk8wJEW080CHAE3cFeKJEn4nk65Ybj+0QgLFdwhssyy5peLyLLVmbxJhbr6mXMpojhqa2Ye0+9Aq6G13bdG2wrJNvulWJ0Jv9FjfYRnpBOiZ/Mdli/Q+HfYi7fe9usOzs72fxl71/ueU2rK2/uO9itPVoa3zQ/CM5zCnJwbYz2yy2Ma3rNIR5h904cDoZDp6XSy9jyWHLY0Kb43N4rc9rJn8X1iRCCicFlE4N9ymFk3WH6pZsw9r6kapIk/06vSDdqkRoQMiABtuQwfwQhFtl80RowIAB0Gg02LhxI6ZOnYr4+HgMHToUcrkcWq0Wbm5uUCob/kUuXboUjzzyCHr16iUte+CBB7BixQqEhITAzc0NCQkJ6Nu3L58jZIXconIUlpoeuKb2cDZ5G2OjfDYN0GmA8kKjxYXOSlQGNzR8+IZKJxmutB+KoA6DAXW44S3RPmGAi+HAf/HwVlSeXW6xDW31jee6/17xOy4WXcRFzUWkZH9l1S4IAC5yF7R1D4KXIhCyal+UlXnjeqEHrhWXwC34M4ttTIvpisk9+iLCzx0uihujbHad1iLZ9N2kNhOi8ofQKywOTA1R1b9LCQA6+bWF0skZVWbecK10ckYnP1NP3wHULmo4y50t3lWidml4PGFzxNAcbbQW3i7e8HVt+D+h3i7WXWow1Ya19bu26Wr2wGlNIjSq/SiTSYg1muNzoNbL5omQQqHAunXrMGXKFCxYsAA1NTU4cMDwHJbu3bsjISEBEyZMqFfv/Pnz2Lp1a73Las8//zwuX76MN998E/n5+ejbty/Wr19vi11p1XKLyjH43ZQGn19Sy0XhhH3zB5lMhq7n56HhQ+JNijJv/Cx3/uM5OgEo1wsAlsdSXG8/Aegzpd5ybUUV8rTWDb5bezEZqy/vwvWKbJRUm75LxZR7yqfhkuZepBXW356Ta65VbfQM80Gntl71lns5q6xKQLycG769F2h6EgMAPdtFYtNDn1t8xkfPdg3fZRTkGYQvJ36BQl1hg+WAIdFp8LbYOm18MeHW22iuGJrSRlOTObqzNEd/uB3auB36tTUx3Aq7PFl6woQJOHfuHI4dO4bY2Fj4+xu+nDMzM03Wueuuu6DR1D+Aubq6YuXKlVi5cmVLhXtHKiytRJvqPPOPOq/2QmFp5Y1ESK8Hrp4Ezu0Fzu2BX85Rq7aV+8BSBPcYakiA3NSGB6UAyD+8FbBwNgcA3s/4DOsup6CssgbllTUorzK8gK+qRg9XJy1gxVWhc9WnjV72p69UQ18ZAFHtBqVPqsX6R656Q19hSIJCfd3QrZ0KUe28ERWsQm6pCm/9ajkGUzq2CUPVpQWoMvM2QiU80bFNw+8QApqexNRtx9I65gR5BplNMmzRhr1jaGoyV1tu74PW7RBDU90On0Nz9IfboY3boV/fHENJcQli0MA7BxvJ5neN3W4c9a6x335LR/jWARZffpf18G7cLb8CnN0DnN9rGN/TSLv7/gcuYb2MniCsq9YjK2MXPqte15TdsJp3VSzc9Z1RWe6PslI1tGVO0FXr/3j43XsW6w9TLcfD9/RBVJAKKnfjS7eGB8c9ZvFszCoTD54DbHiZklqNKyVXmnTQaWr92yGGKyVXMCZ5zK0/gK8ZYmiuNsigOT/L5jp+MxFy0ETo/ImDuCtptMX1quEEBW5cPiuHK4469cAPuBdZVWqsk8dbbGO07h84Jeo8O0pRBKXPMbj4HAaUlu9saVMcCy+vDlC7K6F2d4baw9nwr7sSvxVk47+Zmy220dAt2xVVNfi/Xw/jrV+fvqX6tXKLyjE4IcniGZ19z09kMkPUSExCyJTmOn7z7fNklgJ6XNAHYb/+XuzT98QxfSdUwnBGJEqWgSvOVtz+rnKFk6c7qpzToFUeQrFTOmDVs3sNXhoyw+ytztYkQg1xVcqtHltjbnxOsI8b9j0/kWd0iFpAc1zqJDKHiZCDyswvhYcVz9/5391vQdZ5NMIUTphT+4LLP/49k+GMMb+Zv/1dKQT6B5xEkfYTFOmKpOXRbaMR4d4D2y9+ZLKuNZp6q7Ofa1uUXphv8V1GfsPN3yEU7OPGRIeIqBViIuRgTl3WYOuX36FT7vtYGG75GT6r2geib492DZafL1JZfLVElUyGfdcNT4YNcAvA+LvGY8JdExDmHYaUC7/hs/OJTTob09RbndUeznCGL3QVPibruyicoPYw/cBPIiJqvZgItWaNeL3F+auF+G7nJnTN+RT/kKch3VmJSifLz/Cp+/ydW9XVpyfm9Z6J2HaxRg/lao67pZp6q3Owjxv2zR/Ey1pERA6KiVBrVZQN8V5vyGpMP0dHyF1w5ZFd+OW77bg3LwmzZfmAHNBDhhLfKAC/W9xMgT4P3+d8j+tlN14Sml+ej+vl15Gtte6x+E/3eAEDQnrWW95cY2uaOoaAl7WIiBwXE6FWKi/vMgLMJEEAIKvRwW/bSIyW1QAyoMTJG7ruf0abgXPgee0X4KdFFrez/Px64HzTHlBp7pUDTEKIiMiemAi1UtryKgRYsZ6zrAYXnTtB2WcWQvv/GZ5KVwBATeE5q7bjo/RGkFcw/N394e/mjzZubeDvZvi5uKoYiw5ZTqaIiIhuV0yE7nBHeixDn4nzpPlL2kv4/Nzn+Pzc51bVX/vQh2bfA0RERNSaMRFqxa5Ycfu7X/seKKsqw95Le/H5uc9xPO+4DSMkIiK6vTERaqVSr2ZheYjlZ/j0yVyP46fmo7TK8JwcJ5kTHmj3AKLbRmPFzyuaFMPt8C4hIiKipmAi1Iro9QLfnr6GnfsPY2TRclSGWn6Gzw9FxwAAIZ4hmNhxIsZ1GIe2Hm1xpeQKVqWualIS0xwv4SMiIrInJkJ20piXbJZX1mDH8Rx8/MNF9Cj8BvHKRGS7VAOwnGD09IrCM31fxH1t74OT7MZltOZKYvj4eyIias2YCNlBblE5Br+bAl213uQ6Lgon7Hg6FnvSr2HT4UzUlBVhqfJjjHM+DAAo8e4CwPILSx+P+DPuD2r4PV1MYoiIyNExEbKDwtJKs0kQAOiq9Zjw/iFU1wjEyE5jpdsaBIp8ZChdsDtqGP5PdxWosJwIufkGN1fYREREdxwmQnYiUxRZfNGnrNoT76p3YUDl5/jGwx1fqsJwSgFAc9Lq7Zh7mCEREZGjYyJkB/kVV+HZ4R3AqcbkOjK9E+aUy/GNkxZvuraDXmYYGC2XydG3XV/cG3AvVv2yylYhExER3ZGYCNlBueac2SQIAISTHms89AAMA6Z7+PfA6PajMTx8ONq4tUF6QToTISIioiZiImQHTjqtVesF6WV4uNtUjOw8GaFeoUZlfIYPERFR0zERuo29ePfLGBH95wbL+AwfIiKipmMidBuTOZl+fQbA29+JiIiayvyRllpESUW1Vet5uDBPJSIiaklMhGysRFeNz4/nWrWur7tzC0dDRETk2JgI2ZAQAq9+/isul/9u71CIiIgITIRsavNPWdh1Igvl/octruusF1ArPW0QFRERkePiIBQb+TVHgzd3pUMdkIxSVw08avSIv56PgJqGnyeklikQ1KaTjaMkIiJyLEyEbEBTXoV5W4/Dxf0YqtscAwAsK5dj0CPbAVfvhiu5twF8QhsuIyIiombBRKiFCSHw8v+dwNXiTPi0/wzlAKaVVWPwlK8Bdbi9wyMiInJoHCPUwj4+lIk96VmIDH0P5U5AD10Vnhu3hUkQERHRbYCJUAs6nlWI5btPo0/blbjsUgWfmhq82+8fUAb3sndoREREBCZCLaawtBLPbDmOAd6fIM2nAACw/K4paNv1T3aOjIiIiGoxEWoBer3Ai5+l4q6KL3EqMA0A8BfVPeg34DU7R0ZERER1cbB0C1j7/UVozu2HiNiNMiclohVqzB37ib3DIiIiopvwjFAz++liAZL27MPdQR/hvIsSbaDAW+M/g0KutHdoREREdBOeEbpFuUXlKCytNFpWVFaJJVv3YYr6HazwdoVMAG8N/jf8PdvaKUoiIiIyh4nQLcgtKscT7/4fPGo0RsvdUYFXPT7CQj/D2Z//d9f/Q0zYAHuESERERFZgInQLSq5lINH5JZQp9EbLK2QyvOrfBhVOTuhZXoExfqPsFCERERFZg4nQLfhdex7PhAag0klmcp1TLi4oKr4IoJvtAiMiIqJG4WDpW6CtLjGbBAFApZMM2uoSG0VEREREt4KJEBERETksJkJERETksOySCKWlpSE6OhpqtRoLFiyAEMLs+kIIvP322+jYsSP8/Pwwb948lJaW1lvvsccew1//+teWCpuIiIjuMDZPhHQ6HcaOHYvevXvj2LFjSE9PR2Jiotk669evx8qVK7FlyxYcOnQIR48exZw5c4zW+eabb7Bv3z68+eabLRg9ERER3Ulsngh99dVX0Gg0WLFiBTp06IBly5Zh/fr1Zuts3LgRCxYswP33349OnTphyZIl2Llzp1ReXl6OuXPnIj4+Hj4+Pi28B4CHi3U321m7HhEREdmHzROhEydOoE+fPnB3dwcAdO/eHenp6Wbr5OfnIywsTJqXy+WQy+XS/Jtvvony8nIoFArs27fP7KU2nU4HrVZrNDWWr69/s65HRERE9mHzREir1SIyMlKal8lkkMvlKCwsNFnn3nvvRXJysjS/YcMGDB8+HACQlZWFFStW4K677kJWVhYWLFiASZMmmUyGli9fDpVKJU2hoaGN3ge1X2c4O5l/d5izkxJqv86NbpuIiIhsx+bXbhQKBVxcXIyWubq6oqysDGq1usE6y5Ytw8iRI9G/f39otVqcPHkS33//PQAgMTERgYGB2Lt3L1xcXPDcc88hPDwce/fulZKlul599VW8+OKL0rxWq210MhTkGYQvJn6JQp3p5E3tokaQZ1Cj2iUiIiLbsnki5Ovri7S0NKNlxcXFcHZ2NlknIiIC6enpOHPmDF5++WUEBgaif//+AICcnBwMGTJESq68vLzQsWNHZGRkNNiWi4tLvUTsVgR5BjHRISIiauVsfmksOjoaR44ckeYzMzOh0+ng6+trtp5MJoO3tze+/fZbxMfHS8tDQ0NRXl4uzev1euTk5CA8PLz5gyciIqI7is0ToQEDBkCj0WDjxo0AgPj4eAwdOhRyuRxarRZVVVUm6y5duhSPPPIIevXqJS179NFHsWvXLuzYsQM5OTl49dVXodPp8MADD7T4vhAREVHrZpcxQuvWrcOUKVOwYMEC1NTU4MCBAwAMd5AlJCRgwoQJ9eqdP38eW7durXdZrVOnTvj000/x2muv4cyZM+jQoQN27twJLy8vW+wOERERtWIyYemxzi0kNzcXx44dQ2xsLPz97XebuVarhUqlgkajgbe3t93iICIiIus11/Hbbk/8Cw4ORnBwsL02T0RERMSXrhIREZHjYiJEREREDouJEBERETksJkJERETksJgIERERkcNiIkREREQOi4kQEREROSwmQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRERE5LCYCBEREZHDYiJEREREDouJEBERETksJkJERETksJgIERERkcNiIkREREQOi4kQEREROSwmQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRERE5LCYCBEREZHDYiJEREREDouJEBERETksJkJERETksJgIERERkcNiIkREREQOi4kQEREROSwmQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRERE5LCYCBEREZHDYiJEREREDouJEBERETksJkJERETksJgIERERkcOySyKUlpaG6OhoqNVqLFiwAEIIs+sLIfD222+jY8eO8PPzw7x581BaWlpvvaqqKtxzzz1ISUlpociJiIjoTmLzREin02Hs2LHo3bs3jh07hvT0dCQmJpqts379eqxcuRJbtmzBoUOHcPToUcyZM6feem+//TbS0tJaKHIiIiK609g8Efrqq6+g0WiwYsUKdOjQAcuWLcP69evN1tm4cSMWLFiA+++/H506dcKSJUuwc+dOo3XOnTuHd999FxERES0YPREREd1JbJ4InThxAn369IG7uzsAoHv37khPTzdbJz8/H2FhYdK8XC6HXC43Wmf27Nl45ZVXEB4ebrYtnU4HrVZrNBEREZFjanQidPbs2SZtUKvVIjIyUpqXyWSQy+UoLCw0Wefee+9FcnKyNL9hwwYMHz7caF6j0eCll16yuP3ly5dDpVJJU2ho6K3tCBEREbV6jU6EevTogV69euGtt95CRkZGozeoUCjg4uJitMzV1RVlZWUm6yxbtgxHjx5F//790aNHD3z66ad45plnAADXr1/Hq6++ivXr10OhUFjc/quvvgqNRiNN2dnZjd4HIiIiujM0OhHKz8/H3/72N/z666/o3bs3YmJi8K9//Qs5OTlW1ff19cX169eNlhUXF8PZ2dlknYiICKSnp2PdunUICwvDsGHD0L9/fwDA888/jxkzZuDee++1avsuLi7w9vY2moiIiMgxyYSle9fNqK6uxoYNG/Dyyy+juLgYsbGxiI+PR2xsrMk6+/btw+zZs3Hu3DkAQGZmJrp06YKSkpJ6435ulpubi7vuuguHDh1Cr169DDsgk8HLywtOToacrqSkBK6urnjttdfwyiuvWNwHrVYLlUoFjUbDpIiIiKiVaK7jt+VrSQ04d+4cduzYgc8//xynTp3CyJEjMXnyZJSVleHhhx/G5cuXTdYdMGAANBoNNm7ciKlTpyI+Ph5Dhw6FXC6HVquFm5sblEplg3WXLl2KRx55REqCANS7PPfYY4/h+eefx4gRI25l14iIiMiBNDoRuueee3DhwgU89NBDeOGFFzBu3Dh4eHgAMCQl/v7+5jeoUGDdunWYMmUKFixYgJqaGhw4cACA4Q6yhIQETJgwoV698+fPY+vWrfWeE3Tz7fKurq5o27YtfHx8GrtrRERE5GAafWls8+bNGD9+PLy8vJq04dzcXBw7dgyxsbEWk6eWxEtjRERErU9zHb+bNEYIAPLy8hAQENCUJuyKiRAREVHr01zH70bfNZaeno5evXph+/btAIAhQ4YgKiqqyc8XIiIiIrK1RidCs2fPxuDBg6UHGh45cgRjx45t8N1fRERERLezRl8a8/LywtmzZxEUFCQty83NRdeuXaHRaJo9wJbGS2NEREStj90ujd1zzz3YtGmT0bJNmzYhKirqloMgIiIisodG3z6/evVqjBw5Ep988gkiIiKQkZGBwsJCfP311y0RHxEREVGLuaW7xoqLi7Fr1y7k5uYiNDQUo0ePbvLt9PbCS2NEREStj12fLO3l5YUpU6YYLbt+/bpdnwdERERE1FiNToTS09OxYMECnD17FjU1NQAAIQQuX74MnU7X7AESERERtZRGD5Z+8skn0bFjRwwYMAC9e/fG6tWr4erqivj4+JaIj4iIiKjFNDoRSktLw9/+9jfMmjULly5dwsiRI/HRRx8hMTGxBcIjIiIiajmNToTuvvtufPzxx+jRowcuXLiA/Px8BAQE1HsLPBEREdHtrtGJ0MqVK5GQkACtVosZM2agffv2uO+++zB+/PiWiI+IiIioxdzS7fO1VWQyGQ4cOICSkhKMGDECcrm82QNsabx9noiIqPWx6+3zMplM+nngwIG3vHEiIiIie2r0pbH3338fly9fbolYiIiIiGzqlsYInTx5siViISIiIrKpRidCixYtwtKlS1FSUtIS8RARERHZTKPHCJ0/fx56vR4dO3bE1KlT4eHhIZW9/vrrzRocERERUUtqdCKUmZmJTp06oVOnTsjLy2uJmIiIiIhsotGJ0IYNG1oiDiIiIiKba3QilJWVZbIsLCysScEQERER2VKjE6GIiAjIZDKjhyrWqn0bPREREVFr0Oi7xvR6PWpqaqDX61FaWor9+/dj0KBB+O6771oiPiIiIqIWc0uv2LhZaWkpBgwYgJ9//rk5YrIpvmKDiIio9Wmu43ejzwg1JC8vD1euXGmOpoiIiIhsptFjhCIjI+uNC7py5Qqef/755oyLiIiIqMU1OhFKTEw0mpfJZAgJCUH79u2bKyYiIiIim2h0InTz2+bz8vIQEBDQbAERERER2Uqjxwilp6ejV69e2L59OwBgyJAhiIqKwtmzZ5s9OCIiIqKW1OhEaPbs2Rg8eDCGDx8OADhy5AjGjh2LOXPmNHtwRERERC2p0bfPe3l54ezZswgKCpKW5ebmomvXrtBoNM0eYEvj7fNEREStj91un7/nnnuwadMmo2WbNm1CVFTULQdBREREZA+NHiy9evVqjBw5Ep988gkiIiKQkZGBwsJCfP311y0RHxEREVGLuaUnSxcXF+OLL75ATk4OQkNDMXr0aHh5ebVEfC2Ol8aIiIhan+Y6fjf6jBBgGCf0+OOPAzDcPt9akyAiIiJybLx9noiIiBwWb58nIiIih8Xb5zlGiIiIqNXh7fNERERETdTk2+cvXryIoqIi3j5PRERErU6jE6GePXvi3Llz0u3zjz32GFQqFf7zn/+gR48eLREjERERUYu4pdvnL126hCtXrmDfvn344YcfUF1djT59+jR3bEREREQtyqoxQlevXsWmTZswdepUBAUF4b777sP27duxf/9+fPDBBygoKEBKSorVG01LS0N0dDTUajUWLFgAS+O1hRB4++230bFjR/j5+WHevHkoLS2VytetW4egoCAolUoMHz4cV65csToWIiIiclxWJULt2rXD9OnTkZubi/Xr10Or1eLw4cNwc3PDgAED4OHhYfUGdTodxo4di969e+PYsWNIT09HYmKi2Trr16/HypUrsWXLFhw6dAhHjx6Vbtc/ePAgFi1ahE2bNiEjIwMVFRWYP3++1fEQERGR47IqEdq7dy8WLFiAoqIijB8/Hj179sSMGTOg0+mQl5fXqA1+9dVX0Gg0WLFiBTp06IBly5Zh/fr1Zuts3LgRCxYswP33349OnTphyZIl2LlzJwDgt99+w5o1azB06FCEhITgySefxLFjx0y2pdPpoNVqjSYiIiJyTFYlQkOGDEF8fDx+/vlnXLlyBa+99hqEEGjTpg1iYmLQqVMnzJ0716oNnjhxAn369IG7uzsAoHv37khPTzdbJz8/H2FhYdK8XC6HXC4HAMyYMQOTJk2Syn777TfcddddJttavnw5VCqVNIWGhloVNxEREd15Gv0cIT8/Pzz++OP4+OOPkZ2djbS0NDz99NPIysqyqr5Wq0VkZKQ0L5PJIJfLUVhYaLLOvffei+TkZGl+w4YN0pOt6yooKMDatWvNJmWvvvoqNBqNNGVnZ1sVNxEREd15bumusbq6dOmCLl264Pnnn7dugwoFXFxcjJa5urqirKwMarW6wTrLli3DyJEj0b9/f2i1Wpw8eRLff/99vfXmzp2L2NhYjB492uT2XVxc6m2fiIiIHFOTE6HG8vX1RVpamtGy4uJiODs7m6wTERGB9PR0nDlzBi+//DICAwPRv39/o3U+/vhjfP/990hNTW2JsImIiOgOZPNEKDo6Gh999JE0n5mZCZ1OB19fX7P1ZDIZvL298e233+LQoUNGZUePHsXzzz+PXbt2ITAwsEXiJiIiojtPo8cINdWAAQOg0WiwceNGAEB8fDyGDh0KuVwOrVaLqqoqk3WXLl2KRx55BL169ZKWXbt2DWPHjsXChQvRu3dvlJSUoKSkpMX3g4iIiFq/Rr99vjkkJydjypQp8PLyQk1NDQ4cOICoqChEREQgISEBEyZMqFfn/Pnz6N27N9LS0ozu9EpISMALL7xQb31rd4tvnyciImp9muv4bZdECAByc3Nx7NgxxMbGwt/f3x4hAGAiRERE1Bo11/Hb5mOEagUHByM4ONhemyciIiKy/RghIiIiotsFEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIiclhMhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIiclhMhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIiclhMhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHJZdEqG0tDRER0dDrVZjwYIFEEKYXV8IgbfffhsdO3aEn58f5s2bh9LSUqn8wIED6NKlC/z8/LBixYqWDp+IiIjuEDZPhHQ6HcaOHYvevXvj2LFjSE9PR2Jiotk669evx8qVK7FlyxYcOnQIR48exZw5cwAA169fx7hx4/D444/j8OHD2LJlC/bv32+DPSEiIqLWTiYsnY5pZsnJyXjqqaeQk5MDd3d3nDhxAvPmzcPBgwdN1hkwYAD+9Kc/4bnnngMA7N69G4899hi0Wi0SEhLwwQcf4PTp05DJZNi5cye2b9+OzZs3N9iWTqeDTqeT5rVaLUJDQ6HRaODt7d28O0tEREQtQqvVQqVSNfn4bfMzQidOnECfPn3g7u4OAOjevTvS09PN1snPz0dYWJg0L5fLIZfLpfYGDx4MmUwGALj//vtx/Phxk20tX74cKpVKmkJDQ5u6S0RERNRK2TwR0mq1iIyMlOZlMhnkcjkKCwtN1rn33nuRnJwszW/YsAHDhw9vsD1vb2/k5uaabOvVV1+FRqORpuzs7CbsDREREbVmCptvUKGAi4uL0TJXV1eUlZVBrVY3WGfZsmUYOXIk+vfvD61Wi5MnT+L7779vsL3atkxxcXGpt30iIiJyTDY/I+Tr64vr168bLSsuLoazs7PJOhEREUhPT8e6desQFhaGYcOGoX///g22Z6ktIiIiolo2T4Sio6Nx5MgRaT4zMxM6nQ6+vr5m68lkMnh7e+Pbb79FfHy8yfZSU1MRHBzc/IETERHRHcfmidCAAQOg0WiwceNGAEB8fDyGDh0KuVwOrVaLqqoqk3WXLl2KRx55BL169ZKWjRs3DgcPHsT+/ftRXV2Nd999Fw899FCL7wcRERG1fja/fR4w3EI/ZcoUeHl5oaamBgcOHEBUVBQiIiKQkJCACRMm1Ktz/vx59O7dG2lpafXu9Hr//ffx/PPPQ6VSwcPDAz/99BMCAwOtiqW5br8jIiIi22mu47ddEiEAyM3NxbFjxxAbGwt/f/8mt3f+/HmcPn0aAwcObNQHwkSIiIio9Wn1idDtgokQERFR69NqH6hIREREdLtgIkREREQOi4kQEREROSwmQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRERE5LCYCBEREZHDYiJEREREDouJEBERETksJkJERETksJgIERERkcNiIkREREQOi4kQEREROSwmQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRERE5LCYCBEREZHDYiJEREREDouJEBERETksJkJERETksJgIERERkcNS2DuA1qKmpgZVVVX2DoPIISiVSsjlcnuHQUQOgImQBUIIXL16FUVFRfYOhcih+Pj4oG3btpDJZPYOhYjuYEyELKhNggICAuDu7s4vZaIWJoRAWVkZ8vLyAABBQUF2joiI7mRMhMyoqamRkqA2bdrYOxwih+Hm5gYAyMvLQ0BAAC+TEVGL4WBpM2rHBLm7u9s5EiLHU/t3x7F5RNSSmAhZgZfDiGyPf3dEZAtMhIiIiMhhcYxQC8stKkdhaaXJcrWHM4J93GwYEREREdXiGaEWlFtUjsHvpmDMewdNToPfTUFuUXmzb7uoqAgPP/wwPDw80KtXLxw7dqzZt2FJXFwcZDKZ0TRmzBibx9FSIiIipP0KDAzErFmzUF5u+Xcpk8mQmppab3lmZiZkMpnRoxomTJiAuLg4i23++OOP6NatG5RKJaKionDw4EEAQEpKCnx8fOrFnZycXC+mzMxMi3GbWsea+kREtyMmQi2osLQSumq92XV01XqzZ4xu1ZNPPonS0lKkpqZi5syZGDdunFUH6eY2atQoFBYWStOnn35qVb3apOB2ERcXh8TExHrLN2/ejIKCAiQnJyMlJQXLly9v8VgiIiKM5jUaDSZOnIi5c+fi8uXLGDZsGB5//HEIIaxus7CwEGFhYc0cKRHR7Y+XxhpJCIHyqhqr1q1oxHplldVm13FTyq1ODDIyMrBz507k5uYiKCgIHTt2xFtvvYV9+/Zh9OjRVrXRXJRKZb0zEncSDw8P+Pr6om/fvpg6dSqOHDli8xjOnDmDZ555BnPnzgUAPPfcc/j3v/+Nq1evWt3Gnfw7IiIyh2eEGqm8qgZdX//GqunhDw5b1ebDHxy22Ja1yRcAHDp0CO3btzd6EN28efOgUqkwffp0xMXFYfPmzejUqRNWrVolrZOWloZ+/fpBpVJh1KhRyMnJkcr27NmDLl26wN3dHQ888AAuXLgglW3evBkRERHw8PDAyJEjUVBQYDHG6dOnY9GiRZg3bx48PT3RtWtXnD59GgDg6uqKyMhIAJAuPdVNMGQyGU6dOoXZs2fD19cXGo1GKlu9ejUiIiLQrl07xMXFQa83nJEbNGgQ/vKXv6Bz584ICAgwutw0ZMgQvPvuu9L8hx9+iL59+1rch7oKCwuxZ88etG/fHoDhlu+XX34ZQUFBiIiIwGeffdao9hojJiYGixYtkuZ/++03eHt7IyAgwOo2Grq09dNPP6F79+7w9vbGsmXLrC47f/48hg4dCpVKhQcffFDqRykpKYiIiMB///tfhIeHQ61WY+XKlY3cWyKi5sVE6A6Um5uLwMBAo2Uvv/wy+vXrBwD45ptv8P7772PFihWYMGECAKCkpATDhw/HsGHDcPLkSYSGhmL8+PFSIjF16lTMmDEDZ8+eRbdu3fDaa69J9Z588knEx8cjPT0dCoXCKKn48ssv4ePjI02bNm2SytauXQtPT0+kpaUhICBAuqx07do1nDhxAgCkS2rR0dFG+zNz5kx4e3sjKSkJHh4eAIAdO3ZgyZIlSExMxBdffIEtW7YYHWh37tyJxMREfP7551i1ahWSkpIAAI8++ih27NghrZecnIzJkyfj3LlzUtzx8fGYO3euNK/T6QAATzzxBHx8fODn5wc3Nze8/vrrAID4+Hjs2LEDe/fuxXvvvYepU6ciIyOj0b/LWkuWLJG2nZWVJf08Y8aMeusuW7YMs2bNkh5CqNFojH4HWVlZFrdXUVGBSZMmYeLEiThx4gQOHz5sVVl1dTXGjBmDDh06IC0tDZGRkZg+fbpUXlBQgPj4eHz55ZdYsmQJFixYYJdLtkREEuHgNBqNACA0Gk29svLycpGeni7Ky8ulZXq9XpTqqqya/pdRIMIXfmFx+l9GgcW29Hq91fv05ptviv79+zdYNm3aNBEQECCKioqMlm/dulXcfffd0nxFRYXw8vIShw8fFkIIERERIZYuXSq0Wq3Q6/WiurpaCCFEWVmZcHNzE4mJiaKsrEzo9XpRU1MjhBBi8eLFYtiwYSIjI0OaiouLpTh69uwpbW/t2rVi0KBB0nxGRoYw1T0BiFmzZtVbPnz4cPGPf/xDmt+yZYvo1KmTEEKIgQMHir///e9S2bPPPiumTZsmhBDi+vXrQqlUipycHFFcXCzc3NxETk6OqKyslOJ+7rnnxDvvvCPN6/V6ER4eLtauXSuOHz8uFAqFOHLkiNR+hw4dxJo1a6T5Pn36iNWrV0vx//LLL/Xir93nwsJCadn48ePF4sWLxe+//y5tOzg4WPo5Ly/PqI3Vq1eLdu3aSW3s379feHl5Gf0OgoODRVJSUr3PNCMjQ5pPSUkRPj4+oqqqSgghRHp6urSOubIffvhBKJVK6e/pzJkzQiaTieLiYrF//34BQKSmpgohhNDpdAKAyMzMrPdZCNHw3x8RUS1zx+/G4BmhRpLJZHB3Vlg1uSqtey2Aq1Jusa3GDBz28fFBYWGh0bLY2FisWbMGgOHsjkqlMirPzs6WLkcBgIuLC9q1a4fs7GwAwLZt25CSkoKgoCD069cPx48fB2B4FcL27duxbt06+Pv7Y8SIEbh48aLUjru7OyIiIqTJ09NTKhs0aJD0s7Ozc6MG9z777LP1lmVnZ0uXpgCgffv2UvwAEBoaKv0cHByMa9euAQD8/PwwaNAgJCUlYffu3bjvvvsQHBwMpVIpxV171qd2vvb3ERAQgJ49e2L8+PFYu3at1H5ubi7mz58vnYX5+eefrToTY4parZa2rVAopJ/9/f2lddLS0rBw4UJs2rTJaMyPk5OT0e9AobA8NPDKlSsICgqS1q37uZory83NRXV1NcLCwuDj44OYmBgIIaTLY2q1Gj169ABg+J0DaNTvnYiouTERugP17NkTZ8+ehVarlZZlZGRIdwXVXkqqKywszOjSTUVFBS5fvoywsDCUlpaitLQUe/fuxe+//47+/fvjqaeeAmC41KFWq3Ho0CFcu3YNAQEBeOGFF6yK09vb22SZk5Oha5o6SJrah7pJ2IULF4zuhKo7BiYrK8toDNXkyZOxY8cO6bJYYz399NP49NNPpc88JCQEH330EVJTU5GamooTJ07gr3/9q9k21Go1ABjdPl9UVARfX1+L2y8oKMCkSZPwyiuvYPDgwY2O/2YBAQG4du2adGm0bhJnriwkJATt2rWT9rt2Cg8PB2D+d05EZA92SYTS0tIQHR0NtVqNBQsWWPU/wnfeeQeBgYHw9vbGn/70J6MBuebK7Ent4QwXhfmP2EXhBLWHc7NuNzY2FlFRUZg1axYuXryIpUuXoqqqyugMzM3GjBmD4uJiLFmyBJcuXcJzzz2Hjh07Ijo6Gnq9HqNHj8bmzZuRn58PJycn6SCYn5+PIUOG4Ouvv4ZWqzUqAwyDhouKiqSp7sBmc4KCguDh4YFdu3bh0qVLVt2NNWvWLCQkJODAgQP45ZdfEBcXhzlz5kjlH330EQ4fPoyDBw9i27ZtmDRpklQ2ceJEHDlyBLt378bDDz9cr+24uDijsS43Gzx4MEJCQrB582YAwLRp05CYmIiqqiopSakdkwQYXiaak5MjTUVFRVCpVOjZsyfeeOMN5OTkICkpCT/++CMGDhxotK2bBzXr9Xo88sgj6NChA5599lmUlJSgpKQENTXWD7C/WZ8+faBUKrF8+XJcunQJCxcutKosJiYGXl5e2LVrF5RKJQ4cOICYmBiUlpbecixERC2q6VfpGqeiokJERESI2bNni/Pnz4tRo0aJjz/+2GydAwcOiKioKHHmzBlx7tw5MWrUKGl8h7kyazR2jFBj5RSWiV9zikxOOYVlt9y22e3m5IiRI0cKV1dX0atXL3H06FEhhGFszuLFixusc/LkSREbGyu8vLzEiBEjRHZ2tlS2fft20blzZ+Hq6iq6desmDhw4IJWtXr1aRERECFdXVxETEyPS0tKEEIYxQgCMJrlc3mAcGzZsEAMHDjSKZ9u2baJdu3bC3d1dLFy4UFqOm8az1PXee++JsLAw0bZtW7F48WJpvNLAgQPF3LlzRZcuXYSfn59444036tUdOXKkGDx4cMMfaAPCw8ONxtqsWLFC9OjRQwghRGVlpZg/f74IDAwUfn5+4qWXXpLGVd38mQAQs2fPFkIIcerUKdG/f3/h4eEhIiMjxQcffGAxjl9++aXBNvfv3y/2798vVCqV2bhrY7r5M/3hhx9EVFSUaNOmjfj73/9utI65srNnz4rBgwcLDw8PERUVJXbv3i2EMIxXCg8Pt7jdWhwjRETmNNcYIZsnQklJSUKtVovS0lIhhBCpqanigQceMFvnnXfeEQsWLJDmN23aJPr27WuxzBotnQjR7WHgwIFiw4YNDZYVFhaK3NxcMXLkSPHhhx/aNjAyiX9/RGROqx0sfeLECfTp0wfu7u4AgO7duyM9Pd1snW7duuHzzz/HhQsXkJeXh/Xr12PYsGEWyxqi0+mg1WqNJnJsv/32GyIjI1FRUYEnnnjC3uEQEZEN2fzJ0lqt1ujuJJlMBrlcjsLCQmmw6M1GjBiBjh074q677gIAREdH45VXXrFY1pDly5djyZIlzbU71EqkpKSYLIuJiZGeC0RERI7F5meEFAoFXFxcjJa5urqirKzMZJ3PPvsMly5dwpkzZ1BQUIBu3brhz3/+s8Wyhrz66qvQaDTSVPf2aiIiInIsNj8j5Ovri7S0NKNlxcXF0jNFGrJt2zY8/fTT6NSpEwAgISEBKpUKRUVFZssaen+Si4tLvUSMiIiIHJPNzwhFR0cb3QqdmZkJnU5n9lkp1dXV0sPvAMMD3QCgpqbGbBkRERGROTY/IzRgwABoNBps3LgRU6dORXx8PIYOHQq5XA6tVgs3NzcolUqjOg888ABWrFiBkJAQuLm5ISEhAX379kWbNm3MlhERERGZY/NESKFQYN26dZgyZQoWLFiAmpoaHDhwAIDhDrKEhATpRaC1nn/+eVy+fBlvvvkm8vPz0bdvX6xfv95iGREREZE5MiHs86Kf3NxcHDt2DLGxsUbvS7I1rVYLlUoFjUZT7/H/FRUVyMjIQGRkJFxdXW9tA0XZQJmZJ127twF8Qk2XE1mppKQEv/76K0JDQxESEmLvcJqsWf7+iOiOZe743Rh2e9dYcHAwxo8fb9ckqMUVZQOregPrBpqeVvU2rNfcmy4qwsMPPwwPDw/06tULx44da/ZtWBIXFweZTGY0jRkzxuZxtJTal6/KZDIEBgZi1qxZKC8vt1hPJpMhNTW13vLMzEzIZDKjd41NmDABcXFxFttMSUlBREQE5s2bh7vvvhv//Oc/peU33zQQERGB5OTkejHd/OqOhuI2tY419YmIbkd86WpLKisAqi08n6ZaZ/6M0S168sknUVpaitTUVMycORPjxo2z6iDd3EaNGoXCwkJp+vTTT62qV5sU3C7i4uKQmJhYb/nmzZtRUFCA5ORkpKSkYPny5S0eS0REhNF8TU0NnnrqKWzduhXHjx/HN998g7/97W+oqKiwus3CwkKjF9QSETkKm48RavWEAKpMP/PISLWViUd1OVBp4aWUSnfAysQgIyMDO3fuRG5uLoKCgtCxY0e89dZb2LdvH0aPHm1dTM1EqVQ2+BiDO4WHhwd8fX3Rt29fTJ061aqXwza34uJiLFy4EMOHDwcA9OjRA5WVlWafzXWzO/l3RERkDs8INVZVGbCsnXXTxyOsa/PjEZbbsjb5AnDo0CG0b98eQUFB0rJ58+ZBpVJh+vTpiIuLw+bNm9GpUyesWrVKWictLQ39+vWDSqXCqFGjkJOTI5Xt2bMHXbp0gbu7Ox544AFcuHBBKtu8eTMiIiLg4eGBkSNHoqDA8hmu6dOnY9GiRZg3bx48PT3RtWtXnD59GoDhAZu1Tx+vvfRUN8GQyWQ4deoUZs+eDV9fX6M32q9evRoRERFo164d4uLioNfrAQCDBg3CX/7yF3Tu3BkBAQFGl5uGDBmCd999V5r/8MMP0bdvX4v7UFdhYSH27NmD9u3bAwCqqqrw8ssvIygoCBEREfjss88a1V5j+Pj4YPbs2QAAIQTi4+MxcOBAs4+kuFlDl7Z++ukndO/eHd7e3li2bJnVZefPn8fQoUOhUqnw4IMPSv2o9vLdf//7X4SHh0OtVmPlypW3sMdERM2HidAdKDc3F4GBgUbLXn75ZfTr1w8A8M033+D999/HihUrpDv0SkpKMHz4cAwbNgwnT55EaGgoxo8fLyUSU6dOxYwZM3D27Fl069YNr732mlTvySefRHx8PNLT06FQKIySii+//BI+Pj7StGnTJqls7dq18PT0RFpaGgICAqTLSteuXcOJEycAQLqkFh0dbbQ/M2fOhLe3N5KSkuDh4QEA2LFjB5YsWYLExER88cUX2LJli9GBdufOnUhMTMTnn3+OVatWISkpCQDw6KOPYseOHdJ6ycnJmDx5Ms6dOyfFHR8fj7lz50rzta/keOKJJ+Dj4wM/Pz+4ubnh9ddfBwDEx8djx44d2Lt3L9577z1MnToVGRkZjf5d1lqyZIm07aysLOnnGTNmSOtcv34dQUFBSEhIwMcffywt12g0Rr+DrKwsi9urqKjApEmTMHHiRJw4cQKHDx+2qqy6uhpjxoxBhw4dkJaWhsjISEyfPl0qLygoQHx8PL788kssWbIECxYssMslWyIiSXO8AbY1a/Tb5/V6IXQl1k2XfhRisbfl6dKPltvS663epzfffFP079+/wbJp06aJgIAAUVRUZLR869at4u6775bmKyoqhJeXlzh8+LAQQoiIiAixdOlSodVqhV6vF9XV1UIIIcrKyoSbm5tITEwUZWVlQq/Xi5qaGiGEEIsXLxbDhg0TGRkZ0lRcXCzF0bNnT2l7a9euFYMGDZLmMzIyhKnuCUDMmjWr3vLhw4eLf/zjH9L8li1bRKdOnYQQhrfP//3vf5fKnn32WTFt2jQhhBDXr18XSqVS5OTkiOLiYuHm5iZycnJEZWWlFPdzzz0n3nnnHWler9eL8PBwsXbtWnH8+HGhUCjEkSNHpPY7dOgg1qxZI8336dNHrF69Wor/l19+qRd/7T4XFhZKy8aPHy8WL14sfv/9d2nbwcHB0s95eXlGbZw5c0ZMnjxZ9OrVS+j1erF//37h5eVl9DsIDg4WSUlJ9T7TjIwMaT4lJUX4+PiIqqoqIYQQ6enp0jrmyn744QehVCqlv6czZ84ImUwmiouLxf79+wUAkZqaKoQQQqfTCQAiMzOz3mchBN8+T0Tmtdq3z7d6Mhng7GHdpHCzrk2Fm+W2GjFw2MfHB4WFhUbLYmNjsWbNGgCGszsqlcqoPDs72+hluC4uLmjXrp30LrZt27YhJSUFQUFB6NevH44fPw4AcHNzw/bt27Fu3Tr4+/tjxIgRuHjxotSOu7s7IiIipMnT01MqGzRokPSzs7MzRCOe5PDss8/WW5adnS1dmgKA9u3bG71LLjT0xmMKgoODpSeS+/n5YdCgQUhKSsLu3btx3333ITg4GEqlUoq79qxP7XztQO6AgAD07NkT48ePx9q1a6X2c3NzMX/+fOkszM8//2zVmRhT1Gq1tG2FQiH9fPNdl506dcLGjRtx+vRp6ayak5OT0e9AobA8NPDKlSsICgqS1q37uZory83NRXV1NcLCwuDj44OYmBgIIaTLY2q1Gj169AAA6bU6jfm9ExE1NyZCd6CePXvi7Nmz0Gq10rKMjAzprqDaS0l1hYWFGV26qaiowOXLlxEWFobS0lKUlpZi7969+P3339G/f3889dRTAAyXOtRqNQ4dOoRr164hICAAL7zwglVxmnvug5OToWuaOkia2oe6SdiFCxeM7oSqOwYmKyvLaAzV5MmTsWPHDumyWGM9/fTT+PTTT6XPPCQkBB999BFSU1ORmpqKEydO4K9//avZNtRqNQAY3T5fVFRkcazPqVOnMHXqVGneyckJTk5OkMvljd6PWgEBAbh27Zp0abRuEmeuLCQkBO3atZP2u3YKDw8HYP53TkRkD0yEWpJ7G0Bh4QWvChfDes0oNjYWUVFRmDVrFi5evIilS5eiqqrK6AzMzcaMGYPi4mIsWbIEly5dwnPPPYeOHTsiOjoaer0eo0ePxubNm5Gfnw8nJyfpIJifn48hQ4bg66+/hlarNSoDDIOGi4qKpKnuwGZzgoKC4OHhgV27duHSpUtW3Y01a9YsJCQk4MCBA/jll18QFxeHOXPmSOUfffQRDh8+jIMHD2Lbtm2YNGmSVDZx4kQcOXIEu3fvxsMPP1yv7bi4OKOxLjcbPHgwQkJCsHnzZgDAtGnTkJiYiKqqKhQUFGDSpEnSmCQAyMvLQ05OjjQVFRVBpVKhZ8+eeOONN5CTk4OkpCT8+OOPGDhwoNG2bh7U3KFDB3z77bdYsmQJsrOz8frrryM0NBTdunWz+JmZ0qdPHyiVSixfvhyXLl3CwoULrSqLiYmBl5cXdu3aBaVSiQMHDiAmJgalpRbuiiQispdmuEzXqjV6jFBjFWYJkfuL6akw69bbNiMnJ0eMHDlSuLq6il69eomjR48KIQxjcxYvXtxgnZMnT4rY2Fjh5eUlRowYIbKzs6Wy7du3i86dOwtXV1fRrVs3ceDAAals9erVIiIiQri6uoqYmBiRlpYmhDCMEQJgNMnl8gbj2LBhgxg4cKBRPNu2bRPt2rUT7u7uYuHChdJy3DSepa733ntPhIWFibZt24rFixdL45UGDhwo5s6dK7p06SL8/PzEG2+8Ua/uyJEjxeDBgxv+QBsQHh5uNNZmxYoVokePHkIIISorK8X8+fNFYGCg8PPzEy+99JI0rurmzwSAmD17thBCiFOnTon+/fsLDw8PERkZKT744AOrYvnf//4n7rvvPuHh4SGGDRsmLly4IIQQYv/+/UKlUpmNuzammz/TH374QURFRYk2bdqIv//970brmCs7e/asGDx4sPDw8BBRUVFi9+7dUizh4eEWt1uLY4SIyJzmGiNkt1ds3C5a/BUbdFsYNGgQpk+f3uBZnaKiIpSVlWHmzJmYNGkSZs6cafsAqR7+/RGROa3+FRtEt4vffvsNkZGRqKiowBNPPGHvcIiIyIb4ZGlyCCkpKSbLYmJipOcCERGRY+EZISIiInJYTISIiIjIYTERIiIiIofFRIiIiIgcFhMhIiIicli8a6yFXSm5gkJdoclytYsaQZ5BJsuJiIio5fCMUAu6UnIFY5LHYPIXk01OY5LH4ErJlWbfdlFRER5++GF4eHigV69eOHbsWLNvw5K4uDjIZDKjacyYMTaPo6XUvnxVJpMhMDAQs2bNQnl5ucV6MpkMqamp9ZZnZmZCJpMZvWtswoQJiIuLa1Rcs2fPluqkpKTAx8enXtzJycn1Yrr51R0NxW1qHWvqExHdjpgItaBCXSEqayrNrlNZU2n2jNGtevLJJ1FaWorU1FTMnDkT48aNs+og3dxGjRqFwsJCafr000+tqlebFNwu4uLikJiYWG/55s2bUVBQgOTkZKSkpGD58uUtHktERITJsoMHD+LDDz9sdJuFhYVGL6glInIUvDTWSEIIlFdbl1BUVFdYvV5ZVZnZddwUblYnBhkZGdi5cydyc3MRFBSEjh074q233sK+ffswevRoq9poLkqlst4ZiTuJh4cHfH190bdvX0ydOtWql8O2lMrKSsyaNQudO3dudN07+XdERGQOzwg1Unl1OWK2xlg1Tft6mlVtTvt6msW2rE2+AODQoUNo3749goJujD2aN28eVCoVpk+fjri4OGzevBmdOnXCqlWrpHXS0tLQr18/qFQqjBo1Cjk5OVLZnj170KVLF7i7u+OBBx7AhQsXpLLNmzcjIiICHh4eGDlyJAoKCizGOH36dCxatAjz5s2Dp6cnunbtitOnTwMAXF1dERkZCQDSpae6CYZMJsOpU6cwe/Zs+Pr6Gr3RfvXq1YiIiEC7du0QFxcHvV4PwPCusb/85S/o3LkzAgICjC43DRkyBO+++640/+GHH6Jv374W96GuwsJC7NmzB+3btwcAVFVV4eWXX0ZQUBAiIiLw2WefNaq9W7F8+XKEh4fj0UcfbXTdhi5t/fTTT+jevTu8vb2xbNkyq8vOnz+PoUOHQqVS4cEHH5T6UUpKCiIiIvDf//4X4eHhUKvVWLlyZaNjJSJqTkyE7kC5ubkIDAw0Wvbyyy+jX79+AIBvvvkG77//PlasWIEJEyYAAEpKSjB8+HAMGzYMJ0+eRGhoKMaPHy8lElOnTsWMGTNw9uxZdOvWDa+99ppU78knn0R8fDzS09OhUCiMkoovv/wSPj4+0rRp0yapbO3atfD09ERaWhoCAgKky0rXrl3DiRMnAEC6pBYdHW20PzNnzoS3tzeSkpLg4eEBANixYweWLFmCxMREfPHFF9iyZYvRgXbnzp1ITEzE559/jlWrViEpKQkA8Oijj2LHjh3SesnJyZg8eTLOnTsnxR0fH4+5c+dK87Wv5HjiiSfg4+MDPz8/uLm54fXXXwcAxMfHY8eOHdi7dy/ee+89TJ06FRkZGY3+XdZasmSJtO2srCzp5xkzZgAAzpw5g1WrVmHt2rX16mo0GqPfQVZWlsXtVVRUYNKkSZg4cSJOnDiBw4cPW1VWXV2NMWPGoEOHDkhLS0NkZKTRi24LCgoQHx+PL7/8EkuWLMGCBQvscsmWiKgWL401kpvCDT9N+cmqdc/8fsaqs0KfjPgEnX3NX85wU7hZtU3AcDZCLpebLL948SLOnj0LlUolLdu1axe8vLywePFiAMDKlSvh7++Po0ePok+fPnBzc4NOp4NKpcIHH3wgJUhyuRxKpRI6nQ4BAQH473//CyGE1O6DDz6IdevWSfN+fn7SzyEhIXjrrbcAAFOmTMG2bdsAACqVSnqTsKlLNt27d8c777xjtGzdunV4/vnnMWjQIACG5OGNN97A888/DwCYNWsW+vTpA8CQwOzcuRMTJ07En/70J/z1r39Fbm4uVCoV9u/fj3Xr1iEgIEAa1JyQkICQkBA8/PDDAABnZ2cAwL/+9S9ER0fj/vvvxxtvvCHt3yeffIIFCxagW7du6NatG3r27ImvvvoKc+fONfl7MefZZ5/FtGmGvtSvXz8cPHgQgOHSnBBCGiDd0DgfLy8vo8HZtQmxOT/99BPKysqwaNEiKBQKvP322/jiiy8slh05cgQXL17E0aNH4e3tjYULF6JLly4oKSkBYEic16xZg27duuHuu+/Gc889h7y8PISHh9/S50JE1FQ8I9RIMpkM7kp3qyZXhatVbboqXC221ZiBwz4+PigsNB6AHRsbizVr1gAwnN2pmwQBQHZ2tnQ5CgBcXFzQrl07ZGdnAwC2bduGlJQUBAUFoV+/fjh+/DgAwM3NDdu3b8e6devg7++PESNG4OLFi1I77u7uiIiIkCZPT0+prDZhAQyJRd0EypJnn3223rLs7Gzp0hQAtG/fXoofAEJDQ6Wfg4ODce3aNQCG5GzQoEFISkrC7t27cd999yE4OBhKpVKKu/asT+187e8jICAAPXv2xPjx443OxuTm5mL+/PnSWZiff/7ZqjMxpqjVamnbCoVC+tnf3x8fffQR9Hq9ySTLycnJ6HegUFj+/8+VK1cQFBQkrVv3czVXlpubi+rqaoSFhcHHxwcxMTEQQkiXx9RqNXr06AHgRjLZmN87EVFz4xmhO1DPnj1x9uxZaLVa6cxKRkYGwsLC8NNPP0mXkuoKCwszunRTUVGBy5cvIywsDKWlpSgtLcXevXtRWVmJ119/HU899RR+/fVXFBQUQK1W49ChQygtLcWcOXPwwgsvYNeuXRbjrI2tIU5OhhxdCNFgEmhqH+omYRcuXDA6Q1J3DExWVpbRGKrJkydj8+bNCAoKwuTJky3GfrOnn34a48aNQ0JCAry9vRESEoI333xTOgNVXl5udn8BQ5IAGB59UHsmrKioCL6+vmbrbd26FT///LNUv6LCMEj/2LFjmD9/fqP3BTAkeNeuXYNer4eTk5NREmeuLCQkBO3atZPOWAGGS3Ph4eG4evWqxc+AiMjWeEaoBald1HCWO5tdx1nuDLWLulm3Gxsbi6ioKMyaNQsXL17E0qVLUVVVZXQG5mZjxoxBcXExlixZgkuXLuG5555Dx44dER0dDb1ej9GjR2Pz5s3Iz8+Hk5OTdGksPz8fQ4YMwddffw2tVmtUBhgu0xUVFUlT3YHN5gQFBcHDwwO7du3CpUuXrLoba9asWUhISMCBAwfwyy+/IC4uDnPmzJHKP/roIxw+fBgHDx7Etm3bMGnSJKls4sSJOHLkCHbv3i1d/qorLi7OaKzLzQYPHoyQkBBs3rwZADBt2jQkJiaiqqoKBQUFmDRpkjQmCQDy8vKQk5MjTUVFRVCpVOjZsyfeeOMN5OTkICkpCT/++CMGDhxotK2bBzVv27YN6enpSE1NRWpqKubMmYM5c+bgo48+sviZmdKnTx8olUosX74cly5dwsKFC60qi4mJgZeXF3bt2gWlUokDBw4gJiYGpaWltxwLEVGLEg5Oo9EIAEKj0dQrKy8vF+np6aK8vPyW279cfFmcyj9lcrpcfLkp4ZuUk5MjRo4cKVxdXUWvXr3E0aNHhRBCTJs2TSxevLjBOidPnhSxsbHCy8tLjBgxQmRnZ0tl27dvF507dxaurq6iW7du4sCBA1LZ6tWrRUREhHB1dRUxMTEiLS1NCCHE4sWLBQCjSS6XNxjHhg0bxMCBA43i2bZtm2jXrp1wd3cXCxculJYDEBkZGQ3uw3vvvSfCwsJE27ZtxeLFi0VNTY0QQoiBAweKuXPnii5dugg/Pz/xxhtv1Ks7cuRIMXjw4IY/0AaEh4eLpKQkaX7FihWiR48eQgghKisrxfz580VgYKDw8/MTL730kqiurpbiv3maPXu2EEKIU6dOif79+wsPDw8RGRkpPvjgA6vjqbV48WLps92/f79QqVRm466N6ebP9IcffhBRUVGiTZs24u9//7vROubKzp49KwYPHiw8PDxEVFSU2L17txRLeHi4xe3Wao6/PyK6c5k7fjeGTAjHvkCv1WqhUqmg0WjqnbavqKhARkYGIiMj4epq3Xgfuj0NGjQI06dPb/CsTlFREcrKyjBz5kxMmjQJM2fOtH2AVA///ojIHHPH78bgpTFyeL/99hsiIyNRUVGBJ554wt7hEBGRDXGwNDmElJQUk2UxMTHSc4GIiMix8IwQEREROSwmQlZw8GFURHbBvzsisgUmQmYolUoAQFmZ+ReiElHzq/27q/07JCJqCRwjZIZcLoePjw/y8vIAGJ6S3JgnPBNR4wkhUFZWhry8PPj4+Jh9XQwRUVMxEbKgbdu2ACAlQ0RkGz4+PtLfHxFRS2EiZIFMJkNQUBACAgJQVVVl73CIHIJSqeSZICKyCSZCVpLL5fxiJiIiusNwsDQRERE5LCZCRERE5LCYCBEREZHDcvgxQrUPbdNqtXaOhIiIiKxVe9xu6sNXHT4RKigoAACEhobaORIiIiJqrIKCAqhUqluu7/CJkK+vLwAgKyurSR9kU2i1WoSGhiI7Oxve3t6MgTHYPYbbJQ7GwBgYA2MwRaPRICwsTDqO3yqHT4ScnAzDpFQqlV0POgDg7e3NGBjDbRXD7RIHY2AMjIExmFJ7HL/l+s0UBxEREVGrw0SIiIiIHJbDJ0IuLi5YvHgxXFxcGANjYAy3WRyMgTEwBsbQ0jHIRFPvOyMiIiJqpRz+jBARERE5LiZCRERE5LCYCBEREZHDYiJkZzt37kT79u2hUCgQExOD06dP2zWeESNGIDEx0W7bf+WVVzB27Fi7bHvTpk0ICwuDp6cnhg4diszMTLvEYS8FBQWIjIw02m9b98+GYqjLFv3TXAy26p8NxeBo/dNU37Nln7RmWy3dJy3FYIs+aSoGe/TJgoIC/Pjjj8jPz2++RoUD+/XXX8V9990nfHx8xPz584Ver7fp9s+fPy/UarX49NNPxdWrV8UjjzwiYmNjbRpDXZs3bxYAxIYNG+yy/V9//VV4eXmJ8+fP23zb58+fF6GhoeLnn38Wly5dEk899ZQYOHCgzbafn58vIiIiREZGhrTMlv3z+vXrok+fPgKAFIOt+2dDMdRli/5pLgZb9U9Tvwtb9s/k5GQRGRkp5HK5uP/++0V6eroQwnZ90lTfs2WftGZbLd0nLcVgiz5p7ndh6+/Mbdu2CR8fH3HvvfcKNzc3sW3bNiFE0/ulwyZCFRUVIiIiQsyePVucP39ejBo1Snz88cc2jWHXrl1izZo10vy+ffuEs7OzTWOoVVBQIAIDA0WnTp3skgjp9XoRGxsrFi1aZPNtCyHE9u3bxSOPPCLN//DDDyIoKMgm227owGfr/jlkyBCRkJBgFIOt+2dDMdSyVf80FYMt+2dDMdiyf5o68NmyT5rqe7bsk5a2ZYs+aS4GW/VJUzHY+juzsLBQ+Pn5iV9//VUIIcTGjRtFWFhYs/RLh02EkpKShFqtFqWlpUIIIVJTU8UDDzxg15jWrFkjunbtapdtT58+XcyZM0dMmzbNLonQ2rVrhbu7u/j444/Frl27RGVlpU23f+rUKdGmTRtx/PhxUVRUJB577DExdepUm2y7oQOfrfvnhQsXhBDC5NkYIVq+f5qLwVb901QMtuyfDcVgy/5p6sBnz+9MU33Plt+ZN2/LHt+ZdWOw13dmbQy2/s7MysoSmzdvluZPnDghvLy8mqVfOmwiFBcXJ0aOHCnN6/V6oVar7RaPTqcTHTp0EKtWrbL5tvft2ydCQ0OFRqOxSyJUXFws/P39RY8ePcQbb7whHnzwQdGnTx9RXl5u0zhmz54tAAgAIjIyUuTl5dlkuw0d+OzVP00lQrbsnzfHYI/+WTcGe/XPmz8He/XP2gOfvfqkqb5nyz5587bs0SfrxmCvPnnz52CvPllZWSn+3//7f2LatGnN0i8ddrC0VqtFZGSkNC+TySCXy1FYWGiXeF577TV4enpi1qxZNt1uRUUFZs+ejTVr1tjtxXmff/45SktLsW/fPixatAh79uxBUVERNm7caLMYjhw5gl27duGnn35CcXExHn/8cYwaNQrCBs8bbd++fb1l7J8G7J8G9uqflZWVePfddzF37ly79UlTfc+WfbLutuzVJ+vGYK8+WTcGe/XJEydOIDAwEHv27EFCQkKz9EuHTYQUCkW9x3K7urqirKzM5rHs3bsXH3zwAbZu3QqlUmnTbb/55puIjo7G6NGjbbrdunJychATEwNfX18Aht9N9+7dkZGRYbMYPv30Uzz22GO4//774enpiaVLl+LixYs4ceKEzWKoi/3TgP3TwF79s+6Bzx590lTfs2WfvHlb9uiTN8dgjz55cwz26pPdu3fHd999h6ioKDz55JPN0i8VzR1ka+Hr64u0tDSjZcXFxXB2drZpHBcvXsQTTzyBNWvWoGvXrjbdNgBs3boV169fh4+PDwCgrKwMn332GY4ePYr333/fJjGEhoaivLzcaNmlS5fw4IMP2mT7AFBdXW30P4ji4mKUlpaipqbGZjHUxf5pwP5pYI/+WXvgO3LkCJRKpc37pKm+Z8s+2dC2bN0nG4rB1n2yoRjs9Z0pk8nQs2dPJCYmIjw8HMuXL296v2zWC3etyHfffSfuuusuaT4jI0O4urqK6upqm8VQVlYmunTpIv7yl7+I4uJiabLlbfzZ2dkiIyNDmv70pz+Jd955R1y/ft1mMRQUFAiVSiXWrFkjsrOzxb///W/h4uJictBuS9i2bZtwc3MTK1asEFu2bBEPPvigCAsLs+mgbdQZE2Kv/lk3Bnv1z7ox2Kt/1o3BXv2zbgy27p8XLlwQ/v7+RoNTbdknTfU9W/ZJU9vKysqyWZ80FUN+fr7N+qSpGLZu3WrTPvndd9+J+fPnS/OXL18WMplMJCcnN7lfOmwiVFVVJfz9/cUnn3wihDAM+hozZoxNY0hKSpIGmtWdbJkA3Mxed40dPnxYxMbGCjc3NxEZGSmSkpJsun29Xi/i4uJEWFiYUCqVomfPnuLYsWM2jaHu795e/RM33blmj/5pbhv2GCwthH36Z90YbNk/TR34KisrbdYnTfW9f/3rXzbrk9b2/5bsk+ZisFWfNBXDxYsXbfqdmZubK7y8vMTatWtFVlaWmDp1qnjooYea5bvSod8+n5ycjClTpsDLyws1NTU4cOAAoqKi7B0WOSiZTIaMjAxEREQAYP8k+0hOTsbEiRPrLc/IyEBqair7JNnNN998gxdeeAE5OTl46KGH8P7778Pf37/J35UOnQgBQG5uLo4dO4bY2Fj4+/vbOxwiI+yfdLthn6TbUVP6pcMnQkREROS4HPb2eSIiIiImQkREROSwmAgRERGRw2IiRERERA6LiRARERE5LCZCRHTbSklJgUwmM5o8PT1bZFuJiYkYNGhQi7RNRLcvh33XGBG1Dt7e3rh06ZI0L5PJ7BgNEd1pmAgR0W1NJpNJL7gkImpuvDRGRK1OXFwcRo4ciYEDB0KlUuGxxx6DVquVyr///nvce++9UKvVmDJlCoqKiqSy7777Dt27d4eXlxdGjhyJnJwco7Y//PBDBAYGIiAgAP/3f/9nq10iIjthIkREtzWNRgMfHx9pmj17NgDg66+/xowZM3Ds2DFkZmZi0aJFAIDs7GyMGjUK8+bNw88//4ySkhJMnz4dAJCZmYlx48bhxRdfxOnTp+Hj44NnnnlG2tapU6ewY8cOHDx4ENOnT8eLL75o8/0lItviKzaI6LaVkpKCcePG4eTJk9IyT09PrFq1Ct9++y0OHjwIAEhKSsILL7yAzMxMLF++HPv378eePXsAAJcvX0ZwcDCuXLmCjz/+GD/88AO++uorAEBOTg5SU1MxZswYJCYm4umnn0ZmZiYCAwNx9uxZdOrUCfyKJLqzcYwQEd3WnJycEBERUW95aGio9HNwcDCuXbsGwHBGqH379lJZu3bt4OLiguzsbOTk5Bi1FRISgpCQEGm+S5cuCAwMBAA4Ozs3854Q0e2Il8aIqFXKzMyUfs7KykJQUBAAICwsDBcvXpTKcnNzodPpEBYWhtDQUGRkZEhlZ8+eRc+ePaHX6wEY7lAjIsfCRIiIbmtCCBQVFRlNNTU1OHLkCD755BOcO3cOb7/9NiZNmgQA+POf/4wff/wRH374ITIyMvD0009jwoQJCAwMxOOPP44ffvgBiYmJyM7OxtKlSxEQEAAnJ34VEjkq/vUT0W1Nq9VCrVYbTYcPH8bYsWOxceNG3HfffejQoQMWL14MwHC568svv8Tq1avRs2dPeHh4YMOGDQCAiIgI7Ny5EytWrEBUVBSKioqkMiJyTBwsTUStTlxcHDIzM5GYmGjvUIioleMZISIiInJYPCNEREREDotnhIiIiMhhMREiIiIih8VEiIiIiBwWEyEiIiJyWEyEiIiIyGExESIiIiKHxUSIiIiIHBYTISIiInJY/x98xr97oeEmbQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'CrossEntropy+ReLU+1Hidden': [exec_result.loc[3].loss_validate, exec_result.loc[3].acc_validate],\n",
    "                   'CrossEntropy+ReLU+2Hidden': [exec_result2.loc[0].loss_validate, exec_result2.loc[0].acc_validate],\n",
    "                   'CrossEntropy+ReLU+3Hidden': [exec_result2.loc[1].loss_validate, exec_result2.loc[1].acc_validate],\n",
    "                   'CrossEntropy+ReLU+4Hidden': [exec_result2.loc[2].loss_validate, exec_result2.loc[2].acc_validate]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、结果记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层隐含层的感知机\n",
    "| model | batch size | learning rate SGD | momentum | weight decay | acc validate | acc test |\n",
    "|----------|----------|----------|---------|----------|----------|----------|\n",
    "| Euclidean+Sigmoid     | 10 | 0.001 | 0.55 | 0.0001    | 0.9046 | 0.9161 | \n",
    "| Euclidean+ReLU        | 10 | 0.001 | 0.99 | 0.0001    | 0.9678 | 0.9661 | \n",
    "| CrossEntropy+Sigmoid  | 10 | 0.001 | 0.55 | 0.00001   | 0.9257 | 0.9267 | \n",
    "| CrossEntropy+ReLU     | 10 | 0.001 | 0.99 | 0.00001   | 0.9786 | 0.9802 | \n",
    "*  <font color=\"red\">注:batch size-批大小; learning rate SGD-学习率;momentum-动量;weight decay-权重衰减率。</font>\n",
    "\n",
    "### 具有多层隐含层的多层感知机\n",
    "\n",
    "| model | hid_layers | neurons | train time | acc validate | acc test |\n",
    "|----------|----------|----------|---------|----------|----------|\n",
    "| CrossEntropy+ReLU | 1 | 128               | 321.255   | 0.9799 | 0.9812 | \n",
    "| CrossEntropy+ReLU | 2 | 512,128           | 997.467   | 0.9841 | 0.9851 | \n",
    "| CrossEntropy+ReLU | 2 | 256,64            | 552.641   | 0.9801 | 0.9831 | \n",
    "| CrossEntropy+ReLU | 2 | 300,100           | 640.250   | 0.9816 | 0.9829 | \n",
    "| CrossEntropy+ReLU | 3 | 512,256,128       | 1326.537  | 0.9801 | 0.9818 | \n",
    "| CrossEntropy+ReLU | 4 | 512,256,128,64    | 1421.584  | 0.9802 | 0.9833 | \n",
    "*  <font color=\"red\">注:hid_layers-隐含层的个数; neurons-全连接层神经元的数量;train time-模型训练共花费时间。单位(秒)。</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cf1e4799745e9ccc5e4a5d8e027719ef2e43f9255145d7e4068adaea12ce15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
