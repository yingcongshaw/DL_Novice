{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试加入动量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from criterion import EuclideanLossLayer,SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "exec_result = pd.DataFrame(columns=['mode','batch_size','learning_rate_SGD', 'momentum','weight_decay','time','loss_validate','acc_validate'])\n",
    "\n",
    "max_epoch = 20\n",
    "disp_freq = 50\n",
    "init_std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shaw/work/DL/hw2/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 5.0869\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.8104\t Accuracy 0.0798\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.2760\t Accuracy 0.0867\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.0801\t Accuracy 0.0956\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9752\t Accuracy 0.0999\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.9067\t Accuracy 0.1054\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8585\t Accuracy 0.1084\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8207\t Accuracy 0.1137\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7903\t Accuracy 0.1208\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7643\t Accuracy 0.1290\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7432\t Accuracy 0.1336\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7241\t Average training accuracy 0.1402\n",
      "Epoch [0]\t Average validation loss 0.5194\t Average validation accuracy 0.2218\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4886\t Accuracy 0.2400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.5113\t Accuracy 0.2425\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.5049\t Accuracy 0.2522\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.5009\t Accuracy 0.2597\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4964\t Accuracy 0.2666\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4908\t Accuracy 0.2773\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4862\t Accuracy 0.2852\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4820\t Accuracy 0.2904\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4777\t Accuracy 0.2988\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4734\t Accuracy 0.3072\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4701\t Accuracy 0.3143\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4660\t Average training accuracy 0.3227\n",
      "Epoch [1]\t Average validation loss 0.4167\t Average validation accuracy 0.4354\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3912\t Accuracy 0.4500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4145\t Accuracy 0.4351\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4109\t Accuracy 0.4447\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4101\t Accuracy 0.4470\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4084\t Accuracy 0.4488\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4054\t Accuracy 0.4561\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4032\t Accuracy 0.4617\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4015\t Accuracy 0.4642\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3994\t Accuracy 0.4686\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3975\t Accuracy 0.4732\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3960\t Accuracy 0.4770\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3939\t Average training accuracy 0.4816\n",
      "Epoch [2]\t Average validation loss 0.3641\t Average validation accuracy 0.5536\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3425\t Accuracy 0.6100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3648\t Accuracy 0.5443\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3625\t Accuracy 0.5513\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3630\t Accuracy 0.5500\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3624\t Accuracy 0.5512\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3606\t Accuracy 0.5565\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3593\t Accuracy 0.5600\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3588\t Accuracy 0.5600\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3577\t Accuracy 0.5623\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3567\t Accuracy 0.5645\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3561\t Accuracy 0.5658\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3547\t Average training accuracy 0.5693\n",
      "Epoch [3]\t Average validation loss 0.3335\t Average validation accuracy 0.6236\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3150\t Accuracy 0.6700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3360\t Accuracy 0.6104\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3344\t Accuracy 0.6173\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3355\t Accuracy 0.6136\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3354\t Accuracy 0.6138\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3342\t Accuracy 0.6170\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3335\t Accuracy 0.6203\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3335\t Accuracy 0.6197\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3329\t Accuracy 0.6207\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3324\t Accuracy 0.6229\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3322\t Accuracy 0.6233\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3313\t Average training accuracy 0.6257\n",
      "Epoch [4]\t Average validation loss 0.3145\t Average validation accuracy 0.6698\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2984\t Accuracy 0.7000\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3182\t Accuracy 0.6504\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3170\t Accuracy 0.6610\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3184\t Accuracy 0.6566\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3186\t Accuracy 0.6567\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3178\t Accuracy 0.6588\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3173\t Accuracy 0.6615\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3177\t Accuracy 0.6613\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3173\t Accuracy 0.6621\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3171\t Accuracy 0.6634\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3171\t Accuracy 0.6632\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3165\t Average training accuracy 0.6651\n",
      "Epoch [5]\t Average validation loss 0.3023\t Average validation accuracy 0.7080\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2881\t Accuracy 0.7100\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3067\t Accuracy 0.6861\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3058\t Accuracy 0.6962\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3074\t Accuracy 0.6909\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3078\t Accuracy 0.6908\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3072\t Accuracy 0.6916\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3069\t Accuracy 0.6936\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3074\t Accuracy 0.6926\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3072\t Accuracy 0.6933\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3072\t Accuracy 0.6942\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3073\t Accuracy 0.6935\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3069\t Average training accuracy 0.6951\n",
      "Epoch [6]\t Average validation loss 0.2944\t Average validation accuracy 0.7382\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2818\t Accuracy 0.7600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2993\t Accuracy 0.7078\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2986\t Accuracy 0.7168\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3003\t Accuracy 0.7124\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3008\t Accuracy 0.7123\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3003\t Accuracy 0.7130\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3002\t Accuracy 0.7151\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3009\t Accuracy 0.7140\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3007\t Accuracy 0.7147\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3009\t Accuracy 0.7151\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3011\t Accuracy 0.7142\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3008\t Average training accuracy 0.7156\n",
      "Epoch [7]\t Average validation loss 0.2895\t Average validation accuracy 0.7632\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2782\t Accuracy 0.7800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2947\t Accuracy 0.7288\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2942\t Accuracy 0.7344\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2959\t Accuracy 0.7289\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2965\t Accuracy 0.7285\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2961\t Accuracy 0.7295\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2960\t Accuracy 0.7312\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2968\t Accuracy 0.7304\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2967\t Accuracy 0.7313\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2970\t Accuracy 0.7314\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2972\t Accuracy 0.7305\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2970\t Average training accuracy 0.7314\n",
      "Epoch [8]\t Average validation loss 0.2866\t Average validation accuracy 0.7798\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2764\t Accuracy 0.8000\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2920\t Accuracy 0.7429\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2917\t Accuracy 0.7469\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2934\t Accuracy 0.7419\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2940\t Accuracy 0.7411\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2937\t Accuracy 0.7420\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2937\t Accuracy 0.7434\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2945\t Accuracy 0.7424\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2945\t Accuracy 0.7432\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2949\t Accuracy 0.7430\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2951\t Accuracy 0.7424\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2950\t Average training accuracy 0.7432\n",
      "Epoch [9]\t Average validation loss 0.2853\t Average validation accuracy 0.7900\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2761\t Accuracy 0.8100\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2908\t Accuracy 0.7543\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2906\t Accuracy 0.7564\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2923\t Accuracy 0.7505\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2929\t Accuracy 0.7498\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2927\t Accuracy 0.7506\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2927\t Accuracy 0.7517\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2936\t Accuracy 0.7507\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2937\t Accuracy 0.7513\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2940\t Accuracy 0.7514\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2943\t Accuracy 0.7506\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2942\t Average training accuracy 0.7513\n",
      "Epoch [10]\t Average validation loss 0.2851\t Average validation accuracy 0.7982\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2767\t Accuracy 0.8200\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2907\t Accuracy 0.7608\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2905\t Accuracy 0.7620\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2923\t Accuracy 0.7566\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2929\t Accuracy 0.7557\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2928\t Accuracy 0.7566\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2928\t Accuracy 0.7578\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2937\t Accuracy 0.7571\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2938\t Accuracy 0.7576\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2942\t Accuracy 0.7577\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2945\t Accuracy 0.7570\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2944\t Average training accuracy 0.7576\n",
      "Epoch [11]\t Average validation loss 0.2858\t Average validation accuracy 0.8060\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2782\t Accuracy 0.8200\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2914\t Accuracy 0.7675\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2913\t Accuracy 0.7684\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2930\t Accuracy 0.7628\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2937\t Accuracy 0.7616\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2936\t Accuracy 0.7623\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2936\t Accuracy 0.7633\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2946\t Accuracy 0.7626\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2947\t Accuracy 0.7628\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2951\t Accuracy 0.7629\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2954\t Accuracy 0.7623\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2953\t Average training accuracy 0.7629\n",
      "Epoch [12]\t Average validation loss 0.2872\t Average validation accuracy 0.8104\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2802\t Accuracy 0.8200\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2928\t Accuracy 0.7724\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2927\t Accuracy 0.7725\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2944\t Accuracy 0.7664\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2951\t Accuracy 0.7656\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2950\t Accuracy 0.7663\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2951\t Accuracy 0.7669\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2961\t Accuracy 0.7664\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2962\t Accuracy 0.7664\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2966\t Accuracy 0.7664\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2969\t Accuracy 0.7657\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2969\t Average training accuracy 0.7663\n",
      "Epoch [13]\t Average validation loss 0.2892\t Average validation accuracy 0.8134\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2827\t Accuracy 0.8200\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2947\t Accuracy 0.7757\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2947\t Accuracy 0.7760\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2963\t Accuracy 0.7696\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2970\t Accuracy 0.7688\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2970\t Accuracy 0.7696\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2970\t Accuracy 0.7702\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2980\t Accuracy 0.7694\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2981\t Accuracy 0.7697\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2986\t Accuracy 0.7696\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2989\t Accuracy 0.7690\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2989\t Average training accuracy 0.7695\n",
      "Epoch [14]\t Average validation loss 0.2915\t Average validation accuracy 0.8148\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2855\t Accuracy 0.8200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2970\t Accuracy 0.7767\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2970\t Accuracy 0.7774\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2986\t Accuracy 0.7711\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2993\t Accuracy 0.7706\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2993\t Accuracy 0.7711\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2993\t Accuracy 0.7718\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3003\t Accuracy 0.7709\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3004\t Accuracy 0.7712\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3009\t Accuracy 0.7712\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3012\t Accuracy 0.7705\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3012\t Average training accuracy 0.7711\n",
      "Epoch [15]\t Average validation loss 0.2942\t Average validation accuracy 0.8148\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2886\t Accuracy 0.8300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2996\t Accuracy 0.7767\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2996\t Accuracy 0.7774\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3012\t Accuracy 0.7716\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3019\t Accuracy 0.7708\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3018\t Accuracy 0.7716\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3019\t Accuracy 0.7725\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3029\t Accuracy 0.7716\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3030\t Accuracy 0.7722\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3035\t Accuracy 0.7722\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3038\t Accuracy 0.7715\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3038\t Average training accuracy 0.7720\n",
      "Epoch [16]\t Average validation loss 0.2971\t Average validation accuracy 0.8166\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2919\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3024\t Accuracy 0.7776\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3024\t Accuracy 0.7782\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3040\t Accuracy 0.7723\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3047\t Accuracy 0.7715\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3047\t Accuracy 0.7720\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3047\t Accuracy 0.7727\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3057\t Accuracy 0.7719\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3058\t Accuracy 0.7725\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3063\t Accuracy 0.7725\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3066\t Accuracy 0.7720\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3066\t Average training accuracy 0.7725\n",
      "Epoch [17]\t Average validation loss 0.3003\t Average validation accuracy 0.8174\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2954\t Accuracy 0.8300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3054\t Accuracy 0.7784\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3055\t Accuracy 0.7789\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3069\t Accuracy 0.7724\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3077\t Accuracy 0.7717\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3076\t Accuracy 0.7723\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3077\t Accuracy 0.7727\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3087\t Accuracy 0.7719\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3088\t Accuracy 0.7725\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3093\t Accuracy 0.7726\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3096\t Accuracy 0.7721\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3096\t Average training accuracy 0.7726\n",
      "Epoch [18]\t Average validation loss 0.3035\t Average validation accuracy 0.8168\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2990\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3085\t Accuracy 0.7780\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3086\t Accuracy 0.7780\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3100\t Accuracy 0.7720\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3108\t Accuracy 0.7712\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3108\t Accuracy 0.7717\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3108\t Accuracy 0.7720\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3118\t Accuracy 0.7712\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3119\t Accuracy 0.7719\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3124\t Accuracy 0.7720\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3127\t Accuracy 0.7715\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3127\t Average training accuracy 0.7721\n",
      "Epoch [19]\t Average validation loss 0.3069\t Average validation accuracy 0.8180\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.3385\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9910\t Accuracy 0.1504\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.6277\t Accuracy 0.1824\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.4532\t Accuracy 0.1995\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.3390\t Accuracy 0.2183\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.2505\t Accuracy 0.2358\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.1771\t Accuracy 0.2513\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.1188\t Accuracy 0.2656\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.0706\t Accuracy 0.2784\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.0285\t Accuracy 0.2899\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.9920\t Accuracy 0.3019\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9592\t Average training accuracy 0.3141\n",
      "Epoch [0]\t Average validation loss 0.6066\t Average validation accuracy 0.4478\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.5545\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.5966\t Accuracy 0.4567\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.5836\t Accuracy 0.4668\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.5781\t Accuracy 0.4656\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.5731\t Accuracy 0.4681\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.5652\t Accuracy 0.4731\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.5557\t Accuracy 0.4788\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.5486\t Accuracy 0.4814\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.5416\t Accuracy 0.4855\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.5347\t Accuracy 0.4892\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.5281\t Accuracy 0.4944\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5212\t Average training accuracy 0.5002\n",
      "Epoch [1]\t Average validation loss 0.4397\t Average validation accuracy 0.5730\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.4079\t Accuracy 0.6300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4365\t Accuracy 0.5761\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4315\t Accuracy 0.5793\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4313\t Accuracy 0.5769\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4303\t Accuracy 0.5756\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4271\t Accuracy 0.5798\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4227\t Accuracy 0.5837\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4199\t Accuracy 0.5842\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4168\t Accuracy 0.5874\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4137\t Accuracy 0.5894\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4106\t Accuracy 0.5928\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4069\t Average training accuracy 0.5962\n",
      "Epoch [2]\t Average validation loss 0.3604\t Average validation accuracy 0.6630\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3373\t Accuracy 0.7400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3602\t Accuracy 0.6610\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3578\t Accuracy 0.6587\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3591\t Accuracy 0.6541\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3592\t Accuracy 0.6513\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3573\t Accuracy 0.6535\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3548\t Accuracy 0.6553\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3536\t Accuracy 0.6545\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3519\t Accuracy 0.6566\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3503\t Accuracy 0.6579\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3484\t Accuracy 0.6602\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3461\t Average training accuracy 0.6633\n",
      "Epoch [3]\t Average validation loss 0.3132\t Average validation accuracy 0.7200\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2954\t Accuracy 0.7700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3149\t Accuracy 0.7120\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3137\t Accuracy 0.7091\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3156\t Accuracy 0.7043\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3161\t Accuracy 0.7017\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3149\t Accuracy 0.7034\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3132\t Accuracy 0.7063\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3129\t Accuracy 0.7050\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3118\t Accuracy 0.7062\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3109\t Accuracy 0.7066\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3097\t Accuracy 0.7081\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3080\t Average training accuracy 0.7107\n",
      "Epoch [4]\t Average validation loss 0.2820\t Average validation accuracy 0.7608\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2676\t Accuracy 0.8000\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2850\t Accuracy 0.7492\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2845\t Accuracy 0.7458\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2867\t Accuracy 0.7409\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2873\t Accuracy 0.7378\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2865\t Accuracy 0.7395\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2853\t Accuracy 0.7425\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2854\t Accuracy 0.7410\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2847\t Accuracy 0.7413\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2842\t Accuracy 0.7415\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2834\t Accuracy 0.7428\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2821\t Average training accuracy 0.7450\n",
      "Epoch [5]\t Average validation loss 0.2601\t Average validation accuracy 0.7896\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2481\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2640\t Accuracy 0.7784\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2639\t Accuracy 0.7761\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2663\t Accuracy 0.7694\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2670\t Accuracy 0.7675\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2663\t Accuracy 0.7686\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2654\t Accuracy 0.7709\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2659\t Accuracy 0.7692\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2654\t Accuracy 0.7693\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2651\t Accuracy 0.7692\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2646\t Accuracy 0.7699\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2635\t Average training accuracy 0.7718\n",
      "Epoch [6]\t Average validation loss 0.2441\t Average validation accuracy 0.8130\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2340\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2487\t Accuracy 0.7969\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2490\t Accuracy 0.7960\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2514\t Accuracy 0.7911\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2521\t Accuracy 0.7891\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2516\t Accuracy 0.7897\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2509\t Accuracy 0.7915\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2516\t Accuracy 0.7898\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2513\t Accuracy 0.7896\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2511\t Accuracy 0.7897\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2507\t Accuracy 0.7901\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2499\t Average training accuracy 0.7919\n",
      "Epoch [7]\t Average validation loss 0.2321\t Average validation accuracy 0.8296\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2237\t Accuracy 0.8200\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2374\t Accuracy 0.8116\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2378\t Accuracy 0.8114\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2403\t Accuracy 0.8062\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2410\t Accuracy 0.8054\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2405\t Accuracy 0.8057\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2400\t Accuracy 0.8072\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2408\t Accuracy 0.8054\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2406\t Accuracy 0.8055\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2406\t Accuracy 0.8055\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2404\t Accuracy 0.8057\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2396\t Average training accuracy 0.8069\n",
      "Epoch [8]\t Average validation loss 0.2229\t Average validation accuracy 0.8456\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2156\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2288\t Accuracy 0.8249\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2294\t Accuracy 0.8229\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2319\t Accuracy 0.8179\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2326\t Accuracy 0.8179\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2322\t Accuracy 0.8183\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2318\t Accuracy 0.8192\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2327\t Accuracy 0.8175\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2326\t Accuracy 0.8170\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2327\t Accuracy 0.8176\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2325\t Accuracy 0.8173\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2319\t Average training accuracy 0.8183\n",
      "Epoch [9]\t Average validation loss 0.2159\t Average validation accuracy 0.8576\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2095\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2224\t Accuracy 0.8316\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2230\t Accuracy 0.8317\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2256\t Accuracy 0.8273\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2262\t Accuracy 0.8269\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2259\t Accuracy 0.8276\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2256\t Accuracy 0.8277\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2266\t Accuracy 0.8263\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2265\t Accuracy 0.8260\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2266\t Accuracy 0.8264\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2266\t Accuracy 0.8258\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2260\t Average training accuracy 0.8267\n",
      "Epoch [10]\t Average validation loss 0.2106\t Average validation accuracy 0.8664\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2050\t Accuracy 0.8700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2175\t Accuracy 0.8375\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2183\t Accuracy 0.8373\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2208\t Accuracy 0.8326\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2214\t Accuracy 0.8336\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2211\t Accuracy 0.8341\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2209\t Accuracy 0.8339\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2220\t Accuracy 0.8330\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2219\t Accuracy 0.8327\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2221\t Accuracy 0.8331\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2221\t Accuracy 0.8326\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2216\t Average training accuracy 0.8333\n",
      "Epoch [11]\t Average validation loss 0.2067\t Average validation accuracy 0.8750\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2017\t Accuracy 0.8700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2139\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2147\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2172\t Accuracy 0.8395\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2179\t Accuracy 0.8399\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2176\t Accuracy 0.8409\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2174\t Accuracy 0.8409\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2185\t Accuracy 0.8397\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2185\t Accuracy 0.8392\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2187\t Accuracy 0.8393\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2188\t Accuracy 0.8385\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2183\t Average training accuracy 0.8392\n",
      "Epoch [12]\t Average validation loss 0.2037\t Average validation accuracy 0.8812\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1994\t Accuracy 0.8700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2113\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2121\t Accuracy 0.8492\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2146\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2153\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2150\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2149\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2160\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2160\t Accuracy 0.8434\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2163\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2164\t Accuracy 0.8428\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2159\t Average training accuracy 0.8433\n",
      "Epoch [13]\t Average validation loss 0.2016\t Average validation accuracy 0.8842\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1978\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2094\t Accuracy 0.8551\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2103\t Accuracy 0.8530\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2128\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2134\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2132\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2131\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2143\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2143\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2146\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2147\t Accuracy 0.8457\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2143\t Average training accuracy 0.8460\n",
      "Epoch [14]\t Average validation loss 0.2001\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1967\t Accuracy 0.8800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2082\t Accuracy 0.8563\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2091\t Accuracy 0.8544\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2115\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2122\t Accuracy 0.8495\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2119\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2119\t Accuracy 0.8495\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2131\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2131\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2134\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2136\t Accuracy 0.8470\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2132\t Average training accuracy 0.8474\n",
      "Epoch [15]\t Average validation loss 0.1991\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1962\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2074\t Accuracy 0.8551\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2083\t Accuracy 0.8544\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2108\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2114\t Accuracy 0.8496\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2112\t Accuracy 0.8505\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2111\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2124\t Accuracy 0.8489\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2124\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2128\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2129\t Accuracy 0.8481\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2126\t Average training accuracy 0.8485\n",
      "Epoch [16]\t Average validation loss 0.1985\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1959\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2070\t Accuracy 0.8547\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2080\t Accuracy 0.8548\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2104\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2110\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2108\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2108\t Accuracy 0.8505\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2120\t Accuracy 0.8493\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2121\t Accuracy 0.8492\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2125\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2126\t Accuracy 0.8488\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2123\t Average training accuracy 0.8492\n",
      "Epoch [17]\t Average validation loss 0.1983\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1959\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2070\t Accuracy 0.8547\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2079\t Accuracy 0.8555\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2103\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2110\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2108\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2108\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2120\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2121\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2125\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2126\t Accuracy 0.8494\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2123\t Average training accuracy 0.8499\n",
      "Epoch [18]\t Average validation loss 0.1984\t Average validation accuracy 0.8884\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1962\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2072\t Accuracy 0.8557\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2081\t Accuracy 0.8559\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2105\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2112\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2110\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2110\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2123\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2123\t Accuracy 0.8501\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2127\t Accuracy 0.8504\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2129\t Accuracy 0.8498\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2126\t Average training accuracy 0.8503\n",
      "Epoch [19]\t Average validation loss 0.1988\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7268\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6990\t Accuracy 0.1133\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.6530\t Accuracy 0.1057\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.6107\t Accuracy 0.1069\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5695\t Accuracy 0.1124\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.5346\t Accuracy 0.1178\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.5058\t Accuracy 0.1267\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.4821\t Accuracy 0.1327\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.4587\t Accuracy 0.1399\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.4401\t Accuracy 0.1447\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.4227\t Accuracy 0.1502\n",
      "\n",
      "Epoch [0]\t Average training loss 2.4069\t Average training accuracy 0.1553\n",
      "Epoch [0]\t Average validation loss 2.2420\t Average validation accuracy 0.2084\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.2140\t Accuracy 0.2600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.2284\t Accuracy 0.2225\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.2243\t Accuracy 0.2244\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.2201\t Accuracy 0.2264\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.2119\t Accuracy 0.2354\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.2057\t Accuracy 0.2396\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1996\t Accuracy 0.2457\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1955\t Accuracy 0.2503\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1895\t Accuracy 0.2560\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1854\t Accuracy 0.2602\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1810\t Accuracy 0.2665\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1762\t Average training accuracy 0.2736\n",
      "Epoch [1]\t Average validation loss 2.1208\t Average validation accuracy 0.3456\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.1011\t Accuracy 0.4300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.1178\t Accuracy 0.3665\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.1170\t Accuracy 0.3668\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.1163\t Accuracy 0.3686\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.1116\t Accuracy 0.3764\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.1079\t Accuracy 0.3848\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.1037\t Accuracy 0.3933\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.1020\t Accuracy 0.3989\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0979\t Accuracy 0.4065\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0955\t Accuracy 0.4108\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0926\t Accuracy 0.4167\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0892\t Average training accuracy 0.4239\n",
      "Epoch [2]\t Average validation loss 2.0449\t Average validation accuracy 0.5094\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0271\t Accuracy 0.5500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0461\t Accuracy 0.5125\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0460\t Accuracy 0.5108\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0465\t Accuracy 0.5070\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0433\t Accuracy 0.5125\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0405\t Accuracy 0.5156\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0371\t Accuracy 0.5204\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0365\t Accuracy 0.5220\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0333\t Accuracy 0.5269\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0318\t Accuracy 0.5292\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 2.0297\t Accuracy 0.5323\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0270\t Average training accuracy 0.5368\n",
      "Epoch [3]\t Average validation loss 1.9881\t Average validation accuracy 0.6152\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9714\t Accuracy 0.6700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9924\t Accuracy 0.5876\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9927\t Accuracy 0.5887\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9940\t Accuracy 0.5823\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9918\t Accuracy 0.5857\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9897\t Accuracy 0.5869\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9869\t Accuracy 0.5891\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9871\t Accuracy 0.5887\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9846\t Accuracy 0.5917\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9838\t Accuracy 0.5922\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9823\t Accuracy 0.5940\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9802\t Average training accuracy 0.5973\n",
      "Epoch [4]\t Average validation loss 1.9454\t Average validation accuracy 0.6620\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9295\t Accuracy 0.7000\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9518\t Accuracy 0.6314\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9524\t Accuracy 0.6320\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9543\t Accuracy 0.6239\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9530\t Accuracy 0.6274\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9513\t Accuracy 0.6269\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9490\t Accuracy 0.6277\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9499\t Accuracy 0.6271\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9479\t Accuracy 0.6284\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9477\t Accuracy 0.6294\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9466\t Accuracy 0.6300\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9449\t Average training accuracy 0.6325\n",
      "Epoch [5]\t Average validation loss 1.9133\t Average validation accuracy 0.6876\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8982\t Accuracy 0.7200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9215\t Accuracy 0.6541\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9223\t Accuracy 0.6562\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9246\t Accuracy 0.6472\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9240\t Accuracy 0.6505\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9227\t Accuracy 0.6493\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.9207\t Accuracy 0.6499\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9221\t Accuracy 0.6490\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9205\t Accuracy 0.6502\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9207\t Accuracy 0.6508\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.9199\t Accuracy 0.6508\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9185\t Average training accuracy 0.6529\n",
      "Epoch [6]\t Average validation loss 1.8896\t Average validation accuracy 0.7078\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8751\t Accuracy 0.7400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.8990\t Accuracy 0.6708\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.8999\t Accuracy 0.6726\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.9026\t Accuracy 0.6658\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.9025\t Accuracy 0.6682\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.9015\t Accuracy 0.6659\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.8998\t Accuracy 0.6655\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.9016\t Accuracy 0.6644\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.9003\t Accuracy 0.6654\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.9007\t Accuracy 0.6655\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.9002\t Accuracy 0.6654\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8992\t Average training accuracy 0.6669\n",
      "Epoch [7]\t Average validation loss 1.8723\t Average validation accuracy 0.7202\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8583\t Accuracy 0.7700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8827\t Accuracy 0.6808\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8836\t Accuracy 0.6817\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8865\t Accuracy 0.6756\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8869\t Accuracy 0.6774\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8861\t Accuracy 0.6755\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8846\t Accuracy 0.6751\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8867\t Accuracy 0.6741\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8856\t Accuracy 0.6750\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8864\t Accuracy 0.6751\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8861\t Accuracy 0.6747\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8852\t Average training accuracy 0.6759\n",
      "Epoch [8]\t Average validation loss 1.8601\t Average validation accuracy 0.7296\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8464\t Accuracy 0.7700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8711\t Accuracy 0.6906\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8721\t Accuracy 0.6904\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8752\t Accuracy 0.6842\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8759\t Accuracy 0.6851\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8753\t Accuracy 0.6826\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8740\t Accuracy 0.6817\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8763\t Accuracy 0.6803\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8754\t Accuracy 0.6810\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8764\t Accuracy 0.6812\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8762\t Accuracy 0.6810\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8755\t Average training accuracy 0.6822\n",
      "Epoch [9]\t Average validation loss 1.8519\t Average validation accuracy 0.7354\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8385\t Accuracy 0.7700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8633\t Accuracy 0.6927\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8644\t Accuracy 0.6938\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8675\t Accuracy 0.6874\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8685\t Accuracy 0.6881\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8680\t Accuracy 0.6855\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8669\t Accuracy 0.6851\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8694\t Accuracy 0.6838\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8687\t Accuracy 0.6848\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8697\t Accuracy 0.6850\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8697\t Accuracy 0.6849\n",
      "\n",
      "Epoch [10]\t Average training loss 1.8692\t Average training accuracy 0.6861\n",
      "Epoch [10]\t Average validation loss 1.8468\t Average validation accuracy 0.7384\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8336\t Accuracy 0.7800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8585\t Accuracy 0.6961\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8595\t Accuracy 0.6977\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8628\t Accuracy 0.6920\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8640\t Accuracy 0.6922\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8636\t Accuracy 0.6893\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8626\t Accuracy 0.6885\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.8652\t Accuracy 0.6868\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.8647\t Accuracy 0.6875\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.8658\t Accuracy 0.6878\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.8659\t Accuracy 0.6876\n",
      "\n",
      "Epoch [11]\t Average training loss 1.8655\t Average training accuracy 0.6887\n",
      "Epoch [11]\t Average validation loss 1.8442\t Average validation accuracy 0.7394\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8311\t Accuracy 0.7900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.8560\t Accuracy 0.6988\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.8570\t Accuracy 0.6994\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.8603\t Accuracy 0.6938\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.8617\t Accuracy 0.6936\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.8614\t Accuracy 0.6912\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.8605\t Accuracy 0.6900\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.8632\t Accuracy 0.6885\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.8628\t Accuracy 0.6890\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.8640\t Accuracy 0.6894\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.8642\t Accuracy 0.6893\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8638\t Average training accuracy 0.6903\n",
      "Epoch [12]\t Average validation loss 1.8435\t Average validation accuracy 0.7438\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8305\t Accuracy 0.7900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.8553\t Accuracy 0.6998\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.8563\t Accuracy 0.7004\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.8596\t Accuracy 0.6950\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.8612\t Accuracy 0.6947\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.8610\t Accuracy 0.6924\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.8602\t Accuracy 0.6909\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.8629\t Accuracy 0.6895\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.8626\t Accuracy 0.6898\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.8638\t Accuracy 0.6901\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.8641\t Accuracy 0.6900\n",
      "\n",
      "Epoch [13]\t Average training loss 1.8638\t Average training accuracy 0.6910\n",
      "Epoch [13]\t Average validation loss 1.8443\t Average validation accuracy 0.7450\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8314\t Accuracy 0.7900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.8561\t Accuracy 0.6996\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.8571\t Accuracy 0.6994\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.8604\t Accuracy 0.6940\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.8620\t Accuracy 0.6941\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.8619\t Accuracy 0.6924\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.8611\t Accuracy 0.6907\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.8639\t Accuracy 0.6893\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.8636\t Accuracy 0.6897\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.8650\t Accuracy 0.6901\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.8652\t Accuracy 0.6898\n",
      "\n",
      "Epoch [14]\t Average training loss 1.8650\t Average training accuracy 0.6910\n",
      "Epoch [14]\t Average validation loss 1.8462\t Average validation accuracy 0.7444\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.8334\t Accuracy 0.7800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.8579\t Accuracy 0.7002\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.8589\t Accuracy 0.6995\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.8622\t Accuracy 0.6938\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.8639\t Accuracy 0.6938\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.8638\t Accuracy 0.6920\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.8631\t Accuracy 0.6901\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.8659\t Accuracy 0.6885\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.8657\t Accuracy 0.6890\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.8671\t Accuracy 0.6894\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.8674\t Accuracy 0.6888\n",
      "\n",
      "Epoch [15]\t Average training loss 1.8672\t Average training accuracy 0.6900\n",
      "Epoch [15]\t Average validation loss 1.8490\t Average validation accuracy 0.7428\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.8362\t Accuracy 0.7800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.8606\t Accuracy 0.6988\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.8615\t Accuracy 0.6989\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.8648\t Accuracy 0.6932\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.8666\t Accuracy 0.6935\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.8666\t Accuracy 0.6916\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.8659\t Accuracy 0.6893\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.8687\t Accuracy 0.6876\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.8686\t Accuracy 0.6881\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.8699\t Accuracy 0.6884\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.8703\t Accuracy 0.6878\n",
      "\n",
      "Epoch [16]\t Average training loss 1.8702\t Average training accuracy 0.6888\n",
      "Epoch [16]\t Average validation loss 1.8526\t Average validation accuracy 0.7422\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.8398\t Accuracy 0.7800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.8639\t Accuracy 0.6980\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.8648\t Accuracy 0.6987\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.8680\t Accuracy 0.6926\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.8700\t Accuracy 0.6925\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.8699\t Accuracy 0.6905\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.8693\t Accuracy 0.6878\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.8721\t Accuracy 0.6859\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.8720\t Accuracy 0.6865\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.8734\t Accuracy 0.6868\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.8737\t Accuracy 0.6861\n",
      "\n",
      "Epoch [17]\t Average training loss 1.8737\t Average training accuracy 0.6872\n",
      "Epoch [17]\t Average validation loss 1.8566\t Average validation accuracy 0.7402\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.8437\t Accuracy 0.7600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.8677\t Accuracy 0.6953\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.8686\t Accuracy 0.6957\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.8718\t Accuracy 0.6905\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.8737\t Accuracy 0.6905\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.8737\t Accuracy 0.6886\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.8732\t Accuracy 0.6860\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.8760\t Accuracy 0.6841\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.8758\t Accuracy 0.6848\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.8772\t Accuracy 0.6849\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.8776\t Accuracy 0.6843\n",
      "\n",
      "Epoch [18]\t Average training loss 1.8776\t Average training accuracy 0.6854\n",
      "Epoch [18]\t Average validation loss 1.8610\t Average validation accuracy 0.7384\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.8481\t Accuracy 0.7600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.8719\t Accuracy 0.6941\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.8727\t Accuracy 0.6942\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.8758\t Accuracy 0.6885\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.8778\t Accuracy 0.6884\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.8778\t Accuracy 0.6866\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.8773\t Accuracy 0.6838\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.8801\t Accuracy 0.6820\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.8800\t Accuracy 0.6827\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.8814\t Accuracy 0.6826\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.8817\t Accuracy 0.6821\n",
      "\n",
      "Epoch [19]\t Average training loss 1.8817\t Average training accuracy 0.6832\n",
      "Epoch [19]\t Average validation loss 1.8656\t Average validation accuracy 0.7348\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.1388\t Accuracy 0.0600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.8563\t Accuracy 0.1002\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.7576\t Accuracy 0.1094\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.6583\t Accuracy 0.1242\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5737\t Accuracy 0.1363\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.5062\t Accuracy 0.1499\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.4410\t Accuracy 0.1647\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3870\t Accuracy 0.1833\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3356\t Accuracy 0.2022\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2901\t Accuracy 0.2213\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.2481\t Accuracy 0.2403\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2065\t Average training accuracy 0.2612\n",
      "Epoch [0]\t Average validation loss 1.7288\t Average validation accuracy 0.5094\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.7071\t Accuracy 0.5500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.7228\t Accuracy 0.5196\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.7004\t Accuracy 0.5317\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6817\t Accuracy 0.5425\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.6636\t Accuracy 0.5536\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.6455\t Accuracy 0.5620\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.6237\t Accuracy 0.5730\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.6113\t Accuracy 0.5779\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5936\t Accuracy 0.5864\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5791\t Accuracy 0.5937\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5645\t Accuracy 0.5999\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5473\t Average training accuracy 0.6084\n",
      "Epoch [1]\t Average validation loss 1.3163\t Average validation accuracy 0.7232\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.2936\t Accuracy 0.7300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3369\t Accuracy 0.6996\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3255\t Accuracy 0.7041\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.3198\t Accuracy 0.7050\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3129\t Accuracy 0.7090\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3034\t Accuracy 0.7109\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.2909\t Accuracy 0.7160\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2881\t Accuracy 0.7162\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2783\t Accuracy 0.7198\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2715\t Accuracy 0.7224\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2643\t Accuracy 0.7243\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2540\t Average training accuracy 0.7279\n",
      "Epoch [2]\t Average validation loss 1.0911\t Average validation accuracy 0.7964\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.0780\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1262\t Accuracy 0.7657\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1202\t Accuracy 0.7700\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1202\t Accuracy 0.7664\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1182\t Accuracy 0.7690\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1126\t Accuracy 0.7687\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.1049\t Accuracy 0.7713\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.1064\t Accuracy 0.7704\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.1005\t Accuracy 0.7727\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0973\t Accuracy 0.7737\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0938\t Accuracy 0.7741\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0870\t Average training accuracy 0.7765\n",
      "Epoch [3]\t Average validation loss 0.9585\t Average validation accuracy 0.8316\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.9535\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.0010\t Accuracy 0.7996\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.9981\t Accuracy 0.8037\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0013\t Accuracy 0.7982\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0018\t Accuracy 0.7990\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.9982\t Accuracy 0.7986\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.9932\t Accuracy 0.8007\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.9970\t Accuracy 0.7992\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.9931\t Accuracy 0.8009\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.9919\t Accuracy 0.8010\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.9904\t Accuracy 0.8008\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9856\t Average training accuracy 0.8024\n",
      "Epoch [4]\t Average validation loss 0.8761\t Average validation accuracy 0.8548\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8778\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9225\t Accuracy 0.8196\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9217\t Accuracy 0.8222\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9269\t Accuracy 0.8173\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9286\t Accuracy 0.8179\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9262\t Accuracy 0.8174\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9228\t Accuracy 0.8187\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9278\t Accuracy 0.8170\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9251\t Accuracy 0.8189\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9251\t Accuracy 0.8186\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9246\t Accuracy 0.8185\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9211\t Average training accuracy 0.8195\n",
      "Epoch [5]\t Average validation loss 0.8230\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8313\t Accuracy 0.8700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8714\t Accuracy 0.8361\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8722\t Accuracy 0.8367\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8785\t Accuracy 0.8306\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8811\t Accuracy 0.8308\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8793\t Accuracy 0.8299\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8769\t Accuracy 0.8305\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8826\t Accuracy 0.8289\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8807\t Accuracy 0.8304\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8814\t Accuracy 0.8298\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8816\t Accuracy 0.8294\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8788\t Average training accuracy 0.8300\n",
      "Epoch [6]\t Average validation loss 0.7879\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8017\t Accuracy 0.8800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8373\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8393\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8464\t Accuracy 0.8381\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8494\t Accuracy 0.8380\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8481\t Accuracy 0.8376\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8462\t Accuracy 0.8383\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8524\t Accuracy 0.8366\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8381\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8521\t Accuracy 0.8377\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8528\t Accuracy 0.8372\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8506\t Average training accuracy 0.8375\n",
      "Epoch [7]\t Average validation loss 0.7645\t Average validation accuracy 0.8798\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.7826\t Accuracy 0.9000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8143\t Accuracy 0.8516\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8172\t Accuracy 0.8503\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8248\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8282\t Accuracy 0.8436\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8271\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8257\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8321\t Accuracy 0.8424\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8311\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8325\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8334\t Accuracy 0.8431\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8316\t Average training accuracy 0.8434\n",
      "Epoch [8]\t Average validation loss 0.7488\t Average validation accuracy 0.8826\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7705\t Accuracy 0.9000\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.7989\t Accuracy 0.8567\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8025\t Accuracy 0.8545\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8104\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8140\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8130\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8120\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8185\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8177\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8193\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8204\t Accuracy 0.8473\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8189\t Average training accuracy 0.8477\n",
      "Epoch [9]\t Average validation loss 0.7385\t Average validation accuracy 0.8862\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7626\t Accuracy 0.9100\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.7888\t Accuracy 0.8600\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.7928\t Accuracy 0.8574\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8010\t Accuracy 0.8521\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8047\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8038\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8030\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8096\t Accuracy 0.8506\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8090\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8107\t Accuracy 0.8514\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8119\t Accuracy 0.8507\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8106\t Average training accuracy 0.8509\n",
      "Epoch [10]\t Average validation loss 0.7320\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7579\t Accuracy 0.9100\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7824\t Accuracy 0.8635\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.7867\t Accuracy 0.8613\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.7950\t Accuracy 0.8563\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.7989\t Accuracy 0.8556\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.7980\t Accuracy 0.8550\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.7973\t Accuracy 0.8556\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8039\t Accuracy 0.8542\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8035\t Accuracy 0.8555\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8053\t Accuracy 0.8546\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8066\t Accuracy 0.8538\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8054\t Average training accuracy 0.8538\n",
      "Epoch [11]\t Average validation loss 0.7283\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7554\t Accuracy 0.9100\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7786\t Accuracy 0.8663\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.7831\t Accuracy 0.8640\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.7915\t Accuracy 0.8585\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.7955\t Accuracy 0.8577\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.7946\t Accuracy 0.8573\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.7941\t Accuracy 0.8579\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8007\t Accuracy 0.8568\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8004\t Accuracy 0.8579\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8023\t Accuracy 0.8570\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8036\t Accuracy 0.8561\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8025\t Average training accuracy 0.8561\n",
      "Epoch [12]\t Average validation loss 0.7265\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7545\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7767\t Accuracy 0.8680\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.7814\t Accuracy 0.8650\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.7899\t Accuracy 0.8598\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.7940\t Accuracy 0.8583\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.7931\t Accuracy 0.8582\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.7926\t Accuracy 0.8586\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.7993\t Accuracy 0.8576\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.7990\t Accuracy 0.8586\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8009\t Accuracy 0.8578\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8023\t Accuracy 0.8570\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8013\t Average training accuracy 0.8571\n",
      "Epoch [13]\t Average validation loss 0.7262\t Average validation accuracy 0.8918\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7550\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7764\t Accuracy 0.8686\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7811\t Accuracy 0.8659\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.7896\t Accuracy 0.8606\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.7938\t Accuracy 0.8592\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.7929\t Accuracy 0.8593\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7925\t Accuracy 0.8597\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.7991\t Accuracy 0.8587\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.7989\t Accuracy 0.8597\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8009\t Accuracy 0.8590\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8022\t Accuracy 0.8582\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8013\t Average training accuracy 0.8581\n",
      "Epoch [14]\t Average validation loss 0.7271\t Average validation accuracy 0.8924\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7564\t Accuracy 0.9100\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7771\t Accuracy 0.8686\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7818\t Accuracy 0.8658\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7904\t Accuracy 0.8605\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7946\t Accuracy 0.8597\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7936\t Accuracy 0.8597\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7934\t Accuracy 0.8604\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7999\t Accuracy 0.8594\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7998\t Accuracy 0.8604\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8018\t Accuracy 0.8597\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8031\t Accuracy 0.8589\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8023\t Average training accuracy 0.8587\n",
      "Epoch [15]\t Average validation loss 0.7287\t Average validation accuracy 0.8938\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7586\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7785\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7833\t Accuracy 0.8668\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7918\t Accuracy 0.8613\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.7961\t Accuracy 0.8605\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7951\t Accuracy 0.8606\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7949\t Accuracy 0.8611\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8014\t Accuracy 0.8601\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8013\t Accuracy 0.8610\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8033\t Accuracy 0.8603\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8046\t Accuracy 0.8594\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8038\t Average training accuracy 0.8593\n",
      "Epoch [16]\t Average validation loss 0.7308\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7612\t Accuracy 0.9200\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7804\t Accuracy 0.8698\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7852\t Accuracy 0.8667\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.7938\t Accuracy 0.8613\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.7980\t Accuracy 0.8607\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.7970\t Accuracy 0.8608\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.7968\t Accuracy 0.8614\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8033\t Accuracy 0.8605\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8032\t Accuracy 0.8612\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8052\t Accuracy 0.8606\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8065\t Accuracy 0.8597\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8058\t Average training accuracy 0.8596\n",
      "Epoch [17]\t Average validation loss 0.7333\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7643\t Accuracy 0.9200\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7827\t Accuracy 0.8692\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.7875\t Accuracy 0.8662\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.7961\t Accuracy 0.8610\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8003\t Accuracy 0.8603\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.7992\t Accuracy 0.8607\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.7991\t Accuracy 0.8614\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8056\t Accuracy 0.8605\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8055\t Accuracy 0.8613\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8075\t Accuracy 0.8606\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8087\t Accuracy 0.8599\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8081\t Average training accuracy 0.8598\n",
      "Epoch [18]\t Average validation loss 0.7360\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7676\t Accuracy 0.9200\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.7852\t Accuracy 0.8704\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.7900\t Accuracy 0.8672\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.7985\t Accuracy 0.8615\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8028\t Accuracy 0.8607\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8017\t Accuracy 0.8610\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8015\t Accuracy 0.8617\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8080\t Accuracy 0.8608\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8079\t Accuracy 0.8615\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8099\t Accuracy 0.8609\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8111\t Accuracy 0.8601\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8105\t Average training accuracy 0.8601\n",
      "Epoch [19]\t Average validation loss 0.7388\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 6.6780\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9615\t Accuracy 0.0912\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.3106\t Accuracy 0.1018\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.0812\t Accuracy 0.1105\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9605\t Accuracy 0.1208\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8828\t Accuracy 0.1316\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8287\t Accuracy 0.1407\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7878\t Accuracy 0.1491\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7545\t Accuracy 0.1589\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7274\t Accuracy 0.1675\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7047\t Accuracy 0.1757\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6849\t Average training accuracy 0.1853\n",
      "Epoch [0]\t Average validation loss 0.4688\t Average validation accuracy 0.3148\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4281\t Accuracy 0.3500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4635\t Accuracy 0.3286\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4611\t Accuracy 0.3282\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4588\t Accuracy 0.3323\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4559\t Accuracy 0.3386\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4513\t Accuracy 0.3495\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4474\t Accuracy 0.3562\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4445\t Accuracy 0.3596\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4405\t Accuracy 0.3678\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4370\t Accuracy 0.3751\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4338\t Accuracy 0.3825\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4303\t Average training accuracy 0.3903\n",
      "Epoch [1]\t Average validation loss 0.3824\t Average validation accuracy 0.5144\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3553\t Accuracy 0.5900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3837\t Accuracy 0.5067\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3832\t Accuracy 0.5085\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3830\t Accuracy 0.5095\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3821\t Accuracy 0.5115\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3796\t Accuracy 0.5181\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3777\t Accuracy 0.5234\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3768\t Accuracy 0.5249\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3747\t Accuracy 0.5296\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3731\t Accuracy 0.5337\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3715\t Accuracy 0.5377\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3695\t Average training accuracy 0.5431\n",
      "Epoch [2]\t Average validation loss 0.3385\t Average validation accuracy 0.6312\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3192\t Accuracy 0.6900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3430\t Accuracy 0.6157\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3431\t Accuracy 0.6156\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3438\t Accuracy 0.6132\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3437\t Accuracy 0.6123\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3421\t Accuracy 0.6149\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3410\t Accuracy 0.6179\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3410\t Accuracy 0.6181\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3398\t Accuracy 0.6204\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3389\t Accuracy 0.6231\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3381\t Accuracy 0.6252\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3369\t Average training accuracy 0.6285\n",
      "Epoch [3]\t Average validation loss 0.3135\t Average validation accuracy 0.6958\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2992\t Accuracy 0.7800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3198\t Accuracy 0.6767\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3201\t Accuracy 0.6724\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3213\t Accuracy 0.6683\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3215\t Accuracy 0.6672\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3204\t Accuracy 0.6680\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3197\t Accuracy 0.6691\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3201\t Accuracy 0.6685\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3194\t Accuracy 0.6697\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3190\t Accuracy 0.6717\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3186\t Accuracy 0.6730\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3177\t Average training accuracy 0.6754\n",
      "Epoch [4]\t Average validation loss 0.2985\t Average validation accuracy 0.7334\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2875\t Accuracy 0.8100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3058\t Accuracy 0.7096\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3062\t Accuracy 0.7068\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3077\t Accuracy 0.7022\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3081\t Accuracy 0.7004\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3072\t Accuracy 0.7005\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3067\t Accuracy 0.7013\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3075\t Accuracy 0.7004\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3070\t Accuracy 0.7012\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3068\t Accuracy 0.7024\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3066\t Accuracy 0.7037\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3060\t Average training accuracy 0.7056\n",
      "Epoch [5]\t Average validation loss 0.2894\t Average validation accuracy 0.7646\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2808\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2972\t Accuracy 0.7318\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2977\t Accuracy 0.7293\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2993\t Accuracy 0.7240\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2998\t Accuracy 0.7215\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2992\t Accuracy 0.7213\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2989\t Accuracy 0.7225\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2997\t Accuracy 0.7214\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2994\t Accuracy 0.7220\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2994\t Accuracy 0.7227\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2994\t Accuracy 0.7237\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2989\t Average training accuracy 0.7256\n",
      "Epoch [6]\t Average validation loss 0.2841\t Average validation accuracy 0.7816\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2771\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2922\t Accuracy 0.7455\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2927\t Accuracy 0.7448\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2944\t Accuracy 0.7390\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2950\t Accuracy 0.7367\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2945\t Accuracy 0.7365\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2942\t Accuracy 0.7377\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2952\t Accuracy 0.7364\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2950\t Accuracy 0.7367\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2952\t Accuracy 0.7376\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2952\t Accuracy 0.7385\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2948\t Average training accuracy 0.7403\n",
      "Epoch [7]\t Average validation loss 0.2814\t Average validation accuracy 0.7958\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2756\t Accuracy 0.8200\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2895\t Accuracy 0.7582\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2901\t Accuracy 0.7564\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2918\t Accuracy 0.7507\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2924\t Accuracy 0.7488\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2921\t Accuracy 0.7483\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2919\t Accuracy 0.7491\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2929\t Accuracy 0.7476\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2928\t Accuracy 0.7477\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2930\t Accuracy 0.7484\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2931\t Accuracy 0.7493\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2929\t Average training accuracy 0.7509\n",
      "Epoch [8]\t Average validation loss 0.2805\t Average validation accuracy 0.8044\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2756\t Accuracy 0.8200\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2886\t Accuracy 0.7667\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2892\t Accuracy 0.7640\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2909\t Accuracy 0.7577\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2916\t Accuracy 0.7565\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2913\t Accuracy 0.7559\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2912\t Accuracy 0.7568\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2922\t Accuracy 0.7553\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2922\t Accuracy 0.7556\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2924\t Accuracy 0.7565\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2926\t Accuracy 0.7569\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2924\t Average training accuracy 0.7583\n",
      "Epoch [9]\t Average validation loss 0.2809\t Average validation accuracy 0.8116\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2767\t Accuracy 0.8200\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2889\t Accuracy 0.7733\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2895\t Accuracy 0.7688\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2912\t Accuracy 0.7632\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2919\t Accuracy 0.7619\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2917\t Accuracy 0.7606\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2916\t Accuracy 0.7612\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2927\t Accuracy 0.7598\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2927\t Accuracy 0.7601\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2930\t Accuracy 0.7609\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2932\t Accuracy 0.7612\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2930\t Average training accuracy 0.7628\n",
      "Epoch [10]\t Average validation loss 0.2823\t Average validation accuracy 0.8150\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2787\t Accuracy 0.8100\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2902\t Accuracy 0.7749\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2908\t Accuracy 0.7708\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2924\t Accuracy 0.7658\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2931\t Accuracy 0.7652\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2930\t Accuracy 0.7644\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2929\t Accuracy 0.7645\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2940\t Accuracy 0.7634\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2941\t Accuracy 0.7637\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2944\t Accuracy 0.7646\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2946\t Accuracy 0.7649\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2945\t Average training accuracy 0.7666\n",
      "Epoch [11]\t Average validation loss 0.2845\t Average validation accuracy 0.8190\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2812\t Accuracy 0.8200\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2922\t Accuracy 0.7780\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2927\t Accuracy 0.7740\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2944\t Accuracy 0.7678\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2951\t Accuracy 0.7673\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2949\t Accuracy 0.7667\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2949\t Accuracy 0.7671\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2960\t Accuracy 0.7665\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2961\t Accuracy 0.7671\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2965\t Accuracy 0.7678\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2967\t Accuracy 0.7680\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2966\t Average training accuracy 0.7697\n",
      "Epoch [12]\t Average validation loss 0.2872\t Average validation accuracy 0.8206\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2843\t Accuracy 0.8200\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2947\t Accuracy 0.7776\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2952\t Accuracy 0.7750\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2968\t Accuracy 0.7693\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2975\t Accuracy 0.7685\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2974\t Accuracy 0.7680\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2974\t Accuracy 0.7685\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2986\t Accuracy 0.7683\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2986\t Accuracy 0.7689\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2990\t Accuracy 0.7696\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2993\t Accuracy 0.7696\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2992\t Average training accuracy 0.7713\n",
      "Epoch [13]\t Average validation loss 0.2904\t Average validation accuracy 0.8218\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2876\t Accuracy 0.8200\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2976\t Accuracy 0.7780\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2981\t Accuracy 0.7769\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2997\t Accuracy 0.7708\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.3004\t Accuracy 0.7695\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.3003\t Accuracy 0.7690\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.3003\t Accuracy 0.7696\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.3014\t Accuracy 0.7694\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.3015\t Accuracy 0.7703\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.3019\t Accuracy 0.7711\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.3022\t Accuracy 0.7711\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3022\t Average training accuracy 0.7726\n",
      "Epoch [14]\t Average validation loss 0.2938\t Average validation accuracy 0.8238\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2913\t Accuracy 0.8200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.3008\t Accuracy 0.7776\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.3013\t Accuracy 0.7762\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.3028\t Accuracy 0.7714\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.3035\t Accuracy 0.7704\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.3035\t Accuracy 0.7700\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.3035\t Accuracy 0.7706\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3046\t Accuracy 0.7702\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3047\t Accuracy 0.7713\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3051\t Accuracy 0.7720\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3054\t Accuracy 0.7720\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3054\t Average training accuracy 0.7734\n",
      "Epoch [15]\t Average validation loss 0.2975\t Average validation accuracy 0.8238\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2951\t Accuracy 0.8200\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.3042\t Accuracy 0.7763\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.3047\t Accuracy 0.7760\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3062\t Accuracy 0.7714\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3069\t Accuracy 0.7709\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3069\t Accuracy 0.7705\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3069\t Accuracy 0.7709\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3080\t Accuracy 0.7705\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3081\t Accuracy 0.7714\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3085\t Accuracy 0.7721\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3088\t Accuracy 0.7721\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3088\t Average training accuracy 0.7734\n",
      "Epoch [16]\t Average validation loss 0.3014\t Average validation accuracy 0.8216\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2991\t Accuracy 0.8200\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3078\t Accuracy 0.7751\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3083\t Accuracy 0.7756\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3097\t Accuracy 0.7712\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3104\t Accuracy 0.7705\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3104\t Accuracy 0.7701\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3104\t Accuracy 0.7705\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3115\t Accuracy 0.7700\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3116\t Accuracy 0.7710\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3120\t Accuracy 0.7715\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3123\t Accuracy 0.7715\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3123\t Average training accuracy 0.7727\n",
      "Epoch [17]\t Average validation loss 0.3053\t Average validation accuracy 0.8212\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.3031\t Accuracy 0.8200\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3115\t Accuracy 0.7747\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3119\t Accuracy 0.7752\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3133\t Accuracy 0.7709\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3140\t Accuracy 0.7702\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3140\t Accuracy 0.7699\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3141\t Accuracy 0.7699\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3151\t Accuracy 0.7694\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3152\t Accuracy 0.7702\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3157\t Accuracy 0.7707\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3159\t Accuracy 0.7708\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3159\t Average training accuracy 0.7719\n",
      "Epoch [18]\t Average validation loss 0.3093\t Average validation accuracy 0.8202\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.3072\t Accuracy 0.8200\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3153\t Accuracy 0.7737\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3157\t Accuracy 0.7742\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3170\t Accuracy 0.7697\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3177\t Accuracy 0.7693\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3177\t Accuracy 0.7691\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3177\t Accuracy 0.7693\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3188\t Accuracy 0.7688\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3189\t Accuracy 0.7696\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3193\t Accuracy 0.7702\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3195\t Accuracy 0.7702\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3196\t Average training accuracy 0.7711\n",
      "Epoch [19]\t Average validation loss 0.3133\t Average validation accuracy 0.8194\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.7768\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.2032\t Accuracy 0.1304\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.7970\t Accuracy 0.1454\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.5905\t Accuracy 0.1622\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.4587\t Accuracy 0.1747\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.3578\t Accuracy 0.1888\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.2777\t Accuracy 0.2043\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.2102\t Accuracy 0.2181\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.1546\t Accuracy 0.2309\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.1065\t Accuracy 0.2420\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.0652\t Accuracy 0.2535\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0279\t Average training accuracy 0.2654\n",
      "Epoch [0]\t Average validation loss 0.6284\t Average validation accuracy 0.3970\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.5638\t Accuracy 0.5000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.6155\t Accuracy 0.4082\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.6042\t Accuracy 0.4154\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.5950\t Accuracy 0.4217\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.5873\t Accuracy 0.4248\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.5779\t Accuracy 0.4306\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.5681\t Accuracy 0.4402\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.5591\t Accuracy 0.4470\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.5513\t Accuracy 0.4525\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.5436\t Accuracy 0.4586\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.5367\t Accuracy 0.4640\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5291\t Average training accuracy 0.4715\n",
      "Epoch [1]\t Average validation loss 0.4387\t Average validation accuracy 0.5608\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3986\t Accuracy 0.6200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4377\t Accuracy 0.5598\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4341\t Accuracy 0.5599\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4316\t Accuracy 0.5606\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4292\t Accuracy 0.5610\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4254\t Accuracy 0.5653\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4209\t Accuracy 0.5718\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4173\t Accuracy 0.5746\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4139\t Accuracy 0.5774\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4106\t Accuracy 0.5813\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4075\t Accuracy 0.5844\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4037\t Average training accuracy 0.5896\n",
      "Epoch [2]\t Average validation loss 0.3528\t Average validation accuracy 0.6606\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3234\t Accuracy 0.7000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3560\t Accuracy 0.6524\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3545\t Accuracy 0.6484\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3542\t Accuracy 0.6471\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3534\t Accuracy 0.6471\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3514\t Accuracy 0.6501\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3488\t Accuracy 0.6541\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3472\t Accuracy 0.6546\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3453\t Accuracy 0.6549\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3436\t Accuracy 0.6583\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3419\t Accuracy 0.6600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3396\t Average training accuracy 0.6632\n",
      "Epoch [3]\t Average validation loss 0.3037\t Average validation accuracy 0.7230\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2802\t Accuracy 0.7400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3090\t Accuracy 0.7086\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3084\t Accuracy 0.7055\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3091\t Accuracy 0.7042\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3089\t Accuracy 0.7034\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3077\t Accuracy 0.7049\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3061\t Accuracy 0.7072\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3055\t Accuracy 0.7070\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3043\t Accuracy 0.7069\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3033\t Accuracy 0.7090\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3024\t Accuracy 0.7100\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3008\t Average training accuracy 0.7121\n",
      "Epoch [4]\t Average validation loss 0.2722\t Average validation accuracy 0.7728\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2532\t Accuracy 0.7700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2786\t Accuracy 0.7490\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2786\t Accuracy 0.7446\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2799\t Accuracy 0.7432\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2800\t Accuracy 0.7417\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2792\t Accuracy 0.7425\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2781\t Accuracy 0.7441\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2781\t Accuracy 0.7439\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2773\t Accuracy 0.7442\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2767\t Accuracy 0.7457\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2762\t Accuracy 0.7463\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2750\t Average training accuracy 0.7475\n",
      "Epoch [5]\t Average validation loss 0.2507\t Average validation accuracy 0.8016\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2349\t Accuracy 0.7900\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2578\t Accuracy 0.7825\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2581\t Accuracy 0.7757\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2597\t Accuracy 0.7729\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2600\t Accuracy 0.7716\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2595\t Accuracy 0.7714\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2587\t Accuracy 0.7720\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2591\t Accuracy 0.7711\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2585\t Accuracy 0.7714\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2581\t Accuracy 0.7724\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2579\t Accuracy 0.7726\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2570\t Average training accuracy 0.7735\n",
      "Epoch [6]\t Average validation loss 0.2354\t Average validation accuracy 0.8196\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2219\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2429\t Accuracy 0.8020\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2434\t Accuracy 0.7978\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2452\t Accuracy 0.7942\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2457\t Accuracy 0.7924\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2453\t Accuracy 0.7920\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2448\t Accuracy 0.7923\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2454\t Accuracy 0.7908\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2450\t Accuracy 0.7910\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2448\t Accuracy 0.7916\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2447\t Accuracy 0.7914\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2440\t Average training accuracy 0.7922\n",
      "Epoch [7]\t Average validation loss 0.2242\t Average validation accuracy 0.8342\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2125\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2321\t Accuracy 0.8159\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2327\t Accuracy 0.8114\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2347\t Accuracy 0.8073\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2352\t Accuracy 0.8066\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2350\t Accuracy 0.8059\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2346\t Accuracy 0.8062\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2354\t Accuracy 0.8050\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2351\t Accuracy 0.8054\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2350\t Accuracy 0.8059\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2351\t Accuracy 0.8053\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2345\t Average training accuracy 0.8058\n",
      "Epoch [8]\t Average validation loss 0.2160\t Average validation accuracy 0.8484\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2054\t Accuracy 0.8700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2242\t Accuracy 0.8280\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2249\t Accuracy 0.8241\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2269\t Accuracy 0.8186\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2275\t Accuracy 0.8184\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2274\t Accuracy 0.8172\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2272\t Accuracy 0.8170\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2281\t Accuracy 0.8155\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2278\t Accuracy 0.8159\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2279\t Accuracy 0.8162\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2280\t Accuracy 0.8158\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2275\t Average training accuracy 0.8161\n",
      "Epoch [9]\t Average validation loss 0.2100\t Average validation accuracy 0.8586\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2002\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2184\t Accuracy 0.8371\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2192\t Accuracy 0.8337\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2213\t Accuracy 0.8275\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2219\t Accuracy 0.8270\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2219\t Accuracy 0.8259\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2217\t Accuracy 0.8257\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2227\t Accuracy 0.8239\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2226\t Accuracy 0.8245\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2227\t Accuracy 0.8245\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2229\t Accuracy 0.8241\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2225\t Average training accuracy 0.8243\n",
      "Epoch [10]\t Average validation loss 0.2057\t Average validation accuracy 0.8670\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1964\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2143\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2151\t Accuracy 0.8405\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2172\t Accuracy 0.8346\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2179\t Accuracy 0.8340\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2179\t Accuracy 0.8322\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2178\t Accuracy 0.8319\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2189\t Accuracy 0.8301\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2187\t Accuracy 0.8309\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2190\t Accuracy 0.8308\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2192\t Accuracy 0.8304\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2188\t Average training accuracy 0.8304\n",
      "Epoch [11]\t Average validation loss 0.2026\t Average validation accuracy 0.8730\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1940\t Accuracy 0.9000\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2113\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2121\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2143\t Accuracy 0.8393\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2151\t Accuracy 0.8381\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2151\t Accuracy 0.8360\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2151\t Accuracy 0.8358\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2162\t Accuracy 0.8340\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2161\t Accuracy 0.8350\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2164\t Accuracy 0.8349\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2166\t Accuracy 0.8347\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2163\t Average training accuracy 0.8346\n",
      "Epoch [12]\t Average validation loss 0.2005\t Average validation accuracy 0.8746\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1924\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2093\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2102\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2124\t Accuracy 0.8419\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2132\t Accuracy 0.8411\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2132\t Accuracy 0.8397\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2132\t Accuracy 0.8397\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2144\t Accuracy 0.8381\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2143\t Accuracy 0.8393\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2146\t Accuracy 0.8391\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2149\t Accuracy 0.8388\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2146\t Average training accuracy 0.8387\n",
      "Epoch [13]\t Average validation loss 0.1991\t Average validation accuracy 0.8760\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1916\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2081\t Accuracy 0.8541\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2089\t Accuracy 0.8505\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2111\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2120\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2120\t Accuracy 0.8427\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2121\t Accuracy 0.8427\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2133\t Accuracy 0.8412\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2132\t Accuracy 0.8422\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2136\t Accuracy 0.8420\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2138\t Accuracy 0.8416\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2136\t Average training accuracy 0.8414\n",
      "Epoch [14]\t Average validation loss 0.1983\t Average validation accuracy 0.8798\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1913\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2074\t Accuracy 0.8567\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2082\t Accuracy 0.8524\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2105\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2113\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2114\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2115\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2127\t Accuracy 0.8423\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2127\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2130\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2133\t Accuracy 0.8429\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2131\t Average training accuracy 0.8428\n",
      "Epoch [15]\t Average validation loss 0.1980\t Average validation accuracy 0.8810\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1913\t Accuracy 0.8800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2072\t Accuracy 0.8559\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2080\t Accuracy 0.8527\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2103\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2111\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2112\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2113\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2125\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2125\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2129\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2132\t Accuracy 0.8441\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2130\t Average training accuracy 0.8440\n",
      "Epoch [16]\t Average validation loss 0.1980\t Average validation accuracy 0.8830\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1917\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2073\t Accuracy 0.8565\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2081\t Accuracy 0.8529\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2104\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2113\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2113\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2115\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2127\t Accuracy 0.8440\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2127\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2131\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2133\t Accuracy 0.8449\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2132\t Average training accuracy 0.8448\n",
      "Epoch [17]\t Average validation loss 0.1983\t Average validation accuracy 0.8844\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1924\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2077\t Accuracy 0.8571\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2085\t Accuracy 0.8535\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2108\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2117\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2117\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2119\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2131\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2131\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2135\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2138\t Accuracy 0.8452\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2136\t Average training accuracy 0.8452\n",
      "Epoch [18]\t Average validation loss 0.1989\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1932\t Accuracy 0.8700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2083\t Accuracy 0.8555\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2091\t Accuracy 0.8527\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2114\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2123\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2123\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2125\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2137\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2137\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2141\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2144\t Accuracy 0.8447\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2142\t Average training accuracy 0.8446\n",
      "Epoch [19]\t Average validation loss 0.1996\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.8288\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6805\t Accuracy 0.1098\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.6174\t Accuracy 0.1081\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.5666\t Accuracy 0.1114\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5306\t Accuracy 0.1153\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.4972\t Accuracy 0.1232\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.4701\t Accuracy 0.1311\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.4470\t Accuracy 0.1385\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.4261\t Accuracy 0.1442\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.4074\t Accuracy 0.1500\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3906\t Accuracy 0.1560\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3757\t Average training accuracy 0.1610\n",
      "Epoch [0]\t Average validation loss 2.2142\t Average validation accuracy 0.2472\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1947\t Accuracy 0.2300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.1988\t Accuracy 0.2459\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.1960\t Accuracy 0.2508\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.1905\t Accuracy 0.2617\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.1872\t Accuracy 0.2710\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1819\t Accuracy 0.2846\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1777\t Accuracy 0.2989\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1744\t Accuracy 0.3073\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1705\t Accuracy 0.3163\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1668\t Accuracy 0.3243\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1629\t Accuracy 0.3333\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1591\t Average training accuracy 0.3419\n",
      "Epoch [1]\t Average validation loss 2.1103\t Average validation accuracy 0.4520\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0816\t Accuracy 0.5400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.1000\t Accuracy 0.4588\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.0992\t Accuracy 0.4566\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.0967\t Accuracy 0.4575\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.0953\t Accuracy 0.4582\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.0918\t Accuracy 0.4618\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.0890\t Accuracy 0.4650\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.0878\t Accuracy 0.4659\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0852\t Accuracy 0.4675\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0830\t Accuracy 0.4698\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0804\t Accuracy 0.4740\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0778\t Average training accuracy 0.4781\n",
      "Epoch [2]\t Average validation loss 2.0386\t Average validation accuracy 0.5428\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0121\t Accuracy 0.6000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0331\t Accuracy 0.5510\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0328\t Accuracy 0.5464\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0316\t Accuracy 0.5423\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0312\t Accuracy 0.5411\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0286\t Accuracy 0.5425\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0263\t Accuracy 0.5443\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0263\t Accuracy 0.5434\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0244\t Accuracy 0.5441\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0231\t Accuracy 0.5461\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 2.0213\t Accuracy 0.5488\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0193\t Average training accuracy 0.5520\n",
      "Epoch [3]\t Average validation loss 1.9853\t Average validation accuracy 0.6182\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9615\t Accuracy 0.7200\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9837\t Accuracy 0.6071\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9837\t Accuracy 0.6006\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9836\t Accuracy 0.5972\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9839\t Accuracy 0.5940\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9819\t Accuracy 0.5947\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9801\t Accuracy 0.5958\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9809\t Accuracy 0.5949\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9795\t Accuracy 0.5962\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9789\t Accuracy 0.5973\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9776\t Accuracy 0.5993\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9761\t Average training accuracy 0.6018\n",
      "Epoch [4]\t Average validation loss 1.9460\t Average validation accuracy 0.6628\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9244\t Accuracy 0.7400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9474\t Accuracy 0.6406\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9477\t Accuracy 0.6351\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9484\t Accuracy 0.6321\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9492\t Accuracy 0.6289\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9477\t Accuracy 0.6286\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9462\t Accuracy 0.6299\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9476\t Accuracy 0.6286\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9466\t Accuracy 0.6298\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9465\t Accuracy 0.6307\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9456\t Accuracy 0.6324\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9445\t Average training accuracy 0.6344\n",
      "Epoch [5]\t Average validation loss 1.9174\t Average validation accuracy 0.6878\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8976\t Accuracy 0.7800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9211\t Accuracy 0.6627\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9215\t Accuracy 0.6579\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9229\t Accuracy 0.6550\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9241\t Accuracy 0.6521\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9229\t Accuracy 0.6512\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.9216\t Accuracy 0.6523\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9235\t Accuracy 0.6511\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9228\t Accuracy 0.6520\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9230\t Accuracy 0.6526\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.9225\t Accuracy 0.6542\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9217\t Average training accuracy 0.6559\n",
      "Epoch [6]\t Average validation loss 1.8970\t Average validation accuracy 0.7086\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8786\t Accuracy 0.7800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.9024\t Accuracy 0.6810\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.9029\t Accuracy 0.6752\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.9047\t Accuracy 0.6711\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.9062\t Accuracy 0.6676\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.9053\t Accuracy 0.6663\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.9042\t Accuracy 0.6671\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.9064\t Accuracy 0.6666\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.9060\t Accuracy 0.6674\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.9065\t Accuracy 0.6675\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.9062\t Accuracy 0.6688\n",
      "\n",
      "Epoch [7]\t Average training loss 1.9056\t Average training accuracy 0.6705\n",
      "Epoch [7]\t Average validation loss 1.8828\t Average validation accuracy 0.7230\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8656\t Accuracy 0.7700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8895\t Accuracy 0.6908\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8901\t Accuracy 0.6860\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8922\t Accuracy 0.6831\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8940\t Accuracy 0.6793\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8932\t Accuracy 0.6783\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8923\t Accuracy 0.6792\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8948\t Accuracy 0.6780\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8945\t Accuracy 0.6789\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8952\t Accuracy 0.6789\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8951\t Accuracy 0.6800\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8947\t Average training accuracy 0.6811\n",
      "Epoch [8]\t Average validation loss 1.8734\t Average validation accuracy 0.7320\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8571\t Accuracy 0.7700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8811\t Accuracy 0.6965\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8817\t Accuracy 0.6936\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8841\t Accuracy 0.6893\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8860\t Accuracy 0.6859\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8854\t Accuracy 0.6847\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8847\t Accuracy 0.6853\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8873\t Accuracy 0.6841\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8871\t Accuracy 0.6853\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8880\t Accuracy 0.6855\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8880\t Accuracy 0.6865\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8877\t Average training accuracy 0.6873\n",
      "Epoch [9]\t Average validation loss 1.8677\t Average validation accuracy 0.7374\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8522\t Accuracy 0.7700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8761\t Accuracy 0.7008\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8768\t Accuracy 0.6973\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8793\t Accuracy 0.6930\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8813\t Accuracy 0.6902\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8809\t Accuracy 0.6892\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8802\t Accuracy 0.6895\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8830\t Accuracy 0.6886\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8829\t Accuracy 0.6900\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8839\t Accuracy 0.6902\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8840\t Accuracy 0.6911\n",
      "\n",
      "Epoch [10]\t Average training loss 1.8838\t Average training accuracy 0.6916\n",
      "Epoch [10]\t Average validation loss 1.8649\t Average validation accuracy 0.7402\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8500\t Accuracy 0.7700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8738\t Accuracy 0.7039\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8745\t Accuracy 0.7003\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8771\t Accuracy 0.6954\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8792\t Accuracy 0.6924\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8789\t Accuracy 0.6917\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8783\t Accuracy 0.6919\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.8811\t Accuracy 0.6913\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.8811\t Accuracy 0.6923\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.8822\t Accuracy 0.6924\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.8824\t Accuracy 0.6932\n",
      "\n",
      "Epoch [11]\t Average training loss 1.8823\t Average training accuracy 0.6937\n",
      "Epoch [11]\t Average validation loss 1.8643\t Average validation accuracy 0.7422\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8498\t Accuracy 0.7800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.8735\t Accuracy 0.7071\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.8742\t Accuracy 0.7031\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.8769\t Accuracy 0.6975\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.8791\t Accuracy 0.6947\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.8788\t Accuracy 0.6938\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.8783\t Accuracy 0.6938\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.8812\t Accuracy 0.6929\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.8812\t Accuracy 0.6938\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.8823\t Accuracy 0.6937\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.8826\t Accuracy 0.6945\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8826\t Average training accuracy 0.6948\n",
      "Epoch [12]\t Average validation loss 1.8653\t Average validation accuracy 0.7408\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8513\t Accuracy 0.7700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.8748\t Accuracy 0.7076\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.8754\t Accuracy 0.7029\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.8782\t Accuracy 0.6975\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.8804\t Accuracy 0.6947\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.8802\t Accuracy 0.6941\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.8797\t Accuracy 0.6940\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.8826\t Accuracy 0.6930\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.8827\t Accuracy 0.6939\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.8839\t Accuracy 0.6938\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.8842\t Accuracy 0.6945\n",
      "\n",
      "Epoch [13]\t Average training loss 1.8842\t Average training accuracy 0.6947\n",
      "Epoch [13]\t Average validation loss 1.8676\t Average validation accuracy 0.7362\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8539\t Accuracy 0.7700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.8772\t Accuracy 0.7065\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.8778\t Accuracy 0.7024\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.8806\t Accuracy 0.6969\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.8829\t Accuracy 0.6937\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.8827\t Accuracy 0.6931\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.8822\t Accuracy 0.6929\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.8851\t Accuracy 0.6917\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.8852\t Accuracy 0.6926\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.8864\t Accuracy 0.6924\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.8868\t Accuracy 0.6932\n",
      "\n",
      "Epoch [14]\t Average training loss 1.8869\t Average training accuracy 0.6933\n",
      "Epoch [14]\t Average validation loss 1.8709\t Average validation accuracy 0.7342\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.8573\t Accuracy 0.7600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.8805\t Accuracy 0.7037\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.8810\t Accuracy 0.6994\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.8838\t Accuracy 0.6942\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.8861\t Accuracy 0.6912\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.8860\t Accuracy 0.6907\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.8856\t Accuracy 0.6904\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.8884\t Accuracy 0.6892\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.8886\t Accuracy 0.6900\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.8898\t Accuracy 0.6899\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.8902\t Accuracy 0.6905\n",
      "\n",
      "Epoch [15]\t Average training loss 1.8903\t Average training accuracy 0.6907\n",
      "Epoch [15]\t Average validation loss 1.8748\t Average validation accuracy 0.7324\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.8614\t Accuracy 0.7500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.8843\t Accuracy 0.7006\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.8848\t Accuracy 0.6963\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.8877\t Accuracy 0.6917\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.8900\t Accuracy 0.6887\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.8899\t Accuracy 0.6884\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.8895\t Accuracy 0.6880\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.8923\t Accuracy 0.6866\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.8925\t Accuracy 0.6875\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.8937\t Accuracy 0.6876\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.8941\t Accuracy 0.6879\n",
      "\n",
      "Epoch [16]\t Average training loss 1.8942\t Average training accuracy 0.6881\n",
      "Epoch [16]\t Average validation loss 1.8792\t Average validation accuracy 0.7272\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.8660\t Accuracy 0.7400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.8887\t Accuracy 0.6971\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.8891\t Accuracy 0.6932\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.8919\t Accuracy 0.6882\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.8943\t Accuracy 0.6852\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.8942\t Accuracy 0.6852\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.8938\t Accuracy 0.6846\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.8966\t Accuracy 0.6830\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.8968\t Accuracy 0.6836\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.8980\t Accuracy 0.6837\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.8984\t Accuracy 0.6841\n",
      "\n",
      "Epoch [17]\t Average training loss 1.8985\t Average training accuracy 0.6844\n",
      "Epoch [17]\t Average validation loss 1.8840\t Average validation accuracy 0.7226\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.8708\t Accuracy 0.7400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.8933\t Accuracy 0.6931\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.8937\t Accuracy 0.6903\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.8965\t Accuracy 0.6852\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.8988\t Accuracy 0.6820\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.8988\t Accuracy 0.6821\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.8984\t Accuracy 0.6814\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.9012\t Accuracy 0.6798\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.9013\t Accuracy 0.6804\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.9025\t Accuracy 0.6805\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.9029\t Accuracy 0.6807\n",
      "\n",
      "Epoch [18]\t Average training loss 1.9031\t Average training accuracy 0.6809\n",
      "Epoch [18]\t Average validation loss 1.8889\t Average validation accuracy 0.7178\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.8758\t Accuracy 0.7400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.8981\t Accuracy 0.6865\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.8984\t Accuracy 0.6852\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.9012\t Accuracy 0.6799\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.9035\t Accuracy 0.6774\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.9035\t Accuracy 0.6775\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.9031\t Accuracy 0.6769\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.9059\t Accuracy 0.6752\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.9060\t Accuracy 0.6757\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.9072\t Accuracy 0.6758\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.9076\t Accuracy 0.6758\n",
      "\n",
      "Epoch [19]\t Average training loss 1.9078\t Average training accuracy 0.6761\n",
      "Epoch [19]\t Average validation loss 1.8940\t Average validation accuracy 0.7138\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4757\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.3704\t Accuracy 0.1147\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3145\t Accuracy 0.1377\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2676\t Accuracy 0.1623\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2260\t Accuracy 0.1864\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.1869\t Accuracy 0.2080\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1499\t Accuracy 0.2290\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1183\t Accuracy 0.2476\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.0837\t Accuracy 0.2692\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0534\t Accuracy 0.2888\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0241\t Accuracy 0.3075\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9964\t Average training accuracy 0.3252\n",
      "Epoch [0]\t Average validation loss 1.6608\t Average validation accuracy 0.5400\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6846\t Accuracy 0.5400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.6559\t Accuracy 0.5376\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6379\t Accuracy 0.5515\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6259\t Accuracy 0.5528\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.6146\t Accuracy 0.5611\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.5990\t Accuracy 0.5691\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.5837\t Accuracy 0.5765\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.5742\t Accuracy 0.5823\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5577\t Accuracy 0.5902\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5449\t Accuracy 0.5958\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5312\t Accuracy 0.6019\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5174\t Average training accuracy 0.6087\n",
      "Epoch [1]\t Average validation loss 1.3178\t Average validation accuracy 0.7058\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.3465\t Accuracy 0.6800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3367\t Accuracy 0.6818\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3278\t Accuracy 0.6872\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.3258\t Accuracy 0.6865\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3221\t Accuracy 0.6885\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3131\t Accuracy 0.6911\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.3043\t Accuracy 0.6942\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.3020\t Accuracy 0.6962\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2920\t Accuracy 0.6999\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2855\t Accuracy 0.7020\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2776\t Accuracy 0.7044\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2691\t Average training accuracy 0.7085\n",
      "Epoch [2]\t Average validation loss 1.1202\t Average validation accuracy 0.7762\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.1532\t Accuracy 0.7600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1521\t Accuracy 0.7494\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1480\t Accuracy 0.7546\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1508\t Accuracy 0.7495\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1505\t Accuracy 0.7490\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1447\t Accuracy 0.7489\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.1393\t Accuracy 0.7506\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.1405\t Accuracy 0.7513\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.1339\t Accuracy 0.7533\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.1305\t Accuracy 0.7544\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.1257\t Accuracy 0.7551\n",
      "\n",
      "Epoch [3]\t Average training loss 1.1200\t Average training accuracy 0.7575\n",
      "Epoch [3]\t Average validation loss 0.9973\t Average validation accuracy 0.8166\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.0351\t Accuracy 0.7800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.0368\t Accuracy 0.7822\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.0356\t Accuracy 0.7863\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0411\t Accuracy 0.7809\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0426\t Accuracy 0.7801\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.0386\t Accuracy 0.7804\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.0350\t Accuracy 0.7818\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.0381\t Accuracy 0.7822\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.0335\t Accuracy 0.7836\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.0319\t Accuracy 0.7841\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.0290\t Accuracy 0.7843\n",
      "\n",
      "Epoch [4]\t Average training loss 1.0249\t Average training accuracy 0.7857\n",
      "Epoch [4]\t Average validation loss 0.9178\t Average validation accuracy 0.8386\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.9590\t Accuracy 0.7900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9616\t Accuracy 0.8039\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9625\t Accuracy 0.8055\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9696\t Accuracy 0.7995\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9721\t Accuracy 0.7995\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9690\t Accuracy 0.7995\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9666\t Accuracy 0.8010\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9708\t Accuracy 0.8012\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9675\t Accuracy 0.8028\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9670\t Accuracy 0.8031\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9652\t Accuracy 0.8032\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9622\t Average training accuracy 0.8044\n",
      "Epoch [5]\t Average validation loss 0.8649\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.9084\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.9111\t Accuracy 0.8204\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.9135\t Accuracy 0.8211\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.9216\t Accuracy 0.8152\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.9245\t Accuracy 0.8157\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.9222\t Accuracy 0.8155\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.9205\t Accuracy 0.8163\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.9254\t Accuracy 0.8162\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.9229\t Accuracy 0.8174\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.9231\t Accuracy 0.8175\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.9222\t Accuracy 0.8175\n",
      "\n",
      "Epoch [6]\t Average training loss 0.9199\t Average training accuracy 0.8183\n",
      "Epoch [6]\t Average validation loss 0.8292\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8737\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8767\t Accuracy 0.8318\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8802\t Accuracy 0.8307\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8888\t Accuracy 0.8248\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8920\t Accuracy 0.8255\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8901\t Accuracy 0.8258\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8889\t Accuracy 0.8264\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8942\t Accuracy 0.8258\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8924\t Accuracy 0.8269\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8930\t Accuracy 0.8269\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8926\t Accuracy 0.8270\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8908\t Average training accuracy 0.8273\n",
      "Epoch [7]\t Average validation loss 0.8048\t Average validation accuracy 0.8704\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8498\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8531\t Accuracy 0.8384\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8572\t Accuracy 0.8371\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8662\t Accuracy 0.8317\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8696\t Accuracy 0.8329\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8679\t Accuracy 0.8331\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8671\t Accuracy 0.8343\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8727\t Accuracy 0.8334\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8713\t Accuracy 0.8341\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8722\t Accuracy 0.8339\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8722\t Accuracy 0.8338\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8707\t Average training accuracy 0.8341\n",
      "Epoch [8]\t Average validation loss 0.7881\t Average validation accuracy 0.8754\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8330\t Accuracy 0.8700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8367\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8414\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8383\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8541\t Accuracy 0.8390\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8526\t Accuracy 0.8390\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8520\t Accuracy 0.8398\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8578\t Accuracy 0.8390\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8567\t Accuracy 0.8399\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8578\t Accuracy 0.8397\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8581\t Accuracy 0.8394\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8568\t Average training accuracy 0.8396\n",
      "Epoch [9]\t Average validation loss 0.7767\t Average validation accuracy 0.8792\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8213\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8255\t Accuracy 0.8502\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8305\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8398\t Accuracy 0.8422\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8434\t Accuracy 0.8429\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8420\t Accuracy 0.8433\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8416\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8475\t Accuracy 0.8432\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8467\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8479\t Accuracy 0.8437\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8483\t Accuracy 0.8434\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8473\t Average training accuracy 0.8436\n",
      "Epoch [10]\t Average validation loss 0.7691\t Average validation accuracy 0.8812\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8133\t Accuracy 0.8700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8179\t Accuracy 0.8547\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8232\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8325\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8362\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8348\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8345\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8405\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8398\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8412\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8417\t Accuracy 0.8467\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8408\t Average training accuracy 0.8470\n",
      "Epoch [11]\t Average validation loss 0.7642\t Average validation accuracy 0.8834\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8080\t Accuracy 0.8700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8129\t Accuracy 0.8569\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8183\t Accuracy 0.8543\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8277\t Accuracy 0.8485\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8313\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8300\t Accuracy 0.8496\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8298\t Accuracy 0.8505\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8359\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8353\t Accuracy 0.8501\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8368\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8374\t Accuracy 0.8490\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8366\t Average training accuracy 0.8491\n",
      "Epoch [12]\t Average validation loss 0.7612\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8045\t Accuracy 0.8700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8098\t Accuracy 0.8582\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8153\t Accuracy 0.8560\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8246\t Accuracy 0.8499\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8283\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8270\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8269\t Accuracy 0.8522\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8330\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8325\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8340\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8347\t Accuracy 0.8506\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8340\t Average training accuracy 0.8507\n",
      "Epoch [13]\t Average validation loss 0.7595\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8021\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8081\t Accuracy 0.8604\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8136\t Accuracy 0.8577\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8229\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8266\t Accuracy 0.8524\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8253\t Accuracy 0.8529\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8252\t Accuracy 0.8536\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8313\t Accuracy 0.8525\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8309\t Accuracy 0.8532\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8325\t Accuracy 0.8527\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8332\t Accuracy 0.8524\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8325\t Average training accuracy 0.8524\n",
      "Epoch [14]\t Average validation loss 0.7588\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8009\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8073\t Accuracy 0.8618\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8129\t Accuracy 0.8587\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8221\t Accuracy 0.8530\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8258\t Accuracy 0.8537\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8245\t Accuracy 0.8540\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8244\t Accuracy 0.8545\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8305\t Accuracy 0.8534\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8302\t Accuracy 0.8541\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8318\t Accuracy 0.8537\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8325\t Accuracy 0.8535\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8319\t Average training accuracy 0.8533\n",
      "Epoch [15]\t Average validation loss 0.7589\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8004\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8073\t Accuracy 0.8622\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8129\t Accuracy 0.8594\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8220\t Accuracy 0.8544\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8257\t Accuracy 0.8546\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8244\t Accuracy 0.8551\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8243\t Accuracy 0.8554\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8305\t Accuracy 0.8543\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8302\t Accuracy 0.8548\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8318\t Accuracy 0.8545\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8325\t Accuracy 0.8543\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8320\t Average training accuracy 0.8541\n",
      "Epoch [16]\t Average validation loss 0.7596\t Average validation accuracy 0.8914\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8005\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8078\t Accuracy 0.8629\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8134\t Accuracy 0.8604\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8224\t Accuracy 0.8552\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8261\t Accuracy 0.8555\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8248\t Accuracy 0.8560\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8247\t Accuracy 0.8564\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8309\t Accuracy 0.8554\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8307\t Accuracy 0.8560\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8322\t Accuracy 0.8556\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8330\t Accuracy 0.8553\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8325\t Average training accuracy 0.8551\n",
      "Epoch [17]\t Average validation loss 0.7606\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8008\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8087\t Accuracy 0.8641\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8142\t Accuracy 0.8615\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8231\t Accuracy 0.8564\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8269\t Accuracy 0.8565\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8255\t Accuracy 0.8570\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8254\t Accuracy 0.8574\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8316\t Accuracy 0.8563\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8314\t Accuracy 0.8568\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8330\t Accuracy 0.8564\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8338\t Accuracy 0.8560\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8333\t Average training accuracy 0.8559\n",
      "Epoch [18]\t Average validation loss 0.7618\t Average validation accuracy 0.8924\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8014\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8098\t Accuracy 0.8647\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8152\t Accuracy 0.8619\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8241\t Accuracy 0.8570\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8279\t Accuracy 0.8571\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8265\t Accuracy 0.8575\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8264\t Accuracy 0.8580\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8326\t Accuracy 0.8570\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8324\t Accuracy 0.8574\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8340\t Accuracy 0.8571\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8348\t Accuracy 0.8566\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8343\t Average training accuracy 0.8566\n",
      "Epoch [19]\t Average validation loss 0.7632\t Average validation accuracy 0.8930\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 11.0893\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.1018\t Accuracy 0.0982\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.3824\t Accuracy 0.0997\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1288\t Accuracy 0.1037\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9911\t Accuracy 0.1115\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.9026\t Accuracy 0.1206\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8364\t Accuracy 0.1337\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7876\t Accuracy 0.1445\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7486\t Accuracy 0.1570\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7167\t Accuracy 0.1706\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6891\t Accuracy 0.1853\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6654\t Average training accuracy 0.2007\n",
      "Epoch [0]\t Average validation loss 0.4150\t Average validation accuracy 0.3880\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4015\t Accuracy 0.4600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4148\t Accuracy 0.3914\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4070\t Accuracy 0.4121\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4046\t Accuracy 0.4182\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4010\t Accuracy 0.4291\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3969\t Accuracy 0.4410\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3918\t Accuracy 0.4569\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3887\t Accuracy 0.4655\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3853\t Accuracy 0.4755\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3825\t Accuracy 0.4843\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3793\t Accuracy 0.4938\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3761\t Average training accuracy 0.5026\n",
      "Epoch [1]\t Average validation loss 0.3355\t Average validation accuracy 0.6302\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3228\t Accuracy 0.7300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3403\t Accuracy 0.6092\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3368\t Accuracy 0.6217\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3374\t Accuracy 0.6170\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3367\t Accuracy 0.6187\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3352\t Accuracy 0.6229\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3330\t Accuracy 0.6295\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3325\t Accuracy 0.6315\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3312\t Accuracy 0.6358\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3304\t Accuracy 0.6390\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3292\t Accuracy 0.6426\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3279\t Average training accuracy 0.6465\n",
      "Epoch [2]\t Average validation loss 0.3059\t Average validation accuracy 0.7270\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2954\t Accuracy 0.7500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3123\t Accuracy 0.6898\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3104\t Accuracy 0.6995\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3120\t Accuracy 0.6934\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3122\t Accuracy 0.6935\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3115\t Accuracy 0.6955\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3104\t Accuracy 0.6988\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3107\t Accuracy 0.6993\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3103\t Accuracy 0.7016\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3102\t Accuracy 0.7033\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3097\t Accuracy 0.7054\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3091\t Average training accuracy 0.7077\n",
      "Epoch [3]\t Average validation loss 0.2946\t Average validation accuracy 0.7676\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2861\t Accuracy 0.7900\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3015\t Accuracy 0.7308\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3004\t Accuracy 0.7377\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3022\t Accuracy 0.7311\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3028\t Accuracy 0.7300\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3026\t Accuracy 0.7311\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3019\t Accuracy 0.7332\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3027\t Accuracy 0.7329\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3026\t Accuracy 0.7345\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3029\t Accuracy 0.7354\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3028\t Accuracy 0.7372\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3025\t Average training accuracy 0.7390\n",
      "Epoch [4]\t Average validation loss 0.2919\t Average validation accuracy 0.7980\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2849\t Accuracy 0.8000\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2988\t Accuracy 0.7490\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2982\t Accuracy 0.7554\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3001\t Accuracy 0.7496\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3009\t Accuracy 0.7489\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3008\t Accuracy 0.7498\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3005\t Accuracy 0.7514\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3015\t Accuracy 0.7509\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3016\t Accuracy 0.7523\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3020\t Accuracy 0.7528\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3021\t Accuracy 0.7541\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3021\t Average training accuracy 0.7555\n",
      "Epoch [5]\t Average validation loss 0.2939\t Average validation accuracy 0.8130\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2881\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3005\t Accuracy 0.7637\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3002\t Accuracy 0.7678\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3021\t Accuracy 0.7619\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3030\t Accuracy 0.7611\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3030\t Accuracy 0.7621\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3029\t Accuracy 0.7628\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3040\t Accuracy 0.7626\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3042\t Accuracy 0.7635\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3047\t Accuracy 0.7638\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3050\t Accuracy 0.7648\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3050\t Average training accuracy 0.7657\n",
      "Epoch [6]\t Average validation loss 0.2985\t Average validation accuracy 0.8194\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2936\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.3048\t Accuracy 0.7692\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.3047\t Accuracy 0.7715\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3065\t Accuracy 0.7658\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3074\t Accuracy 0.7662\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3075\t Accuracy 0.7668\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3075\t Accuracy 0.7673\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3086\t Accuracy 0.7670\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3089\t Accuracy 0.7679\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3095\t Accuracy 0.7684\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3098\t Accuracy 0.7690\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3099\t Average training accuracy 0.7699\n",
      "Epoch [7]\t Average validation loss 0.3046\t Average validation accuracy 0.8210\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.3004\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.3105\t Accuracy 0.7741\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.3105\t Accuracy 0.7745\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.3122\t Accuracy 0.7685\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.3132\t Accuracy 0.7690\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.3134\t Accuracy 0.7694\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.3134\t Accuracy 0.7702\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.3145\t Accuracy 0.7696\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.3148\t Accuracy 0.7702\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.3154\t Accuracy 0.7705\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.3158\t Accuracy 0.7709\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3160\t Average training accuracy 0.7718\n",
      "Epoch [8]\t Average validation loss 0.3115\t Average validation accuracy 0.8218\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.3078\t Accuracy 0.8200\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.3170\t Accuracy 0.7737\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.3171\t Accuracy 0.7738\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.3187\t Accuracy 0.7679\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.3196\t Accuracy 0.7684\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.3199\t Accuracy 0.7688\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.3199\t Accuracy 0.7699\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.3210\t Accuracy 0.7691\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.3214\t Accuracy 0.7700\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.3220\t Accuracy 0.7700\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.3223\t Accuracy 0.7703\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3225\t Average training accuracy 0.7710\n",
      "Epoch [9]\t Average validation loss 0.3188\t Average validation accuracy 0.8212\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.3154\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.3238\t Accuracy 0.7720\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.3240\t Accuracy 0.7709\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.3255\t Accuracy 0.7650\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.3264\t Accuracy 0.7655\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.3266\t Accuracy 0.7664\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.3267\t Accuracy 0.7673\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.3278\t Accuracy 0.7667\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.3281\t Accuracy 0.7675\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.3287\t Accuracy 0.7678\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.3291\t Accuracy 0.7680\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3293\t Average training accuracy 0.7688\n",
      "Epoch [10]\t Average validation loss 0.3261\t Average validation accuracy 0.8160\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.3230\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.3307\t Accuracy 0.7675\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.3309\t Accuracy 0.7673\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.3323\t Accuracy 0.7610\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.3332\t Accuracy 0.7610\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.3334\t Accuracy 0.7619\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.3335\t Accuracy 0.7631\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.3345\t Accuracy 0.7626\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.3348\t Accuracy 0.7635\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.3354\t Accuracy 0.7637\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.3358\t Accuracy 0.7639\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3360\t Average training accuracy 0.7645\n",
      "Epoch [11]\t Average validation loss 0.3331\t Average validation accuracy 0.8134\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.3303\t Accuracy 0.8200\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.3374\t Accuracy 0.7645\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.3376\t Accuracy 0.7625\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.3389\t Accuracy 0.7557\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.3397\t Accuracy 0.7559\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.3399\t Accuracy 0.7568\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.3400\t Accuracy 0.7580\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.3410\t Accuracy 0.7576\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.3413\t Accuracy 0.7582\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.3419\t Accuracy 0.7584\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.3422\t Accuracy 0.7584\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3424\t Average training accuracy 0.7589\n",
      "Epoch [12]\t Average validation loss 0.3399\t Average validation accuracy 0.8050\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.3372\t Accuracy 0.8200\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.3438\t Accuracy 0.7618\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.3440\t Accuracy 0.7564\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.3452\t Accuracy 0.7510\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.3460\t Accuracy 0.7511\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.3462\t Accuracy 0.7516\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.3463\t Accuracy 0.7526\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.3472\t Accuracy 0.7523\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.3475\t Accuracy 0.7529\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.3480\t Accuracy 0.7529\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.3483\t Accuracy 0.7529\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3485\t Average training accuracy 0.7533\n",
      "Epoch [13]\t Average validation loss 0.3462\t Average validation accuracy 0.7964\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.3437\t Accuracy 0.8200\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.3498\t Accuracy 0.7525\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.3500\t Accuracy 0.7480\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.3511\t Accuracy 0.7429\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.3519\t Accuracy 0.7436\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.3520\t Accuracy 0.7439\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.3521\t Accuracy 0.7446\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.3530\t Accuracy 0.7441\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.3533\t Accuracy 0.7450\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.3537\t Accuracy 0.7447\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.3540\t Accuracy 0.7447\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3542\t Average training accuracy 0.7452\n",
      "Epoch [14]\t Average validation loss 0.3521\t Average validation accuracy 0.7890\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.3497\t Accuracy 0.8100\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.3554\t Accuracy 0.7416\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.3556\t Accuracy 0.7395\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.3566\t Accuracy 0.7345\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.3573\t Accuracy 0.7352\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.3575\t Accuracy 0.7354\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.3575\t Accuracy 0.7354\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3584\t Accuracy 0.7349\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3586\t Accuracy 0.7361\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3591\t Accuracy 0.7355\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3593\t Accuracy 0.7355\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3595\t Average training accuracy 0.7362\n",
      "Epoch [15]\t Average validation loss 0.3575\t Average validation accuracy 0.7812\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.3552\t Accuracy 0.8000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.3605\t Accuracy 0.7325\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.3607\t Accuracy 0.7298\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3616\t Accuracy 0.7254\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3623\t Accuracy 0.7260\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3624\t Accuracy 0.7268\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3625\t Accuracy 0.7266\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3633\t Accuracy 0.7262\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3635\t Accuracy 0.7275\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3639\t Accuracy 0.7264\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3642\t Accuracy 0.7263\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3644\t Average training accuracy 0.7269\n",
      "Epoch [16]\t Average validation loss 0.3625\t Average validation accuracy 0.7702\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.3602\t Accuracy 0.7800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3652\t Accuracy 0.7190\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3654\t Accuracy 0.7169\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3662\t Accuracy 0.7137\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3669\t Accuracy 0.7144\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3670\t Accuracy 0.7158\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3671\t Accuracy 0.7155\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3678\t Accuracy 0.7150\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3680\t Accuracy 0.7164\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3684\t Accuracy 0.7155\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3686\t Accuracy 0.7154\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3688\t Average training accuracy 0.7160\n",
      "Epoch [17]\t Average validation loss 0.3670\t Average validation accuracy 0.7600\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.3647\t Accuracy 0.7600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3695\t Accuracy 0.7071\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3696\t Accuracy 0.7061\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3704\t Accuracy 0.7032\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3710\t Accuracy 0.7038\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3712\t Accuracy 0.7051\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3712\t Accuracy 0.7048\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3719\t Accuracy 0.7042\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3721\t Accuracy 0.7057\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3724\t Accuracy 0.7046\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3726\t Accuracy 0.7044\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3728\t Average training accuracy 0.7051\n",
      "Epoch [18]\t Average validation loss 0.3710\t Average validation accuracy 0.7480\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.3688\t Accuracy 0.7400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3733\t Accuracy 0.6955\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3735\t Accuracy 0.6928\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3742\t Accuracy 0.6907\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3748\t Accuracy 0.6913\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3749\t Accuracy 0.6929\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3749\t Accuracy 0.6927\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3756\t Accuracy 0.6917\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3758\t Accuracy 0.6935\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3761\t Accuracy 0.6926\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3763\t Accuracy 0.6923\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3764\t Average training accuracy 0.6933\n",
      "Epoch [19]\t Average validation loss 0.3747\t Average validation accuracy 0.7362\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5557\t Accuracy 0.2400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.5578\t Accuracy 0.1859\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.2917\t Accuracy 0.2109\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1475\t Accuracy 0.2305\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.0481\t Accuracy 0.2512\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.9708\t Accuracy 0.2720\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.9099\t Accuracy 0.2919\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8603\t Accuracy 0.3097\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8184\t Accuracy 0.3278\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7821\t Accuracy 0.3439\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7513\t Accuracy 0.3580\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7235\t Average training accuracy 0.3727\n",
      "Epoch [0]\t Average validation loss 0.4203\t Average validation accuracy 0.5560\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4127\t Accuracy 0.5600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4152\t Accuracy 0.5692\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4062\t Accuracy 0.5758\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4030\t Accuracy 0.5737\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3986\t Accuracy 0.5765\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3924\t Accuracy 0.5847\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3867\t Accuracy 0.5919\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3820\t Accuracy 0.5968\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3769\t Accuracy 0.6034\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3721\t Accuracy 0.6099\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3678\t Accuracy 0.6162\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3630\t Average training accuracy 0.6232\n",
      "Epoch [1]\t Average validation loss 0.3000\t Average validation accuracy 0.7240\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2992\t Accuracy 0.7700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3040\t Accuracy 0.7143\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3013\t Accuracy 0.7167\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3021\t Accuracy 0.7103\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3013\t Accuracy 0.7103\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2991\t Accuracy 0.7138\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2970\t Accuracy 0.7166\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2957\t Accuracy 0.7184\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2937\t Accuracy 0.7211\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2920\t Accuracy 0.7238\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2903\t Accuracy 0.7264\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2881\t Average training accuracy 0.7300\n",
      "Epoch [2]\t Average validation loss 0.2525\t Average validation accuracy 0.8008\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2516\t Accuracy 0.8200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2592\t Accuracy 0.7776\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2583\t Accuracy 0.7787\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2600\t Accuracy 0.7730\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2601\t Accuracy 0.7717\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2590\t Accuracy 0.7728\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2582\t Accuracy 0.7735\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2579\t Accuracy 0.7744\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2569\t Accuracy 0.7756\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2562\t Accuracy 0.7775\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2554\t Accuracy 0.7792\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2541\t Average training accuracy 0.7813\n",
      "Epoch [3]\t Average validation loss 0.2279\t Average validation accuracy 0.8410\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2262\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2359\t Accuracy 0.8110\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2356\t Accuracy 0.8117\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2379\t Accuracy 0.8055\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2383\t Accuracy 0.8042\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2377\t Accuracy 0.8047\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2373\t Accuracy 0.8054\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2376\t Accuracy 0.8056\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2371\t Accuracy 0.8069\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2368\t Accuracy 0.8080\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2365\t Accuracy 0.8092\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2356\t Average training accuracy 0.8104\n",
      "Epoch [4]\t Average validation loss 0.2140\t Average validation accuracy 0.8612\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2123\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2226\t Accuracy 0.8298\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2227\t Accuracy 0.8301\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2251\t Accuracy 0.8246\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2257\t Accuracy 0.8230\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2254\t Accuracy 0.8239\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2253\t Accuracy 0.8247\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2260\t Accuracy 0.8240\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2256\t Accuracy 0.8250\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2256\t Accuracy 0.8256\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2255\t Accuracy 0.8260\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2249\t Average training accuracy 0.8269\n",
      "Epoch [5]\t Average validation loss 0.2061\t Average validation accuracy 0.8754\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2047\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2152\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2154\t Accuracy 0.8430\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2179\t Accuracy 0.8366\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2186\t Accuracy 0.8350\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2185\t Accuracy 0.8351\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2185\t Accuracy 0.8353\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2194\t Accuracy 0.8345\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2192\t Accuracy 0.8355\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2194\t Accuracy 0.8358\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2195\t Accuracy 0.8359\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2190\t Average training accuracy 0.8365\n",
      "Epoch [6]\t Average validation loss 0.2019\t Average validation accuracy 0.8816\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2006\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2114\t Accuracy 0.8531\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2117\t Accuracy 0.8499\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2142\t Accuracy 0.8432\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2150\t Accuracy 0.8414\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2149\t Accuracy 0.8411\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2151\t Accuracy 0.8416\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2161\t Accuracy 0.8407\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2160\t Accuracy 0.8415\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2163\t Accuracy 0.8419\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2165\t Accuracy 0.8420\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2162\t Average training accuracy 0.8422\n",
      "Epoch [7]\t Average validation loss 0.2003\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1991\t Accuracy 0.8700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2099\t Accuracy 0.8584\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2103\t Accuracy 0.8547\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2129\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2137\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2137\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2138\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2150\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2149\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2153\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2156\t Accuracy 0.8465\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2153\t Average training accuracy 0.8465\n",
      "Epoch [8]\t Average validation loss 0.2003\t Average validation accuracy 0.8866\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1991\t Accuracy 0.8800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2101\t Accuracy 0.8559\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2105\t Accuracy 0.8540\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2131\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2139\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2139\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2141\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2152\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2152\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2157\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2160\t Accuracy 0.8471\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2157\t Average training accuracy 0.8471\n",
      "Epoch [9]\t Average validation loss 0.2013\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1998\t Accuracy 0.8900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2111\t Accuracy 0.8559\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2115\t Accuracy 0.8541\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2141\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2149\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2149\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2151\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2163\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2164\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2168\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2171\t Accuracy 0.8467\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2169\t Average training accuracy 0.8469\n",
      "Epoch [10]\t Average validation loss 0.2028\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2011\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2126\t Accuracy 0.8549\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2130\t Accuracy 0.8532\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2156\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2164\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2164\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2166\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2178\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2179\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2184\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2186\t Accuracy 0.8458\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2185\t Average training accuracy 0.8460\n",
      "Epoch [11]\t Average validation loss 0.2045\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2023\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2142\t Accuracy 0.8545\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2147\t Accuracy 0.8529\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2172\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2180\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2180\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2182\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2195\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2195\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2200\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2203\t Accuracy 0.8444\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2201\t Average training accuracy 0.8447\n",
      "Epoch [12]\t Average validation loss 0.2063\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2038\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2159\t Accuracy 0.8537\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2164\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2189\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2197\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2197\t Accuracy 0.8422\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2199\t Accuracy 0.8422\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2211\t Accuracy 0.8415\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2212\t Accuracy 0.8422\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2217\t Accuracy 0.8423\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2220\t Accuracy 0.8420\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2218\t Average training accuracy 0.8424\n",
      "Epoch [13]\t Average validation loss 0.2080\t Average validation accuracy 0.8828\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2053\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2176\t Accuracy 0.8520\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2181\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2206\t Accuracy 0.8418\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2214\t Accuracy 0.8412\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2213\t Accuracy 0.8399\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2216\t Accuracy 0.8400\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2228\t Accuracy 0.8394\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2228\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2233\t Accuracy 0.8403\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2236\t Accuracy 0.8403\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2234\t Average training accuracy 0.8406\n",
      "Epoch [14]\t Average validation loss 0.2098\t Average validation accuracy 0.8808\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2068\t Accuracy 0.8700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2192\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2197\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2222\t Accuracy 0.8398\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2229\t Accuracy 0.8392\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2229\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2231\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2243\t Accuracy 0.8373\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2243\t Accuracy 0.8379\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2248\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2251\t Accuracy 0.8382\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2249\t Average training accuracy 0.8384\n",
      "Epoch [15]\t Average validation loss 0.2113\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2082\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2207\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2212\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2236\t Accuracy 0.8378\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2243\t Accuracy 0.8372\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2242\t Accuracy 0.8363\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2244\t Accuracy 0.8361\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2256\t Accuracy 0.8354\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2257\t Accuracy 0.8360\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2261\t Accuracy 0.8365\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2264\t Accuracy 0.8364\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2262\t Average training accuracy 0.8365\n",
      "Epoch [16]\t Average validation loss 0.2125\t Average validation accuracy 0.8760\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2093\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2219\t Accuracy 0.8441\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2224\t Accuracy 0.8425\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2247\t Accuracy 0.8357\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2255\t Accuracy 0.8349\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2254\t Accuracy 0.8341\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2256\t Accuracy 0.8338\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2268\t Accuracy 0.8332\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2268\t Accuracy 0.8338\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2273\t Accuracy 0.8343\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2275\t Accuracy 0.8343\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2273\t Average training accuracy 0.8344\n",
      "Epoch [17]\t Average validation loss 0.2136\t Average validation accuracy 0.8736\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2102\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2229\t Accuracy 0.8410\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2234\t Accuracy 0.8399\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2257\t Accuracy 0.8336\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2265\t Accuracy 0.8325\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2263\t Accuracy 0.8318\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2265\t Accuracy 0.8314\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2277\t Accuracy 0.8310\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2277\t Accuracy 0.8317\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2282\t Accuracy 0.8322\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2284\t Accuracy 0.8323\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2282\t Average training accuracy 0.8326\n",
      "Epoch [18]\t Average validation loss 0.2145\t Average validation accuracy 0.8720\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2109\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2238\t Accuracy 0.8398\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2242\t Accuracy 0.8386\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2265\t Accuracy 0.8321\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2272\t Accuracy 0.8313\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2271\t Accuracy 0.8304\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2273\t Accuracy 0.8301\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2285\t Accuracy 0.8295\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2285\t Accuracy 0.8302\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2289\t Accuracy 0.8306\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2291\t Accuracy 0.8308\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2289\t Average training accuracy 0.8311\n",
      "Epoch [19]\t Average validation loss 0.2152\t Average validation accuracy 0.8710\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.8747\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6632\t Accuracy 0.0620\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.6007\t Accuracy 0.0697\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.5399\t Accuracy 0.0846\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.4908\t Accuracy 0.0911\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.4491\t Accuracy 0.1047\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.4137\t Accuracy 0.1203\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3849\t Accuracy 0.1388\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3596\t Accuracy 0.1569\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.3383\t Accuracy 0.1749\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3195\t Accuracy 0.1914\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3030\t Average training accuracy 0.2077\n",
      "Epoch [0]\t Average validation loss 2.1151\t Average validation accuracy 0.4110\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1103\t Accuracy 0.4300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.1072\t Accuracy 0.4327\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0988\t Accuracy 0.4408\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0936\t Accuracy 0.4515\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.0887\t Accuracy 0.4580\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.0825\t Accuracy 0.4678\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.0766\t Accuracy 0.4750\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.0728\t Accuracy 0.4817\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.0671\t Accuracy 0.4894\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.0628\t Accuracy 0.4957\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.0581\t Accuracy 0.5026\n",
      "\n",
      "Epoch [1]\t Average training loss 2.0535\t Average training accuracy 0.5090\n",
      "Epoch [1]\t Average validation loss 1.9859\t Average validation accuracy 0.6162\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.9784\t Accuracy 0.6500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.9878\t Accuracy 0.6067\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.9826\t Accuracy 0.6085\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.9812\t Accuracy 0.6062\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.9798\t Accuracy 0.6051\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.9764\t Accuracy 0.6070\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.9731\t Accuracy 0.6079\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.9725\t Accuracy 0.6094\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.9693\t Accuracy 0.6129\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.9676\t Accuracy 0.6153\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.9653\t Accuracy 0.6181\n",
      "\n",
      "Epoch [2]\t Average training loss 1.9628\t Average training accuracy 0.6210\n",
      "Epoch [2]\t Average validation loss 1.9162\t Average validation accuracy 0.6900\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.9077\t Accuracy 0.7300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.9234\t Accuracy 0.6625\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.9202\t Accuracy 0.6644\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.9210\t Accuracy 0.6604\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.9215\t Accuracy 0.6581\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.9196\t Accuracy 0.6583\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.9178\t Accuracy 0.6573\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.9190\t Accuracy 0.6568\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.9173\t Accuracy 0.6596\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.9170\t Accuracy 0.6607\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.9160\t Accuracy 0.6620\n",
      "\n",
      "Epoch [3]\t Average training loss 1.9147\t Average training accuracy 0.6634\n",
      "Epoch [3]\t Average validation loss 1.8806\t Average validation accuracy 0.7192\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.8713\t Accuracy 0.7400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.8907\t Accuracy 0.6822\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.8887\t Accuracy 0.6841\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.8906\t Accuracy 0.6805\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.8923\t Accuracy 0.6781\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.8913\t Accuracy 0.6774\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.8903\t Accuracy 0.6764\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.8926\t Accuracy 0.6754\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.8917\t Accuracy 0.6779\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.8923\t Accuracy 0.6784\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.8920\t Accuracy 0.6792\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8915\t Average training accuracy 0.6800\n",
      "Epoch [4]\t Average validation loss 1.8650\t Average validation accuracy 0.7328\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.8551\t Accuracy 0.7500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.8766\t Accuracy 0.6939\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.8753\t Accuracy 0.6938\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.8779\t Accuracy 0.6897\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.8801\t Accuracy 0.6869\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.8797\t Accuracy 0.6863\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.8792\t Accuracy 0.6856\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.8820\t Accuracy 0.6843\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.8816\t Accuracy 0.6864\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.8827\t Accuracy 0.6863\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.8829\t Accuracy 0.6867\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8828\t Average training accuracy 0.6870\n",
      "Epoch [5]\t Average validation loss 1.8613\t Average validation accuracy 0.7368\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8509\t Accuracy 0.7400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.8734\t Accuracy 0.6945\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.8726\t Accuracy 0.6945\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.8755\t Accuracy 0.6895\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.8781\t Accuracy 0.6871\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.8780\t Accuracy 0.6866\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.8778\t Accuracy 0.6855\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.8809\t Accuracy 0.6842\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.8808\t Accuracy 0.6865\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.8821\t Accuracy 0.6865\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.8826\t Accuracy 0.6867\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8827\t Average training accuracy 0.6872\n",
      "Epoch [6]\t Average validation loss 1.8645\t Average validation accuracy 0.7370\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8536\t Accuracy 0.7400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.8765\t Accuracy 0.6941\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.8761\t Accuracy 0.6936\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.8791\t Accuracy 0.6883\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.8819\t Accuracy 0.6851\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.8820\t Accuracy 0.6846\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.8819\t Accuracy 0.6832\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.8851\t Accuracy 0.6817\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.8852\t Accuracy 0.6838\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.8867\t Accuracy 0.6835\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.8873\t Accuracy 0.6836\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8876\t Average training accuracy 0.6842\n",
      "Epoch [7]\t Average validation loss 1.8716\t Average validation accuracy 0.7304\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8603\t Accuracy 0.7300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8833\t Accuracy 0.6896\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8830\t Accuracy 0.6883\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8861\t Accuracy 0.6832\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8890\t Accuracy 0.6805\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8892\t Accuracy 0.6805\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8892\t Accuracy 0.6787\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8924\t Accuracy 0.6768\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8926\t Accuracy 0.6788\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8941\t Accuracy 0.6783\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8948\t Accuracy 0.6785\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8952\t Average training accuracy 0.6792\n",
      "Epoch [8]\t Average validation loss 1.8807\t Average validation accuracy 0.7254\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8690\t Accuracy 0.7300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8919\t Accuracy 0.6833\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8918\t Accuracy 0.6820\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8948\t Accuracy 0.6772\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8977\t Accuracy 0.6739\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8980\t Accuracy 0.6741\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8980\t Accuracy 0.6726\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.9011\t Accuracy 0.6708\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.9014\t Accuracy 0.6728\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.9029\t Accuracy 0.6723\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.9036\t Accuracy 0.6726\n",
      "\n",
      "Epoch [9]\t Average training loss 1.9040\t Average training accuracy 0.6732\n",
      "Epoch [9]\t Average validation loss 1.8906\t Average validation accuracy 0.7160\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8787\t Accuracy 0.7400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.9013\t Accuracy 0.6757\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.9012\t Accuracy 0.6739\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.9042\t Accuracy 0.6696\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.9071\t Accuracy 0.6669\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.9074\t Accuracy 0.6671\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.9074\t Accuracy 0.6652\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.9105\t Accuracy 0.6636\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.9108\t Accuracy 0.6654\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.9123\t Accuracy 0.6648\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.9129\t Accuracy 0.6651\n",
      "\n",
      "Epoch [10]\t Average training loss 1.9134\t Average training accuracy 0.6655\n",
      "Epoch [10]\t Average validation loss 1.9008\t Average validation accuracy 0.7060\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8886\t Accuracy 0.7400\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.9108\t Accuracy 0.6669\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.9108\t Accuracy 0.6668\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.9137\t Accuracy 0.6617\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.9166\t Accuracy 0.6589\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.9169\t Accuracy 0.6590\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.9169\t Accuracy 0.6571\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.9198\t Accuracy 0.6554\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.9201\t Accuracy 0.6568\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.9216\t Accuracy 0.6563\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.9222\t Accuracy 0.6566\n",
      "\n",
      "Epoch [11]\t Average training loss 1.9227\t Average training accuracy 0.6568\n",
      "Epoch [11]\t Average validation loss 1.9107\t Average validation accuracy 0.6974\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8983\t Accuracy 0.7200\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.9201\t Accuracy 0.6602\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.9201\t Accuracy 0.6585\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.9229\t Accuracy 0.6517\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.9258\t Accuracy 0.6484\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.9260\t Accuracy 0.6490\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.9260\t Accuracy 0.6473\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.9289\t Accuracy 0.6456\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.9292\t Accuracy 0.6468\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.9306\t Accuracy 0.6463\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.9312\t Accuracy 0.6467\n",
      "\n",
      "Epoch [12]\t Average training loss 1.9316\t Average training accuracy 0.6470\n",
      "Epoch [12]\t Average validation loss 1.9201\t Average validation accuracy 0.6874\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.9075\t Accuracy 0.7100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.9290\t Accuracy 0.6490\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.9290\t Accuracy 0.6478\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.9317\t Accuracy 0.6402\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.9345\t Accuracy 0.6374\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.9347\t Accuracy 0.6386\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.9347\t Accuracy 0.6371\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.9375\t Accuracy 0.6356\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.9377\t Accuracy 0.6363\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.9391\t Accuracy 0.6358\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.9397\t Accuracy 0.6363\n",
      "\n",
      "Epoch [13]\t Average training loss 1.9401\t Average training accuracy 0.6364\n",
      "Epoch [13]\t Average validation loss 1.9290\t Average validation accuracy 0.6754\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.9161\t Accuracy 0.6700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.9372\t Accuracy 0.6394\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.9373\t Accuracy 0.6369\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.9399\t Accuracy 0.6297\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.9426\t Accuracy 0.6269\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.9429\t Accuracy 0.6284\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.9428\t Accuracy 0.6270\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.9455\t Accuracy 0.6254\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.9457\t Accuracy 0.6257\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.9470\t Accuracy 0.6251\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.9476\t Accuracy 0.6253\n",
      "\n",
      "Epoch [14]\t Average training loss 1.9480\t Average training accuracy 0.6254\n",
      "Epoch [14]\t Average validation loss 1.9372\t Average validation accuracy 0.6640\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.9241\t Accuracy 0.6600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.9449\t Accuracy 0.6278\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.9450\t Accuracy 0.6249\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.9475\t Accuracy 0.6177\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.9502\t Accuracy 0.6153\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.9504\t Accuracy 0.6167\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.9503\t Accuracy 0.6156\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.9529\t Accuracy 0.6140\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.9531\t Accuracy 0.6143\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.9544\t Accuracy 0.6139\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.9549\t Accuracy 0.6142\n",
      "\n",
      "Epoch [15]\t Average training loss 1.9553\t Average training accuracy 0.6141\n",
      "Epoch [15]\t Average validation loss 1.9447\t Average validation accuracy 0.6524\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.9315\t Accuracy 0.6600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.9520\t Accuracy 0.6208\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.9520\t Accuracy 0.6146\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.9545\t Accuracy 0.6070\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.9571\t Accuracy 0.6041\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.9573\t Accuracy 0.6055\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.9572\t Accuracy 0.6048\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.9597\t Accuracy 0.6029\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.9599\t Accuracy 0.6031\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.9611\t Accuracy 0.6025\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.9616\t Accuracy 0.6026\n",
      "\n",
      "Epoch [16]\t Average training loss 1.9620\t Average training accuracy 0.6026\n",
      "Epoch [16]\t Average validation loss 1.9516\t Average validation accuracy 0.6378\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.9383\t Accuracy 0.6500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.9585\t Accuracy 0.6098\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.9585\t Accuracy 0.6027\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.9609\t Accuracy 0.5952\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.9635\t Accuracy 0.5927\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.9637\t Accuracy 0.5945\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.9635\t Accuracy 0.5934\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.9659\t Accuracy 0.5916\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.9661\t Accuracy 0.5920\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.9673\t Accuracy 0.5916\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.9678\t Accuracy 0.5917\n",
      "\n",
      "Epoch [17]\t Average training loss 1.9681\t Average training accuracy 0.5916\n",
      "Epoch [17]\t Average validation loss 1.9580\t Average validation accuracy 0.6252\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.9444\t Accuracy 0.6500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.9644\t Accuracy 0.5994\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.9644\t Accuracy 0.5913\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.9667\t Accuracy 0.5840\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.9693\t Accuracy 0.5811\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.9695\t Accuracy 0.5833\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.9693\t Accuracy 0.5824\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.9716\t Accuracy 0.5806\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.9718\t Accuracy 0.5809\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.9729\t Accuracy 0.5807\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.9734\t Accuracy 0.5808\n",
      "\n",
      "Epoch [18]\t Average training loss 1.9737\t Average training accuracy 0.5806\n",
      "Epoch [18]\t Average validation loss 1.9637\t Average validation accuracy 0.6148\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.9501\t Accuracy 0.6500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.9698\t Accuracy 0.5937\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.9698\t Accuracy 0.5843\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.9720\t Accuracy 0.5754\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.9746\t Accuracy 0.5716\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.9747\t Accuracy 0.5733\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.9746\t Accuracy 0.5727\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.9768\t Accuracy 0.5707\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.9770\t Accuracy 0.5712\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.9781\t Accuracy 0.5706\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.9785\t Accuracy 0.5706\n",
      "\n",
      "Epoch [19]\t Average training loss 1.9788\t Average training accuracy 0.5701\n",
      "Epoch [19]\t Average validation loss 1.9690\t Average validation accuracy 0.6052\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7457\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5057\t Accuracy 0.0704\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3816\t Accuracy 0.1050\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2851\t Accuracy 0.1546\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2081\t Accuracy 0.1969\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.1378\t Accuracy 0.2384\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.0724\t Accuracy 0.2780\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.0193\t Accuracy 0.3111\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.9667\t Accuracy 0.3430\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.9187\t Accuracy 0.3726\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.8744\t Accuracy 0.3982\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8326\t Average training accuracy 0.4223\n",
      "Epoch [0]\t Average validation loss 1.3301\t Average validation accuracy 0.7020\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.3776\t Accuracy 0.6700\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.3503\t Accuracy 0.6888\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.3228\t Accuracy 0.7033\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.3066\t Accuracy 0.7061\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.2918\t Accuracy 0.7119\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.2735\t Accuracy 0.7174\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.2537\t Accuracy 0.7254\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.2443\t Accuracy 0.7285\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.2286\t Accuracy 0.7339\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.2152\t Accuracy 0.7394\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.2019\t Accuracy 0.7433\n",
      "\n",
      "Epoch [1]\t Average training loss 1.1879\t Average training accuracy 0.7481\n",
      "Epoch [1]\t Average validation loss 0.9727\t Average validation accuracy 0.8346\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.0296\t Accuracy 0.7900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.0194\t Accuracy 0.8006\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.0112\t Accuracy 0.8015\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.0093\t Accuracy 0.8007\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.0056\t Accuracy 0.8024\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.9977\t Accuracy 0.8041\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.9887\t Accuracy 0.8071\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.9896\t Accuracy 0.8063\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.9832\t Accuracy 0.8082\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.9788\t Accuracy 0.8097\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.9740\t Accuracy 0.8101\n",
      "\n",
      "Epoch [2]\t Average training loss 0.9679\t Average training accuracy 0.8115\n",
      "Epoch [2]\t Average validation loss 0.8343\t Average validation accuracy 0.8680\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8931\t Accuracy 0.8200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8882\t Accuracy 0.8306\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8873\t Accuracy 0.8298\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8905\t Accuracy 0.8276\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8906\t Accuracy 0.8286\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8865\t Accuracy 0.8297\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8818\t Accuracy 0.8322\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8861\t Accuracy 0.8312\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8832\t Accuracy 0.8322\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8820\t Accuracy 0.8334\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8804\t Accuracy 0.8332\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8772\t Average training accuracy 0.8338\n",
      "Epoch [3]\t Average validation loss 0.7748\t Average validation accuracy 0.8812\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8344\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8303\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8325\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8380\t Accuracy 0.8397\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8398\t Accuracy 0.8407\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8372\t Accuracy 0.8413\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8345\t Accuracy 0.8432\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8402\t Accuracy 0.8423\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8388\t Accuracy 0.8430\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8389\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8387\t Accuracy 0.8435\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8369\t Average training accuracy 0.8439\n",
      "Epoch [4]\t Average validation loss 0.7487\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8064\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8042\t Accuracy 0.8516\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8079\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8146\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8172\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8154\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8138\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8200\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8194\t Accuracy 0.8494\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8202\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8207\t Accuracy 0.8495\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8196\t Average training accuracy 0.8499\n",
      "Epoch [5]\t Average validation loss 0.7390\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.7945\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.7939\t Accuracy 0.8586\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.7984\t Accuracy 0.8574\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8057\t Accuracy 0.8536\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8089\t Accuracy 0.8531\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8073\t Accuracy 0.8535\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8063\t Accuracy 0.8543\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8128\t Accuracy 0.8533\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8126\t Accuracy 0.8535\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8138\t Accuracy 0.8539\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8146\t Accuracy 0.8533\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8139\t Average training accuracy 0.8535\n",
      "Epoch [6]\t Average validation loss 0.7378\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.7912\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.7920\t Accuracy 0.8622\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.7969\t Accuracy 0.8598\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8044\t Accuracy 0.8560\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8079\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8065\t Accuracy 0.8563\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8059\t Accuracy 0.8568\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8124\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8124\t Accuracy 0.8562\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8138\t Accuracy 0.8564\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8148\t Accuracy 0.8555\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8143\t Average training accuracy 0.8556\n",
      "Epoch [7]\t Average validation loss 0.7408\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.7925\t Accuracy 0.8700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.7942\t Accuracy 0.8641\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.7993\t Accuracy 0.8610\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8069\t Accuracy 0.8570\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8106\t Accuracy 0.8572\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8092\t Accuracy 0.8579\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8087\t Accuracy 0.8583\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8153\t Accuracy 0.8577\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8154\t Accuracy 0.8579\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8169\t Accuracy 0.8579\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8179\t Accuracy 0.8570\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8175\t Average training accuracy 0.8570\n",
      "Epoch [8]\t Average validation loss 0.7456\t Average validation accuracy 0.8918\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7959\t Accuracy 0.8700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.7982\t Accuracy 0.8653\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8033\t Accuracy 0.8617\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8110\t Accuracy 0.8577\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8147\t Accuracy 0.8581\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8134\t Accuracy 0.8585\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8130\t Accuracy 0.8589\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8194\t Accuracy 0.8583\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8196\t Accuracy 0.8585\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8212\t Accuracy 0.8584\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8222\t Accuracy 0.8577\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8218\t Average training accuracy 0.8577\n",
      "Epoch [9]\t Average validation loss 0.7509\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7998\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8027\t Accuracy 0.8655\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8079\t Accuracy 0.8615\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8155\t Accuracy 0.8575\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8193\t Accuracy 0.8575\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8178\t Accuracy 0.8581\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8175\t Accuracy 0.8587\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8239\t Accuracy 0.8581\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8240\t Accuracy 0.8583\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8256\t Accuracy 0.8583\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8266\t Accuracy 0.8575\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8263\t Average training accuracy 0.8575\n",
      "Epoch [10]\t Average validation loss 0.7560\t Average validation accuracy 0.8946\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8036\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8071\t Accuracy 0.8645\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8122\t Accuracy 0.8606\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8198\t Accuracy 0.8568\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8236\t Accuracy 0.8574\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8221\t Accuracy 0.8581\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8217\t Accuracy 0.8588\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8281\t Accuracy 0.8582\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8282\t Accuracy 0.8583\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8298\t Accuracy 0.8582\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8308\t Accuracy 0.8575\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8304\t Average training accuracy 0.8575\n",
      "Epoch [11]\t Average validation loss 0.7606\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8070\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8110\t Accuracy 0.8645\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8161\t Accuracy 0.8608\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8237\t Accuracy 0.8571\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8274\t Accuracy 0.8573\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8259\t Accuracy 0.8583\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8255\t Accuracy 0.8591\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8318\t Accuracy 0.8586\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8319\t Accuracy 0.8588\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8335\t Accuracy 0.8586\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8344\t Accuracy 0.8580\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8340\t Average training accuracy 0.8580\n",
      "Epoch [12]\t Average validation loss 0.7646\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8099\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8144\t Accuracy 0.8655\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8195\t Accuracy 0.8617\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8270\t Accuracy 0.8579\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8307\t Accuracy 0.8579\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8291\t Accuracy 0.8586\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8287\t Accuracy 0.8592\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8349\t Accuracy 0.8585\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8350\t Accuracy 0.8587\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8366\t Accuracy 0.8584\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8375\t Accuracy 0.8580\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8371\t Average training accuracy 0.8580\n",
      "Epoch [13]\t Average validation loss 0.7679\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8124\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8172\t Accuracy 0.8663\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8223\t Accuracy 0.8624\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8298\t Accuracy 0.8581\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8335\t Accuracy 0.8580\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8318\t Accuracy 0.8588\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8314\t Accuracy 0.8594\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8376\t Accuracy 0.8587\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8376\t Accuracy 0.8589\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8392\t Accuracy 0.8585\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8400\t Accuracy 0.8580\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8397\t Average training accuracy 0.8581\n",
      "Epoch [14]\t Average validation loss 0.7707\t Average validation accuracy 0.8952\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8146\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8195\t Accuracy 0.8657\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8246\t Accuracy 0.8620\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8321\t Accuracy 0.8577\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8358\t Accuracy 0.8575\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8340\t Accuracy 0.8586\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8337\t Accuracy 0.8592\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8397\t Accuracy 0.8585\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8398\t Accuracy 0.8589\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8414\t Accuracy 0.8586\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8421\t Accuracy 0.8581\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8418\t Average training accuracy 0.8581\n",
      "Epoch [15]\t Average validation loss 0.7730\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8163\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8214\t Accuracy 0.8657\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8265\t Accuracy 0.8617\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8339\t Accuracy 0.8579\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8377\t Accuracy 0.8577\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8358\t Accuracy 0.8586\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8354\t Accuracy 0.8592\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8415\t Accuracy 0.8585\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8415\t Accuracy 0.8588\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8431\t Accuracy 0.8584\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8438\t Accuracy 0.8580\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8435\t Average training accuracy 0.8581\n",
      "Epoch [16]\t Average validation loss 0.7748\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8176\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8229\t Accuracy 0.8659\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8280\t Accuracy 0.8620\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8354\t Accuracy 0.8581\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8391\t Accuracy 0.8577\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8372\t Accuracy 0.8586\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8369\t Accuracy 0.8593\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8428\t Accuracy 0.8585\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8429\t Accuracy 0.8589\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8444\t Accuracy 0.8585\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8452\t Accuracy 0.8582\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8448\t Average training accuracy 0.8581\n",
      "Epoch [17]\t Average validation loss 0.7763\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8187\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8241\t Accuracy 0.8657\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8292\t Accuracy 0.8620\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8366\t Accuracy 0.8583\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8403\t Accuracy 0.8579\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8384\t Accuracy 0.8588\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8380\t Accuracy 0.8594\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8439\t Accuracy 0.8586\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8440\t Accuracy 0.8588\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8455\t Accuracy 0.8585\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8462\t Accuracy 0.8581\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8458\t Average training accuracy 0.8580\n",
      "Epoch [18]\t Average validation loss 0.7775\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8195\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8251\t Accuracy 0.8645\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8302\t Accuracy 0.8613\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8375\t Accuracy 0.8579\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8412\t Accuracy 0.8577\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8393\t Accuracy 0.8586\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8389\t Accuracy 0.8593\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8448\t Accuracy 0.8585\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8448\t Accuracy 0.8588\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8464\t Accuracy 0.8584\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8470\t Accuracy 0.8579\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8467\t Average training accuracy 0.8578\n",
      "Epoch [19]\t Average validation loss 0.7784\t Average validation accuracy 0.8952\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 1.4575\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.7051\t Accuracy 0.1427\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.5878\t Accuracy 0.2282\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.5277\t Accuracy 0.2974\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.4880\t Accuracy 0.3572\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.4590\t Accuracy 0.4057\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4371\t Accuracy 0.4434\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4206\t Accuracy 0.4741\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4069\t Accuracy 0.5011\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3957\t Accuracy 0.5240\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3865\t Accuracy 0.5436\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3786\t Average training accuracy 0.5614\n",
      "Epoch [0]\t Average validation loss 0.2895\t Average validation accuracy 0.7834\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2805\t Accuracy 0.8000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2952\t Accuracy 0.7541\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2951\t Accuracy 0.7559\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2969\t Accuracy 0.7532\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2980\t Accuracy 0.7540\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2986\t Accuracy 0.7558\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2993\t Accuracy 0.7576\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3010\t Accuracy 0.7578\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3021\t Accuracy 0.7597\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3034\t Accuracy 0.7613\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3046\t Accuracy 0.7619\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3056\t Average training accuracy 0.7638\n",
      "Epoch [1]\t Average validation loss 0.3108\t Average validation accuracy 0.8240\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3077\t Accuracy 0.8100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3172\t Accuracy 0.7714\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3188\t Accuracy 0.7689\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3213\t Accuracy 0.7659\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3233\t Accuracy 0.7651\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3247\t Accuracy 0.7641\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3260\t Accuracy 0.7631\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3282\t Accuracy 0.7609\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3296\t Accuracy 0.7612\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3313\t Accuracy 0.7609\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3327\t Accuracy 0.7605\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3340\t Average training accuracy 0.7603\n",
      "Epoch [2]\t Average validation loss 0.3434\t Average validation accuracy 0.8114\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3420\t Accuracy 0.8100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3481\t Accuracy 0.7486\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3494\t Accuracy 0.7394\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3513\t Accuracy 0.7383\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3529\t Accuracy 0.7361\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3540\t Accuracy 0.7353\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3549\t Accuracy 0.7335\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3566\t Accuracy 0.7308\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3577\t Accuracy 0.7304\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3589\t Accuracy 0.7294\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3599\t Accuracy 0.7268\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3609\t Average training accuracy 0.7260\n",
      "Epoch [3]\t Average validation loss 0.3675\t Average validation accuracy 0.7774\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3666\t Accuracy 0.8000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3707\t Accuracy 0.7018\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3716\t Accuracy 0.6938\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3728\t Accuracy 0.6941\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3740\t Accuracy 0.6893\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3747\t Accuracy 0.6878\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3753\t Accuracy 0.6849\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3764\t Accuracy 0.6821\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3771\t Accuracy 0.6818\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3780\t Accuracy 0.6801\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3787\t Accuracy 0.6774\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3793\t Average training accuracy 0.6765\n",
      "Epoch [4]\t Average validation loss 0.3831\t Average validation accuracy 0.7380\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.3823\t Accuracy 0.7500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3853\t Accuracy 0.6476\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3859\t Accuracy 0.6379\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3867\t Accuracy 0.6399\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3876\t Accuracy 0.6344\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3880\t Accuracy 0.6349\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3884\t Accuracy 0.6314\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3892\t Accuracy 0.6287\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3897\t Accuracy 0.6279\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3902\t Accuracy 0.6261\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3907\t Accuracy 0.6240\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3911\t Average training accuracy 0.6229\n",
      "Epoch [5]\t Average validation loss 0.3931\t Average validation accuracy 0.6992\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.3923\t Accuracy 0.7200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3947\t Accuracy 0.5965\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3950\t Accuracy 0.5834\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3956\t Accuracy 0.5863\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3963\t Accuracy 0.5817\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3966\t Accuracy 0.5841\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3968\t Accuracy 0.5809\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3974\t Accuracy 0.5780\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3977\t Accuracy 0.5780\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3982\t Accuracy 0.5759\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3985\t Accuracy 0.5747\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3987\t Average training accuracy 0.5741\n",
      "Epoch [6]\t Average validation loss 0.3997\t Average validation accuracy 0.6622\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.3988\t Accuracy 0.7000\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.4008\t Accuracy 0.5600\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.4011\t Accuracy 0.5416\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.4015\t Accuracy 0.5440\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.4021\t Accuracy 0.5388\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.4023\t Accuracy 0.5428\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.4024\t Accuracy 0.5408\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.4029\t Accuracy 0.5379\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.4031\t Accuracy 0.5382\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.4034\t Accuracy 0.5353\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.4037\t Accuracy 0.5343\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4039\t Average training accuracy 0.5342\n",
      "Epoch [7]\t Average validation loss 0.4041\t Average validation accuracy 0.6286\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.4032\t Accuracy 0.6600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.4050\t Accuracy 0.5302\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.4052\t Accuracy 0.5084\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.4056\t Accuracy 0.5119\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.4060\t Accuracy 0.5056\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.4062\t Accuracy 0.5096\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.4062\t Accuracy 0.5083\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.4066\t Accuracy 0.5058\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.4068\t Accuracy 0.5066\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.4071\t Accuracy 0.5034\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.4072\t Accuracy 0.5028\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4074\t Average training accuracy 0.5032\n",
      "Epoch [8]\t Average validation loss 0.4073\t Average validation accuracy 0.5972\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.4063\t Accuracy 0.5900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.4080\t Accuracy 0.5069\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.4081\t Accuracy 0.4848\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.4084\t Accuracy 0.4870\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.4088\t Accuracy 0.4801\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.4089\t Accuracy 0.4845\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.4090\t Accuracy 0.4832\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.4093\t Accuracy 0.4810\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.4094\t Accuracy 0.4822\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.4097\t Accuracy 0.4793\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.4098\t Accuracy 0.4787\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4099\t Average training accuracy 0.4796\n",
      "Epoch [9]\t Average validation loss 0.4095\t Average validation accuracy 0.5726\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.4086\t Accuracy 0.5700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.4101\t Accuracy 0.4910\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.4102\t Accuracy 0.4692\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.4105\t Accuracy 0.4701\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.4108\t Accuracy 0.4627\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.4109\t Accuracy 0.4672\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.4109\t Accuracy 0.4659\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.4112\t Accuracy 0.4636\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.4114\t Accuracy 0.4651\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.4115\t Accuracy 0.4622\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.4117\t Accuracy 0.4617\n",
      "\n",
      "Epoch [10]\t Average training loss 0.4117\t Average training accuracy 0.4623\n",
      "Epoch [10]\t Average validation loss 0.4112\t Average validation accuracy 0.5536\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.4102\t Accuracy 0.5600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.4117\t Accuracy 0.4759\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.4117\t Accuracy 0.4555\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.4120\t Accuracy 0.4538\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.4123\t Accuracy 0.4465\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.4124\t Accuracy 0.4506\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.4124\t Accuracy 0.4496\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.4127\t Accuracy 0.4474\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.4128\t Accuracy 0.4491\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.4129\t Accuracy 0.4468\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.4130\t Accuracy 0.4466\n",
      "\n",
      "Epoch [11]\t Average training loss 0.4131\t Average training accuracy 0.4474\n",
      "Epoch [11]\t Average validation loss 0.4124\t Average validation accuracy 0.5344\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.4114\t Accuracy 0.5300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.4129\t Accuracy 0.4633\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.4129\t Accuracy 0.4434\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.4131\t Accuracy 0.4415\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.4134\t Accuracy 0.4339\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.4135\t Accuracy 0.4378\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.4135\t Accuracy 0.4369\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.4138\t Accuracy 0.4349\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.4138\t Accuracy 0.4368\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.4140\t Accuracy 0.4344\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.4141\t Accuracy 0.4344\n",
      "\n",
      "Epoch [12]\t Average training loss 0.4141\t Average training accuracy 0.4350\n",
      "Epoch [12]\t Average validation loss 0.4134\t Average validation accuracy 0.5180\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.4124\t Accuracy 0.5300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.4138\t Accuracy 0.4535\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.4138\t Accuracy 0.4336\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.4140\t Accuracy 0.4310\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.4143\t Accuracy 0.4235\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.4144\t Accuracy 0.4276\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.4144\t Accuracy 0.4269\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.4146\t Accuracy 0.4248\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.4147\t Accuracy 0.4266\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.4148\t Accuracy 0.4246\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.4149\t Accuracy 0.4248\n",
      "\n",
      "Epoch [13]\t Average training loss 0.4150\t Average training accuracy 0.4253\n",
      "Epoch [13]\t Average validation loss 0.4142\t Average validation accuracy 0.5076\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.4131\t Accuracy 0.5300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.4145\t Accuracy 0.4451\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.4145\t Accuracy 0.4260\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.4147\t Accuracy 0.4234\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.4150\t Accuracy 0.4158\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.4151\t Accuracy 0.4192\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.4150\t Accuracy 0.4188\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.4153\t Accuracy 0.4170\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.4153\t Accuracy 0.4190\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.4155\t Accuracy 0.4168\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.4155\t Accuracy 0.4171\n",
      "\n",
      "Epoch [14]\t Average training loss 0.4156\t Average training accuracy 0.4176\n",
      "Epoch [14]\t Average validation loss 0.4148\t Average validation accuracy 0.4974\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.4137\t Accuracy 0.5200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.4151\t Accuracy 0.4353\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.4151\t Accuracy 0.4173\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.4153\t Accuracy 0.4146\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.4155\t Accuracy 0.4074\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.4156\t Accuracy 0.4112\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.4156\t Accuracy 0.4112\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.4158\t Accuracy 0.4097\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.4158\t Accuracy 0.4119\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.4160\t Accuracy 0.4098\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.4160\t Accuracy 0.4100\n",
      "\n",
      "Epoch [15]\t Average training loss 0.4161\t Average training accuracy 0.4105\n",
      "Epoch [15]\t Average validation loss 0.4152\t Average validation accuracy 0.4888\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.4141\t Accuracy 0.4900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.4155\t Accuracy 0.4280\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.4155\t Accuracy 0.4100\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.4157\t Accuracy 0.4071\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.4160\t Accuracy 0.4002\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.4160\t Accuracy 0.4042\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.4160\t Accuracy 0.4044\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.4162\t Accuracy 0.4030\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.4163\t Accuracy 0.4055\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.4164\t Accuracy 0.4033\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.4164\t Accuracy 0.4038\n",
      "\n",
      "Epoch [16]\t Average training loss 0.4165\t Average training accuracy 0.4043\n",
      "Epoch [16]\t Average validation loss 0.4156\t Average validation accuracy 0.4828\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.4145\t Accuracy 0.4700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.4159\t Accuracy 0.4224\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.4159\t Accuracy 0.4050\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.4161\t Accuracy 0.4017\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.4163\t Accuracy 0.3950\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.4164\t Accuracy 0.3989\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.4163\t Accuracy 0.3994\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.4166\t Accuracy 0.3981\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.4166\t Accuracy 0.4006\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.4167\t Accuracy 0.3986\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.4168\t Accuracy 0.3990\n",
      "\n",
      "Epoch [17]\t Average training loss 0.4168\t Average training accuracy 0.3994\n",
      "Epoch [17]\t Average validation loss 0.4160\t Average validation accuracy 0.4762\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.4148\t Accuracy 0.4700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.4162\t Accuracy 0.4167\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.4162\t Accuracy 0.4005\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.4164\t Accuracy 0.3970\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.4166\t Accuracy 0.3907\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.4167\t Accuracy 0.3947\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.4166\t Accuracy 0.3953\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.4168\t Accuracy 0.3940\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.4169\t Accuracy 0.3965\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.4170\t Accuracy 0.3945\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.4171\t Accuracy 0.3951\n",
      "\n",
      "Epoch [18]\t Average training loss 0.4171\t Average training accuracy 0.3953\n",
      "Epoch [18]\t Average validation loss 0.4162\t Average validation accuracy 0.4712\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.4151\t Accuracy 0.4700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.4165\t Accuracy 0.4125\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.4164\t Accuracy 0.3962\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.4166\t Accuracy 0.3925\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.4169\t Accuracy 0.3864\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.4169\t Accuracy 0.3905\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.4169\t Accuracy 0.3912\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.4171\t Accuracy 0.3902\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.4171\t Accuracy 0.3926\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.4172\t Accuracy 0.3906\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.4173\t Accuracy 0.3913\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4173\t Average training accuracy 0.3915\n",
      "Epoch [19]\t Average validation loss 0.4164\t Average validation accuracy 0.4676\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.0019\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.1484\t Accuracy 0.2378\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8364\t Accuracy 0.3505\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6984\t Accuracy 0.4175\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6133\t Accuracy 0.4693\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5534\t Accuracy 0.5133\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.5096\t Accuracy 0.5492\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4766\t Accuracy 0.5770\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4494\t Accuracy 0.6021\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4273\t Accuracy 0.6238\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.4088\t Accuracy 0.6413\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3928\t Average training accuracy 0.6573\n",
      "Epoch [0]\t Average validation loss 0.2147\t Average validation accuracy 0.8570\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2178\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2205\t Accuracy 0.8294\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2191\t Accuracy 0.8301\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2203\t Accuracy 0.8246\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2198\t Accuracy 0.8266\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2182\t Accuracy 0.8292\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2174\t Accuracy 0.8313\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2177\t Accuracy 0.8319\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2169\t Accuracy 0.8336\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2167\t Accuracy 0.8348\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2165\t Accuracy 0.8354\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2158\t Average training accuracy 0.8363\n",
      "Epoch [1]\t Average validation loss 0.1970\t Average validation accuracy 0.8826\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2002\t Accuracy 0.8700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.2069\t Accuracy 0.8520\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2074\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2103\t Accuracy 0.8397\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2115\t Accuracy 0.8402\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2117\t Accuracy 0.8409\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2123\t Accuracy 0.8413\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2140\t Accuracy 0.8413\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2143\t Accuracy 0.8416\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2151\t Accuracy 0.8413\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2157\t Accuracy 0.8409\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2159\t Average training accuracy 0.8410\n",
      "Epoch [2]\t Average validation loss 0.2053\t Average validation accuracy 0.8790\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2055\t Accuracy 0.8600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2154\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2160\t Accuracy 0.8418\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2188\t Accuracy 0.8346\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2198\t Accuracy 0.8348\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8348\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2205\t Accuracy 0.8355\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2221\t Accuracy 0.8354\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2223\t Accuracy 0.8356\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2230\t Accuracy 0.8354\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2234\t Accuracy 0.8348\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2234\t Average training accuracy 0.8350\n",
      "Epoch [3]\t Average validation loss 0.2116\t Average validation accuracy 0.8758\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2097\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.8363\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2218\t Accuracy 0.8330\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2242\t Accuracy 0.8270\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2251\t Accuracy 0.8277\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2251\t Accuracy 0.8275\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2254\t Accuracy 0.8285\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2267\t Accuracy 0.8283\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2268\t Accuracy 0.8285\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2273\t Accuracy 0.8285\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2276\t Accuracy 0.8282\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2274\t Average training accuracy 0.8284\n",
      "Epoch [4]\t Average validation loss 0.2142\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2111\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2237\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2240\t Accuracy 0.8276\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2263\t Accuracy 0.8224\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2271\t Accuracy 0.8235\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2269\t Accuracy 0.8232\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2271\t Accuracy 0.8240\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2283\t Accuracy 0.8237\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2283\t Accuracy 0.8239\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2287\t Accuracy 0.8239\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2288\t Accuracy 0.8238\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2287\t Average training accuracy 0.8240\n",
      "Epoch [5]\t Average validation loss 0.2146\t Average validation accuracy 0.8640\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2109\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2240\t Accuracy 0.8257\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2243\t Accuracy 0.8246\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2265\t Accuracy 0.8187\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2272\t Accuracy 0.8197\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2270\t Accuracy 0.8196\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2271\t Accuracy 0.8205\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2283\t Accuracy 0.8207\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2282\t Accuracy 0.8208\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2286\t Accuracy 0.8208\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2287\t Accuracy 0.8209\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2285\t Average training accuracy 0.8212\n",
      "Epoch [6]\t Average validation loss 0.2141\t Average validation accuracy 0.8624\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2100\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2235\t Accuracy 0.8212\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2237\t Accuracy 0.8211\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2259\t Accuracy 0.8160\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2266\t Accuracy 0.8170\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2264\t Accuracy 0.8169\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2265\t Accuracy 0.8177\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2276\t Accuracy 0.8182\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2276\t Accuracy 0.8188\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2279\t Accuracy 0.8187\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2280\t Accuracy 0.8190\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2278\t Average training accuracy 0.8195\n",
      "Epoch [7]\t Average validation loss 0.2133\t Average validation accuracy 0.8618\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2091\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2227\t Accuracy 0.8206\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2229\t Accuracy 0.8198\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2251\t Accuracy 0.8146\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2258\t Accuracy 0.8159\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2256\t Accuracy 0.8156\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2257\t Accuracy 0.8164\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2269\t Accuracy 0.8171\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2268\t Accuracy 0.8177\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2272\t Accuracy 0.8178\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2273\t Accuracy 0.8182\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2270\t Average training accuracy 0.8187\n",
      "Epoch [8]\t Average validation loss 0.2126\t Average validation accuracy 0.8622\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2082\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2219\t Accuracy 0.8194\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2222\t Accuracy 0.8196\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2244\t Accuracy 0.8146\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2251\t Accuracy 0.8155\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2249\t Accuracy 0.8151\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2250\t Accuracy 0.8159\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2262\t Accuracy 0.8166\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2261\t Accuracy 0.8172\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2265\t Accuracy 0.8173\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2265\t Accuracy 0.8176\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2263\t Average training accuracy 0.8181\n",
      "Epoch [9]\t Average validation loss 0.2119\t Average validation accuracy 0.8622\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2074\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.8196\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2215\t Accuracy 0.8197\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2237\t Accuracy 0.8147\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2244\t Accuracy 0.8156\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2242\t Accuracy 0.8151\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2243\t Accuracy 0.8160\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2255\t Accuracy 0.8167\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2255\t Accuracy 0.8175\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2258\t Accuracy 0.8174\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2259\t Accuracy 0.8177\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2257\t Average training accuracy 0.8182\n",
      "Epoch [10]\t Average validation loss 0.2113\t Average validation accuracy 0.8626\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2068\t Accuracy 0.8400\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2207\t Accuracy 0.8206\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2209\t Accuracy 0.8206\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2231\t Accuracy 0.8154\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2238\t Accuracy 0.8160\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2236\t Accuracy 0.8155\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2237\t Accuracy 0.8163\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2250\t Accuracy 0.8171\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2249\t Accuracy 0.8178\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2253\t Accuracy 0.8178\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2253\t Accuracy 0.8181\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2251\t Average training accuracy 0.8186\n",
      "Epoch [11]\t Average validation loss 0.2107\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2062\t Accuracy 0.8400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2202\t Accuracy 0.8208\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2204\t Accuracy 0.8206\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2226\t Accuracy 0.8154\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2234\t Accuracy 0.8162\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2231\t Accuracy 0.8157\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2233\t Accuracy 0.8166\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2245\t Accuracy 0.8173\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2244\t Accuracy 0.8180\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2248\t Accuracy 0.8180\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2249\t Accuracy 0.8183\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2247\t Average training accuracy 0.8188\n",
      "Epoch [12]\t Average validation loss 0.2103\t Average validation accuracy 0.8626\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2058\t Accuracy 0.8400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2197\t Accuracy 0.8208\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2200\t Accuracy 0.8206\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2222\t Accuracy 0.8157\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2229\t Accuracy 0.8163\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2227\t Accuracy 0.8158\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2228\t Accuracy 0.8167\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2240\t Accuracy 0.8174\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2240\t Accuracy 0.8182\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2244\t Accuracy 0.8182\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2244\t Accuracy 0.8185\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2242\t Average training accuracy 0.8189\n",
      "Epoch [13]\t Average validation loss 0.2099\t Average validation accuracy 0.8624\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2053\t Accuracy 0.8400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2193\t Accuracy 0.8208\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2196\t Accuracy 0.8207\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2218\t Accuracy 0.8158\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2225\t Accuracy 0.8162\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2223\t Accuracy 0.8159\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2225\t Accuracy 0.8169\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2237\t Accuracy 0.8176\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2236\t Accuracy 0.8184\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2240\t Accuracy 0.8183\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2241\t Accuracy 0.8187\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2239\t Average training accuracy 0.8191\n",
      "Epoch [14]\t Average validation loss 0.2095\t Average validation accuracy 0.8626\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2050\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2190\t Accuracy 0.8210\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2193\t Accuracy 0.8208\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2215\t Accuracy 0.8161\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2222\t Accuracy 0.8164\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2220\t Accuracy 0.8161\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2221\t Accuracy 0.8170\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2233\t Accuracy 0.8177\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2233\t Accuracy 0.8185\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2237\t Accuracy 0.8184\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2238\t Accuracy 0.8188\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2236\t Average training accuracy 0.8193\n",
      "Epoch [15]\t Average validation loss 0.2092\t Average validation accuracy 0.8632\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2046\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2187\t Accuracy 0.8210\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2190\t Accuracy 0.8209\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2212\t Accuracy 0.8161\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2219\t Accuracy 0.8163\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2217\t Accuracy 0.8162\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2218\t Accuracy 0.8171\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2231\t Accuracy 0.8178\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2230\t Accuracy 0.8186\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2234\t Accuracy 0.8186\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2235\t Accuracy 0.8190\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2233\t Average training accuracy 0.8194\n",
      "Epoch [16]\t Average validation loss 0.2089\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2043\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2184\t Accuracy 0.8212\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2187\t Accuracy 0.8213\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2209\t Accuracy 0.8164\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2217\t Accuracy 0.8166\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2215\t Accuracy 0.8165\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2216\t Accuracy 0.8173\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2228\t Accuracy 0.8180\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2228\t Accuracy 0.8188\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2232\t Accuracy 0.8188\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2232\t Accuracy 0.8192\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2230\t Average training accuracy 0.8196\n",
      "Epoch [17]\t Average validation loss 0.2087\t Average validation accuracy 0.8626\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2041\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2182\t Accuracy 0.8214\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2185\t Accuracy 0.8214\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2207\t Accuracy 0.8165\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2214\t Accuracy 0.8166\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2212\t Accuracy 0.8165\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2214\t Accuracy 0.8174\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2226\t Accuracy 0.8181\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2225\t Accuracy 0.8188\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2229\t Accuracy 0.8188\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2230\t Accuracy 0.8193\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2228\t Average training accuracy 0.8197\n",
      "Epoch [18]\t Average validation loss 0.2085\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2039\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2180\t Accuracy 0.8214\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2183\t Accuracy 0.8217\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2205\t Accuracy 0.8167\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2212\t Accuracy 0.8168\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2210\t Accuracy 0.8167\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2211\t Accuracy 0.8176\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2224\t Accuracy 0.8182\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2223\t Accuracy 0.8190\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2227\t Accuracy 0.8190\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2228\t Accuracy 0.8194\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2226\t Average training accuracy 0.8198\n",
      "Epoch [19]\t Average validation loss 0.2083\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.9382\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4774\t Accuracy 0.1210\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3265\t Accuracy 0.2124\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2478\t Accuracy 0.2873\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.1937\t Accuracy 0.3452\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.1482\t Accuracy 0.3945\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1117\t Accuracy 0.4320\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.0837\t Accuracy 0.4610\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.0575\t Accuracy 0.4878\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0368\t Accuracy 0.5074\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0184\t Accuracy 0.5239\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0021\t Average training accuracy 0.5385\n",
      "Epoch [0]\t Average validation loss 1.8117\t Average validation accuracy 0.7294\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.8009\t Accuracy 0.7400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.8224\t Accuracy 0.6927\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.8209\t Accuracy 0.6879\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.8235\t Accuracy 0.6829\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.8264\t Accuracy 0.6829\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.8260\t Accuracy 0.6843\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.8261\t Accuracy 0.6839\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.8300\t Accuracy 0.6850\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.8310\t Accuracy 0.6877\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.8337\t Accuracy 0.6864\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.8354\t Accuracy 0.6858\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8369\t Average training accuracy 0.6852\n",
      "Epoch [1]\t Average validation loss 1.8346\t Average validation accuracy 0.7160\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.8235\t Accuracy 0.7400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.8493\t Accuracy 0.6735\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.8518\t Accuracy 0.6691\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.8571\t Accuracy 0.6646\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.8625\t Accuracy 0.6642\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.8645\t Accuracy 0.6652\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.8665\t Accuracy 0.6618\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.8717\t Accuracy 0.6615\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.8741\t Accuracy 0.6634\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.8777\t Accuracy 0.6601\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.8803\t Accuracy 0.6584\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8826\t Average training accuracy 0.6571\n",
      "Epoch [2]\t Average validation loss 1.8913\t Average validation accuracy 0.6806\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.8796\t Accuracy 0.7200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.9026\t Accuracy 0.6453\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.9047\t Accuracy 0.6321\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.9090\t Accuracy 0.6246\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.9135\t Accuracy 0.6217\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.9152\t Accuracy 0.6249\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.9166\t Accuracy 0.6210\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.9207\t Accuracy 0.6197\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.9226\t Accuracy 0.6208\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.9254\t Accuracy 0.6167\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.9274\t Accuracy 0.6146\n",
      "\n",
      "Epoch [3]\t Average training loss 1.9291\t Average training accuracy 0.6133\n",
      "Epoch [3]\t Average validation loss 1.9334\t Average validation accuracy 0.6390\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9212\t Accuracy 0.6700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9418\t Accuracy 0.6127\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9432\t Accuracy 0.5946\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9466\t Accuracy 0.5828\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9503\t Accuracy 0.5770\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9514\t Accuracy 0.5824\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9522\t Accuracy 0.5793\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9555\t Accuracy 0.5759\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9568\t Accuracy 0.5770\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9590\t Accuracy 0.5725\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9604\t Accuracy 0.5705\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9616\t Average training accuracy 0.5696\n",
      "Epoch [4]\t Average validation loss 1.9617\t Average validation accuracy 0.6014\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9491\t Accuracy 0.6500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9681\t Accuracy 0.5839\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9690\t Accuracy 0.5591\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9718\t Accuracy 0.5456\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9749\t Accuracy 0.5375\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9757\t Accuracy 0.5433\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9761\t Accuracy 0.5411\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9788\t Accuracy 0.5366\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9797\t Accuracy 0.5377\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9814\t Accuracy 0.5334\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9825\t Accuracy 0.5319\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9834\t Average training accuracy 0.5310\n",
      "Epoch [5]\t Average validation loss 1.9807\t Average validation accuracy 0.5690\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.9676\t Accuracy 0.6300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9858\t Accuracy 0.5614\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9863\t Accuracy 0.5334\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9887\t Accuracy 0.5177\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9915\t Accuracy 0.5092\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9921\t Accuracy 0.5153\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.9922\t Accuracy 0.5137\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9945\t Accuracy 0.5083\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9952\t Accuracy 0.5097\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9966\t Accuracy 0.5055\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.9974\t Accuracy 0.5038\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9981\t Average training accuracy 0.5031\n",
      "Epoch [6]\t Average validation loss 1.9937\t Average validation accuracy 0.5406\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.9802\t Accuracy 0.6100\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.9979\t Accuracy 0.5420\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.9982\t Accuracy 0.5133\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 2.0003\t Accuracy 0.4962\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 2.0029\t Accuracy 0.4877\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 2.0033\t Accuracy 0.4944\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 2.0033\t Accuracy 0.4926\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 2.0054\t Accuracy 0.4867\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 2.0059\t Accuracy 0.4880\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 2.0071\t Accuracy 0.4842\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 2.0078\t Accuracy 0.4827\n",
      "\n",
      "Epoch [7]\t Average training loss 2.0084\t Average training accuracy 0.4820\n",
      "Epoch [7]\t Average validation loss 2.0028\t Average validation accuracy 0.5194\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.9890\t Accuracy 0.5700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 2.0064\t Accuracy 0.5237\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 2.0065\t Accuracy 0.4960\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 2.0085\t Accuracy 0.4798\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 2.0109\t Accuracy 0.4714\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 2.0112\t Accuracy 0.4784\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 2.0111\t Accuracy 0.4769\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 2.0130\t Accuracy 0.4710\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 2.0134\t Accuracy 0.4723\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 2.0145\t Accuracy 0.4688\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 2.0151\t Accuracy 0.4672\n",
      "\n",
      "Epoch [8]\t Average training loss 2.0156\t Average training accuracy 0.4664\n",
      "Epoch [8]\t Average validation loss 2.0093\t Average validation accuracy 0.4992\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.9952\t Accuracy 0.5600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 2.0124\t Accuracy 0.5098\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 2.0125\t Accuracy 0.4834\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 2.0143\t Accuracy 0.4668\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 2.0166\t Accuracy 0.4588\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 2.0169\t Accuracy 0.4652\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 2.0168\t Accuracy 0.4639\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 2.0185\t Accuracy 0.4583\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 2.0188\t Accuracy 0.4596\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 2.0199\t Accuracy 0.4562\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 2.0203\t Accuracy 0.4545\n",
      "\n",
      "Epoch [9]\t Average training loss 2.0208\t Average training accuracy 0.4539\n",
      "Epoch [9]\t Average validation loss 2.0140\t Average validation accuracy 0.4824\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.9996\t Accuracy 0.5500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 2.0169\t Accuracy 0.4965\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 2.0168\t Accuracy 0.4717\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 2.0186\t Accuracy 0.4560\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 2.0208\t Accuracy 0.4483\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 2.0210\t Accuracy 0.4551\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 2.0208\t Accuracy 0.4536\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 2.0225\t Accuracy 0.4483\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 2.0228\t Accuracy 0.4495\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 2.0237\t Accuracy 0.4463\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 2.0242\t Accuracy 0.4447\n",
      "\n",
      "Epoch [10]\t Average training loss 2.0246\t Average training accuracy 0.4441\n",
      "Epoch [10]\t Average validation loss 2.0175\t Average validation accuracy 0.4702\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 2.0029\t Accuracy 0.5300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 2.0201\t Accuracy 0.4873\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 2.0200\t Accuracy 0.4627\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 2.0217\t Accuracy 0.4479\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 2.0239\t Accuracy 0.4406\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 2.0241\t Accuracy 0.4471\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 2.0239\t Accuracy 0.4455\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 2.0255\t Accuracy 0.4398\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 2.0257\t Accuracy 0.4411\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 2.0266\t Accuracy 0.4378\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 2.0270\t Accuracy 0.4364\n",
      "\n",
      "Epoch [11]\t Average training loss 2.0274\t Average training accuracy 0.4359\n",
      "Epoch [11]\t Average validation loss 2.0201\t Average validation accuracy 0.4592\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 2.0053\t Accuracy 0.5300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 2.0226\t Accuracy 0.4776\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 2.0224\t Accuracy 0.4541\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 2.0241\t Accuracy 0.4403\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 2.0262\t Accuracy 0.4335\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 2.0264\t Accuracy 0.4396\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 2.0262\t Accuracy 0.4380\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 2.0277\t Accuracy 0.4326\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 2.0279\t Accuracy 0.4339\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 2.0288\t Accuracy 0.4308\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 2.0292\t Accuracy 0.4296\n",
      "\n",
      "Epoch [12]\t Average training loss 2.0295\t Average training accuracy 0.4291\n",
      "Epoch [12]\t Average validation loss 2.0222\t Average validation accuracy 0.4502\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 2.0072\t Accuracy 0.5400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 2.0245\t Accuracy 0.4694\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 2.0243\t Accuracy 0.4470\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 2.0259\t Accuracy 0.4340\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 2.0280\t Accuracy 0.4280\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 2.0282\t Accuracy 0.4332\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 2.0279\t Accuracy 0.4317\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 2.0295\t Accuracy 0.4264\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 2.0296\t Accuracy 0.4278\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 2.0305\t Accuracy 0.4247\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 2.0308\t Accuracy 0.4236\n",
      "\n",
      "Epoch [13]\t Average training loss 2.0312\t Average training accuracy 0.4232\n",
      "Epoch [13]\t Average validation loss 2.0237\t Average validation accuracy 0.4442\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 2.0086\t Accuracy 0.5300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 2.0259\t Accuracy 0.4627\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 2.0257\t Accuracy 0.4420\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 2.0273\t Accuracy 0.4290\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 2.0294\t Accuracy 0.4231\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 2.0296\t Accuracy 0.4282\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 2.0293\t Accuracy 0.4268\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 2.0308\t Accuracy 0.4215\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 2.0309\t Accuracy 0.4229\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 2.0318\t Accuracy 0.4200\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 2.0321\t Accuracy 0.4189\n",
      "\n",
      "Epoch [14]\t Average training loss 2.0325\t Average training accuracy 0.4186\n",
      "Epoch [14]\t Average validation loss 2.0249\t Average validation accuracy 0.4400\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 2.0097\t Accuracy 0.5200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 2.0271\t Accuracy 0.4590\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 2.0268\t Accuracy 0.4383\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 2.0284\t Accuracy 0.4254\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 2.0305\t Accuracy 0.4197\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 2.0306\t Accuracy 0.4240\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 2.0304\t Accuracy 0.4227\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 2.0318\t Accuracy 0.4175\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 2.0320\t Accuracy 0.4190\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 2.0328\t Accuracy 0.4160\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 2.0332\t Accuracy 0.4149\n",
      "\n",
      "Epoch [15]\t Average training loss 2.0335\t Average training accuracy 0.4148\n",
      "Epoch [15]\t Average validation loss 2.0259\t Average validation accuracy 0.4338\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 2.0105\t Accuracy 0.5200\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 2.0280\t Accuracy 0.4557\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 2.0277\t Accuracy 0.4350\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 2.0292\t Accuracy 0.4221\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 2.0313\t Accuracy 0.4167\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 2.0315\t Accuracy 0.4212\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 2.0312\t Accuracy 0.4200\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 2.0327\t Accuracy 0.4147\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 2.0328\t Accuracy 0.4161\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 2.0336\t Accuracy 0.4132\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 2.0340\t Accuracy 0.4121\n",
      "\n",
      "Epoch [16]\t Average training loss 2.0343\t Average training accuracy 0.4121\n",
      "Epoch [16]\t Average validation loss 2.0267\t Average validation accuracy 0.4308\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 2.0112\t Accuracy 0.5100\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 2.0287\t Accuracy 0.4514\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 2.0284\t Accuracy 0.4320\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 2.0300\t Accuracy 0.4193\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 2.0320\t Accuracy 0.4141\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 2.0322\t Accuracy 0.4187\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 2.0319\t Accuracy 0.4174\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 2.0333\t Accuracy 0.4120\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 2.0334\t Accuracy 0.4133\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 2.0343\t Accuracy 0.4105\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 2.0346\t Accuracy 0.4095\n",
      "\n",
      "Epoch [17]\t Average training loss 2.0349\t Average training accuracy 0.4094\n",
      "Epoch [17]\t Average validation loss 2.0274\t Average validation accuracy 0.4258\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 2.0118\t Accuracy 0.5100\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 2.0293\t Accuracy 0.4473\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 2.0290\t Accuracy 0.4284\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 2.0305\t Accuracy 0.4157\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 2.0326\t Accuracy 0.4109\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 2.0328\t Accuracy 0.4156\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 2.0325\t Accuracy 0.4144\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 2.0339\t Accuracy 0.4091\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 2.0340\t Accuracy 0.4104\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 2.0348\t Accuracy 0.4075\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 2.0351\t Accuracy 0.4065\n",
      "\n",
      "Epoch [18]\t Average training loss 2.0354\t Average training accuracy 0.4064\n",
      "Epoch [18]\t Average validation loss 2.0279\t Average validation accuracy 0.4238\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 2.0122\t Accuracy 0.5000\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 2.0298\t Accuracy 0.4443\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 2.0295\t Accuracy 0.4256\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 2.0310\t Accuracy 0.4134\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 2.0331\t Accuracy 0.4085\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 2.0332\t Accuracy 0.4131\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 2.0329\t Accuracy 0.4121\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 2.0343\t Accuracy 0.4067\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 2.0344\t Accuracy 0.4081\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 2.0352\t Accuracy 0.4052\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 2.0356\t Accuracy 0.4043\n",
      "\n",
      "Epoch [19]\t Average training loss 2.0359\t Average training accuracy 0.4042\n",
      "Epoch [19]\t Average validation loss 2.0283\t Average validation accuracy 0.4220\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5926\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.1407\t Accuracy 0.2629\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.8585\t Accuracy 0.4289\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.6792\t Accuracy 0.5168\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.5509\t Accuracy 0.5733\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.4473\t Accuracy 0.6143\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.3667\t Accuracy 0.6456\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.3093\t Accuracy 0.6687\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.2568\t Accuracy 0.6874\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.2148\t Accuracy 0.7023\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.1790\t Accuracy 0.7146\n",
      "\n",
      "Epoch [0]\t Average training loss 1.1478\t Average training accuracy 0.7255\n",
      "Epoch [0]\t Average validation loss 0.7498\t Average validation accuracy 0.8854\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.7906\t Accuracy 0.8700\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8024\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8047\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8110\t Accuracy 0.8442\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8135\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8106\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8093\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8148\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8145\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8159\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8167\t Accuracy 0.8495\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8163\t Average training accuracy 0.8499\n",
      "Epoch [1]\t Average validation loss 0.7434\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.7791\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.7960\t Accuracy 0.8682\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8021\t Accuracy 0.8635\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8113\t Accuracy 0.8581\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8164\t Accuracy 0.8579\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8156\t Accuracy 0.8590\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8162\t Accuracy 0.8600\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8231\t Accuracy 0.8593\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8240\t Accuracy 0.8594\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8265\t Accuracy 0.8588\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8280\t Accuracy 0.8583\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8283\t Average training accuracy 0.8581\n",
      "Epoch [2]\t Average validation loss 0.7649\t Average validation accuracy 0.8988\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.7999\t Accuracy 0.8900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8144\t Accuracy 0.8678\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8198\t Accuracy 0.8639\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8284\t Accuracy 0.8585\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8330\t Accuracy 0.8580\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8317\t Accuracy 0.8594\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8319\t Accuracy 0.8600\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8382\t Accuracy 0.8595\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8386\t Accuracy 0.8594\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8406\t Accuracy 0.8588\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8416\t Accuracy 0.8584\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8416\t Average training accuracy 0.8584\n",
      "Epoch [3]\t Average validation loss 0.7754\t Average validation accuracy 0.8952\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8104\t Accuracy 0.8900\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8232\t Accuracy 0.8661\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8281\t Accuracy 0.8625\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8362\t Accuracy 0.8572\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8405\t Accuracy 0.8572\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8387\t Accuracy 0.8588\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8386\t Accuracy 0.8594\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8446\t Accuracy 0.8589\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8447\t Accuracy 0.8589\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8465\t Accuracy 0.8582\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8472\t Accuracy 0.8576\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8470\t Average training accuracy 0.8576\n",
      "Epoch [4]\t Average validation loss 0.7793\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8143\t Accuracy 0.8800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8263\t Accuracy 0.8647\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8309\t Accuracy 0.8617\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8388\t Accuracy 0.8570\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8430\t Accuracy 0.8572\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8410\t Accuracy 0.8588\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8408\t Accuracy 0.8594\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8467\t Accuracy 0.8589\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8467\t Accuracy 0.8589\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8483\t Accuracy 0.8583\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8489\t Accuracy 0.8577\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8486\t Average training accuracy 0.8575\n",
      "Epoch [5]\t Average validation loss 0.7805\t Average validation accuracy 0.8938\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8155\t Accuracy 0.8800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8271\t Accuracy 0.8625\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8317\t Accuracy 0.8610\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8395\t Accuracy 0.8562\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8436\t Accuracy 0.8564\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8415\t Accuracy 0.8577\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8412\t Accuracy 0.8581\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8470\t Accuracy 0.8577\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8470\t Accuracy 0.8579\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8486\t Accuracy 0.8573\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8491\t Accuracy 0.8569\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8488\t Average training accuracy 0.8565\n",
      "Epoch [6]\t Average validation loss 0.7805\t Average validation accuracy 0.8934\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8156\t Accuracy 0.8800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8271\t Accuracy 0.8620\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8316\t Accuracy 0.8612\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8394\t Accuracy 0.8558\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8434\t Accuracy 0.8556\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8413\t Accuracy 0.8571\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8410\t Accuracy 0.8577\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8467\t Accuracy 0.8570\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8467\t Accuracy 0.8574\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8482\t Accuracy 0.8568\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8487\t Accuracy 0.8564\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8484\t Average training accuracy 0.8560\n",
      "Epoch [7]\t Average validation loss 0.7801\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8151\t Accuracy 0.8800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8266\t Accuracy 0.8616\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8312\t Accuracy 0.8610\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8389\t Accuracy 0.8558\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8429\t Accuracy 0.8553\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8408\t Accuracy 0.8566\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8404\t Accuracy 0.8572\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8461\t Accuracy 0.8566\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8461\t Accuracy 0.8567\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8476\t Accuracy 0.8561\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8481\t Accuracy 0.8557\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8478\t Average training accuracy 0.8553\n",
      "Epoch [8]\t Average validation loss 0.7795\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8144\t Accuracy 0.8800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8260\t Accuracy 0.8608\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8306\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8383\t Accuracy 0.8550\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8423\t Accuracy 0.8545\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8401\t Accuracy 0.8557\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8397\t Accuracy 0.8564\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8454\t Accuracy 0.8559\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8454\t Accuracy 0.8560\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8469\t Accuracy 0.8555\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8473\t Accuracy 0.8551\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8471\t Average training accuracy 0.8547\n",
      "Epoch [9]\t Average validation loss 0.7787\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8135\t Accuracy 0.8800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8253\t Accuracy 0.8612\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8299\t Accuracy 0.8598\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8376\t Accuracy 0.8546\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8416\t Accuracy 0.8540\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8394\t Accuracy 0.8551\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8390\t Accuracy 0.8558\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8447\t Accuracy 0.8554\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8446\t Accuracy 0.8554\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8462\t Accuracy 0.8549\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8466\t Accuracy 0.8546\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8463\t Average training accuracy 0.8542\n",
      "Epoch [10]\t Average validation loss 0.7779\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8126\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8246\t Accuracy 0.8612\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8292\t Accuracy 0.8595\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8369\t Accuracy 0.8541\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8409\t Accuracy 0.8536\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8387\t Accuracy 0.8545\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8383\t Accuracy 0.8554\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8439\t Accuracy 0.8549\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8438\t Accuracy 0.8549\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8454\t Accuracy 0.8545\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8458\t Accuracy 0.8542\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8455\t Average training accuracy 0.8538\n",
      "Epoch [11]\t Average validation loss 0.7771\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8117\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8238\t Accuracy 0.8598\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8285\t Accuracy 0.8588\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8362\t Accuracy 0.8535\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8402\t Accuracy 0.8532\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8380\t Accuracy 0.8542\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8375\t Accuracy 0.8551\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8432\t Accuracy 0.8546\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8431\t Accuracy 0.8546\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8447\t Accuracy 0.8542\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8451\t Accuracy 0.8539\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8448\t Average training accuracy 0.8534\n",
      "Epoch [12]\t Average validation loss 0.7763\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8109\t Accuracy 0.8700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8231\t Accuracy 0.8590\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8278\t Accuracy 0.8584\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8356\t Accuracy 0.8532\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8395\t Accuracy 0.8531\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8373\t Accuracy 0.8540\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8368\t Accuracy 0.8550\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8425\t Accuracy 0.8545\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8424\t Accuracy 0.8544\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8439\t Accuracy 0.8541\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8444\t Accuracy 0.8537\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8441\t Average training accuracy 0.8532\n",
      "Epoch [13]\t Average validation loss 0.7756\t Average validation accuracy 0.8894\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8100\t Accuracy 0.8700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8224\t Accuracy 0.8584\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8271\t Accuracy 0.8584\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8349\t Accuracy 0.8534\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8388\t Accuracy 0.8529\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8366\t Accuracy 0.8536\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8361\t Accuracy 0.8546\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8418\t Accuracy 0.8542\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8417\t Accuracy 0.8541\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8433\t Accuracy 0.8538\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8437\t Accuracy 0.8535\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8434\t Average training accuracy 0.8530\n",
      "Epoch [14]\t Average validation loss 0.7748\t Average validation accuracy 0.8900\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8092\t Accuracy 0.8700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8217\t Accuracy 0.8582\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8265\t Accuracy 0.8581\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8343\t Accuracy 0.8532\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8382\t Accuracy 0.8524\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8359\t Accuracy 0.8532\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8355\t Accuracy 0.8543\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8411\t Accuracy 0.8539\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8410\t Accuracy 0.8537\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8426\t Accuracy 0.8534\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8430\t Accuracy 0.8532\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8427\t Average training accuracy 0.8527\n",
      "Epoch [15]\t Average validation loss 0.7741\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8084\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8211\t Accuracy 0.8580\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8259\t Accuracy 0.8579\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8336\t Accuracy 0.8531\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8376\t Accuracy 0.8523\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8353\t Accuracy 0.8532\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8349\t Accuracy 0.8542\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8405\t Accuracy 0.8538\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8404\t Accuracy 0.8536\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8420\t Accuracy 0.8533\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8424\t Accuracy 0.8530\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8421\t Average training accuracy 0.8525\n",
      "Epoch [16]\t Average validation loss 0.7734\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8077\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8205\t Accuracy 0.8578\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8253\t Accuracy 0.8578\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8331\t Accuracy 0.8528\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8370\t Accuracy 0.8521\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8347\t Accuracy 0.8531\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8343\t Accuracy 0.8540\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8399\t Accuracy 0.8535\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8398\t Accuracy 0.8533\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8413\t Accuracy 0.8530\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8417\t Accuracy 0.8528\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8415\t Average training accuracy 0.8524\n",
      "Epoch [17]\t Average validation loss 0.7728\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8070\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8199\t Accuracy 0.8578\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8247\t Accuracy 0.8579\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8325\t Accuracy 0.8525\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8364\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8342\t Accuracy 0.8529\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8337\t Accuracy 0.8538\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8393\t Accuracy 0.8532\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8392\t Accuracy 0.8531\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8408\t Accuracy 0.8529\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8412\t Accuracy 0.8526\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8409\t Average training accuracy 0.8523\n",
      "Epoch [18]\t Average validation loss 0.7722\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8063\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8193\t Accuracy 0.8580\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8242\t Accuracy 0.8577\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8320\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8359\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8336\t Accuracy 0.8525\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8331\t Accuracy 0.8534\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8388\t Accuracy 0.8529\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8387\t Accuracy 0.8527\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8402\t Accuracy 0.8525\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8406\t Accuracy 0.8522\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8404\t Average training accuracy 0.8519\n",
      "Epoch [19]\t Average validation loss 0.7716\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.5494\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9284\t Accuracy 0.1290\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4951\t Accuracy 0.2050\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.2040\t Accuracy 0.2472\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.0344\t Accuracy 0.2489\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.9371\t Accuracy 0.2308\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8704\t Accuracy 0.2099\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8188\t Accuracy 0.1923\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7764\t Accuracy 0.1825\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7423\t Accuracy 0.1748\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7139\t Accuracy 0.1701\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6908\t Average training accuracy 0.1663\n",
      "Epoch [0]\t Average validation loss 0.4493\t Average validation accuracy 0.0988\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4504\t Accuracy 0.0800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4488\t Accuracy 0.1225\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4476\t Accuracy 0.1386\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4462\t Accuracy 0.1435\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4452\t Accuracy 0.1599\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4440\t Accuracy 0.1667\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4430\t Accuracy 0.1771\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4421\t Accuracy 0.1905\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4409\t Accuracy 0.2020\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4398\t Accuracy 0.2111\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4388\t Accuracy 0.2204\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4380\t Average training accuracy 0.2283\n",
      "Epoch [1]\t Average validation loss 0.4290\t Average validation accuracy 0.1698\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.4299\t Accuracy 0.1800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4271\t Accuracy 0.3010\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4274\t Accuracy 0.2979\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4273\t Accuracy 0.2909\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4269\t Accuracy 0.2903\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4264\t Accuracy 0.2895\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4262\t Accuracy 0.2920\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4262\t Accuracy 0.2969\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4260\t Accuracy 0.2997\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4257\t Accuracy 0.3016\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4256\t Accuracy 0.3046\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4256\t Average training accuracy 0.3067\n",
      "Epoch [2]\t Average validation loss 0.4252\t Average validation accuracy 0.1916\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.4261\t Accuracy 0.2100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.4241\t Accuracy 0.3104\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.4247\t Accuracy 0.3046\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.4249\t Accuracy 0.2979\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.4248\t Accuracy 0.2964\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.4246\t Accuracy 0.2949\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.4246\t Accuracy 0.2968\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.4248\t Accuracy 0.3009\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.4247\t Accuracy 0.3037\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.4246\t Accuracy 0.3055\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.4246\t Accuracy 0.3083\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4246\t Average training accuracy 0.3101\n",
      "Epoch [3]\t Average validation loss 0.4249\t Average validation accuracy 0.1912\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.4258\t Accuracy 0.2100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.4238\t Accuracy 0.3118\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.4244\t Accuracy 0.3061\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.4247\t Accuracy 0.2991\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.4245\t Accuracy 0.2974\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.4243\t Accuracy 0.2962\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.4243\t Accuracy 0.2982\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.4245\t Accuracy 0.3024\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.4244\t Accuracy 0.3054\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.4242\t Accuracy 0.3071\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.4242\t Accuracy 0.3100\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4243\t Average training accuracy 0.3118\n",
      "Epoch [4]\t Average validation loss 0.4245\t Average validation accuracy 0.1928\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.4254\t Accuracy 0.2100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.4234\t Accuracy 0.3137\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.4241\t Accuracy 0.3080\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.4243\t Accuracy 0.3013\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.4242\t Accuracy 0.2991\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.4240\t Accuracy 0.2978\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.4240\t Accuracy 0.2997\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.4242\t Accuracy 0.3037\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.4241\t Accuracy 0.3068\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.4239\t Accuracy 0.3085\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.4239\t Accuracy 0.3113\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4240\t Average training accuracy 0.3130\n",
      "Epoch [5]\t Average validation loss 0.4242\t Average validation accuracy 0.1946\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.4252\t Accuracy 0.2100\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.4232\t Accuracy 0.3147\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.4239\t Accuracy 0.3096\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.4241\t Accuracy 0.3028\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.4240\t Accuracy 0.3005\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.4237\t Accuracy 0.2989\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.4238\t Accuracy 0.3010\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.4239\t Accuracy 0.3048\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.4238\t Accuracy 0.3079\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.4237\t Accuracy 0.3097\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.4237\t Accuracy 0.3125\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4238\t Average training accuracy 0.3142\n",
      "Epoch [6]\t Average validation loss 0.4240\t Average validation accuracy 0.1978\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.4250\t Accuracy 0.2100\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.4230\t Accuracy 0.3159\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.4237\t Accuracy 0.3110\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.4239\t Accuracy 0.3048\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.4238\t Accuracy 0.3019\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.4236\t Accuracy 0.3002\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.4236\t Accuracy 0.3021\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.4238\t Accuracy 0.3059\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.4237\t Accuracy 0.3089\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.4236\t Accuracy 0.3107\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.4236\t Accuracy 0.3135\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4236\t Average training accuracy 0.3151\n",
      "Epoch [7]\t Average validation loss 0.4239\t Average validation accuracy 0.2024\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.4249\t Accuracy 0.2100\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.4229\t Accuracy 0.3163\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.4236\t Accuracy 0.3116\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.4238\t Accuracy 0.3060\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.4237\t Accuracy 0.3028\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.4235\t Accuracy 0.3012\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.4235\t Accuracy 0.3029\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.4237\t Accuracy 0.3065\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.4236\t Accuracy 0.3095\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.4235\t Accuracy 0.3112\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.4235\t Accuracy 0.3139\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4235\t Average training accuracy 0.3156\n",
      "Epoch [8]\t Average validation loss 0.4238\t Average validation accuracy 0.2084\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.4248\t Accuracy 0.2200\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.4228\t Accuracy 0.3169\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.4235\t Accuracy 0.3128\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.4238\t Accuracy 0.3071\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.4236\t Accuracy 0.3041\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.4234\t Accuracy 0.3022\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.4235\t Accuracy 0.3036\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.4236\t Accuracy 0.3073\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.4235\t Accuracy 0.3101\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.4234\t Accuracy 0.3119\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.4234\t Accuracy 0.3146\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4235\t Average training accuracy 0.3162\n",
      "Epoch [9]\t Average validation loss 0.4238\t Average validation accuracy 0.2152\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.4247\t Accuracy 0.2200\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.4228\t Accuracy 0.3176\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.4235\t Accuracy 0.3137\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.4237\t Accuracy 0.3080\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.4236\t Accuracy 0.3052\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.4234\t Accuracy 0.3032\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.4234\t Accuracy 0.3045\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.4236\t Accuracy 0.3079\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.4235\t Accuracy 0.3105\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.4233\t Accuracy 0.3124\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.4234\t Accuracy 0.3149\n",
      "\n",
      "Epoch [10]\t Average training loss 0.4234\t Average training accuracy 0.3165\n",
      "Epoch [10]\t Average validation loss 0.4237\t Average validation accuracy 0.2206\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.4246\t Accuracy 0.2200\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.4227\t Accuracy 0.3175\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.4234\t Accuracy 0.3137\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.4237\t Accuracy 0.3083\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.4235\t Accuracy 0.3052\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.4233\t Accuracy 0.3034\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.4234\t Accuracy 0.3047\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.4235\t Accuracy 0.3081\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.4234\t Accuracy 0.3107\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.4233\t Accuracy 0.3125\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.4233\t Accuracy 0.3150\n",
      "\n",
      "Epoch [11]\t Average training loss 0.4234\t Average training accuracy 0.3165\n",
      "Epoch [11]\t Average validation loss 0.4236\t Average validation accuracy 0.2276\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.4246\t Accuracy 0.2400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.4227\t Accuracy 0.3186\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.4234\t Accuracy 0.3143\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.4237\t Accuracy 0.3099\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.4235\t Accuracy 0.3065\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.4233\t Accuracy 0.3044\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.4233\t Accuracy 0.3055\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.4235\t Accuracy 0.3086\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.4234\t Accuracy 0.3109\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.4233\t Accuracy 0.3129\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.4233\t Accuracy 0.3154\n",
      "\n",
      "Epoch [12]\t Average training loss 0.4233\t Average training accuracy 0.3167\n",
      "Epoch [12]\t Average validation loss 0.4236\t Average validation accuracy 0.2344\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.4245\t Accuracy 0.2500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.4227\t Accuracy 0.3192\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.4234\t Accuracy 0.3144\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.4236\t Accuracy 0.3104\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.4235\t Accuracy 0.3072\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.4233\t Accuracy 0.3048\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.4233\t Accuracy 0.3056\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.4235\t Accuracy 0.3088\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.4233\t Accuracy 0.3109\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.4232\t Accuracy 0.3131\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.4233\t Accuracy 0.3154\n",
      "\n",
      "Epoch [13]\t Average training loss 0.4233\t Average training accuracy 0.3167\n",
      "Epoch [13]\t Average validation loss 0.4235\t Average validation accuracy 0.2430\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.4245\t Accuracy 0.2600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.4226\t Accuracy 0.3210\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.4234\t Accuracy 0.3149\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.4236\t Accuracy 0.3114\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.4235\t Accuracy 0.3080\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.4232\t Accuracy 0.3055\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.4233\t Accuracy 0.3063\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.4234\t Accuracy 0.3095\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.4233\t Accuracy 0.3115\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.4232\t Accuracy 0.3136\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.4232\t Accuracy 0.3159\n",
      "\n",
      "Epoch [14]\t Average training loss 0.4233\t Average training accuracy 0.3171\n",
      "Epoch [14]\t Average validation loss 0.4235\t Average validation accuracy 0.2536\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.4244\t Accuracy 0.3100\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.4226\t Accuracy 0.3225\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.4233\t Accuracy 0.3159\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.4236\t Accuracy 0.3136\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.4234\t Accuracy 0.3097\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.4232\t Accuracy 0.3067\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.4233\t Accuracy 0.3075\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.4234\t Accuracy 0.3106\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.4233\t Accuracy 0.3125\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.4232\t Accuracy 0.3147\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.4232\t Accuracy 0.3168\n",
      "\n",
      "Epoch [15]\t Average training loss 0.4232\t Average training accuracy 0.3177\n",
      "Epoch [15]\t Average validation loss 0.4234\t Average validation accuracy 0.2610\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.4243\t Accuracy 0.3100\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.4226\t Accuracy 0.3224\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.4233\t Accuracy 0.3162\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.4235\t Accuracy 0.3143\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.4234\t Accuracy 0.3100\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.4232\t Accuracy 0.3074\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.4232\t Accuracy 0.3081\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.4234\t Accuracy 0.3114\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.4232\t Accuracy 0.3130\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.4231\t Accuracy 0.3153\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.4232\t Accuracy 0.3173\n",
      "\n",
      "Epoch [16]\t Average training loss 0.4232\t Average training accuracy 0.3183\n",
      "Epoch [16]\t Average validation loss 0.4234\t Average validation accuracy 0.2672\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.4243\t Accuracy 0.3200\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.4225\t Accuracy 0.3224\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.4233\t Accuracy 0.3162\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.4235\t Accuracy 0.3151\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.4234\t Accuracy 0.3107\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.4232\t Accuracy 0.3082\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.4232\t Accuracy 0.3092\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.4233\t Accuracy 0.3123\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.4232\t Accuracy 0.3137\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.4231\t Accuracy 0.3160\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.4231\t Accuracy 0.3180\n",
      "\n",
      "Epoch [17]\t Average training loss 0.4232\t Average training accuracy 0.3190\n",
      "Epoch [17]\t Average validation loss 0.4233\t Average validation accuracy 0.2760\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.4242\t Accuracy 0.3200\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.4225\t Accuracy 0.3247\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.4233\t Accuracy 0.3178\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.4235\t Accuracy 0.3170\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.4233\t Accuracy 0.3124\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.4231\t Accuracy 0.3098\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.4232\t Accuracy 0.3104\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.4233\t Accuracy 0.3132\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.4232\t Accuracy 0.3144\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.4231\t Accuracy 0.3168\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.4231\t Accuracy 0.3188\n",
      "\n",
      "Epoch [18]\t Average training loss 0.4231\t Average training accuracy 0.3197\n",
      "Epoch [18]\t Average validation loss 0.4233\t Average validation accuracy 0.2868\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.4241\t Accuracy 0.3200\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.4225\t Accuracy 0.3251\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.4232\t Accuracy 0.3186\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.4235\t Accuracy 0.3180\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.4233\t Accuracy 0.3133\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.4231\t Accuracy 0.3104\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.4231\t Accuracy 0.3111\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.4233\t Accuracy 0.3136\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.4231\t Accuracy 0.3147\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.4230\t Accuracy 0.3171\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.4231\t Accuracy 0.3191\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4231\t Average training accuracy 0.3199\n",
      "Epoch [19]\t Average validation loss 0.4232\t Average validation accuracy 0.2958\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.2548\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2365\t Accuracy 0.2669\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8220\t Accuracy 0.3479\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6730\t Accuracy 0.4046\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5881\t Accuracy 0.4447\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5314\t Accuracy 0.4855\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4908\t Accuracy 0.5138\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4605\t Accuracy 0.5432\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4357\t Accuracy 0.5661\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4153\t Accuracy 0.5848\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3980\t Accuracy 0.6025\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3837\t Average training accuracy 0.6187\n",
      "Epoch [0]\t Average validation loss 0.2263\t Average validation accuracy 0.8274\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2211\t Accuracy 0.8200\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2346\t Accuracy 0.7771\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2333\t Accuracy 0.7830\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2346\t Accuracy 0.7781\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2349\t Accuracy 0.7799\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2341\t Accuracy 0.7831\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2337\t Accuracy 0.7855\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2344\t Accuracy 0.7892\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2341\t Accuracy 0.7908\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2341\t Accuracy 0.7922\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2338\t Accuracy 0.7930\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2333\t Average training accuracy 0.7925\n",
      "Epoch [1]\t Average validation loss 0.2149\t Average validation accuracy 0.8402\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2102\t Accuracy 0.8100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.2236\t Accuracy 0.7990\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2233\t Accuracy 0.7966\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2253\t Accuracy 0.7948\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2263\t Accuracy 0.7966\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2261\t Accuracy 0.7988\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2262\t Accuracy 0.8015\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2272\t Accuracy 0.8045\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2272\t Accuracy 0.8061\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2274\t Accuracy 0.8068\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2274\t Accuracy 0.8065\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2271\t Average training accuracy 0.8058\n",
      "Epoch [2]\t Average validation loss 0.2111\t Average validation accuracy 0.8496\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2062\t Accuracy 0.8300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2201\t Accuracy 0.8094\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2201\t Accuracy 0.8060\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2222\t Accuracy 0.8026\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2232\t Accuracy 0.8033\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2231\t Accuracy 0.8051\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2233\t Accuracy 0.8069\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2245\t Accuracy 0.8094\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2246\t Accuracy 0.8107\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2249\t Accuracy 0.8111\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2250\t Accuracy 0.8105\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2248\t Average training accuracy 0.8097\n",
      "Epoch [3]\t Average validation loss 0.2096\t Average validation accuracy 0.8516\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2044\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2188\t Accuracy 0.8120\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2189\t Accuracy 0.8079\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2210\t Accuracy 0.8041\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2220\t Accuracy 0.8049\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2219\t Accuracy 0.8067\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2221\t Accuracy 0.8084\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2234\t Accuracy 0.8107\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2235\t Accuracy 0.8118\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2239\t Accuracy 0.8122\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2240\t Accuracy 0.8116\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2238\t Average training accuracy 0.8108\n",
      "Epoch [4]\t Average validation loss 0.2088\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2036\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2182\t Accuracy 0.8118\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2183\t Accuracy 0.8083\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2204\t Accuracy 0.8045\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2215\t Accuracy 0.8052\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2214\t Accuracy 0.8070\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2215\t Accuracy 0.8088\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2228\t Accuracy 0.8112\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2229\t Accuracy 0.8124\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2233\t Accuracy 0.8127\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2234\t Accuracy 0.8121\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2233\t Average training accuracy 0.8113\n",
      "Epoch [5]\t Average validation loss 0.2084\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2031\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2178\t Accuracy 0.8104\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2179\t Accuracy 0.8077\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2200\t Accuracy 0.8045\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2211\t Accuracy 0.8052\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2210\t Accuracy 0.8072\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2212\t Accuracy 0.8090\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2224\t Accuracy 0.8112\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2226\t Accuracy 0.8124\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2230\t Accuracy 0.8128\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2231\t Accuracy 0.8122\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2229\t Average training accuracy 0.8115\n",
      "Epoch [6]\t Average validation loss 0.2081\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2027\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2175\t Accuracy 0.8102\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2176\t Accuracy 0.8076\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2197\t Accuracy 0.8047\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2208\t Accuracy 0.8054\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2207\t Accuracy 0.8073\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2209\t Accuracy 0.8092\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2221\t Accuracy 0.8115\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2223\t Accuracy 0.8127\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2227\t Accuracy 0.8132\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2228\t Accuracy 0.8126\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2227\t Average training accuracy 0.8120\n",
      "Epoch [7]\t Average validation loss 0.2079\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2025\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2173\t Accuracy 0.8102\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2174\t Accuracy 0.8076\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2196\t Accuracy 0.8050\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2206\t Accuracy 0.8057\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2205\t Accuracy 0.8076\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2207\t Accuracy 0.8096\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2220\t Accuracy 0.8117\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2221\t Accuracy 0.8130\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2226\t Accuracy 0.8133\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2227\t Accuracy 0.8128\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2225\t Average training accuracy 0.8122\n",
      "Epoch [8]\t Average validation loss 0.2077\t Average validation accuracy 0.8528\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2023\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2172\t Accuracy 0.8106\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2173\t Accuracy 0.8079\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2194\t Accuracy 0.8052\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2205\t Accuracy 0.8060\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2204\t Accuracy 0.8076\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2206\t Accuracy 0.8096\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2219\t Accuracy 0.8117\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2220\t Accuracy 0.8131\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2225\t Accuracy 0.8134\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2226\t Accuracy 0.8129\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2224\t Average training accuracy 0.8123\n",
      "Epoch [9]\t Average validation loss 0.2076\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2022\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2172\t Accuracy 0.8110\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2173\t Accuracy 0.8079\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2194\t Accuracy 0.8054\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2204\t Accuracy 0.8060\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2203\t Accuracy 0.8078\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2205\t Accuracy 0.8097\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2218\t Accuracy 0.8119\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2219\t Accuracy 0.8132\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2224\t Accuracy 0.8136\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2225\t Accuracy 0.8131\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2223\t Average training accuracy 0.8124\n",
      "Epoch [10]\t Average validation loss 0.2075\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2021\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2171\t Accuracy 0.8114\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2172\t Accuracy 0.8081\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2193\t Accuracy 0.8056\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2203\t Accuracy 0.8062\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2203\t Accuracy 0.8081\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2204\t Accuracy 0.8099\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2217\t Accuracy 0.8121\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2218\t Accuracy 0.8133\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2223\t Accuracy 0.8137\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2224\t Accuracy 0.8132\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2223\t Average training accuracy 0.8126\n",
      "Epoch [11]\t Average validation loss 0.2075\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2020\t Accuracy 0.8300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2170\t Accuracy 0.8110\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2171\t Accuracy 0.8076\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2192\t Accuracy 0.8054\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2203\t Accuracy 0.8061\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2202\t Accuracy 0.8081\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2204\t Accuracy 0.8098\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2216\t Accuracy 0.8119\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2218\t Accuracy 0.8131\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2222\t Accuracy 0.8136\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2223\t Accuracy 0.8131\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2222\t Average training accuracy 0.8125\n",
      "Epoch [12]\t Average validation loss 0.2074\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2020\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2170\t Accuracy 0.8116\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2171\t Accuracy 0.8081\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2192\t Accuracy 0.8058\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2202\t Accuracy 0.8063\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2201\t Accuracy 0.8082\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2203\t Accuracy 0.8100\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2216\t Accuracy 0.8121\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2217\t Accuracy 0.8133\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2221\t Accuracy 0.8137\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2223\t Accuracy 0.8132\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2221\t Average training accuracy 0.8126\n",
      "Epoch [13]\t Average validation loss 0.2074\t Average validation accuracy 0.8518\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8114\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2171\t Accuracy 0.8079\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2192\t Accuracy 0.8058\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2202\t Accuracy 0.8064\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2201\t Accuracy 0.8084\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2203\t Accuracy 0.8101\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2215\t Accuracy 0.8122\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2217\t Accuracy 0.8134\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2221\t Accuracy 0.8138\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2222\t Accuracy 0.8133\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2221\t Average training accuracy 0.8127\n",
      "Epoch [14]\t Average validation loss 0.2074\t Average validation accuracy 0.8520\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8114\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8079\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2191\t Accuracy 0.8058\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2202\t Accuracy 0.8064\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2201\t Accuracy 0.8084\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2202\t Accuracy 0.8101\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2215\t Accuracy 0.8123\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2216\t Accuracy 0.8135\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2221\t Accuracy 0.8139\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2222\t Accuracy 0.8135\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2220\t Average training accuracy 0.8128\n",
      "Epoch [15]\t Average validation loss 0.2073\t Average validation accuracy 0.8516\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8110\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8078\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2191\t Accuracy 0.8057\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2201\t Accuracy 0.8063\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8084\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2202\t Accuracy 0.8102\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2214\t Accuracy 0.8124\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2216\t Accuracy 0.8136\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2220\t Accuracy 0.8140\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2221\t Accuracy 0.8136\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2220\t Average training accuracy 0.8129\n",
      "Epoch [16]\t Average validation loss 0.2073\t Average validation accuracy 0.8516\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8110\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8078\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2191\t Accuracy 0.8058\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2201\t Accuracy 0.8066\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8086\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2202\t Accuracy 0.8103\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2214\t Accuracy 0.8125\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2215\t Accuracy 0.8136\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2220\t Accuracy 0.8141\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2221\t Accuracy 0.8136\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2220\t Average training accuracy 0.8130\n",
      "Epoch [17]\t Average validation loss 0.2073\t Average validation accuracy 0.8518\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8110\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8078\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2190\t Accuracy 0.8058\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2201\t Accuracy 0.8066\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8086\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2201\t Accuracy 0.8103\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2214\t Accuracy 0.8125\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2215\t Accuracy 0.8137\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2220\t Accuracy 0.8141\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2221\t Accuracy 0.8136\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2219\t Average training accuracy 0.8130\n",
      "Epoch [18]\t Average validation loss 0.2073\t Average validation accuracy 0.8520\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.8300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8106\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8077\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2190\t Accuracy 0.8057\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2200\t Accuracy 0.8066\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2199\t Accuracy 0.8087\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2201\t Accuracy 0.8104\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2213\t Accuracy 0.8126\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2215\t Accuracy 0.8138\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2219\t Accuracy 0.8142\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2220\t Accuracy 0.8137\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2219\t Average training accuracy 0.8131\n",
      "Epoch [19]\t Average validation loss 0.2072\t Average validation accuracy 0.8520\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.9824\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4299\t Accuracy 0.2276\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3240\t Accuracy 0.3263\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.1892\t Accuracy 0.3179\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.1252\t Accuracy 0.3682\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.1000\t Accuracy 0.4000\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.0873\t Accuracy 0.3832\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.0808\t Accuracy 0.3879\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.0748\t Accuracy 0.3922\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0722\t Accuracy 0.3871\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0703\t Accuracy 0.3906\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0690\t Average training accuracy 0.3898\n",
      "Epoch [0]\t Average validation loss 2.0408\t Average validation accuracy 0.3514\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.0285\t Accuracy 0.4300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0455\t Accuracy 0.4265\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0436\t Accuracy 0.4023\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0439\t Accuracy 0.4010\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.0444\t Accuracy 0.3925\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.0435\t Accuracy 0.3961\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.0427\t Accuracy 0.3963\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.0436\t Accuracy 0.3892\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.0433\t Accuracy 0.3969\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.0442\t Accuracy 0.3902\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.0448\t Accuracy 0.3913\n",
      "\n",
      "Epoch [1]\t Average training loss 2.0455\t Average training accuracy 0.3897\n",
      "Epoch [1]\t Average validation loss 2.0391\t Average validation accuracy 0.3248\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0185\t Accuracy 0.3800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.0421\t Accuracy 0.3959\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.0410\t Accuracy 0.3863\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.0414\t Accuracy 0.3820\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.0423\t Accuracy 0.3789\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.0418\t Accuracy 0.3786\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.0414\t Accuracy 0.3810\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.0426\t Accuracy 0.3747\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0425\t Accuracy 0.3827\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0436\t Accuracy 0.3771\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0443\t Accuracy 0.3783\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0450\t Average training accuracy 0.3771\n",
      "Epoch [2]\t Average validation loss 2.0399\t Average validation accuracy 0.3194\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0182\t Accuracy 0.3600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0425\t Accuracy 0.3898\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0414\t Accuracy 0.3815\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0417\t Accuracy 0.3774\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0425\t Accuracy 0.3742\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0419\t Accuracy 0.3738\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0415\t Accuracy 0.3764\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0426\t Accuracy 0.3702\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0425\t Accuracy 0.3782\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0435\t Accuracy 0.3729\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 2.0442\t Accuracy 0.3741\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0450\t Average training accuracy 0.3730\n",
      "Epoch [3]\t Average validation loss 2.0396\t Average validation accuracy 0.3174\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 2.0175\t Accuracy 0.3600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 2.0421\t Accuracy 0.3878\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 2.0409\t Accuracy 0.3790\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 2.0413\t Accuracy 0.3751\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 2.0421\t Accuracy 0.3720\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 2.0415\t Accuracy 0.3714\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 2.0411\t Accuracy 0.3742\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 2.0422\t Accuracy 0.3681\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 2.0421\t Accuracy 0.3762\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 2.0431\t Accuracy 0.3710\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 2.0437\t Accuracy 0.3722\n",
      "\n",
      "Epoch [4]\t Average training loss 2.0445\t Average training accuracy 0.3712\n",
      "Epoch [4]\t Average validation loss 2.0392\t Average validation accuracy 0.3166\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 2.0168\t Accuracy 0.3600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 2.0416\t Accuracy 0.3865\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 2.0405\t Accuracy 0.3779\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 2.0408\t Accuracy 0.3740\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 2.0416\t Accuracy 0.3710\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 2.0410\t Accuracy 0.3704\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 2.0406\t Accuracy 0.3732\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 2.0417\t Accuracy 0.3672\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 2.0416\t Accuracy 0.3753\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 2.0426\t Accuracy 0.3702\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 2.0433\t Accuracy 0.3713\n",
      "\n",
      "Epoch [5]\t Average training loss 2.0440\t Average training accuracy 0.3703\n",
      "Epoch [5]\t Average validation loss 2.0387\t Average validation accuracy 0.3160\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 2.0163\t Accuracy 0.3600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 2.0412\t Accuracy 0.3857\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 2.0400\t Accuracy 0.3772\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 2.0403\t Accuracy 0.3737\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 2.0411\t Accuracy 0.3707\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 2.0405\t Accuracy 0.3701\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 2.0401\t Accuracy 0.3729\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 2.0412\t Accuracy 0.3669\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 2.0411\t Accuracy 0.3749\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 2.0421\t Accuracy 0.3698\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 2.0428\t Accuracy 0.3709\n",
      "\n",
      "Epoch [6]\t Average training loss 2.0436\t Average training accuracy 0.3699\n",
      "Epoch [6]\t Average validation loss 2.0383\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 2.0157\t Accuracy 0.3600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 2.0407\t Accuracy 0.3853\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 2.0395\t Accuracy 0.3766\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 2.0399\t Accuracy 0.3728\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 2.0407\t Accuracy 0.3701\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 2.0401\t Accuracy 0.3695\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 2.0397\t Accuracy 0.3723\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 2.0408\t Accuracy 0.3664\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 2.0407\t Accuracy 0.3745\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 2.0417\t Accuracy 0.3695\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 2.0424\t Accuracy 0.3705\n",
      "\n",
      "Epoch [7]\t Average training loss 2.0432\t Average training accuracy 0.3695\n",
      "Epoch [7]\t Average validation loss 2.0380\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 2.0153\t Accuracy 0.3600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 2.0404\t Accuracy 0.3849\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 2.0392\t Accuracy 0.3765\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 2.0395\t Accuracy 0.3728\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 2.0403\t Accuracy 0.3701\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 2.0397\t Accuracy 0.3695\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 2.0393\t Accuracy 0.3723\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 2.0404\t Accuracy 0.3663\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 2.0403\t Accuracy 0.3745\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 2.0414\t Accuracy 0.3694\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 2.0420\t Accuracy 0.3704\n",
      "\n",
      "Epoch [8]\t Average training loss 2.0428\t Average training accuracy 0.3694\n",
      "Epoch [8]\t Average validation loss 2.0376\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 2.0149\t Accuracy 0.3600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 2.0400\t Accuracy 0.3847\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 2.0388\t Accuracy 0.3764\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 2.0392\t Accuracy 0.3726\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 2.0400\t Accuracy 0.3699\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 2.0394\t Accuracy 0.3693\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 2.0390\t Accuracy 0.3721\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 2.0401\t Accuracy 0.3662\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 2.0400\t Accuracy 0.3742\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 2.0411\t Accuracy 0.3692\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 2.0417\t Accuracy 0.3702\n",
      "\n",
      "Epoch [9]\t Average training loss 2.0425\t Average training accuracy 0.3691\n",
      "Epoch [9]\t Average validation loss 2.0373\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 2.0146\t Accuracy 0.3600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 2.0397\t Accuracy 0.3845\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 2.0385\t Accuracy 0.3762\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 2.0389\t Accuracy 0.3723\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 2.0397\t Accuracy 0.3698\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 2.0391\t Accuracy 0.3692\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 2.0387\t Accuracy 0.3721\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 2.0398\t Accuracy 0.3662\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 2.0397\t Accuracy 0.3743\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 2.0408\t Accuracy 0.3692\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 2.0414\t Accuracy 0.3702\n",
      "\n",
      "Epoch [10]\t Average training loss 2.0422\t Average training accuracy 0.3692\n",
      "Epoch [10]\t Average validation loss 2.0371\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 2.0143\t Accuracy 0.3600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 2.0395\t Accuracy 0.3847\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 2.0383\t Accuracy 0.3764\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 2.0386\t Accuracy 0.3725\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 2.0394\t Accuracy 0.3699\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 2.0389\t Accuracy 0.3694\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 2.0385\t Accuracy 0.3722\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 2.0396\t Accuracy 0.3662\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 2.0395\t Accuracy 0.3743\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 2.0405\t Accuracy 0.3693\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 2.0412\t Accuracy 0.3703\n",
      "\n",
      "Epoch [11]\t Average training loss 2.0420\t Average training accuracy 0.3693\n",
      "Epoch [11]\t Average validation loss 2.0369\t Average validation accuracy 0.3158\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 2.0140\t Accuracy 0.3600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 2.0392\t Accuracy 0.3843\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 2.0380\t Accuracy 0.3763\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 2.0384\t Accuracy 0.3724\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 2.0392\t Accuracy 0.3699\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 2.0386\t Accuracy 0.3692\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 2.0382\t Accuracy 0.3722\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 2.0394\t Accuracy 0.3662\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 2.0393\t Accuracy 0.3743\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 2.0403\t Accuracy 0.3693\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 2.0410\t Accuracy 0.3703\n",
      "\n",
      "Epoch [12]\t Average training loss 2.0418\t Average training accuracy 0.3693\n",
      "Epoch [12]\t Average validation loss 2.0367\t Average validation accuracy 0.3160\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 2.0138\t Accuracy 0.3600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 2.0390\t Accuracy 0.3841\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 2.0378\t Accuracy 0.3764\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 2.0382\t Accuracy 0.3724\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 2.0390\t Accuracy 0.3700\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 2.0384\t Accuracy 0.3693\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 2.0380\t Accuracy 0.3722\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 2.0392\t Accuracy 0.3662\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 2.0391\t Accuracy 0.3744\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 2.0401\t Accuracy 0.3694\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 2.0408\t Accuracy 0.3703\n",
      "\n",
      "Epoch [13]\t Average training loss 2.0416\t Average training accuracy 0.3693\n",
      "Epoch [13]\t Average validation loss 2.0365\t Average validation accuracy 0.3160\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 2.0135\t Accuracy 0.3600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 2.0388\t Accuracy 0.3841\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 2.0376\t Accuracy 0.3765\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 2.0380\t Accuracy 0.3721\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 2.0388\t Accuracy 0.3697\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 2.0383\t Accuracy 0.3691\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 2.0379\t Accuracy 0.3720\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 2.0390\t Accuracy 0.3660\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 2.0389\t Accuracy 0.3742\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 2.0399\t Accuracy 0.3692\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 2.0406\t Accuracy 0.3701\n",
      "\n",
      "Epoch [14]\t Average training loss 2.0414\t Average training accuracy 0.3692\n",
      "Epoch [14]\t Average validation loss 2.0363\t Average validation accuracy 0.3162\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 2.0133\t Accuracy 0.3600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 2.0387\t Accuracy 0.3843\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 2.0375\t Accuracy 0.3767\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 2.0378\t Accuracy 0.3723\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 2.0386\t Accuracy 0.3699\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 2.0381\t Accuracy 0.3690\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 2.0377\t Accuracy 0.3720\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 2.0388\t Accuracy 0.3660\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 2.0387\t Accuracy 0.3743\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 2.0398\t Accuracy 0.3693\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 2.0404\t Accuracy 0.3702\n",
      "\n",
      "Epoch [15]\t Average training loss 2.0412\t Average training accuracy 0.3692\n",
      "Epoch [15]\t Average validation loss 2.0361\t Average validation accuracy 0.3162\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 2.0132\t Accuracy 0.3600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 2.0385\t Accuracy 0.3849\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 2.0373\t Accuracy 0.3770\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 2.0376\t Accuracy 0.3723\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 2.0385\t Accuracy 0.3699\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 2.0379\t Accuracy 0.3690\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 2.0375\t Accuracy 0.3719\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 2.0386\t Accuracy 0.3660\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 2.0385\t Accuracy 0.3743\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 2.0396\t Accuracy 0.3693\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 2.0403\t Accuracy 0.3702\n",
      "\n",
      "Epoch [16]\t Average training loss 2.0411\t Average training accuracy 0.3693\n",
      "Epoch [16]\t Average validation loss 2.0360\t Average validation accuracy 0.3162\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 2.0130\t Accuracy 0.3600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 2.0383\t Accuracy 0.3845\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 2.0371\t Accuracy 0.3770\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 2.0375\t Accuracy 0.3724\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 2.0383\t Accuracy 0.3699\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 2.0378\t Accuracy 0.3690\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 2.0374\t Accuracy 0.3720\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 2.0385\t Accuracy 0.3660\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 2.0384\t Accuracy 0.3743\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 2.0395\t Accuracy 0.3693\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 2.0401\t Accuracy 0.3702\n",
      "\n",
      "Epoch [17]\t Average training loss 2.0409\t Average training accuracy 0.3693\n",
      "Epoch [17]\t Average validation loss 2.0358\t Average validation accuracy 0.3164\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 2.0128\t Accuracy 0.3600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 2.0382\t Accuracy 0.3841\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 2.0370\t Accuracy 0.3768\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 2.0373\t Accuracy 0.3723\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 2.0382\t Accuracy 0.3699\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 2.0376\t Accuracy 0.3691\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 2.0372\t Accuracy 0.3721\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 2.0384\t Accuracy 0.3662\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 2.0383\t Accuracy 0.3744\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 2.0393\t Accuracy 0.3694\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 2.0400\t Accuracy 0.3702\n",
      "\n",
      "Epoch [18]\t Average training loss 2.0408\t Average training accuracy 0.3693\n",
      "Epoch [18]\t Average validation loss 2.0357\t Average validation accuracy 0.3164\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 2.0126\t Accuracy 0.3600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 2.0381\t Accuracy 0.3839\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 2.0369\t Accuracy 0.3767\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 2.0372\t Accuracy 0.3722\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 2.0381\t Accuracy 0.3698\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 2.0375\t Accuracy 0.3691\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 2.0371\t Accuracy 0.3722\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 2.0382\t Accuracy 0.3663\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 2.0381\t Accuracy 0.3746\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 2.0392\t Accuracy 0.3695\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 2.0399\t Accuracy 0.3704\n",
      "\n",
      "Epoch [19]\t Average training loss 2.0407\t Average training accuracy 0.3695\n",
      "Epoch [19]\t Average validation loss 2.0356\t Average validation accuracy 0.3164\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7321\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.1494\t Accuracy 0.2937\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.6400\t Accuracy 0.5137\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.3682\t Accuracy 0.6056\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.2236\t Accuracy 0.6576\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.1461\t Accuracy 0.6915\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.1021\t Accuracy 0.7130\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.0770\t Accuracy 0.7283\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.0540\t Accuracy 0.7421\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.0358\t Accuracy 0.7528\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.0186\t Accuracy 0.7599\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0034\t Average training accuracy 0.7661\n",
      "Epoch [0]\t Average validation loss 0.7804\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8183\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8300\t Accuracy 0.8516\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8383\t Accuracy 0.8444\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8499\t Accuracy 0.8413\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8559\t Accuracy 0.8428\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8447\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8500\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8442\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8534\t Accuracy 0.8438\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8548\t Accuracy 0.8432\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8552\t Accuracy 0.8416\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8551\t Average training accuracy 0.8403\n",
      "Epoch [1]\t Average validation loss 0.7828\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8202\t Accuracy 0.8800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8309\t Accuracy 0.8510\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8351\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8436\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8441\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8420\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8463\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8463\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8485\t Accuracy 0.8417\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8485\t Average training accuracy 0.8406\n",
      "Epoch [2]\t Average validation loss 0.7777\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8138\t Accuracy 0.8700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8260\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8311\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8399\t Accuracy 0.8409\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8443\t Accuracy 0.8421\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8405\t Accuracy 0.8441\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8384\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8428\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8428\t Accuracy 0.8430\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8447\t Accuracy 0.8423\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8453\t Accuracy 0.8407\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8454\t Average training accuracy 0.8396\n",
      "Epoch [3]\t Average validation loss 0.7750\t Average validation accuracy 0.8862\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8094\t Accuracy 0.8700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8233\t Accuracy 0.8488\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8288\t Accuracy 0.8431\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8377\t Accuracy 0.8399\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8421\t Accuracy 0.8410\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8383\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8363\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8406\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8407\t Accuracy 0.8423\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8426\t Accuracy 0.8418\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8432\t Accuracy 0.8402\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8434\t Average training accuracy 0.8391\n",
      "Epoch [4]\t Average validation loss 0.7730\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8061\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8215\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8274\t Accuracy 0.8415\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8363\t Accuracy 0.8385\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8407\t Accuracy 0.8395\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8369\t Accuracy 0.8424\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8349\t Accuracy 0.8427\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8392\t Accuracy 0.8417\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8394\t Accuracy 0.8416\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8413\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8418\t Accuracy 0.8395\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8420\t Average training accuracy 0.8385\n",
      "Epoch [5]\t Average validation loss 0.7716\t Average validation accuracy 0.8864\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8035\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8201\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8263\t Accuracy 0.8404\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8352\t Accuracy 0.8374\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8397\t Accuracy 0.8384\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8359\t Accuracy 0.8413\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8338\t Accuracy 0.8418\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8382\t Accuracy 0.8410\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8383\t Accuracy 0.8409\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8402\t Accuracy 0.8405\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8408\t Accuracy 0.8390\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8409\t Average training accuracy 0.8381\n",
      "Epoch [6]\t Average validation loss 0.7705\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8016\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8191\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8254\t Accuracy 0.8397\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8343\t Accuracy 0.8360\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8389\t Accuracy 0.8372\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8351\t Accuracy 0.8399\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8331\t Accuracy 0.8410\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8374\t Accuracy 0.8403\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8375\t Accuracy 0.8403\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8394\t Accuracy 0.8401\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8399\t Accuracy 0.8386\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8400\t Average training accuracy 0.8377\n",
      "Epoch [7]\t Average validation loss 0.7696\t Average validation accuracy 0.8848\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8002\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8183\t Accuracy 0.8427\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8247\t Accuracy 0.8399\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8336\t Accuracy 0.8362\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8382\t Accuracy 0.8370\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8344\t Accuracy 0.8397\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8324\t Accuracy 0.8409\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8367\t Accuracy 0.8402\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8368\t Accuracy 0.8401\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8387\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8392\t Accuracy 0.8385\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8393\t Average training accuracy 0.8378\n",
      "Epoch [8]\t Average validation loss 0.7689\t Average validation accuracy 0.8842\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7991\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8177\t Accuracy 0.8414\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8241\t Accuracy 0.8393\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8330\t Accuracy 0.8355\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8377\t Accuracy 0.8364\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8339\t Accuracy 0.8392\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8319\t Accuracy 0.8403\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8362\t Accuracy 0.8397\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8363\t Accuracy 0.8397\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8381\t Accuracy 0.8396\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8387\t Accuracy 0.8382\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8387\t Average training accuracy 0.8374\n",
      "Epoch [9]\t Average validation loss 0.7682\t Average validation accuracy 0.8838\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7982\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8171\t Accuracy 0.8416\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8236\t Accuracy 0.8396\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8326\t Accuracy 0.8353\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8373\t Accuracy 0.8362\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8336\t Accuracy 0.8389\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8314\t Accuracy 0.8401\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8358\t Accuracy 0.8396\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8358\t Accuracy 0.8396\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8377\t Accuracy 0.8396\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8382\t Accuracy 0.8382\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8382\t Average training accuracy 0.8374\n",
      "Epoch [10]\t Average validation loss 0.7678\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7974\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8167\t Accuracy 0.8410\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8232\t Accuracy 0.8395\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8322\t Accuracy 0.8355\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8370\t Accuracy 0.8361\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8332\t Accuracy 0.8386\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8311\t Accuracy 0.8399\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8354\t Accuracy 0.8396\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8355\t Accuracy 0.8397\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8373\t Accuracy 0.8397\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8378\t Accuracy 0.8382\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8378\t Average training accuracy 0.8374\n",
      "Epoch [11]\t Average validation loss 0.7674\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7968\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8164\t Accuracy 0.8394\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8229\t Accuracy 0.8380\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8319\t Accuracy 0.8346\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8367\t Accuracy 0.8353\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8330\t Accuracy 0.8378\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8308\t Accuracy 0.8395\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8351\t Accuracy 0.8393\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8352\t Accuracy 0.8394\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8370\t Accuracy 0.8395\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8375\t Accuracy 0.8380\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8375\t Average training accuracy 0.8374\n",
      "Epoch [12]\t Average validation loss 0.7670\t Average validation accuracy 0.8830\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7962\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8160\t Accuracy 0.8382\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8226\t Accuracy 0.8369\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8316\t Accuracy 0.8334\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8364\t Accuracy 0.8345\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8327\t Accuracy 0.8370\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8306\t Accuracy 0.8390\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8349\t Accuracy 0.8389\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8349\t Accuracy 0.8392\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8367\t Accuracy 0.8393\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8373\t Accuracy 0.8379\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8372\t Average training accuracy 0.8373\n",
      "Epoch [13]\t Average validation loss 0.7667\t Average validation accuracy 0.8830\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7957\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8158\t Accuracy 0.8380\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8224\t Accuracy 0.8366\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8314\t Accuracy 0.8330\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8363\t Accuracy 0.8341\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8326\t Accuracy 0.8366\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8304\t Accuracy 0.8388\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8347\t Accuracy 0.8387\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8347\t Accuracy 0.8391\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8365\t Accuracy 0.8391\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8370\t Accuracy 0.8378\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8370\t Average training accuracy 0.8372\n",
      "Epoch [14]\t Average validation loss 0.7665\t Average validation accuracy 0.8824\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7953\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8157\t Accuracy 0.8373\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8222\t Accuracy 0.8365\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8312\t Accuracy 0.8329\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8361\t Accuracy 0.8339\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8324\t Accuracy 0.8363\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8302\t Accuracy 0.8386\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8345\t Accuracy 0.8386\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8345\t Accuracy 0.8390\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8363\t Accuracy 0.8391\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8369\t Accuracy 0.8377\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8368\t Average training accuracy 0.8372\n",
      "Epoch [15]\t Average validation loss 0.7662\t Average validation accuracy 0.8824\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7949\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8155\t Accuracy 0.8369\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8221\t Accuracy 0.8362\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8311\t Accuracy 0.8325\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8360\t Accuracy 0.8335\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8323\t Accuracy 0.8360\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8301\t Accuracy 0.8383\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8344\t Accuracy 0.8382\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8344\t Accuracy 0.8388\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8362\t Accuracy 0.8388\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8367\t Accuracy 0.8375\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8367\t Average training accuracy 0.8370\n",
      "Epoch [16]\t Average validation loss 0.7661\t Average validation accuracy 0.8820\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7946\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8155\t Accuracy 0.8365\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8220\t Accuracy 0.8357\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8310\t Accuracy 0.8323\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8359\t Accuracy 0.8332\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8322\t Accuracy 0.8356\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8300\t Accuracy 0.8381\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8343\t Accuracy 0.8380\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8343\t Accuracy 0.8385\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8361\t Accuracy 0.8385\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8366\t Accuracy 0.8372\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8365\t Average training accuracy 0.8367\n",
      "Epoch [17]\t Average validation loss 0.7659\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7943\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8154\t Accuracy 0.8359\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8219\t Accuracy 0.8354\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8309\t Accuracy 0.8319\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8358\t Accuracy 0.8329\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8322\t Accuracy 0.8352\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8299\t Accuracy 0.8377\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8342\t Accuracy 0.8377\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8342\t Accuracy 0.8383\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8360\t Accuracy 0.8383\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8365\t Accuracy 0.8370\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8364\t Average training accuracy 0.8366\n",
      "Epoch [18]\t Average validation loss 0.7657\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7941\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8153\t Accuracy 0.8361\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8218\t Accuracy 0.8356\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8308\t Accuracy 0.8319\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8358\t Accuracy 0.8328\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8321\t Accuracy 0.8351\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8299\t Accuracy 0.8377\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8341\t Accuracy 0.8376\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8341\t Accuracy 0.8382\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8359\t Accuracy 0.8383\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8364\t Accuracy 0.8370\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8363\t Average training accuracy 0.8366\n",
      "Epoch [19]\t Average validation loss 0.7655\t Average validation accuracy 0.8826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "for momentum in [0, 0.1, 0.55, 0.9, 0.99]:\n",
    "    #Euclidean+Sigmoid\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]   \n",
    "\n",
    "    #Euclidean+ReLU\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    reluMLP = Network()\n",
    "    # 使用FCLayer和ReLULayer构建多层感知机\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]     \n",
    "\n",
    "    #CrossEntropy+Sigmoid\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]         \n",
    "\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    #CrossEntropy+ReLU\n",
    "    reluMLP = Network()\n",
    "    t1=time.time()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>42.074909</td>\n",
       "      <td>[0.5193544682084369, 0.4167396770427404, 0.364...</td>\n",
       "      <td>[0.2218, 0.4354, 0.5536, 0.6236, 0.66980000000...</td>\n",
       "      <td>0.72049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41.935513</td>\n",
       "      <td>[0.6066053231156066, 0.43967822092945325, 0.36...</td>\n",
       "      <td>[0.4478000000000001, 0.573, 0.6629999999999999...</td>\n",
       "      <td>0.80681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>42.384914</td>\n",
       "      <td>[2.2419760239290825, 2.120830066790953, 2.0448...</td>\n",
       "      <td>[0.2084, 0.34559999999999996, 0.50940000000000...</td>\n",
       "      <td>0.66653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>52.771345</td>\n",
       "      <td>[1.7287515334418808, 1.316340443397868, 1.0911...</td>\n",
       "      <td>[0.5094, 0.7232000000000001, 0.796399999999999...</td>\n",
       "      <td>0.85185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>43.192641</td>\n",
       "      <td>[0.46882086734492245, 0.3824436766165054, 0.33...</td>\n",
       "      <td>[0.3148, 0.5144, 0.6312, 0.6958, 0.7334, 0.764...</td>\n",
       "      <td>0.75270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>46.562965</td>\n",
       "      <td>[0.6284309608096394, 0.43873470187546787, 0.35...</td>\n",
       "      <td>[0.39699999999999996, 0.5608, 0.66060000000000...</td>\n",
       "      <td>0.80322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85.173177</td>\n",
       "      <td>[2.2142067795608527, 2.110336441159032, 2.0385...</td>\n",
       "      <td>[0.24719999999999998, 0.45199999999999996, 0.5...</td>\n",
       "      <td>0.67096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.942119</td>\n",
       "      <td>[1.660839475040853, 1.3178145956194982, 1.1201...</td>\n",
       "      <td>[0.54, 0.7057999999999999, 0.7761999999999999,...</td>\n",
       "      <td>0.84501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>56.571428</td>\n",
       "      <td>[0.41495263560142653, 0.3354805097896369, 0.30...</td>\n",
       "      <td>[0.38800000000000007, 0.6302, 0.727, 0.7676, 0...</td>\n",
       "      <td>0.76113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>86.555884</td>\n",
       "      <td>[0.42031232663562174, 0.3000379899107795, 0.25...</td>\n",
       "      <td>[0.5559999999999999, 0.7240000000000001, 0.800...</td>\n",
       "      <td>0.84956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>66.109829</td>\n",
       "      <td>[2.11506918317018, 1.9859282926490542, 1.91620...</td>\n",
       "      <td>[0.41100000000000003, 0.6162, 0.69, 0.71920000...</td>\n",
       "      <td>0.66902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>81.992782</td>\n",
       "      <td>[1.3300812792308307, 0.9726849778962473, 0.834...</td>\n",
       "      <td>[0.7019999999999998, 0.8346000000000001, 0.868...</td>\n",
       "      <td>0.87942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>88.326414</td>\n",
       "      <td>[0.28946647591604463, 0.3108198537079458, 0.34...</td>\n",
       "      <td>[0.7834, 0.824, 0.8114, 0.7774, 0.738, 0.6992,...</td>\n",
       "      <td>0.60458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>86.308342</td>\n",
       "      <td>[0.21470329552861933, 0.1969618925224044, 0.20...</td>\n",
       "      <td>[0.857, 0.8826, 0.8789999999999999, 0.8758, 0....</td>\n",
       "      <td>0.86517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.614446</td>\n",
       "      <td>[1.8116696729294708, 1.8346117557317194, 1.891...</td>\n",
       "      <td>[0.7293999999999998, 0.7159999999999997, 0.680...</td>\n",
       "      <td>0.51885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.696518</td>\n",
       "      <td>[0.7498260145781994, 0.7434287487200432, 0.764...</td>\n",
       "      <td>[0.8854, 0.8966, 0.8987999999999999, 0.8952000...</td>\n",
       "      <td>0.89176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.974966</td>\n",
       "      <td>[0.44928882557053795, 0.4290476658730572, 0.42...</td>\n",
       "      <td>[0.09880000000000001, 0.16980000000000003, 0.1...</td>\n",
       "      <td>0.22143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>90.321124</td>\n",
       "      <td>[0.22631334871803388, 0.21492353997074543, 0.2...</td>\n",
       "      <td>[0.8274000000000001, 0.8402, 0.849599999999999...</td>\n",
       "      <td>0.85018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91.064870</td>\n",
       "      <td>[2.040807199367551, 2.0390936625070366, 2.0399...</td>\n",
       "      <td>[0.3514, 0.32480000000000003, 0.31940000000000...</td>\n",
       "      <td>0.31851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>103.714352</td>\n",
       "      <td>[0.7803713323824266, 0.7828212393407487, 0.777...</td>\n",
       "      <td>[0.8850000000000001, 0.887, 0.8868, 0.88620000...</td>\n",
       "      <td>0.88410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0      Euclidean_Sigmoid         100              0.001      0.00   \n",
       "1         Euclidean_ReLU         100              0.001      0.00   \n",
       "2   CrossEntropy_Sigmoid         100              0.001      0.00   \n",
       "3      CrossEntropy_ReLU         100              0.001      0.00   \n",
       "4      Euclidean_Sigmoid         100              0.001      0.10   \n",
       "5         Euclidean_ReLU         100              0.001      0.10   \n",
       "6   CrossEntropy_Sigmoid         100              0.001      0.10   \n",
       "7      CrossEntropy_ReLU         100              0.001      0.10   \n",
       "8      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "9         Euclidean_ReLU         100              0.001      0.55   \n",
       "10  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "11     CrossEntropy_ReLU         100              0.001      0.55   \n",
       "12     Euclidean_Sigmoid         100              0.001      0.90   \n",
       "13        Euclidean_ReLU         100              0.001      0.90   \n",
       "14  CrossEntropy_Sigmoid         100              0.001      0.90   \n",
       "15     CrossEntropy_ReLU         100              0.001      0.90   \n",
       "16     Euclidean_Sigmoid         100              0.001      0.99   \n",
       "17        Euclidean_ReLU         100              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         100              0.001      0.99   \n",
       "19     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "0            0.1   42.074909   \n",
       "1            0.1   41.935513   \n",
       "2            0.1   42.384914   \n",
       "3            0.1   52.771345   \n",
       "4            0.1   43.192641   \n",
       "5            0.1   46.562965   \n",
       "6            0.1   85.173177   \n",
       "7            0.1   63.942119   \n",
       "8            0.1   56.571428   \n",
       "9            0.1   86.555884   \n",
       "10           0.1   66.109829   \n",
       "11           0.1   81.992782   \n",
       "12           0.1   88.326414   \n",
       "13           0.1   86.308342   \n",
       "14           0.1   89.614446   \n",
       "15           0.1   89.696518   \n",
       "16           0.1   89.974966   \n",
       "17           0.1   90.321124   \n",
       "18           0.1   91.064870   \n",
       "19           0.1  103.714352   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "0   [0.5193544682084369, 0.4167396770427404, 0.364...   \n",
       "1   [0.6066053231156066, 0.43967822092945325, 0.36...   \n",
       "2   [2.2419760239290825, 2.120830066790953, 2.0448...   \n",
       "3   [1.7287515334418808, 1.316340443397868, 1.0911...   \n",
       "4   [0.46882086734492245, 0.3824436766165054, 0.33...   \n",
       "5   [0.6284309608096394, 0.43873470187546787, 0.35...   \n",
       "6   [2.2142067795608527, 2.110336441159032, 2.0385...   \n",
       "7   [1.660839475040853, 1.3178145956194982, 1.1201...   \n",
       "8   [0.41495263560142653, 0.3354805097896369, 0.30...   \n",
       "9   [0.42031232663562174, 0.3000379899107795, 0.25...   \n",
       "10  [2.11506918317018, 1.9859282926490542, 1.91620...   \n",
       "11  [1.3300812792308307, 0.9726849778962473, 0.834...   \n",
       "12  [0.28946647591604463, 0.3108198537079458, 0.34...   \n",
       "13  [0.21470329552861933, 0.1969618925224044, 0.20...   \n",
       "14  [1.8116696729294708, 1.8346117557317194, 1.891...   \n",
       "15  [0.7498260145781994, 0.7434287487200432, 0.764...   \n",
       "16  [0.44928882557053795, 0.4290476658730572, 0.42...   \n",
       "17  [0.22631334871803388, 0.21492353997074543, 0.2...   \n",
       "18  [2.040807199367551, 2.0390936625070366, 2.0399...   \n",
       "19  [0.7803713323824266, 0.7828212393407487, 0.777...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "0   [0.2218, 0.4354, 0.5536, 0.6236, 0.66980000000...             0.72049  \n",
       "1   [0.4478000000000001, 0.573, 0.6629999999999999...             0.80681  \n",
       "2   [0.2084, 0.34559999999999996, 0.50940000000000...             0.66653  \n",
       "3   [0.5094, 0.7232000000000001, 0.796399999999999...             0.85185  \n",
       "4   [0.3148, 0.5144, 0.6312, 0.6958, 0.7334, 0.764...             0.75270  \n",
       "5   [0.39699999999999996, 0.5608, 0.66060000000000...             0.80322  \n",
       "6   [0.24719999999999998, 0.45199999999999996, 0.5...             0.67096  \n",
       "7   [0.54, 0.7057999999999999, 0.7761999999999999,...             0.84501  \n",
       "8   [0.38800000000000007, 0.6302, 0.727, 0.7676, 0...             0.76113  \n",
       "9   [0.5559999999999999, 0.7240000000000001, 0.800...             0.84956  \n",
       "10  [0.41100000000000003, 0.6162, 0.69, 0.71920000...             0.66902  \n",
       "11  [0.7019999999999998, 0.8346000000000001, 0.868...             0.87942  \n",
       "12  [0.7834, 0.824, 0.8114, 0.7774, 0.738, 0.6992,...             0.60458  \n",
       "13  [0.857, 0.8826, 0.8789999999999999, 0.8758, 0....             0.86517  \n",
       "14  [0.7293999999999998, 0.7159999999999997, 0.680...             0.51885  \n",
       "15  [0.8854, 0.8966, 0.8987999999999999, 0.8952000...             0.89176  \n",
       "16  [0.09880000000000001, 0.16980000000000003, 0.1...             0.22143  \n",
       "17  [0.8274000000000001, 0.8402, 0.849599999999999...             0.85018  \n",
       "18  [0.3514, 0.32480000000000003, 0.31940000000000...             0.31851  \n",
       "19  [0.8850000000000001, 0.887, 0.8868, 0.88620000...             0.88410  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result['acc_validate_float'] = exec_result['acc_validate'].map(lambda x: np.average(x))\n",
    "exec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_index = exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "euclidean_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'].idxmax()\n",
    "euclidean_relu= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_relu= exec_result.loc[best_index]\n",
    "plot_loss_and_acc({'Euclidean+Sigmoid': [euclidean_sigmoid.loss_validate, euclidean_sigmoid.acc_validate],\n",
    "                   'Euclidean+ReLU': [euclidean_relu.loss_validate, euclidean_relu.acc_validate],\n",
    "                   'CrossEntropy+Sigmoid': [crossEntropy_sigmoid.loss_validate, crossEntropy_sigmoid.acc_validate],\n",
    "                   'CrossEntropy+ReLU': [crossEntropy_relu.loss_validate, crossEntropy_relu.acc_validate]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result.to_csv('./result/result_momentum.csv',index=False)\n",
    "# euclidean_sigmoid.momentum,euclidean_relu.momentum,crossEntropy_sigmoid.momentum,crossEntropy_relu.momentum\n",
    "# exec_result=pd.read_csv('./result_momentum.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>42.074909</td>\n",
       "      <td>[0.5193544682084369, 0.4167396770427404, 0.364...</td>\n",
       "      <td>[0.2218, 0.4354, 0.5536, 0.6236, 0.66980000000...</td>\n",
       "      <td>0.72049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41.935513</td>\n",
       "      <td>[0.6066053231156066, 0.43967822092945325, 0.36...</td>\n",
       "      <td>[0.4478000000000001, 0.573, 0.6629999999999999...</td>\n",
       "      <td>0.80681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>42.384914</td>\n",
       "      <td>[2.2419760239290825, 2.120830066790953, 2.0448...</td>\n",
       "      <td>[0.2084, 0.34559999999999996, 0.50940000000000...</td>\n",
       "      <td>0.66653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>52.771345</td>\n",
       "      <td>[1.7287515334418808, 1.316340443397868, 1.0911...</td>\n",
       "      <td>[0.5094, 0.7232000000000001, 0.796399999999999...</td>\n",
       "      <td>0.85185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>43.192641</td>\n",
       "      <td>[0.46882086734492245, 0.3824436766165054, 0.33...</td>\n",
       "      <td>[0.3148, 0.5144, 0.6312, 0.6958, 0.7334, 0.764...</td>\n",
       "      <td>0.75270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>46.562965</td>\n",
       "      <td>[0.6284309608096394, 0.43873470187546787, 0.35...</td>\n",
       "      <td>[0.39699999999999996, 0.5608, 0.66060000000000...</td>\n",
       "      <td>0.80322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85.173177</td>\n",
       "      <td>[2.2142067795608527, 2.110336441159032, 2.0385...</td>\n",
       "      <td>[0.24719999999999998, 0.45199999999999996, 0.5...</td>\n",
       "      <td>0.67096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.942119</td>\n",
       "      <td>[1.660839475040853, 1.3178145956194982, 1.1201...</td>\n",
       "      <td>[0.54, 0.7057999999999999, 0.7761999999999999,...</td>\n",
       "      <td>0.84501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>56.571428</td>\n",
       "      <td>[0.41495263560142653, 0.3354805097896369, 0.30...</td>\n",
       "      <td>[0.38800000000000007, 0.6302, 0.727, 0.7676, 0...</td>\n",
       "      <td>0.76113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>86.555884</td>\n",
       "      <td>[0.42031232663562174, 0.3000379899107795, 0.25...</td>\n",
       "      <td>[0.5559999999999999, 0.7240000000000001, 0.800...</td>\n",
       "      <td>0.84956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>66.109829</td>\n",
       "      <td>[2.11506918317018, 1.9859282926490542, 1.91620...</td>\n",
       "      <td>[0.41100000000000003, 0.6162, 0.69, 0.71920000...</td>\n",
       "      <td>0.66902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.1</td>\n",
       "      <td>81.992782</td>\n",
       "      <td>[1.3300812792308307, 0.9726849778962473, 0.834...</td>\n",
       "      <td>[0.7019999999999998, 0.8346000000000001, 0.868...</td>\n",
       "      <td>0.87942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>88.326414</td>\n",
       "      <td>[0.28946647591604463, 0.3108198537079458, 0.34...</td>\n",
       "      <td>[0.7834, 0.824, 0.8114, 0.7774, 0.738, 0.6992,...</td>\n",
       "      <td>0.60458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>86.308342</td>\n",
       "      <td>[0.21470329552861933, 0.1969618925224044, 0.20...</td>\n",
       "      <td>[0.857, 0.8826, 0.8789999999999999, 0.8758, 0....</td>\n",
       "      <td>0.86517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.614446</td>\n",
       "      <td>[1.8116696729294708, 1.8346117557317194, 1.891...</td>\n",
       "      <td>[0.7293999999999998, 0.7159999999999997, 0.680...</td>\n",
       "      <td>0.51885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.696518</td>\n",
       "      <td>[0.7498260145781994, 0.7434287487200432, 0.764...</td>\n",
       "      <td>[0.8854, 0.8966, 0.8987999999999999, 0.8952000...</td>\n",
       "      <td>0.89176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89.974966</td>\n",
       "      <td>[0.44928882557053795, 0.4290476658730572, 0.42...</td>\n",
       "      <td>[0.09880000000000001, 0.16980000000000003, 0.1...</td>\n",
       "      <td>0.22143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>90.321124</td>\n",
       "      <td>[0.22631334871803388, 0.21492353997074543, 0.2...</td>\n",
       "      <td>[0.8274000000000001, 0.8402, 0.849599999999999...</td>\n",
       "      <td>0.85018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91.064870</td>\n",
       "      <td>[2.040807199367551, 2.0390936625070366, 2.0399...</td>\n",
       "      <td>[0.3514, 0.32480000000000003, 0.31940000000000...</td>\n",
       "      <td>0.31851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.1</td>\n",
       "      <td>103.714352</td>\n",
       "      <td>[0.7803713323824266, 0.7828212393407487, 0.777...</td>\n",
       "      <td>[0.8850000000000001, 0.887, 0.8868, 0.88620000...</td>\n",
       "      <td>0.88410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0      Euclidean_Sigmoid         100              0.001      0.00   \n",
       "1         Euclidean_ReLU         100              0.001      0.00   \n",
       "2   CrossEntropy_Sigmoid         100              0.001      0.00   \n",
       "3      CrossEntropy_ReLU         100              0.001      0.00   \n",
       "4      Euclidean_Sigmoid         100              0.001      0.10   \n",
       "5         Euclidean_ReLU         100              0.001      0.10   \n",
       "6   CrossEntropy_Sigmoid         100              0.001      0.10   \n",
       "7      CrossEntropy_ReLU         100              0.001      0.10   \n",
       "8      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "9         Euclidean_ReLU         100              0.001      0.55   \n",
       "10  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "11     CrossEntropy_ReLU         100              0.001      0.55   \n",
       "12     Euclidean_Sigmoid         100              0.001      0.90   \n",
       "13        Euclidean_ReLU         100              0.001      0.90   \n",
       "14  CrossEntropy_Sigmoid         100              0.001      0.90   \n",
       "15     CrossEntropy_ReLU         100              0.001      0.90   \n",
       "16     Euclidean_Sigmoid         100              0.001      0.99   \n",
       "17        Euclidean_ReLU         100              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         100              0.001      0.99   \n",
       "19     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "0            0.1   42.074909   \n",
       "1            0.1   41.935513   \n",
       "2            0.1   42.384914   \n",
       "3            0.1   52.771345   \n",
       "4            0.1   43.192641   \n",
       "5            0.1   46.562965   \n",
       "6            0.1   85.173177   \n",
       "7            0.1   63.942119   \n",
       "8            0.1   56.571428   \n",
       "9            0.1   86.555884   \n",
       "10           0.1   66.109829   \n",
       "11           0.1   81.992782   \n",
       "12           0.1   88.326414   \n",
       "13           0.1   86.308342   \n",
       "14           0.1   89.614446   \n",
       "15           0.1   89.696518   \n",
       "16           0.1   89.974966   \n",
       "17           0.1   90.321124   \n",
       "18           0.1   91.064870   \n",
       "19           0.1  103.714352   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "0   [0.5193544682084369, 0.4167396770427404, 0.364...   \n",
       "1   [0.6066053231156066, 0.43967822092945325, 0.36...   \n",
       "2   [2.2419760239290825, 2.120830066790953, 2.0448...   \n",
       "3   [1.7287515334418808, 1.316340443397868, 1.0911...   \n",
       "4   [0.46882086734492245, 0.3824436766165054, 0.33...   \n",
       "5   [0.6284309608096394, 0.43873470187546787, 0.35...   \n",
       "6   [2.2142067795608527, 2.110336441159032, 2.0385...   \n",
       "7   [1.660839475040853, 1.3178145956194982, 1.1201...   \n",
       "8   [0.41495263560142653, 0.3354805097896369, 0.30...   \n",
       "9   [0.42031232663562174, 0.3000379899107795, 0.25...   \n",
       "10  [2.11506918317018, 1.9859282926490542, 1.91620...   \n",
       "11  [1.3300812792308307, 0.9726849778962473, 0.834...   \n",
       "12  [0.28946647591604463, 0.3108198537079458, 0.34...   \n",
       "13  [0.21470329552861933, 0.1969618925224044, 0.20...   \n",
       "14  [1.8116696729294708, 1.8346117557317194, 1.891...   \n",
       "15  [0.7498260145781994, 0.7434287487200432, 0.764...   \n",
       "16  [0.44928882557053795, 0.4290476658730572, 0.42...   \n",
       "17  [0.22631334871803388, 0.21492353997074543, 0.2...   \n",
       "18  [2.040807199367551, 2.0390936625070366, 2.0399...   \n",
       "19  [0.7803713323824266, 0.7828212393407487, 0.777...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "0   [0.2218, 0.4354, 0.5536, 0.6236, 0.66980000000...             0.72049  \n",
       "1   [0.4478000000000001, 0.573, 0.6629999999999999...             0.80681  \n",
       "2   [0.2084, 0.34559999999999996, 0.50940000000000...             0.66653  \n",
       "3   [0.5094, 0.7232000000000001, 0.796399999999999...             0.85185  \n",
       "4   [0.3148, 0.5144, 0.6312, 0.6958, 0.7334, 0.764...             0.75270  \n",
       "5   [0.39699999999999996, 0.5608, 0.66060000000000...             0.80322  \n",
       "6   [0.24719999999999998, 0.45199999999999996, 0.5...             0.67096  \n",
       "7   [0.54, 0.7057999999999999, 0.7761999999999999,...             0.84501  \n",
       "8   [0.38800000000000007, 0.6302, 0.727, 0.7676, 0...             0.76113  \n",
       "9   [0.5559999999999999, 0.7240000000000001, 0.800...             0.84956  \n",
       "10  [0.41100000000000003, 0.6162, 0.69, 0.71920000...             0.66902  \n",
       "11  [0.7019999999999998, 0.8346000000000001, 0.868...             0.87942  \n",
       "12  [0.7834, 0.824, 0.8114, 0.7774, 0.738, 0.6992,...             0.60458  \n",
       "13  [0.857, 0.8826, 0.8789999999999999, 0.8758, 0....             0.86517  \n",
       "14  [0.7293999999999998, 0.7159999999999997, 0.680...             0.51885  \n",
       "15  [0.8854, 0.8966, 0.8987999999999999, 0.8952000...             0.89176  \n",
       "16  [0.09880000000000001, 0.16980000000000003, 0.1...             0.22143  \n",
       "17  [0.8274000000000001, 0.8402, 0.849599999999999...             0.85018  \n",
       "18  [0.3514, 0.32480000000000003, 0.31940000000000...             0.31851  \n",
       "19  [0.8850000000000001, 0.887, 0.8868, 0.88620000...             0.88410  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f53642e0490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6), dpi=100)\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['momentum'],exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'], marker='s', label = 'Euclidean+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['momentum'],exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'], marker='s', label = 'Euclidean+ReLU')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['momentum'],exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['momentum'],exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+ReLU')\n",
    "plt.xlabel('momentum')\n",
    "plt.ylabel('acc_validate')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cf1e4799745e9ccc5e4a5d8e027719ef2e43f9255145d7e4068adaea12ce15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
