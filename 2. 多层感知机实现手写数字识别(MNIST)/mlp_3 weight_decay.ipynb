{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程，尝试不同的权重衰减率  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from criterion import EuclideanLossLayer,SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "exec_result = pd.DataFrame(columns=['mode','batch_size','learning_rate_SGD', 'momentum','weight_decay','time','loss_validate','acc_validate'])\n",
    "\n",
    "max_epoch = 20\n",
    "disp_freq = 50\n",
    "init_std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shaw/work/DL/hw2/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 7.7248\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.5896\t Accuracy 0.1167\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.1096\t Accuracy 0.1288\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.9348\t Accuracy 0.1365\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8381\t Accuracy 0.1469\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7734\t Accuracy 0.1605\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.7255\t Accuracy 0.1789\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6909\t Accuracy 0.1907\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6624\t Accuracy 0.2065\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6383\t Accuracy 0.2211\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6175\t Accuracy 0.2368\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5991\t Average training accuracy 0.2541\n",
      "Epoch [0]\t Average validation loss 0.3995\t Average validation accuracy 0.4716\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.3907\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4043\t Accuracy 0.4469\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.3980\t Accuracy 0.4686\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3944\t Accuracy 0.4803\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3903\t Accuracy 0.4910\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3853\t Accuracy 0.5017\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3806\t Accuracy 0.5137\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3785\t Accuracy 0.5195\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3755\t Accuracy 0.5276\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3727\t Accuracy 0.5358\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3701\t Accuracy 0.5419\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3670\t Average training accuracy 0.5494\n",
      "Epoch [1]\t Average validation loss 0.3262\t Average validation accuracy 0.6672\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3175\t Accuracy 0.6800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3338\t Accuracy 0.6325\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3312\t Accuracy 0.6408\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3310\t Accuracy 0.6413\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3298\t Accuracy 0.6441\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3276\t Accuracy 0.6484\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3257\t Accuracy 0.6529\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3257\t Accuracy 0.6549\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3247\t Accuracy 0.6583\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3238\t Accuracy 0.6616\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3230\t Accuracy 0.6635\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3217\t Average training accuracy 0.6675\n",
      "Epoch [2]\t Average validation loss 0.2992\t Average validation accuracy 0.7506\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2915\t Accuracy 0.7300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3074\t Accuracy 0.7067\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3062\t Accuracy 0.7129\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3071\t Accuracy 0.7111\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3069\t Accuracy 0.7120\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3058\t Accuracy 0.7141\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3049\t Accuracy 0.7168\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3057\t Accuracy 0.7168\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3053\t Accuracy 0.7182\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3052\t Accuracy 0.7201\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3050\t Accuracy 0.7208\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3044\t Average training accuracy 0.7233\n",
      "Epoch [3]\t Average validation loss 0.2894\t Average validation accuracy 0.7864\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2828\t Accuracy 0.7800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2976\t Accuracy 0.7420\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2970\t Accuracy 0.7485\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2984\t Accuracy 0.7457\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2987\t Accuracy 0.7453\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2981\t Accuracy 0.7455\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2977\t Accuracy 0.7475\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2988\t Accuracy 0.7468\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2987\t Accuracy 0.7479\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2989\t Accuracy 0.7490\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2990\t Accuracy 0.7492\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2988\t Average training accuracy 0.7511\n",
      "Epoch [4]\t Average validation loss 0.2878\t Average validation accuracy 0.8074\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2821\t Accuracy 0.7800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2957\t Accuracy 0.7610\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2955\t Accuracy 0.7653\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2970\t Accuracy 0.7628\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2975\t Accuracy 0.7629\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2972\t Accuracy 0.7625\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2971\t Accuracy 0.7635\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2983\t Accuracy 0.7625\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2984\t Accuracy 0.7631\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2988\t Accuracy 0.7640\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2991\t Accuracy 0.7636\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2990\t Average training accuracy 0.7649\n",
      "Epoch [5]\t Average validation loss 0.2905\t Average validation accuracy 0.8164\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2856\t Accuracy 0.7800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2980\t Accuracy 0.7720\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2980\t Accuracy 0.7748\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2995\t Accuracy 0.7711\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3002\t Accuracy 0.7710\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3001\t Accuracy 0.7707\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3001\t Accuracy 0.7715\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3014\t Accuracy 0.7701\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3016\t Accuracy 0.7714\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3021\t Accuracy 0.7721\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3025\t Accuracy 0.7715\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3025\t Average training accuracy 0.7726\n",
      "Epoch [6]\t Average validation loss 0.2957\t Average validation accuracy 0.8210\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2914\t Accuracy 0.7900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.3028\t Accuracy 0.7741\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.3028\t Accuracy 0.7767\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3044\t Accuracy 0.7728\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3051\t Accuracy 0.7727\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3052\t Accuracy 0.7729\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3053\t Accuracy 0.7737\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3065\t Accuracy 0.7725\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3068\t Accuracy 0.7740\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3073\t Accuracy 0.7745\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3077\t Accuracy 0.7738\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3079\t Average training accuracy 0.7748\n",
      "Epoch [7]\t Average validation loss 0.3023\t Average validation accuracy 0.8226\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2985\t Accuracy 0.8000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.3089\t Accuracy 0.7751\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.3090\t Accuracy 0.7770\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.3105\t Accuracy 0.7724\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.3113\t Accuracy 0.7723\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.3114\t Accuracy 0.7725\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.3115\t Accuracy 0.7735\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.3127\t Accuracy 0.7725\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.3130\t Accuracy 0.7737\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.3136\t Accuracy 0.7739\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.3140\t Accuracy 0.7735\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3142\t Average training accuracy 0.7743\n",
      "Epoch [8]\t Average validation loss 0.3096\t Average validation accuracy 0.8190\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.3062\t Accuracy 0.8100\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.3157\t Accuracy 0.7735\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.3159\t Accuracy 0.7746\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.3173\t Accuracy 0.7703\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.3181\t Accuracy 0.7705\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.3182\t Accuracy 0.7705\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.3184\t Accuracy 0.7709\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.3196\t Accuracy 0.7701\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.3199\t Accuracy 0.7713\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.3204\t Accuracy 0.7713\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.3209\t Accuracy 0.7711\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3211\t Average training accuracy 0.7717\n",
      "Epoch [9]\t Average validation loss 0.3172\t Average validation accuracy 0.8182\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.3140\t Accuracy 0.8100\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.3228\t Accuracy 0.7712\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.3229\t Accuracy 0.7714\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.3243\t Accuracy 0.7677\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.3251\t Accuracy 0.7675\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.3252\t Accuracy 0.7675\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.3254\t Accuracy 0.7679\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.3265\t Accuracy 0.7674\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.3268\t Accuracy 0.7685\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.3274\t Accuracy 0.7685\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.3278\t Accuracy 0.7682\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3280\t Average training accuracy 0.7687\n",
      "Epoch [10]\t Average validation loss 0.3247\t Average validation accuracy 0.8138\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.3218\t Accuracy 0.8100\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.3298\t Accuracy 0.7690\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.3300\t Accuracy 0.7677\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.3312\t Accuracy 0.7632\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.3320\t Accuracy 0.7631\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.3322\t Accuracy 0.7630\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.3324\t Accuracy 0.7638\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.3334\t Accuracy 0.7630\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.3337\t Accuracy 0.7641\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.3343\t Accuracy 0.7641\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.3347\t Accuracy 0.7635\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3349\t Average training accuracy 0.7640\n",
      "Epoch [11]\t Average validation loss 0.3320\t Average validation accuracy 0.8084\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.3293\t Accuracy 0.8000\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.3367\t Accuracy 0.7643\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.3369\t Accuracy 0.7618\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.3380\t Accuracy 0.7570\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.3388\t Accuracy 0.7574\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.3389\t Accuracy 0.7573\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.3391\t Accuracy 0.7581\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.3401\t Accuracy 0.7575\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.3404\t Accuracy 0.7584\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.3409\t Accuracy 0.7584\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.3413\t Accuracy 0.7579\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3415\t Average training accuracy 0.7586\n",
      "Epoch [12]\t Average validation loss 0.3389\t Average validation accuracy 0.8022\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.3363\t Accuracy 0.8000\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.3432\t Accuracy 0.7588\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.3434\t Accuracy 0.7552\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.3444\t Accuracy 0.7501\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.3451\t Accuracy 0.7507\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.3453\t Accuracy 0.7508\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.3455\t Accuracy 0.7516\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.3464\t Accuracy 0.7511\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.3467\t Accuracy 0.7523\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.3472\t Accuracy 0.7522\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.3475\t Accuracy 0.7518\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3477\t Average training accuracy 0.7523\n",
      "Epoch [13]\t Average validation loss 0.3454\t Average validation accuracy 0.7956\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.3429\t Accuracy 0.7900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.3493\t Accuracy 0.7512\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.3494\t Accuracy 0.7471\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.3504\t Accuracy 0.7419\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.3511\t Accuracy 0.7422\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.3513\t Accuracy 0.7427\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.3514\t Accuracy 0.7433\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.3523\t Accuracy 0.7429\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.3525\t Accuracy 0.7444\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.3530\t Accuracy 0.7444\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.3533\t Accuracy 0.7440\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3535\t Average training accuracy 0.7446\n",
      "Epoch [14]\t Average validation loss 0.3513\t Average validation accuracy 0.7884\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.3490\t Accuracy 0.7900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.3549\t Accuracy 0.7404\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.3551\t Accuracy 0.7383\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.3560\t Accuracy 0.7346\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.3567\t Accuracy 0.7342\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.3568\t Accuracy 0.7349\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.3569\t Accuracy 0.7357\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3578\t Accuracy 0.7354\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3580\t Accuracy 0.7370\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3584\t Accuracy 0.7368\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3587\t Accuracy 0.7364\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3589\t Average training accuracy 0.7370\n",
      "Epoch [15]\t Average validation loss 0.3569\t Average validation accuracy 0.7792\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.3545\t Accuracy 0.7900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.3601\t Accuracy 0.7329\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.3602\t Accuracy 0.7296\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3611\t Accuracy 0.7260\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3617\t Accuracy 0.7262\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3619\t Accuracy 0.7271\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3620\t Accuracy 0.7275\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3628\t Accuracy 0.7269\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3630\t Accuracy 0.7288\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3634\t Accuracy 0.7285\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3636\t Accuracy 0.7279\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3638\t Average training accuracy 0.7286\n",
      "Epoch [16]\t Average validation loss 0.3619\t Average validation accuracy 0.7712\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.3596\t Accuracy 0.7800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3649\t Accuracy 0.7251\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3650\t Accuracy 0.7200\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3658\t Accuracy 0.7156\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3664\t Accuracy 0.7152\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3665\t Accuracy 0.7167\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3666\t Accuracy 0.7172\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3673\t Accuracy 0.7165\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3675\t Accuracy 0.7181\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3679\t Accuracy 0.7180\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3681\t Accuracy 0.7177\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3683\t Average training accuracy 0.7185\n",
      "Epoch [17]\t Average validation loss 0.3664\t Average validation accuracy 0.7604\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.3642\t Accuracy 0.7700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3692\t Accuracy 0.7145\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3693\t Accuracy 0.7099\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3700\t Accuracy 0.7064\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3706\t Accuracy 0.7063\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3707\t Accuracy 0.7077\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3708\t Accuracy 0.7072\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3715\t Accuracy 0.7071\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3716\t Accuracy 0.7086\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3720\t Accuracy 0.7081\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3722\t Accuracy 0.7078\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3724\t Average training accuracy 0.7086\n",
      "Epoch [18]\t Average validation loss 0.3706\t Average validation accuracy 0.7514\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.3684\t Accuracy 0.7700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3731\t Accuracy 0.7027\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3732\t Accuracy 0.6994\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3739\t Accuracy 0.6967\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3744\t Accuracy 0.6963\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3745\t Accuracy 0.6981\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3746\t Accuracy 0.6972\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3752\t Accuracy 0.6966\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3754\t Accuracy 0.6986\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3757\t Accuracy 0.6977\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3759\t Accuracy 0.6972\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3761\t Average training accuracy 0.6982\n",
      "Epoch [19]\t Average validation loss 0.3743\t Average validation accuracy 0.7400\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.6274\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2610\t Accuracy 0.2575\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8275\t Accuracy 0.3911\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6684\t Accuracy 0.4309\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5807\t Accuracy 0.4875\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5237\t Accuracy 0.5197\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4824\t Accuracy 0.5501\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4517\t Accuracy 0.5749\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4263\t Accuracy 0.5980\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4056\t Accuracy 0.6171\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3883\t Accuracy 0.6319\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3743\t Average training accuracy 0.6455\n",
      "Epoch [0]\t Average validation loss 0.2202\t Average validation accuracy 0.8182\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2125\t Accuracy 0.8200\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2301\t Accuracy 0.7559\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2288\t Accuracy 0.7782\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2304\t Accuracy 0.7750\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2309\t Accuracy 0.7794\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2300\t Accuracy 0.7828\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2296\t Accuracy 0.7855\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2302\t Accuracy 0.7902\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2299\t Accuracy 0.7929\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2299\t Accuracy 0.7945\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2296\t Accuracy 0.7951\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2291\t Average training accuracy 0.7949\n",
      "Epoch [1]\t Average validation loss 0.2105\t Average validation accuracy 0.8440\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2050\t Accuracy 0.8400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.2198\t Accuracy 0.8065\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2196\t Accuracy 0.8045\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2218\t Accuracy 0.8023\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2229\t Accuracy 0.8035\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2227\t Accuracy 0.8051\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2229\t Accuracy 0.8067\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2241\t Accuracy 0.8094\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2241\t Accuracy 0.8106\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2245\t Accuracy 0.8109\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2246\t Accuracy 0.8103\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2244\t Average training accuracy 0.8095\n",
      "Epoch [2]\t Average validation loss 0.2089\t Average validation accuracy 0.8492\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2035\t Accuracy 0.8300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2185\t Accuracy 0.8094\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2186\t Accuracy 0.8072\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2208\t Accuracy 0.8047\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2219\t Accuracy 0.8057\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2218\t Accuracy 0.8073\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2219\t Accuracy 0.8087\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2232\t Accuracy 0.8113\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2233\t Accuracy 0.8123\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2237\t Accuracy 0.8124\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2237\t Accuracy 0.8119\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2236\t Average training accuracy 0.8110\n",
      "Epoch [3]\t Average validation loss 0.2084\t Average validation accuracy 0.8494\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2029\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2180\t Accuracy 0.8104\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2182\t Accuracy 0.8077\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2203\t Accuracy 0.8051\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2214\t Accuracy 0.8060\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2213\t Accuracy 0.8076\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2215\t Accuracy 0.8094\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2227\t Accuracy 0.8118\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2228\t Accuracy 0.8129\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2233\t Accuracy 0.8131\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2234\t Accuracy 0.8124\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2232\t Average training accuracy 0.8115\n",
      "Epoch [4]\t Average validation loss 0.2081\t Average validation accuracy 0.8502\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2026\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2178\t Accuracy 0.8108\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2179\t Accuracy 0.8085\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2200\t Accuracy 0.8060\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2211\t Accuracy 0.8067\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2210\t Accuracy 0.8083\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2212\t Accuracy 0.8100\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2225\t Accuracy 0.8123\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2226\t Accuracy 0.8134\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2230\t Accuracy 0.8136\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2231\t Accuracy 0.8130\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2230\t Average training accuracy 0.8121\n",
      "Epoch [5]\t Average validation loss 0.2079\t Average validation accuracy 0.8510\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2024\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2176\t Accuracy 0.8110\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2177\t Accuracy 0.8084\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2199\t Accuracy 0.8061\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2210\t Accuracy 0.8069\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2209\t Accuracy 0.8086\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2210\t Accuracy 0.8103\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2223\t Accuracy 0.8125\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2224\t Accuracy 0.8136\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2229\t Accuracy 0.8138\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2230\t Accuracy 0.8132\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2228\t Average training accuracy 0.8123\n",
      "Epoch [6]\t Average validation loss 0.2077\t Average validation accuracy 0.8518\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2022\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2174\t Accuracy 0.8116\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2176\t Accuracy 0.8092\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2197\t Accuracy 0.8070\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2208\t Accuracy 0.8077\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2207\t Accuracy 0.8091\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2209\t Accuracy 0.8108\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2221\t Accuracy 0.8129\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2223\t Accuracy 0.8140\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2227\t Accuracy 0.8142\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2228\t Accuracy 0.8136\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2227\t Average training accuracy 0.8127\n",
      "Epoch [7]\t Average validation loss 0.2076\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2021\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2173\t Accuracy 0.8118\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2175\t Accuracy 0.8096\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2196\t Accuracy 0.8076\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2207\t Accuracy 0.8082\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2206\t Accuracy 0.8096\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2207\t Accuracy 0.8111\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2220\t Accuracy 0.8132\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2221\t Accuracy 0.8143\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2226\t Accuracy 0.8145\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2227\t Accuracy 0.8139\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2225\t Average training accuracy 0.8130\n",
      "Epoch [8]\t Average validation loss 0.2075\t Average validation accuracy 0.8520\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2020\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2172\t Accuracy 0.8120\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2174\t Accuracy 0.8098\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2195\t Accuracy 0.8077\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2206\t Accuracy 0.8083\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2205\t Accuracy 0.8096\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2206\t Accuracy 0.8113\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2218\t Accuracy 0.8133\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2220\t Accuracy 0.8145\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2224\t Accuracy 0.8146\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2226\t Accuracy 0.8141\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2224\t Average training accuracy 0.8132\n",
      "Epoch [9]\t Average validation loss 0.2074\t Average validation accuracy 0.8526\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2018\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2171\t Accuracy 0.8118\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2173\t Accuracy 0.8098\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2194\t Accuracy 0.8079\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2204\t Accuracy 0.8087\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2204\t Accuracy 0.8100\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2205\t Accuracy 0.8117\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2217\t Accuracy 0.8136\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2219\t Accuracy 0.8149\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2223\t Accuracy 0.8149\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2224\t Accuracy 0.8144\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2223\t Average training accuracy 0.8136\n",
      "Epoch [10]\t Average validation loss 0.2073\t Average validation accuracy 0.8526\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2018\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2171\t Accuracy 0.8122\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2172\t Accuracy 0.8100\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2193\t Accuracy 0.8080\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2204\t Accuracy 0.8088\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2203\t Accuracy 0.8102\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2204\t Accuracy 0.8120\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2216\t Accuracy 0.8139\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2218\t Accuracy 0.8151\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2222\t Accuracy 0.8151\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2223\t Accuracy 0.8146\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2222\t Average training accuracy 0.8138\n",
      "Epoch [11]\t Average validation loss 0.2072\t Average validation accuracy 0.8528\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2017\t Accuracy 0.8300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.8131\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2171\t Accuracy 0.8108\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2192\t Accuracy 0.8084\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2202\t Accuracy 0.8092\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2202\t Accuracy 0.8107\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2203\t Accuracy 0.8124\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2215\t Accuracy 0.8143\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2216\t Accuracy 0.8155\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2221\t Accuracy 0.8155\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2222\t Accuracy 0.8149\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2221\t Average training accuracy 0.8143\n",
      "Epoch [12]\t Average validation loss 0.2071\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2015\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2168\t Accuracy 0.8131\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2170\t Accuracy 0.8106\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2191\t Accuracy 0.8083\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2201\t Accuracy 0.8090\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8107\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2202\t Accuracy 0.8123\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2214\t Accuracy 0.8142\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2215\t Accuracy 0.8154\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2220\t Accuracy 0.8155\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2221\t Accuracy 0.8149\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2219\t Average training accuracy 0.8143\n",
      "Epoch [13]\t Average validation loss 0.2070\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2014\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2167\t Accuracy 0.8131\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2169\t Accuracy 0.8105\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2190\t Accuracy 0.8081\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2200\t Accuracy 0.8091\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2200\t Accuracy 0.8107\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2201\t Accuracy 0.8123\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2213\t Accuracy 0.8141\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2214\t Accuracy 0.8154\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2219\t Accuracy 0.8156\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2220\t Accuracy 0.8150\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2219\t Average training accuracy 0.8144\n",
      "Epoch [14]\t Average validation loss 0.2069\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2014\t Accuracy 0.8300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2167\t Accuracy 0.8131\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2168\t Accuracy 0.8107\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2189\t Accuracy 0.8081\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2200\t Accuracy 0.8090\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2199\t Accuracy 0.8107\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2200\t Accuracy 0.8125\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2212\t Accuracy 0.8143\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2214\t Accuracy 0.8156\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2218\t Accuracy 0.8157\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2219\t Accuracy 0.8151\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2218\t Average training accuracy 0.8145\n",
      "Epoch [15]\t Average validation loss 0.2069\t Average validation accuracy 0.8526\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2014\t Accuracy 0.8300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2166\t Accuracy 0.8135\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2168\t Accuracy 0.8112\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2188\t Accuracy 0.8085\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2199\t Accuracy 0.8092\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2198\t Accuracy 0.8109\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2200\t Accuracy 0.8125\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2212\t Accuracy 0.8143\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2213\t Accuracy 0.8156\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2218\t Accuracy 0.8159\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2219\t Accuracy 0.8152\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2217\t Average training accuracy 0.8147\n",
      "Epoch [16]\t Average validation loss 0.2068\t Average validation accuracy 0.8526\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2013\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2165\t Accuracy 0.8133\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2167\t Accuracy 0.8112\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2188\t Accuracy 0.8085\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2198\t Accuracy 0.8091\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2198\t Accuracy 0.8108\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2199\t Accuracy 0.8125\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2211\t Accuracy 0.8143\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2213\t Accuracy 0.8157\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2217\t Accuracy 0.8159\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2218\t Accuracy 0.8153\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2217\t Average training accuracy 0.8148\n",
      "Epoch [17]\t Average validation loss 0.2068\t Average validation accuracy 0.8526\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2013\t Accuracy 0.8300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2165\t Accuracy 0.8127\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2166\t Accuracy 0.8107\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2187\t Accuracy 0.8081\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2198\t Accuracy 0.8089\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2197\t Accuracy 0.8107\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2198\t Accuracy 0.8124\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2211\t Accuracy 0.8143\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2212\t Accuracy 0.8156\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2217\t Accuracy 0.8159\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2218\t Accuracy 0.8153\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2216\t Average training accuracy 0.8147\n",
      "Epoch [18]\t Average validation loss 0.2068\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2013\t Accuracy 0.8300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2165\t Accuracy 0.8131\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2166\t Accuracy 0.8110\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2187\t Accuracy 0.8085\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2197\t Accuracy 0.8091\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2197\t Accuracy 0.8109\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2198\t Accuracy 0.8125\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2210\t Accuracy 0.8144\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2212\t Accuracy 0.8157\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2216\t Accuracy 0.8159\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2217\t Accuracy 0.8153\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2216\t Average training accuracy 0.8148\n",
      "Epoch [19]\t Average validation loss 0.2067\t Average validation accuracy 0.8528\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4143\t Accuracy 0.1600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.3905\t Accuracy 0.1180\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3470\t Accuracy 0.1357\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3196\t Accuracy 0.1520\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2973\t Accuracy 0.1664\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2790\t Accuracy 0.1806\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2628\t Accuracy 0.1948\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2486\t Accuracy 0.2119\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2352\t Accuracy 0.2306\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2221\t Accuracy 0.2490\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.2107\t Accuracy 0.2659\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2001\t Average training accuracy 0.2822\n",
      "Epoch [0]\t Average validation loss 2.0799\t Average validation accuracy 0.4580\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.0370\t Accuracy 0.6300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0734\t Accuracy 0.4855\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0639\t Accuracy 0.5012\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0599\t Accuracy 0.5044\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.0549\t Accuracy 0.5120\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.0491\t Accuracy 0.5182\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.0431\t Accuracy 0.5247\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.0392\t Accuracy 0.5291\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.0336\t Accuracy 0.5354\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.0284\t Accuracy 0.5415\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.0234\t Accuracy 0.5476\n",
      "\n",
      "Epoch [1]\t Average training loss 2.0185\t Average training accuracy 0.5531\n",
      "Epoch [1]\t Average validation loss 1.9528\t Average validation accuracy 0.6342\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.9165\t Accuracy 0.6800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.9551\t Accuracy 0.6233\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.9497\t Accuracy 0.6277\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.9496\t Accuracy 0.6217\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.9480\t Accuracy 0.6215\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.9450\t Accuracy 0.6211\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.9415\t Accuracy 0.6224\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.9409\t Accuracy 0.6229\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.9379\t Accuracy 0.6254\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.9355\t Accuracy 0.6273\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.9330\t Accuracy 0.6294\n",
      "\n",
      "Epoch [2]\t Average training loss 1.9303\t Average training accuracy 0.6319\n",
      "Epoch [2]\t Average validation loss 1.8854\t Average validation accuracy 0.6882\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.8553\t Accuracy 0.7000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.8927\t Accuracy 0.6633\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.8897\t Accuracy 0.6684\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.8916\t Accuracy 0.6598\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.8921\t Accuracy 0.6592\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.8905\t Accuracy 0.6575\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.8885\t Accuracy 0.6574\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.8897\t Accuracy 0.6568\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.8881\t Accuracy 0.6584\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.8874\t Accuracy 0.6595\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.8863\t Accuracy 0.6608\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8849\t Average training accuracy 0.6624\n",
      "Epoch [3]\t Average validation loss 1.8520\t Average validation accuracy 0.7142\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.8268\t Accuracy 0.7300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.8620\t Accuracy 0.6818\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.8605\t Accuracy 0.6852\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.8635\t Accuracy 0.6764\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.8651\t Accuracy 0.6747\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.8643\t Accuracy 0.6731\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.8632\t Accuracy 0.6731\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.8654\t Accuracy 0.6722\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.8647\t Accuracy 0.6732\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.8649\t Accuracy 0.6740\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.8646\t Accuracy 0.6750\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8640\t Average training accuracy 0.6762\n",
      "Epoch [4]\t Average validation loss 1.8384\t Average validation accuracy 0.7242\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.8168\t Accuracy 0.7400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.8498\t Accuracy 0.6914\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.8491\t Accuracy 0.6928\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.8526\t Accuracy 0.6839\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.8549\t Accuracy 0.6816\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.8546\t Accuracy 0.6800\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.8540\t Accuracy 0.6798\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.8567\t Accuracy 0.6781\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.8565\t Accuracy 0.6795\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.8574\t Accuracy 0.6798\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.8575\t Accuracy 0.6805\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8573\t Average training accuracy 0.6816\n",
      "Epoch [5]\t Average validation loss 1.8363\t Average validation accuracy 0.7270\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8174\t Accuracy 0.7400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.8483\t Accuracy 0.6924\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.8481\t Accuracy 0.6936\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.8519\t Accuracy 0.6852\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.8545\t Accuracy 0.6830\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.8546\t Accuracy 0.6822\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.8542\t Accuracy 0.6819\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.8572\t Accuracy 0.6801\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.8573\t Accuracy 0.6814\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.8585\t Accuracy 0.6815\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.8589\t Accuracy 0.6822\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8590\t Average training accuracy 0.6829\n",
      "Epoch [6]\t Average validation loss 1.8409\t Average validation accuracy 0.7258\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8241\t Accuracy 0.7300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.8529\t Accuracy 0.6912\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.8531\t Accuracy 0.6917\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.8570\t Accuracy 0.6840\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.8598\t Accuracy 0.6819\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.8600\t Accuracy 0.6810\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.8598\t Accuracy 0.6795\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.8629\t Accuracy 0.6780\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.8632\t Accuracy 0.6793\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.8645\t Accuracy 0.6791\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.8651\t Accuracy 0.6797\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8653\t Average training accuracy 0.6802\n",
      "Epoch [7]\t Average validation loss 1.8493\t Average validation accuracy 0.7238\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8339\t Accuracy 0.7200\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8610\t Accuracy 0.6880\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8614\t Accuracy 0.6877\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8652\t Accuracy 0.6797\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8681\t Accuracy 0.6780\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8684\t Accuracy 0.6772\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8683\t Accuracy 0.6754\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8715\t Accuracy 0.6738\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8718\t Accuracy 0.6750\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8732\t Accuracy 0.6747\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8739\t Accuracy 0.6753\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8742\t Average training accuracy 0.6759\n",
      "Epoch [8]\t Average validation loss 1.8595\t Average validation accuracy 0.7182\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8452\t Accuracy 0.7100\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8708\t Accuracy 0.6814\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8713\t Accuracy 0.6805\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8750\t Accuracy 0.6730\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8780\t Accuracy 0.6710\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8783\t Accuracy 0.6708\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8783\t Accuracy 0.6692\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8813\t Accuracy 0.6675\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8817\t Accuracy 0.6687\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8832\t Accuracy 0.6684\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8838\t Accuracy 0.6690\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8842\t Average training accuracy 0.6697\n",
      "Epoch [9]\t Average validation loss 1.8705\t Average validation accuracy 0.7100\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8569\t Accuracy 0.7100\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8813\t Accuracy 0.6743\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8818\t Accuracy 0.6734\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8854\t Accuracy 0.6666\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8884\t Accuracy 0.6641\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8887\t Accuracy 0.6639\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8887\t Accuracy 0.6624\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8917\t Accuracy 0.6607\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8921\t Accuracy 0.6616\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8935\t Accuracy 0.6611\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8942\t Accuracy 0.6616\n",
      "\n",
      "Epoch [10]\t Average training loss 1.8946\t Average training accuracy 0.6623\n",
      "Epoch [10]\t Average validation loss 1.8816\t Average validation accuracy 0.7034\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8685\t Accuracy 0.6900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8918\t Accuracy 0.6659\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8924\t Accuracy 0.6644\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8958\t Accuracy 0.6584\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8988\t Accuracy 0.6558\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8991\t Accuracy 0.6562\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8990\t Accuracy 0.6547\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.9020\t Accuracy 0.6534\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.9024\t Accuracy 0.6540\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.9038\t Accuracy 0.6535\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.9044\t Accuracy 0.6540\n",
      "\n",
      "Epoch [11]\t Average training loss 1.9049\t Average training accuracy 0.6544\n",
      "Epoch [11]\t Average validation loss 1.8924\t Average validation accuracy 0.6946\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8797\t Accuracy 0.6800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.9020\t Accuracy 0.6573\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.9026\t Accuracy 0.6550\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.9059\t Accuracy 0.6485\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.9088\t Accuracy 0.6457\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.9091\t Accuracy 0.6469\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.9090\t Accuracy 0.6455\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.9119\t Accuracy 0.6444\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.9122\t Accuracy 0.6450\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.9136\t Accuracy 0.6444\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.9143\t Accuracy 0.6449\n",
      "\n",
      "Epoch [12]\t Average training loss 1.9147\t Average training accuracy 0.6453\n",
      "Epoch [12]\t Average validation loss 1.9026\t Average validation accuracy 0.6854\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8902\t Accuracy 0.6700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.9117\t Accuracy 0.6475\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.9122\t Accuracy 0.6448\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.9154\t Accuracy 0.6384\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.9183\t Accuracy 0.6354\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.9185\t Accuracy 0.6363\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.9185\t Accuracy 0.6356\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.9212\t Accuracy 0.6345\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.9216\t Accuracy 0.6347\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.9229\t Accuracy 0.6341\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.9235\t Accuracy 0.6347\n",
      "\n",
      "Epoch [13]\t Average training loss 1.9239\t Average training accuracy 0.6354\n",
      "Epoch [13]\t Average validation loss 1.9122\t Average validation accuracy 0.6770\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8999\t Accuracy 0.6700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.9207\t Accuracy 0.6418\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.9213\t Accuracy 0.6367\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.9244\t Accuracy 0.6309\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.9271\t Accuracy 0.6270\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.9274\t Accuracy 0.6280\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.9273\t Accuracy 0.6273\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.9299\t Accuracy 0.6261\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.9303\t Accuracy 0.6259\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.9316\t Accuracy 0.6254\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.9322\t Accuracy 0.6257\n",
      "\n",
      "Epoch [14]\t Average training loss 1.9326\t Average training accuracy 0.6263\n",
      "Epoch [14]\t Average validation loss 1.9211\t Average validation accuracy 0.6666\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.9089\t Accuracy 0.6700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.9291\t Accuracy 0.6327\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.9297\t Accuracy 0.6260\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.9326\t Accuracy 0.6205\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.9354\t Accuracy 0.6163\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.9356\t Accuracy 0.6177\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.9355\t Accuracy 0.6170\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.9380\t Accuracy 0.6158\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.9383\t Accuracy 0.6156\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.9396\t Accuracy 0.6151\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.9402\t Accuracy 0.6155\n",
      "\n",
      "Epoch [15]\t Average training loss 1.9405\t Average training accuracy 0.6161\n",
      "Epoch [15]\t Average validation loss 1.9293\t Average validation accuracy 0.6568\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.9171\t Accuracy 0.6600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.9369\t Accuracy 0.6241\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.9374\t Accuracy 0.6173\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.9402\t Accuracy 0.6108\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.9429\t Accuracy 0.6062\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.9431\t Accuracy 0.6075\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.9430\t Accuracy 0.6061\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.9455\t Accuracy 0.6051\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.9458\t Accuracy 0.6050\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.9470\t Accuracy 0.6045\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.9475\t Accuracy 0.6049\n",
      "\n",
      "Epoch [16]\t Average training loss 1.9479\t Average training accuracy 0.6051\n",
      "Epoch [16]\t Average validation loss 1.9369\t Average validation accuracy 0.6454\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.9247\t Accuracy 0.6600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.9440\t Accuracy 0.6151\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.9445\t Accuracy 0.6075\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.9472\t Accuracy 0.6011\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.9499\t Accuracy 0.5968\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.9501\t Accuracy 0.5983\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.9499\t Accuracy 0.5971\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.9523\t Accuracy 0.5959\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.9526\t Accuracy 0.5957\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.9538\t Accuracy 0.5951\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.9543\t Accuracy 0.5953\n",
      "\n",
      "Epoch [17]\t Average training loss 1.9546\t Average training accuracy 0.5951\n",
      "Epoch [17]\t Average validation loss 1.9438\t Average validation accuracy 0.6336\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.9315\t Accuracy 0.6600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.9506\t Accuracy 0.6057\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.9510\t Accuracy 0.5985\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.9537\t Accuracy 0.5909\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.9563\t Accuracy 0.5864\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.9564\t Accuracy 0.5883\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.9563\t Accuracy 0.5875\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.9586\t Accuracy 0.5860\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.9588\t Accuracy 0.5860\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.9600\t Accuracy 0.5854\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.9604\t Accuracy 0.5856\n",
      "\n",
      "Epoch [18]\t Average training loss 1.9608\t Average training accuracy 0.5853\n",
      "Epoch [18]\t Average validation loss 1.9501\t Average validation accuracy 0.6238\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.9378\t Accuracy 0.6600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.9565\t Accuracy 0.5978\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.9569\t Accuracy 0.5909\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.9595\t Accuracy 0.5827\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.9621\t Accuracy 0.5781\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.9622\t Accuracy 0.5796\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.9621\t Accuracy 0.5783\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.9643\t Accuracy 0.5766\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.9645\t Accuracy 0.5766\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.9657\t Accuracy 0.5757\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.9661\t Accuracy 0.5760\n",
      "\n",
      "Epoch [19]\t Average training loss 1.9664\t Average training accuracy 0.5758\n",
      "Epoch [19]\t Average validation loss 1.9559\t Average validation accuracy 0.6112\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5115\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9572\t Accuracy 0.3900\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4742\t Accuracy 0.5748\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.2361\t Accuracy 0.6532\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.1190\t Accuracy 0.6991\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.0653\t Accuracy 0.7278\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0379\t Accuracy 0.7462\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.0230\t Accuracy 0.7590\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.0057\t Accuracy 0.7692\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.9914\t Accuracy 0.7775\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.9776\t Accuracy 0.7827\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9659\t Average training accuracy 0.7870\n",
      "Epoch [0]\t Average validation loss 0.7804\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8212\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8319\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8401\t Accuracy 0.8426\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8513\t Accuracy 0.8392\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8568\t Accuracy 0.8412\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8527\t Accuracy 0.8433\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8502\t Accuracy 0.8434\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8427\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8534\t Accuracy 0.8423\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8549\t Accuracy 0.8416\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8552\t Accuracy 0.8399\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8550\t Average training accuracy 0.8387\n",
      "Epoch [1]\t Average validation loss 0.7823\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8204\t Accuracy 0.8600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8301\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8343\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8431\t Accuracy 0.8404\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8476\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8439\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8420\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8463\t Accuracy 0.8431\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8463\t Accuracy 0.8426\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8481\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8486\t Accuracy 0.8402\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8486\t Average training accuracy 0.8389\n",
      "Epoch [2]\t Average validation loss 0.7777\t Average validation accuracy 0.8862\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8139\t Accuracy 0.8600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8261\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8311\t Accuracy 0.8423\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8402\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8447\t Accuracy 0.8406\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8409\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8390\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8433\t Accuracy 0.8420\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8434\t Accuracy 0.8415\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8453\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8458\t Accuracy 0.8394\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8458\t Average training accuracy 0.8384\n",
      "Epoch [3]\t Average validation loss 0.7751\t Average validation accuracy 0.8854\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8094\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8234\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8290\t Accuracy 0.8420\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8381\t Accuracy 0.8385\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8427\t Accuracy 0.8399\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8390\t Accuracy 0.8422\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8370\t Accuracy 0.8423\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8413\t Accuracy 0.8414\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8415\t Accuracy 0.8412\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8434\t Accuracy 0.8407\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8439\t Accuracy 0.8392\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8440\t Average training accuracy 0.8381\n",
      "Epoch [4]\t Average validation loss 0.7732\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8058\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8216\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8276\t Accuracy 0.8407\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8367\t Accuracy 0.8369\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8413\t Accuracy 0.8381\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8376\t Accuracy 0.8410\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8356\t Accuracy 0.8413\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8399\t Accuracy 0.8405\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8401\t Accuracy 0.8404\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8420\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8425\t Accuracy 0.8386\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8426\t Average training accuracy 0.8378\n",
      "Epoch [5]\t Average validation loss 0.7717\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8033\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8203\t Accuracy 0.8429\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8265\t Accuracy 0.8386\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8356\t Accuracy 0.8353\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8402\t Accuracy 0.8366\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8366\t Accuracy 0.8395\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8346\t Accuracy 0.8403\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8389\t Accuracy 0.8396\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8390\t Accuracy 0.8396\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8409\t Accuracy 0.8395\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8414\t Accuracy 0.8381\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8415\t Average training accuracy 0.8373\n",
      "Epoch [6]\t Average validation loss 0.7704\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8013\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8192\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8256\t Accuracy 0.8399\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8347\t Accuracy 0.8360\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8394\t Accuracy 0.8368\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8358\t Accuracy 0.8394\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8338\t Accuracy 0.8403\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8381\t Accuracy 0.8396\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8382\t Accuracy 0.8397\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8400\t Accuracy 0.8396\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8405\t Accuracy 0.8381\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8406\t Average training accuracy 0.8374\n",
      "Epoch [7]\t Average validation loss 0.7695\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.7999\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8183\t Accuracy 0.8416\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8248\t Accuracy 0.8387\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8339\t Accuracy 0.8350\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8387\t Accuracy 0.8359\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8351\t Accuracy 0.8385\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8330\t Accuracy 0.8398\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8373\t Accuracy 0.8393\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8374\t Accuracy 0.8394\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8393\t Accuracy 0.8393\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8398\t Accuracy 0.8379\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8398\t Average training accuracy 0.8372\n",
      "Epoch [8]\t Average validation loss 0.7687\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7987\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8177\t Accuracy 0.8414\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8243\t Accuracy 0.8389\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8334\t Accuracy 0.8348\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8382\t Accuracy 0.8355\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8346\t Accuracy 0.8381\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8325\t Accuracy 0.8395\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8368\t Accuracy 0.8391\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8369\t Accuracy 0.8391\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8387\t Accuracy 0.8391\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8392\t Accuracy 0.8377\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8392\t Average training accuracy 0.8370\n",
      "Epoch [9]\t Average validation loss 0.7681\t Average validation accuracy 0.8828\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7977\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8172\t Accuracy 0.8414\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8238\t Accuracy 0.8395\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8329\t Accuracy 0.8350\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8378\t Accuracy 0.8356\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8341\t Accuracy 0.8378\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8320\t Accuracy 0.8393\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8363\t Accuracy 0.8390\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8364\t Accuracy 0.8391\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8382\t Accuracy 0.8392\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8387\t Accuracy 0.8377\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8387\t Average training accuracy 0.8371\n",
      "Epoch [10]\t Average validation loss 0.7677\t Average validation accuracy 0.8832\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7970\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8168\t Accuracy 0.8390\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8234\t Accuracy 0.8378\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8325\t Accuracy 0.8334\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8374\t Accuracy 0.8343\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8338\t Accuracy 0.8367\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8317\t Accuracy 0.8385\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8360\t Accuracy 0.8384\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8360\t Accuracy 0.8386\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8378\t Accuracy 0.8387\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8383\t Accuracy 0.8373\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8383\t Average training accuracy 0.8367\n",
      "Epoch [11]\t Average validation loss 0.7672\t Average validation accuracy 0.8834\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7963\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8165\t Accuracy 0.8371\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8231\t Accuracy 0.8364\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8322\t Accuracy 0.8325\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8371\t Accuracy 0.8337\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8335\t Accuracy 0.8362\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8314\t Accuracy 0.8382\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8356\t Accuracy 0.8381\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8357\t Accuracy 0.8384\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8375\t Accuracy 0.8385\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8380\t Accuracy 0.8372\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8380\t Average training accuracy 0.8366\n",
      "Epoch [12]\t Average validation loss 0.7668\t Average validation accuracy 0.8828\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7957\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8162\t Accuracy 0.8363\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8228\t Accuracy 0.8358\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8319\t Accuracy 0.8321\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8369\t Accuracy 0.8332\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8333\t Accuracy 0.8357\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8311\t Accuracy 0.8379\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8354\t Accuracy 0.8379\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8354\t Accuracy 0.8382\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8372\t Accuracy 0.8384\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8377\t Accuracy 0.8371\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8377\t Average training accuracy 0.8365\n",
      "Epoch [13]\t Average validation loss 0.7665\t Average validation accuracy 0.8818\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7952\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8160\t Accuracy 0.8361\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8226\t Accuracy 0.8352\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8318\t Accuracy 0.8317\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8367\t Accuracy 0.8329\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8331\t Accuracy 0.8352\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8309\t Accuracy 0.8377\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8352\t Accuracy 0.8377\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8352\t Accuracy 0.8380\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8370\t Accuracy 0.8380\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8375\t Accuracy 0.8367\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8374\t Average training accuracy 0.8362\n",
      "Epoch [14]\t Average validation loss 0.7662\t Average validation accuracy 0.8818\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7948\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8158\t Accuracy 0.8357\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8225\t Accuracy 0.8351\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8316\t Accuracy 0.8315\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8366\t Accuracy 0.8325\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8329\t Accuracy 0.8349\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8308\t Accuracy 0.8375\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8350\t Accuracy 0.8376\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8350\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8368\t Accuracy 0.8380\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8373\t Accuracy 0.8367\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8372\t Average training accuracy 0.8363\n",
      "Epoch [15]\t Average validation loss 0.7660\t Average validation accuracy 0.8820\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7944\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8157\t Accuracy 0.8357\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8223\t Accuracy 0.8352\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8314\t Accuracy 0.8316\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8364\t Accuracy 0.8326\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8328\t Accuracy 0.8349\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8306\t Accuracy 0.8375\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8348\t Accuracy 0.8375\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8349\t Accuracy 0.8380\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8366\t Accuracy 0.8380\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8371\t Accuracy 0.8366\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8370\t Average training accuracy 0.8363\n",
      "Epoch [16]\t Average validation loss 0.7658\t Average validation accuracy 0.8820\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7940\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8155\t Accuracy 0.8355\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8222\t Accuracy 0.8352\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8313\t Accuracy 0.8314\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8363\t Accuracy 0.8324\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8327\t Accuracy 0.8345\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8305\t Accuracy 0.8372\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8347\t Accuracy 0.8374\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8347\t Accuracy 0.8378\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8365\t Accuracy 0.8379\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8369\t Accuracy 0.8365\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8369\t Average training accuracy 0.8362\n",
      "Epoch [17]\t Average validation loss 0.7656\t Average validation accuracy 0.8820\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7938\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8155\t Accuracy 0.8353\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8221\t Accuracy 0.8350\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8312\t Accuracy 0.8312\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8362\t Accuracy 0.8322\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8326\t Accuracy 0.8341\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8304\t Accuracy 0.8370\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8346\t Accuracy 0.8372\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8346\t Accuracy 0.8378\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8363\t Accuracy 0.8379\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8368\t Accuracy 0.8366\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8367\t Average training accuracy 0.8363\n",
      "Epoch [18]\t Average validation loss 0.7654\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7936\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8154\t Accuracy 0.8353\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8220\t Accuracy 0.8350\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8311\t Accuracy 0.8314\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8361\t Accuracy 0.8325\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8325\t Accuracy 0.8343\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8303\t Accuracy 0.8371\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8345\t Accuracy 0.8373\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8345\t Accuracy 0.8379\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8362\t Accuracy 0.8379\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8367\t Accuracy 0.8366\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8366\t Average training accuracy 0.8362\n",
      "Epoch [19]\t Average validation loss 0.7652\t Average validation accuracy 0.8826\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.2103\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.1537\t Accuracy 0.0831\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8981\t Accuracy 0.0953\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7987\t Accuracy 0.1143\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7410\t Accuracy 0.1327\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7008\t Accuracy 0.1527\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6704\t Accuracy 0.1693\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6457\t Accuracy 0.1890\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6252\t Accuracy 0.2058\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6069\t Accuracy 0.2242\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5912\t Accuracy 0.2401\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5770\t Average training accuracy 0.2567\n",
      "Epoch [0]\t Average validation loss 0.4160\t Average validation accuracy 0.4710\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4028\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4146\t Accuracy 0.4735\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4118\t Accuracy 0.4738\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4083\t Accuracy 0.4807\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4046\t Accuracy 0.4882\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4001\t Accuracy 0.4979\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3963\t Accuracy 0.5050\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3935\t Accuracy 0.5117\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3903\t Accuracy 0.5181\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3870\t Accuracy 0.5259\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3840\t Accuracy 0.5323\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3807\t Average training accuracy 0.5399\n",
      "Epoch [1]\t Average validation loss 0.3325\t Average validation accuracy 0.6606\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3189\t Accuracy 0.7100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3374\t Accuracy 0.6447\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3371\t Accuracy 0.6404\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3371\t Accuracy 0.6370\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3362\t Accuracy 0.6387\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3342\t Accuracy 0.6428\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3326\t Accuracy 0.6448\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3321\t Accuracy 0.6465\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3308\t Accuracy 0.6488\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3294\t Accuracy 0.6516\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3282\t Accuracy 0.6538\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3264\t Average training accuracy 0.6576\n",
      "Epoch [2]\t Average validation loss 0.2942\t Average validation accuracy 0.7258\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2815\t Accuracy 0.8000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3019\t Accuracy 0.7118\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3024\t Accuracy 0.7072\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3038\t Accuracy 0.7033\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3036\t Accuracy 0.7021\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3025\t Accuracy 0.7042\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3017\t Accuracy 0.7054\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3020\t Accuracy 0.7042\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3014\t Accuracy 0.7055\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3006\t Accuracy 0.7071\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3001\t Accuracy 0.7080\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2989\t Average training accuracy 0.7104\n",
      "Epoch [3]\t Average validation loss 0.2723\t Average validation accuracy 0.7658\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2606\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2814\t Accuracy 0.7447\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2822\t Accuracy 0.7443\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2843\t Accuracy 0.7373\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2845\t Accuracy 0.7363\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2838\t Accuracy 0.7368\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2833\t Accuracy 0.7379\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2840\t Accuracy 0.7362\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2836\t Accuracy 0.7376\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2832\t Accuracy 0.7387\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2829\t Accuracy 0.7390\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2820\t Average training accuracy 0.7409\n",
      "Epoch [4]\t Average validation loss 0.2580\t Average validation accuracy 0.7916\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2473\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2679\t Accuracy 0.7673\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2689\t Accuracy 0.7679\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2714\t Accuracy 0.7598\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2717\t Accuracy 0.7586\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2713\t Accuracy 0.7588\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2710\t Accuracy 0.7593\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2718\t Accuracy 0.7574\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2716\t Accuracy 0.7583\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2714\t Accuracy 0.7589\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2713\t Accuracy 0.7588\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2705\t Average training accuracy 0.7604\n",
      "Epoch [5]\t Average validation loss 0.2479\t Average validation accuracy 0.8066\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2380\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2583\t Accuracy 0.7827\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2593\t Accuracy 0.7838\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2621\t Accuracy 0.7758\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2625\t Accuracy 0.7745\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2622\t Accuracy 0.7741\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2620\t Accuracy 0.7744\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2629\t Accuracy 0.7724\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2629\t Accuracy 0.7729\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2628\t Accuracy 0.7732\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2628\t Accuracy 0.7730\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2621\t Average training accuracy 0.7741\n",
      "Epoch [6]\t Average validation loss 0.2403\t Average validation accuracy 0.8196\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2312\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2510\t Accuracy 0.7947\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2521\t Accuracy 0.7936\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2551\t Accuracy 0.7865\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2555\t Accuracy 0.7854\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2553\t Accuracy 0.7845\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2552\t Accuracy 0.7846\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2562\t Accuracy 0.7832\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2562\t Accuracy 0.7836\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2561\t Accuracy 0.7837\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2562\t Accuracy 0.7834\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2556\t Average training accuracy 0.7843\n",
      "Epoch [7]\t Average validation loss 0.2344\t Average validation accuracy 0.8302\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2260\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2452\t Accuracy 0.8006\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2464\t Accuracy 0.8006\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2495\t Accuracy 0.7937\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2500\t Accuracy 0.7932\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2499\t Accuracy 0.7924\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2498\t Accuracy 0.7927\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2508\t Accuracy 0.7913\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2508\t Accuracy 0.7918\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2508\t Accuracy 0.7915\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2510\t Accuracy 0.7913\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2504\t Average training accuracy 0.7921\n",
      "Epoch [8]\t Average validation loss 0.2296\t Average validation accuracy 0.8402\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2218\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2406\t Accuracy 0.8090\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2418\t Accuracy 0.8076\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2450\t Accuracy 0.8007\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2455\t Accuracy 0.8002\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2454\t Accuracy 0.7994\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2454\t Accuracy 0.7998\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2464\t Accuracy 0.7983\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2465\t Accuracy 0.7989\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2465\t Accuracy 0.7985\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2467\t Accuracy 0.7981\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2461\t Average training accuracy 0.7990\n",
      "Epoch [9]\t Average validation loss 0.2257\t Average validation accuracy 0.8458\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2184\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2367\t Accuracy 0.8145\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2380\t Accuracy 0.8148\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2412\t Accuracy 0.8069\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2417\t Accuracy 0.8061\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2417\t Accuracy 0.8051\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2417\t Accuracy 0.8055\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2427\t Accuracy 0.8040\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2428\t Accuracy 0.8044\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2429\t Accuracy 0.8040\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2431\t Accuracy 0.8036\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2426\t Average training accuracy 0.8044\n",
      "Epoch [10]\t Average validation loss 0.2224\t Average validation accuracy 0.8494\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2156\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2335\t Accuracy 0.8196\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2348\t Accuracy 0.8192\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2381\t Accuracy 0.8119\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2386\t Accuracy 0.8110\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2386\t Accuracy 0.8096\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2386\t Accuracy 0.8100\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2396\t Accuracy 0.8083\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2398\t Accuracy 0.8088\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2399\t Accuracy 0.8083\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2401\t Accuracy 0.8077\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2396\t Average training accuracy 0.8084\n",
      "Epoch [11]\t Average validation loss 0.2197\t Average validation accuracy 0.8530\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2132\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2307\t Accuracy 0.8243\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2321\t Accuracy 0.8237\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2354\t Accuracy 0.8160\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2358\t Accuracy 0.8152\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2359\t Accuracy 0.8137\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2359\t Accuracy 0.8141\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2369\t Accuracy 0.8126\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2371\t Accuracy 0.8131\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2372\t Accuracy 0.8127\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2374\t Accuracy 0.8120\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2370\t Average training accuracy 0.8127\n",
      "Epoch [12]\t Average validation loss 0.2173\t Average validation accuracy 0.8554\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2111\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2283\t Accuracy 0.8284\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2297\t Accuracy 0.8279\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2330\t Accuracy 0.8198\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2335\t Accuracy 0.8189\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2336\t Accuracy 0.8172\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2336\t Accuracy 0.8176\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2346\t Accuracy 0.8160\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2348\t Accuracy 0.8165\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2349\t Accuracy 0.8162\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2352\t Accuracy 0.8155\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2347\t Average training accuracy 0.8161\n",
      "Epoch [13]\t Average validation loss 0.2152\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2093\t Accuracy 0.8700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2262\t Accuracy 0.8320\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2276\t Accuracy 0.8309\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2310\t Accuracy 0.8226\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2314\t Accuracy 0.8219\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2315\t Accuracy 0.8202\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2315\t Accuracy 0.8205\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2326\t Accuracy 0.8188\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2328\t Accuracy 0.8191\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2329\t Accuracy 0.8188\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2332\t Accuracy 0.8182\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2327\t Average training accuracy 0.8188\n",
      "Epoch [14]\t Average validation loss 0.2133\t Average validation accuracy 0.8620\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2077\t Accuracy 0.8700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2244\t Accuracy 0.8341\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2258\t Accuracy 0.8339\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2292\t Accuracy 0.8253\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2296\t Accuracy 0.8248\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2297\t Accuracy 0.8231\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2297\t Accuracy 0.8233\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2308\t Accuracy 0.8215\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2310\t Accuracy 0.8218\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2312\t Accuracy 0.8216\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2314\t Accuracy 0.8208\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2310\t Average training accuracy 0.8214\n",
      "Epoch [15]\t Average validation loss 0.2117\t Average validation accuracy 0.8634\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2063\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2228\t Accuracy 0.8361\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2242\t Accuracy 0.8358\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2276\t Accuracy 0.8275\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2280\t Accuracy 0.8272\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2281\t Accuracy 0.8257\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2282\t Accuracy 0.8257\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2292\t Accuracy 0.8240\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2294\t Accuracy 0.8243\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2296\t Accuracy 0.8240\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2299\t Accuracy 0.8233\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2294\t Average training accuracy 0.8238\n",
      "Epoch [16]\t Average validation loss 0.2103\t Average validation accuracy 0.8644\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2050\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.8375\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2228\t Accuracy 0.8369\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2262\t Accuracy 0.8290\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2266\t Accuracy 0.8293\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2267\t Accuracy 0.8277\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2267\t Accuracy 0.8277\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2278\t Accuracy 0.8261\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2280\t Accuracy 0.8265\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2282\t Accuracy 0.8261\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2285\t Accuracy 0.8254\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2280\t Average training accuracy 0.8260\n",
      "Epoch [17]\t Average validation loss 0.2090\t Average validation accuracy 0.8664\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2039\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2200\t Accuracy 0.8390\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2215\t Accuracy 0.8386\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2249\t Accuracy 0.8305\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2253\t Accuracy 0.8309\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2254\t Accuracy 0.8296\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2255\t Accuracy 0.8295\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2265\t Accuracy 0.8276\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2267\t Accuracy 0.8280\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2269\t Accuracy 0.8276\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2272\t Accuracy 0.8271\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2268\t Average training accuracy 0.8275\n",
      "Epoch [18]\t Average validation loss 0.2079\t Average validation accuracy 0.8674\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2029\t Accuracy 0.9000\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2189\t Accuracy 0.8398\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2204\t Accuracy 0.8396\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2237\t Accuracy 0.8315\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2242\t Accuracy 0.8320\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2243\t Accuracy 0.8308\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2243\t Accuracy 0.8308\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2254\t Accuracy 0.8292\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2256\t Accuracy 0.8296\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2258\t Accuracy 0.8292\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2261\t Accuracy 0.8285\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2256\t Average training accuracy 0.8291\n",
      "Epoch [19]\t Average validation loss 0.2069\t Average validation accuracy 0.8684\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.1232\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2328\t Accuracy 0.2773\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8272\t Accuracy 0.3830\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6665\t Accuracy 0.4601\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5705\t Accuracy 0.5185\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5049\t Accuracy 0.5658\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4561\t Accuracy 0.6046\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4194\t Accuracy 0.6332\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.3892\t Accuracy 0.6577\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3641\t Accuracy 0.6784\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3432\t Accuracy 0.6961\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3251\t Average training accuracy 0.7116\n",
      "Epoch [0]\t Average validation loss 0.1176\t Average validation accuracy 0.9092\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1196\t Accuracy 0.9200\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1282\t Accuracy 0.8869\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1272\t Accuracy 0.8861\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1280\t Accuracy 0.8836\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1262\t Accuracy 0.8866\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1245\t Accuracy 0.8886\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1231\t Accuracy 0.8902\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1224\t Accuracy 0.8905\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1214\t Accuracy 0.8912\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1206\t Accuracy 0.8925\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1200\t Accuracy 0.8930\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1188\t Average training accuracy 0.8938\n",
      "Epoch [1]\t Average validation loss 0.0900\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0882\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1014\t Accuracy 0.9157\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1031\t Accuracy 0.9131\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1056\t Accuracy 0.9091\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1051\t Accuracy 0.9096\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1048\t Accuracy 0.9104\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1047\t Accuracy 0.9105\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1051\t Accuracy 0.9096\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1050\t Accuracy 0.9092\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1050\t Accuracy 0.9094\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1052\t Accuracy 0.9090\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1048\t Average training accuracy 0.9092\n",
      "Epoch [2]\t Average validation loss 0.0840\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0840\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0949\t Accuracy 0.9245\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0968\t Accuracy 0.9192\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0996\t Accuracy 0.9157\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0993\t Accuracy 0.9162\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0993\t Accuracy 0.9163\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0995\t Accuracy 0.9159\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1000\t Accuracy 0.9152\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1001\t Accuracy 0.9147\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1003\t Accuracy 0.9146\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1006\t Accuracy 0.9142\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1003\t Average training accuracy 0.9143\n",
      "Epoch [3]\t Average validation loss 0.0817\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0827\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0924\t Accuracy 0.9278\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0942\t Accuracy 0.9226\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0971\t Accuracy 0.9183\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0968\t Accuracy 0.9189\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0968\t Accuracy 0.9189\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0971\t Accuracy 0.9183\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0977\t Accuracy 0.9177\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0978\t Accuracy 0.9174\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0980\t Accuracy 0.9172\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0984\t Accuracy 0.9167\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0982\t Average training accuracy 0.9165\n",
      "Epoch [4]\t Average validation loss 0.0803\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0811\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0910\t Accuracy 0.9292\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0928\t Accuracy 0.9233\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0957\t Accuracy 0.9194\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0954\t Accuracy 0.9201\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0954\t Accuracy 0.9199\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0957\t Accuracy 0.9196\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0963\t Accuracy 0.9189\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0965\t Accuracy 0.9187\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0967\t Accuracy 0.9185\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0971\t Accuracy 0.9178\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0969\t Average training accuracy 0.9178\n",
      "Epoch [5]\t Average validation loss 0.0794\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0799\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0900\t Accuracy 0.9296\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0917\t Accuracy 0.9236\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0947\t Accuracy 0.9203\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0944\t Accuracy 0.9211\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0944\t Accuracy 0.9211\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0948\t Accuracy 0.9207\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0954\t Accuracy 0.9199\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0955\t Accuracy 0.9196\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0958\t Accuracy 0.9195\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0962\t Accuracy 0.9188\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0960\t Average training accuracy 0.9187\n",
      "Epoch [6]\t Average validation loss 0.0787\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0790\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0893\t Accuracy 0.9306\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0910\t Accuracy 0.9248\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0939\t Accuracy 0.9211\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0937\t Accuracy 0.9220\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0937\t Accuracy 0.9222\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0941\t Accuracy 0.9220\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0947\t Accuracy 0.9211\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0948\t Accuracy 0.9206\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0951\t Accuracy 0.9204\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0955\t Accuracy 0.9196\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0953\t Average training accuracy 0.9195\n",
      "Epoch [7]\t Average validation loss 0.0781\t Average validation accuracy 0.9440\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0784\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0887\t Accuracy 0.9314\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0904\t Accuracy 0.9253\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0934\t Accuracy 0.9217\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0931\t Accuracy 0.9226\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0931\t Accuracy 0.9229\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0935\t Accuracy 0.9226\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0941\t Accuracy 0.9216\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0943\t Accuracy 0.9212\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0945\t Accuracy 0.9210\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0949\t Accuracy 0.9203\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0947\t Average training accuracy 0.9202\n",
      "Epoch [8]\t Average validation loss 0.0776\t Average validation accuracy 0.9438\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0777\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0882\t Accuracy 0.9314\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0899\t Accuracy 0.9257\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0928\t Accuracy 0.9222\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0926\t Accuracy 0.9231\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0926\t Accuracy 0.9233\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0930\t Accuracy 0.9231\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0936\t Accuracy 0.9222\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0937\t Accuracy 0.9217\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0940\t Accuracy 0.9216\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0944\t Accuracy 0.9208\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0942\t Average training accuracy 0.9207\n",
      "Epoch [9]\t Average validation loss 0.0772\t Average validation accuracy 0.9452\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0772\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0878\t Accuracy 0.9324\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0894\t Accuracy 0.9261\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0924\t Accuracy 0.9228\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0921\t Accuracy 0.9237\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0922\t Accuracy 0.9238\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0925\t Accuracy 0.9236\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0931\t Accuracy 0.9226\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0933\t Accuracy 0.9222\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0935\t Accuracy 0.9221\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0939\t Accuracy 0.9214\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0938\t Average training accuracy 0.9213\n",
      "Epoch [10]\t Average validation loss 0.0768\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0768\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0874\t Accuracy 0.9327\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0890\t Accuracy 0.9270\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0920\t Accuracy 0.9236\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0917\t Accuracy 0.9246\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0917\t Accuracy 0.9245\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0921\t Accuracy 0.9242\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0927\t Accuracy 0.9232\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0928\t Accuracy 0.9227\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0931\t Accuracy 0.9225\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0935\t Accuracy 0.9218\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0933\t Average training accuracy 0.9218\n",
      "Epoch [11]\t Average validation loss 0.0764\t Average validation accuracy 0.9468\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0764\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0870\t Accuracy 0.9325\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0886\t Accuracy 0.9269\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0916\t Accuracy 0.9238\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0914\t Accuracy 0.9248\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0914\t Accuracy 0.9247\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0917\t Accuracy 0.9245\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0923\t Accuracy 0.9235\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0925\t Accuracy 0.9230\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0927\t Accuracy 0.9229\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0931\t Accuracy 0.9222\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0930\t Average training accuracy 0.9222\n",
      "Epoch [12]\t Average validation loss 0.0761\t Average validation accuracy 0.9472\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0760\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0867\t Accuracy 0.9329\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0883\t Accuracy 0.9272\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0913\t Accuracy 0.9240\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0910\t Accuracy 0.9251\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0910\t Accuracy 0.9250\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0914\t Accuracy 0.9249\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0920\t Accuracy 0.9239\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0922\t Accuracy 0.9234\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0924\t Accuracy 0.9232\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0928\t Accuracy 0.9225\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0926\t Average training accuracy 0.9226\n",
      "Epoch [13]\t Average validation loss 0.0758\t Average validation accuracy 0.9480\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0756\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0864\t Accuracy 0.9333\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0880\t Accuracy 0.9273\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0910\t Accuracy 0.9241\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0908\t Accuracy 0.9253\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0908\t Accuracy 0.9251\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0912\t Accuracy 0.9250\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0917\t Accuracy 0.9241\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0919\t Accuracy 0.9236\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0922\t Accuracy 0.9235\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0926\t Accuracy 0.9229\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0924\t Average training accuracy 0.9230\n",
      "Epoch [14]\t Average validation loss 0.0756\t Average validation accuracy 0.9482\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0754\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0861\t Accuracy 0.9339\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0878\t Accuracy 0.9277\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0907\t Accuracy 0.9247\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0905\t Accuracy 0.9259\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0905\t Accuracy 0.9256\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0909\t Accuracy 0.9253\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0915\t Accuracy 0.9245\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0917\t Accuracy 0.9241\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0919\t Accuracy 0.9240\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0923\t Accuracy 0.9234\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0922\t Average training accuracy 0.9235\n",
      "Epoch [15]\t Average validation loss 0.0754\t Average validation accuracy 0.9482\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0751\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0858\t Accuracy 0.9343\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0875\t Accuracy 0.9285\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0905\t Accuracy 0.9254\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0903\t Accuracy 0.9266\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0903\t Accuracy 0.9262\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0907\t Accuracy 0.9258\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0912\t Accuracy 0.9249\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0914\t Accuracy 0.9245\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0917\t Accuracy 0.9245\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0921\t Accuracy 0.9238\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0919\t Average training accuracy 0.9240\n",
      "Epoch [16]\t Average validation loss 0.0752\t Average validation accuracy 0.9484\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0748\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0856\t Accuracy 0.9347\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0873\t Accuracy 0.9288\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0902\t Accuracy 0.9258\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0900\t Accuracy 0.9270\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0901\t Accuracy 0.9266\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0905\t Accuracy 0.9262\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0910\t Accuracy 0.9252\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0912\t Accuracy 0.9248\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0915\t Accuracy 0.9248\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0919\t Accuracy 0.9241\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0917\t Average training accuracy 0.9243\n",
      "Epoch [17]\t Average validation loss 0.0750\t Average validation accuracy 0.9496\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0746\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0854\t Accuracy 0.9347\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0871\t Accuracy 0.9292\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0901\t Accuracy 0.9263\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0899\t Accuracy 0.9274\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0899\t Accuracy 0.9270\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0903\t Accuracy 0.9265\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0909\t Accuracy 0.9255\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0910\t Accuracy 0.9251\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0913\t Accuracy 0.9251\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0917\t Accuracy 0.9244\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0915\t Average training accuracy 0.9246\n",
      "Epoch [18]\t Average validation loss 0.0749\t Average validation accuracy 0.9498\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0743\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0852\t Accuracy 0.9351\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0870\t Accuracy 0.9294\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0899\t Accuracy 0.9266\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0897\t Accuracy 0.9277\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0897\t Accuracy 0.9273\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0901\t Accuracy 0.9268\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0907\t Accuracy 0.9257\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0909\t Accuracy 0.9253\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0911\t Accuracy 0.9253\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0916\t Accuracy 0.9247\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0914\t Average training accuracy 0.9247\n",
      "Epoch [19]\t Average validation loss 0.0748\t Average validation accuracy 0.9504\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.3830\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4278\t Accuracy 0.1371\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3773\t Accuracy 0.1515\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3401\t Accuracy 0.1617\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3085\t Accuracy 0.1749\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2822\t Accuracy 0.1901\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2598\t Accuracy 0.2055\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2420\t Accuracy 0.2213\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2239\t Accuracy 0.2382\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2086\t Accuracy 0.2523\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1938\t Accuracy 0.2662\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1796\t Average training accuracy 0.2805\n",
      "Epoch [0]\t Average validation loss 2.0152\t Average validation accuracy 0.4548\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.9688\t Accuracy 0.5300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0122\t Accuracy 0.4551\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.9983\t Accuracy 0.4718\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.9910\t Accuracy 0.4752\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.9813\t Accuracy 0.4777\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.9699\t Accuracy 0.4858\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.9588\t Accuracy 0.4921\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.9519\t Accuracy 0.4976\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.9413\t Accuracy 0.5061\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.9330\t Accuracy 0.5141\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.9240\t Accuracy 0.5204\n",
      "\n",
      "Epoch [1]\t Average training loss 1.9146\t Average training accuracy 0.5278\n",
      "Epoch [1]\t Average validation loss 1.7915\t Average validation accuracy 0.6366\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.7433\t Accuracy 0.7400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.7980\t Accuracy 0.6212\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.7865\t Accuracy 0.6279\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.7829\t Accuracy 0.6231\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.7764\t Accuracy 0.6247\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.7670\t Accuracy 0.6267\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.7578\t Accuracy 0.6301\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.7540\t Accuracy 0.6315\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.7452\t Accuracy 0.6361\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.7391\t Accuracy 0.6397\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.7319\t Accuracy 0.6424\n",
      "\n",
      "Epoch [2]\t Average training loss 1.7242\t Average training accuracy 0.6464\n",
      "Epoch [2]\t Average validation loss 1.6110\t Average validation accuracy 0.7226\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.5642\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.6257\t Accuracy 0.6969\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.6162\t Accuracy 0.6999\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.6154\t Accuracy 0.6946\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.6114\t Accuracy 0.6958\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.6035\t Accuracy 0.6969\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.5957\t Accuracy 0.6994\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.5941\t Accuracy 0.6979\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.5868\t Accuracy 0.7010\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.5823\t Accuracy 0.7031\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.5765\t Accuracy 0.7047\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5700\t Average training accuracy 0.7073\n",
      "Epoch [3]\t Average validation loss 1.4636\t Average validation accuracy 0.7680\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.4204\t Accuracy 0.8100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.4852\t Accuracy 0.7390\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.4775\t Accuracy 0.7418\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.4788\t Accuracy 0.7356\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.4767\t Accuracy 0.7348\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.4699\t Accuracy 0.7359\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.4632\t Accuracy 0.7365\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.4634\t Accuracy 0.7349\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.4571\t Accuracy 0.7372\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.4538\t Accuracy 0.7388\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.4491\t Accuracy 0.7399\n",
      "\n",
      "Epoch [4]\t Average training loss 1.4436\t Average training accuracy 0.7414\n",
      "Epoch [4]\t Average validation loss 1.3420\t Average validation accuracy 0.7944\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.3036\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.3694\t Accuracy 0.7637\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.3633\t Accuracy 0.7641\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.3663\t Accuracy 0.7570\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.3655\t Accuracy 0.7572\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.3596\t Accuracy 0.7580\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.3539\t Accuracy 0.7587\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.3553\t Accuracy 0.7571\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.3499\t Accuracy 0.7592\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.3476\t Accuracy 0.7607\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.3437\t Accuracy 0.7611\n",
      "\n",
      "Epoch [5]\t Average training loss 1.3389\t Average training accuracy 0.7625\n",
      "Epoch [5]\t Average validation loss 1.2409\t Average validation accuracy 0.8156\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.2079\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.2732\t Accuracy 0.7818\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.2685\t Accuracy 0.7812\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.2728\t Accuracy 0.7739\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.2730\t Accuracy 0.7735\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.2678\t Accuracy 0.7743\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.2628\t Accuracy 0.7746\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.2653\t Accuracy 0.7734\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.2605\t Accuracy 0.7755\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.2589\t Accuracy 0.7766\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.2557\t Accuracy 0.7772\n",
      "\n",
      "Epoch [6]\t Average training loss 1.2515\t Average training accuracy 0.7781\n",
      "Epoch [6]\t Average validation loss 1.1562\t Average validation accuracy 0.8300\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.1287\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.1926\t Accuracy 0.7916\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.1891\t Accuracy 0.7917\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.1945\t Accuracy 0.7854\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.1954\t Accuracy 0.7854\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.1907\t Accuracy 0.7864\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.1864\t Accuracy 0.7870\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.1896\t Accuracy 0.7860\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.1855\t Accuracy 0.7880\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.1844\t Accuracy 0.7886\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.1818\t Accuracy 0.7892\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1781\t Average training accuracy 0.7901\n",
      "Epoch [7]\t Average validation loss 1.0848\t Average validation accuracy 0.8396\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.0627\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.1246\t Accuracy 0.8035\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.1221\t Accuracy 0.8033\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.1284\t Accuracy 0.7975\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.1298\t Accuracy 0.7967\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.1256\t Accuracy 0.7980\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.1218\t Accuracy 0.7987\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.1256\t Accuracy 0.7973\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.1219\t Accuracy 0.7990\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.1212\t Accuracy 0.7993\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.1191\t Accuracy 0.7999\n",
      "\n",
      "Epoch [8]\t Average training loss 1.1158\t Average training accuracy 0.8004\n",
      "Epoch [8]\t Average validation loss 1.0240\t Average validation accuracy 0.8506\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.0071\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.0668\t Accuracy 0.8110\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.0651\t Accuracy 0.8113\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.0721\t Accuracy 0.8052\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.0740\t Accuracy 0.8047\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.0701\t Accuracy 0.8061\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.0668\t Accuracy 0.8072\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.0710\t Accuracy 0.8058\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.0677\t Accuracy 0.8072\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.0673\t Accuracy 0.8073\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.0656\t Accuracy 0.8076\n",
      "\n",
      "Epoch [9]\t Average training loss 1.0626\t Average training accuracy 0.8081\n",
      "Epoch [9]\t Average validation loss 0.9721\t Average validation accuracy 0.8556\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.9598\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.0172\t Accuracy 0.8188\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.0163\t Accuracy 0.8186\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.0239\t Accuracy 0.8121\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.0260\t Accuracy 0.8115\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.0225\t Accuracy 0.8131\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.0195\t Accuracy 0.8138\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.0241\t Accuracy 0.8122\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.0211\t Accuracy 0.8136\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.0210\t Accuracy 0.8137\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.0195\t Accuracy 0.8140\n",
      "\n",
      "Epoch [10]\t Average training loss 1.0168\t Average training accuracy 0.8144\n",
      "Epoch [10]\t Average validation loss 0.9272\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.9194\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.9744\t Accuracy 0.8253\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.9741\t Accuracy 0.8240\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.9822\t Accuracy 0.8180\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.9846\t Accuracy 0.8172\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.9813\t Accuracy 0.8183\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.9786\t Accuracy 0.8193\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.9834\t Accuracy 0.8176\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.9807\t Accuracy 0.8190\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.9808\t Accuracy 0.8191\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.9797\t Accuracy 0.8192\n",
      "\n",
      "Epoch [11]\t Average training loss 0.9772\t Average training accuracy 0.8195\n",
      "Epoch [11]\t Average validation loss 0.8883\t Average validation accuracy 0.8662\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8845\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.9372\t Accuracy 0.8314\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.9375\t Accuracy 0.8293\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.9460\t Accuracy 0.8230\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.9485\t Accuracy 0.8227\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.9453\t Accuracy 0.8233\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.9430\t Accuracy 0.8244\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.9480\t Accuracy 0.8229\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.9455\t Accuracy 0.8241\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.9458\t Accuracy 0.8242\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.9448\t Accuracy 0.8242\n",
      "\n",
      "Epoch [12]\t Average training loss 0.9425\t Average training accuracy 0.8245\n",
      "Epoch [12]\t Average validation loss 0.8543\t Average validation accuracy 0.8720\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8541\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.9046\t Accuracy 0.8345\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.9054\t Accuracy 0.8329\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.9142\t Accuracy 0.8272\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.9168\t Accuracy 0.8265\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.9139\t Accuracy 0.8273\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.9117\t Accuracy 0.8285\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.9169\t Accuracy 0.8273\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.9146\t Accuracy 0.8286\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.9150\t Accuracy 0.8288\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.9143\t Accuracy 0.8288\n",
      "\n",
      "Epoch [13]\t Average training loss 0.9121\t Average training accuracy 0.8290\n",
      "Epoch [13]\t Average validation loss 0.8243\t Average validation accuracy 0.8746\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8275\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8758\t Accuracy 0.8382\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8771\t Accuracy 0.8367\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8862\t Accuracy 0.8306\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8889\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8861\t Accuracy 0.8313\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8841\t Accuracy 0.8327\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8894\t Accuracy 0.8314\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8873\t Accuracy 0.8325\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8878\t Accuracy 0.8324\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8872\t Accuracy 0.8324\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8853\t Average training accuracy 0.8328\n",
      "Epoch [14]\t Average validation loss 0.7979\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8040\t Accuracy 0.8600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8504\t Accuracy 0.8420\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8520\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8613\t Accuracy 0.8340\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8641\t Accuracy 0.8335\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8614\t Accuracy 0.8349\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8596\t Accuracy 0.8362\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8651\t Accuracy 0.8349\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8630\t Accuracy 0.8361\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8636\t Accuracy 0.8359\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8632\t Accuracy 0.8358\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8614\t Average training accuracy 0.8362\n",
      "Epoch [15]\t Average validation loss 0.7743\t Average validation accuracy 0.8792\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7832\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8276\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8296\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8392\t Accuracy 0.8377\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8420\t Accuracy 0.8374\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8394\t Accuracy 0.8384\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8378\t Accuracy 0.8394\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8433\t Accuracy 0.8381\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8414\t Accuracy 0.8391\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8420\t Accuracy 0.8387\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8418\t Accuracy 0.8385\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8400\t Average training accuracy 0.8388\n",
      "Epoch [16]\t Average validation loss 0.7533\t Average validation accuracy 0.8818\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7646\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8073\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8096\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8193\t Accuracy 0.8413\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8221\t Accuracy 0.8405\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8196\t Accuracy 0.8417\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8182\t Accuracy 0.8424\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8237\t Accuracy 0.8410\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8219\t Accuracy 0.8419\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8227\t Accuracy 0.8416\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8225\t Accuracy 0.8414\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8209\t Average training accuracy 0.8417\n",
      "Epoch [17]\t Average validation loss 0.7344\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7479\t Accuracy 0.8700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7889\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.7916\t Accuracy 0.8488\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8014\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8042\t Accuracy 0.8430\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8019\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8005\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8061\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8044\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8052\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8051\t Accuracy 0.8438\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8036\t Average training accuracy 0.8441\n",
      "Epoch [18]\t Average validation loss 0.7173\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7328\t Accuracy 0.8700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.7724\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.7753\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.7853\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.7881\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.7858\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.7845\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.7901\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.7885\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.7894\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.7894\t Accuracy 0.8461\n",
      "\n",
      "Epoch [19]\t Average training loss 0.7879\t Average training accuracy 0.8464\n",
      "Epoch [19]\t Average validation loss 0.7019\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4204\t Accuracy 0.1200\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9657\t Accuracy 0.3620\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4520\t Accuracy 0.5523\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1735\t Accuracy 0.6370\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.0001\t Accuracy 0.6906\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8856\t Accuracy 0.7264\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8054\t Accuracy 0.7523\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7480\t Accuracy 0.7713\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6998\t Accuracy 0.7871\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6623\t Accuracy 0.7994\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6330\t Accuracy 0.8095\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6078\t Average training accuracy 0.8179\n",
      "Epoch [0]\t Average validation loss 0.2751\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2952\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.3133\t Accuracy 0.9229\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.3235\t Accuracy 0.9170\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3332\t Accuracy 0.9123\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3300\t Accuracy 0.9142\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3277\t Accuracy 0.9151\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3274\t Accuracy 0.9150\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3290\t Accuracy 0.9143\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3279\t Accuracy 0.9142\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3279\t Accuracy 0.9141\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3289\t Accuracy 0.9139\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3279\t Average training accuracy 0.9143\n",
      "Epoch [1]\t Average validation loss 0.2515\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2706\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.2853\t Accuracy 0.9335\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2970\t Accuracy 0.9268\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3058\t Accuracy 0.9219\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3036\t Accuracy 0.9240\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3021\t Accuracy 0.9245\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3027\t Accuracy 0.9243\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3048\t Accuracy 0.9233\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3045\t Accuracy 0.9231\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3052\t Accuracy 0.9229\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3067\t Accuracy 0.9224\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3063\t Average training accuracy 0.9224\n",
      "Epoch [2]\t Average validation loss 0.2400\t Average validation accuracy 0.9472\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2593\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2722\t Accuracy 0.9386\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2839\t Accuracy 0.9318\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2923\t Accuracy 0.9264\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2907\t Accuracy 0.9282\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2896\t Accuracy 0.9287\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2905\t Accuracy 0.9283\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2928\t Accuracy 0.9272\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2927\t Accuracy 0.9270\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2938\t Accuracy 0.9266\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2955\t Accuracy 0.9262\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2953\t Average training accuracy 0.9262\n",
      "Epoch [3]\t Average validation loss 0.2330\t Average validation accuracy 0.9492\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2520\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2643\t Accuracy 0.9406\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2760\t Accuracy 0.9342\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2841\t Accuracy 0.9288\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2828\t Accuracy 0.9305\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2819\t Accuracy 0.9310\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2829\t Accuracy 0.9309\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2853\t Accuracy 0.9299\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2853\t Accuracy 0.9295\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2865\t Accuracy 0.9292\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2883\t Accuracy 0.9288\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2882\t Average training accuracy 0.9287\n",
      "Epoch [4]\t Average validation loss 0.2283\t Average validation accuracy 0.9514\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2471\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2590\t Accuracy 0.9420\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2706\t Accuracy 0.9355\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2786\t Accuracy 0.9304\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2776\t Accuracy 0.9321\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2768\t Accuracy 0.9324\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2779\t Accuracy 0.9322\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2803\t Accuracy 0.9314\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2804\t Accuracy 0.9310\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2816\t Accuracy 0.9307\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2834\t Accuracy 0.9303\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2834\t Average training accuracy 0.9303\n",
      "Epoch [5]\t Average validation loss 0.2249\t Average validation accuracy 0.9534\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2433\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2552\t Accuracy 0.9420\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2668\t Accuracy 0.9366\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2747\t Accuracy 0.9316\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2738\t Accuracy 0.9331\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2732\t Accuracy 0.9335\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2742\t Accuracy 0.9333\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2767\t Accuracy 0.9325\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2768\t Accuracy 0.9320\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2780\t Accuracy 0.9317\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2798\t Accuracy 0.9313\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2799\t Average training accuracy 0.9313\n",
      "Epoch [6]\t Average validation loss 0.2224\t Average validation accuracy 0.9544\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2406\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2524\t Accuracy 0.9429\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2639\t Accuracy 0.9376\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2717\t Accuracy 0.9328\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2710\t Accuracy 0.9342\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2704\t Accuracy 0.9345\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2714\t Accuracy 0.9344\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2739\t Accuracy 0.9336\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2740\t Accuracy 0.9330\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2752\t Accuracy 0.9327\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2771\t Accuracy 0.9323\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2772\t Average training accuracy 0.9323\n",
      "Epoch [7]\t Average validation loss 0.2205\t Average validation accuracy 0.9552\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2385\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2499\t Accuracy 0.9441\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2615\t Accuracy 0.9385\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2693\t Accuracy 0.9336\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2686\t Accuracy 0.9351\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2681\t Accuracy 0.9355\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2692\t Accuracy 0.9354\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2716\t Accuracy 0.9346\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2718\t Accuracy 0.9340\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2730\t Accuracy 0.9337\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2749\t Accuracy 0.9332\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2750\t Average training accuracy 0.9331\n",
      "Epoch [8]\t Average validation loss 0.2189\t Average validation accuracy 0.9552\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2363\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2481\t Accuracy 0.9439\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2596\t Accuracy 0.9386\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2674\t Accuracy 0.9338\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2669\t Accuracy 0.9354\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2663\t Accuracy 0.9357\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2674\t Accuracy 0.9356\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2699\t Accuracy 0.9348\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2701\t Accuracy 0.9343\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2713\t Accuracy 0.9340\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2732\t Accuracy 0.9335\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2733\t Average training accuracy 0.9334\n",
      "Epoch [9]\t Average validation loss 0.2177\t Average validation accuracy 0.9550\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2345\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2466\t Accuracy 0.9441\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2581\t Accuracy 0.9391\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2659\t Accuracy 0.9344\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2654\t Accuracy 0.9361\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2649\t Accuracy 0.9363\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2660\t Accuracy 0.9360\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2684\t Accuracy 0.9354\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2687\t Accuracy 0.9348\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2698\t Accuracy 0.9344\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2717\t Accuracy 0.9339\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2719\t Average training accuracy 0.9338\n",
      "Epoch [10]\t Average validation loss 0.2166\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2333\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2453\t Accuracy 0.9447\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2568\t Accuracy 0.9395\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2647\t Accuracy 0.9350\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2642\t Accuracy 0.9364\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2637\t Accuracy 0.9366\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2648\t Accuracy 0.9363\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2672\t Accuracy 0.9356\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2674\t Accuracy 0.9351\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2686\t Accuracy 0.9347\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2705\t Accuracy 0.9343\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2707\t Average training accuracy 0.9342\n",
      "Epoch [11]\t Average validation loss 0.2156\t Average validation accuracy 0.9562\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2322\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2441\t Accuracy 0.9445\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2557\t Accuracy 0.9395\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2635\t Accuracy 0.9350\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2631\t Accuracy 0.9365\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2625\t Accuracy 0.9367\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2637\t Accuracy 0.9365\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2661\t Accuracy 0.9359\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2663\t Accuracy 0.9354\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2675\t Accuracy 0.9349\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2694\t Accuracy 0.9344\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2696\t Average training accuracy 0.9344\n",
      "Epoch [12]\t Average validation loss 0.2149\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2313\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2432\t Accuracy 0.9451\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2549\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2626\t Accuracy 0.9354\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2622\t Accuracy 0.9367\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2616\t Accuracy 0.9369\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2628\t Accuracy 0.9367\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2652\t Accuracy 0.9361\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2654\t Accuracy 0.9356\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2666\t Accuracy 0.9351\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2685\t Accuracy 0.9347\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2687\t Average training accuracy 0.9347\n",
      "Epoch [13]\t Average validation loss 0.2143\t Average validation accuracy 0.9568\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2307\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2425\t Accuracy 0.9451\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2541\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2618\t Accuracy 0.9355\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2614\t Accuracy 0.9368\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2609\t Accuracy 0.9370\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2620\t Accuracy 0.9369\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2644\t Accuracy 0.9363\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2647\t Accuracy 0.9358\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2658\t Accuracy 0.9353\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2677\t Accuracy 0.9350\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2679\t Average training accuracy 0.9350\n",
      "Epoch [14]\t Average validation loss 0.2137\t Average validation accuracy 0.9570\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2301\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2418\t Accuracy 0.9449\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2534\t Accuracy 0.9400\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2611\t Accuracy 0.9356\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2607\t Accuracy 0.9368\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2602\t Accuracy 0.9370\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2614\t Accuracy 0.9369\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2638\t Accuracy 0.9364\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2640\t Accuracy 0.9359\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2652\t Accuracy 0.9355\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2671\t Accuracy 0.9351\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2673\t Average training accuracy 0.9351\n",
      "Epoch [15]\t Average validation loss 0.2132\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2293\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2412\t Accuracy 0.9451\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2528\t Accuracy 0.9403\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2605\t Accuracy 0.9359\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2601\t Accuracy 0.9372\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2596\t Accuracy 0.9374\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2608\t Accuracy 0.9372\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2632\t Accuracy 0.9366\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2634\t Accuracy 0.9361\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2646\t Accuracy 0.9357\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2665\t Accuracy 0.9353\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2667\t Average training accuracy 0.9353\n",
      "Epoch [16]\t Average validation loss 0.2128\t Average validation accuracy 0.9580\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2289\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2407\t Accuracy 0.9453\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2524\t Accuracy 0.9404\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2600\t Accuracy 0.9360\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2596\t Accuracy 0.9373\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2591\t Accuracy 0.9376\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2603\t Accuracy 0.9374\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2627\t Accuracy 0.9368\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2629\t Accuracy 0.9364\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2641\t Accuracy 0.9359\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2660\t Accuracy 0.9355\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2662\t Average training accuracy 0.9356\n",
      "Epoch [17]\t Average validation loss 0.2125\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2285\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2403\t Accuracy 0.9457\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2519\t Accuracy 0.9407\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2595\t Accuracy 0.9362\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2592\t Accuracy 0.9375\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2587\t Accuracy 0.9377\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2599\t Accuracy 0.9375\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2622\t Accuracy 0.9369\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2625\t Accuracy 0.9365\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2637\t Accuracy 0.9361\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2656\t Accuracy 0.9357\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2658\t Average training accuracy 0.9357\n",
      "Epoch [18]\t Average validation loss 0.2121\t Average validation accuracy 0.9590\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2279\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2398\t Accuracy 0.9457\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2515\t Accuracy 0.9409\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2590\t Accuracy 0.9363\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2587\t Accuracy 0.9375\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2582\t Accuracy 0.9378\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2594\t Accuracy 0.9375\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2618\t Accuracy 0.9370\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2621\t Accuracy 0.9366\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2633\t Accuracy 0.9362\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2652\t Accuracy 0.9358\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2654\t Average training accuracy 0.9358\n",
      "Epoch [19]\t Average validation loss 0.2118\t Average validation accuracy 0.9590\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.6949\t Accuracy 0.0600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.0252\t Accuracy 0.1420\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.7920\t Accuracy 0.1750\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7092\t Accuracy 0.1926\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6623\t Accuracy 0.2087\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.6290\t Accuracy 0.2259\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6044\t Accuracy 0.2429\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.5843\t Accuracy 0.2582\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.5671\t Accuracy 0.2758\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.5528\t Accuracy 0.2905\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5402\t Accuracy 0.3059\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5284\t Average training accuracy 0.3223\n",
      "Epoch [0]\t Average validation loss 0.3984\t Average validation accuracy 0.5068\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.3781\t Accuracy 0.5900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.3983\t Accuracy 0.5043\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.3925\t Accuracy 0.5188\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3922\t Accuracy 0.5193\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3906\t Accuracy 0.5215\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3876\t Accuracy 0.5296\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3849\t Accuracy 0.5347\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3827\t Accuracy 0.5389\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3797\t Accuracy 0.5447\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3773\t Accuracy 0.5492\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3750\t Accuracy 0.5548\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3720\t Average training accuracy 0.5624\n",
      "Epoch [1]\t Average validation loss 0.3307\t Average validation accuracy 0.6606\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3122\t Accuracy 0.6900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3360\t Accuracy 0.6455\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3336\t Accuracy 0.6485\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3355\t Accuracy 0.6410\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3355\t Accuracy 0.6404\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3343\t Accuracy 0.6437\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3333\t Accuracy 0.6469\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3327\t Accuracy 0.6477\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3314\t Accuracy 0.6493\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3304\t Accuracy 0.6509\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3294\t Accuracy 0.6528\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3276\t Average training accuracy 0.6564\n",
      "Epoch [2]\t Average validation loss 0.2980\t Average validation accuracy 0.7288\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2820\t Accuracy 0.7200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3056\t Accuracy 0.7016\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3045\t Accuracy 0.7048\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3072\t Accuracy 0.6989\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3077\t Accuracy 0.6981\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3070\t Accuracy 0.7000\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3065\t Accuracy 0.7014\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3066\t Accuracy 0.7009\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3058\t Accuracy 0.7022\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3054\t Accuracy 0.7029\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3048\t Accuracy 0.7039\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3036\t Average training accuracy 0.7060\n",
      "Epoch [3]\t Average validation loss 0.2782\t Average validation accuracy 0.7632\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2648\t Accuracy 0.7500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2871\t Accuracy 0.7325\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2867\t Accuracy 0.7364\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2898\t Accuracy 0.7301\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2903\t Accuracy 0.7295\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2899\t Accuracy 0.7307\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2897\t Accuracy 0.7318\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2900\t Accuracy 0.7312\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2895\t Accuracy 0.7323\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2894\t Accuracy 0.7324\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2891\t Accuracy 0.7329\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2881\t Average training accuracy 0.7350\n",
      "Epoch [4]\t Average validation loss 0.2647\t Average validation accuracy 0.7828\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2535\t Accuracy 0.7500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2744\t Accuracy 0.7539\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2744\t Accuracy 0.7572\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2777\t Accuracy 0.7526\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2783\t Accuracy 0.7516\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2780\t Accuracy 0.7525\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2779\t Accuracy 0.7532\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2784\t Accuracy 0.7527\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2781\t Accuracy 0.7536\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2781\t Accuracy 0.7532\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2780\t Accuracy 0.7531\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2771\t Average training accuracy 0.7545\n",
      "Epoch [5]\t Average validation loss 0.2548\t Average validation accuracy 0.8002\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2455\t Accuracy 0.7700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2651\t Accuracy 0.7737\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2653\t Accuracy 0.7770\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2688\t Accuracy 0.7702\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2693\t Accuracy 0.7684\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2691\t Accuracy 0.7684\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2692\t Accuracy 0.7692\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2697\t Accuracy 0.7687\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2695\t Accuracy 0.7691\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2696\t Accuracy 0.7684\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2696\t Accuracy 0.7683\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2688\t Average training accuracy 0.7695\n",
      "Epoch [6]\t Average validation loss 0.2472\t Average validation accuracy 0.8150\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2395\t Accuracy 0.7700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2578\t Accuracy 0.7876\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2583\t Accuracy 0.7902\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2619\t Accuracy 0.7829\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2624\t Accuracy 0.7805\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2622\t Accuracy 0.7801\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2623\t Accuracy 0.7806\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2629\t Accuracy 0.7800\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2628\t Accuracy 0.7801\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2629\t Accuracy 0.7793\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2630\t Accuracy 0.7792\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2623\t Average training accuracy 0.7801\n",
      "Epoch [7]\t Average validation loss 0.2412\t Average validation accuracy 0.8256\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2347\t Accuracy 0.7800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2520\t Accuracy 0.7971\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2526\t Accuracy 0.7982\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2564\t Accuracy 0.7908\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2568\t Accuracy 0.7884\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2567\t Accuracy 0.7881\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2568\t Accuracy 0.7883\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2574\t Accuracy 0.7881\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2574\t Accuracy 0.7884\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2575\t Accuracy 0.7876\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2576\t Accuracy 0.7876\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2570\t Average training accuracy 0.7884\n",
      "Epoch [8]\t Average validation loss 0.2362\t Average validation accuracy 0.8336\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2308\t Accuracy 0.7800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2472\t Accuracy 0.8039\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2479\t Accuracy 0.8054\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2518\t Accuracy 0.7989\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2521\t Accuracy 0.7967\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2520\t Accuracy 0.7964\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2522\t Accuracy 0.7963\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2528\t Accuracy 0.7956\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2528\t Accuracy 0.7958\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2531\t Accuracy 0.7950\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2532\t Accuracy 0.7950\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2526\t Average training accuracy 0.7957\n",
      "Epoch [9]\t Average validation loss 0.2320\t Average validation accuracy 0.8408\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2275\t Accuracy 0.8000\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2431\t Accuracy 0.8124\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2440\t Accuracy 0.8129\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2479\t Accuracy 0.8056\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2482\t Accuracy 0.8033\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2481\t Accuracy 0.8025\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2483\t Accuracy 0.8021\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2490\t Accuracy 0.8015\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2490\t Accuracy 0.8016\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2493\t Accuracy 0.8007\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2494\t Accuracy 0.8006\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2488\t Average training accuracy 0.8014\n",
      "Epoch [10]\t Average validation loss 0.2285\t Average validation accuracy 0.8468\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2246\t Accuracy 0.8200\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2396\t Accuracy 0.8188\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2406\t Accuracy 0.8179\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2445\t Accuracy 0.8107\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2449\t Accuracy 0.8086\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2448\t Accuracy 0.8078\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2450\t Accuracy 0.8074\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2457\t Accuracy 0.8068\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2457\t Accuracy 0.8068\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2460\t Accuracy 0.8057\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2462\t Accuracy 0.8054\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2456\t Average training accuracy 0.8061\n",
      "Epoch [11]\t Average validation loss 0.2254\t Average validation accuracy 0.8518\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2221\t Accuracy 0.8200\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2366\t Accuracy 0.8227\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2376\t Accuracy 0.8202\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2416\t Accuracy 0.8138\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2419\t Accuracy 0.8121\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2419\t Accuracy 0.8118\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2421\t Accuracy 0.8113\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2427\t Accuracy 0.8108\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2428\t Accuracy 0.8109\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2431\t Accuracy 0.8097\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2433\t Accuracy 0.8094\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2428\t Average training accuracy 0.8101\n",
      "Epoch [12]\t Average validation loss 0.2226\t Average validation accuracy 0.8542\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2199\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2339\t Accuracy 0.8280\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2350\t Accuracy 0.8241\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2391\t Accuracy 0.8174\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2393\t Accuracy 0.8166\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2393\t Accuracy 0.8156\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2395\t Accuracy 0.8149\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2402\t Accuracy 0.8143\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2403\t Accuracy 0.8147\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2406\t Accuracy 0.8135\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2408\t Accuracy 0.8132\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2402\t Average training accuracy 0.8138\n",
      "Epoch [13]\t Average validation loss 0.2202\t Average validation accuracy 0.8576\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2179\t Accuracy 0.8400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2315\t Accuracy 0.8306\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2327\t Accuracy 0.8273\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2368\t Accuracy 0.8204\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2370\t Accuracy 0.8198\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2370\t Accuracy 0.8188\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2372\t Accuracy 0.8181\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2379\t Accuracy 0.8173\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2380\t Accuracy 0.8177\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2383\t Accuracy 0.8167\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2385\t Accuracy 0.8164\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2380\t Average training accuracy 0.8171\n",
      "Epoch [14]\t Average validation loss 0.2181\t Average validation accuracy 0.8592\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2160\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2293\t Accuracy 0.8335\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2306\t Accuracy 0.8303\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2347\t Accuracy 0.8228\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2349\t Accuracy 0.8225\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2349\t Accuracy 0.8216\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2351\t Accuracy 0.8209\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2358\t Accuracy 0.8203\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2360\t Accuracy 0.8208\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2363\t Accuracy 0.8197\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2365\t Accuracy 0.8194\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2360\t Average training accuracy 0.8200\n",
      "Epoch [15]\t Average validation loss 0.2161\t Average validation accuracy 0.8612\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2144\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2274\t Accuracy 0.8359\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2287\t Accuracy 0.8323\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2328\t Accuracy 0.8248\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2331\t Accuracy 0.8245\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2330\t Accuracy 0.8237\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2332\t Accuracy 0.8229\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2339\t Accuracy 0.8224\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2341\t Accuracy 0.8230\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2344\t Accuracy 0.8221\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2347\t Accuracy 0.8216\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2341\t Average training accuracy 0.8223\n",
      "Epoch [16]\t Average validation loss 0.2143\t Average validation accuracy 0.8628\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2128\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2256\t Accuracy 0.8375\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2269\t Accuracy 0.8339\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2311\t Accuracy 0.8264\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2313\t Accuracy 0.8263\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2313\t Accuracy 0.8255\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2315\t Accuracy 0.8247\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2322\t Accuracy 0.8244\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2324\t Accuracy 0.8252\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2327\t Accuracy 0.8243\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2330\t Accuracy 0.8238\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2325\t Average training accuracy 0.8244\n",
      "Epoch [17]\t Average validation loss 0.2127\t Average validation accuracy 0.8644\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2114\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2239\t Accuracy 0.8390\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2254\t Accuracy 0.8353\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2295\t Accuracy 0.8281\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2297\t Accuracy 0.8280\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2297\t Accuracy 0.8275\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2299\t Accuracy 0.8268\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2306\t Accuracy 0.8264\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2308\t Accuracy 0.8272\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2312\t Accuracy 0.8261\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2314\t Accuracy 0.8257\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2309\t Average training accuracy 0.8262\n",
      "Epoch [18]\t Average validation loss 0.2112\t Average validation accuracy 0.8654\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2101\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2224\t Accuracy 0.8402\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2239\t Accuracy 0.8363\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2281\t Accuracy 0.8297\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2283\t Accuracy 0.8293\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2283\t Accuracy 0.8288\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2285\t Accuracy 0.8280\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2292\t Accuracy 0.8276\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2294\t Accuracy 0.8285\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2297\t Accuracy 0.8275\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2300\t Accuracy 0.8270\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2295\t Average training accuracy 0.8277\n",
      "Epoch [19]\t Average validation loss 0.2098\t Average validation accuracy 0.8664\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.7407\t Accuracy 0.2100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.3253\t Accuracy 0.2625\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8826\t Accuracy 0.3499\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7149\t Accuracy 0.3858\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6174\t Accuracy 0.4310\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5511\t Accuracy 0.4788\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.5005\t Accuracy 0.5227\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4614\t Accuracy 0.5569\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4286\t Accuracy 0.5871\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4016\t Accuracy 0.6130\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3783\t Accuracy 0.6355\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3582\t Average training accuracy 0.6555\n",
      "Epoch [0]\t Average validation loss 0.1304\t Average validation accuracy 0.8944\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1325\t Accuracy 0.9000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1397\t Accuracy 0.8796\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1385\t Accuracy 0.8745\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1395\t Accuracy 0.8709\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1371\t Accuracy 0.8722\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1349\t Accuracy 0.8737\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1329\t Accuracy 0.8762\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1319\t Accuracy 0.8761\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1306\t Accuracy 0.8773\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1292\t Accuracy 0.8788\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1281\t Accuracy 0.8799\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1265\t Average training accuracy 0.8815\n",
      "Epoch [1]\t Average validation loss 0.0938\t Average validation accuracy 0.9232\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0926\t Accuracy 0.9300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1025\t Accuracy 0.9133\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1044\t Accuracy 0.9105\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1072\t Accuracy 0.9063\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1066\t Accuracy 0.9071\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1062\t Accuracy 0.9070\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1059\t Accuracy 0.9065\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1061\t Accuracy 0.9062\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1060\t Accuracy 0.9059\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1058\t Accuracy 0.9063\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1058\t Accuracy 0.9061\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1051\t Average training accuracy 0.9066\n",
      "Epoch [2]\t Average validation loss 0.0844\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0824\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0916\t Accuracy 0.9271\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0938\t Accuracy 0.9215\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0968\t Accuracy 0.9172\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0963\t Accuracy 0.9176\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0961\t Accuracy 0.9175\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0962\t Accuracy 0.9173\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0965\t Accuracy 0.9169\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0966\t Accuracy 0.9162\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0966\t Accuracy 0.9165\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0967\t Accuracy 0.9161\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0963\t Average training accuracy 0.9161\n",
      "Epoch [3]\t Average validation loss 0.0788\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0763\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0853\t Accuracy 0.9341\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0875\t Accuracy 0.9285\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0905\t Accuracy 0.9246\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0900\t Accuracy 0.9253\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0899\t Accuracy 0.9251\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0901\t Accuracy 0.9245\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0904\t Accuracy 0.9240\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0906\t Accuracy 0.9234\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0906\t Accuracy 0.9233\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0909\t Accuracy 0.9226\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0905\t Average training accuracy 0.9226\n",
      "Epoch [4]\t Average validation loss 0.0749\t Average validation accuracy 0.9436\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0726\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0809\t Accuracy 0.9376\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0831\t Accuracy 0.9332\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0860\t Accuracy 0.9293\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0855\t Accuracy 0.9305\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0853\t Accuracy 0.9302\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0856\t Accuracy 0.9299\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0858\t Accuracy 0.9291\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0861\t Accuracy 0.9286\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0861\t Accuracy 0.9283\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0864\t Accuracy 0.9275\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0861\t Average training accuracy 0.9273\n",
      "Epoch [5]\t Average validation loss 0.0719\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0694\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0774\t Accuracy 0.9406\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0796\t Accuracy 0.9364\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0823\t Accuracy 0.9332\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0818\t Accuracy 0.9342\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0817\t Accuracy 0.9339\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0820\t Accuracy 0.9338\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0823\t Accuracy 0.9331\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0825\t Accuracy 0.9326\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0825\t Accuracy 0.9326\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0829\t Accuracy 0.9319\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0827\t Average training accuracy 0.9317\n",
      "Epoch [6]\t Average validation loss 0.0697\t Average validation accuracy 0.9486\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0665\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0745\t Accuracy 0.9435\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0766\t Accuracy 0.9396\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0793\t Accuracy 0.9361\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0788\t Accuracy 0.9375\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0786\t Accuracy 0.9371\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0789\t Accuracy 0.9370\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0791\t Accuracy 0.9363\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0794\t Accuracy 0.9359\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0794\t Accuracy 0.9359\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0798\t Accuracy 0.9353\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0795\t Average training accuracy 0.9351\n",
      "Epoch [7]\t Average validation loss 0.0677\t Average validation accuracy 0.9514\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0637\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0720\t Accuracy 0.9461\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0740\t Accuracy 0.9417\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0766\t Accuracy 0.9381\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0761\t Accuracy 0.9394\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0760\t Accuracy 0.9393\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0763\t Accuracy 0.9391\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0765\t Accuracy 0.9386\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0768\t Accuracy 0.9383\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0768\t Accuracy 0.9384\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0772\t Accuracy 0.9379\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0770\t Average training accuracy 0.9378\n",
      "Epoch [8]\t Average validation loss 0.0661\t Average validation accuracy 0.9538\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0613\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0699\t Accuracy 0.9480\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0718\t Accuracy 0.9449\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0743\t Accuracy 0.9407\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0739\t Accuracy 0.9422\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0737\t Accuracy 0.9418\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0741\t Accuracy 0.9415\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0743\t Accuracy 0.9410\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0746\t Accuracy 0.9407\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0746\t Accuracy 0.9407\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0749\t Accuracy 0.9402\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0747\t Average training accuracy 0.9402\n",
      "Epoch [9]\t Average validation loss 0.0646\t Average validation accuracy 0.9556\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0593\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0680\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0697\t Accuracy 0.9477\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0722\t Accuracy 0.9436\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0718\t Accuracy 0.9448\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0716\t Accuracy 0.9446\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0720\t Accuracy 0.9443\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0723\t Accuracy 0.9436\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0725\t Accuracy 0.9434\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0725\t Accuracy 0.9434\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0729\t Accuracy 0.9427\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0727\t Average training accuracy 0.9427\n",
      "Epoch [10]\t Average validation loss 0.0631\t Average validation accuracy 0.9564\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0581\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0663\t Accuracy 0.9518\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0679\t Accuracy 0.9494\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0703\t Accuracy 0.9458\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0699\t Accuracy 0.9468\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0697\t Accuracy 0.9465\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0701\t Accuracy 0.9462\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0703\t Accuracy 0.9459\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0705\t Accuracy 0.9456\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0706\t Accuracy 0.9457\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0709\t Accuracy 0.9450\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0708\t Average training accuracy 0.9449\n",
      "Epoch [11]\t Average validation loss 0.0618\t Average validation accuracy 0.9568\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0568\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0646\t Accuracy 0.9525\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0662\t Accuracy 0.9505\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0686\t Accuracy 0.9477\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0682\t Accuracy 0.9488\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0680\t Accuracy 0.9486\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0684\t Accuracy 0.9482\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0686\t Accuracy 0.9481\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0688\t Accuracy 0.9478\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0689\t Accuracy 0.9478\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0693\t Accuracy 0.9471\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0691\t Average training accuracy 0.9470\n",
      "Epoch [12]\t Average validation loss 0.0606\t Average validation accuracy 0.9582\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0559\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0631\t Accuracy 0.9561\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0648\t Accuracy 0.9527\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0671\t Accuracy 0.9502\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0667\t Accuracy 0.9508\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0665\t Accuracy 0.9507\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0669\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0671\t Accuracy 0.9498\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0673\t Accuracy 0.9496\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0674\t Accuracy 0.9495\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0678\t Accuracy 0.9490\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0677\t Average training accuracy 0.9488\n",
      "Epoch [13]\t Average validation loss 0.0596\t Average validation accuracy 0.9590\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0619\t Accuracy 0.9571\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0635\t Accuracy 0.9539\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0658\t Accuracy 0.9515\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0654\t Accuracy 0.9521\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0652\t Accuracy 0.9521\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0656\t Accuracy 0.9515\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0658\t Accuracy 0.9514\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0660\t Accuracy 0.9509\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0661\t Accuracy 0.9509\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0665\t Accuracy 0.9503\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0664\t Average training accuracy 0.9502\n",
      "Epoch [14]\t Average validation loss 0.0586\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0548\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0608\t Accuracy 0.9588\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0625\t Accuracy 0.9554\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0647\t Accuracy 0.9530\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0643\t Accuracy 0.9534\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0640\t Accuracy 0.9533\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.9528\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.9527\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0649\t Accuracy 0.9523\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0649\t Accuracy 0.9522\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0654\t Accuracy 0.9517\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0653\t Average training accuracy 0.9517\n",
      "Epoch [15]\t Average validation loss 0.0578\t Average validation accuracy 0.9604\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0542\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0599\t Accuracy 0.9596\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0615\t Accuracy 0.9562\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0637\t Accuracy 0.9538\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0633\t Accuracy 0.9542\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0631\t Accuracy 0.9543\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0635\t Accuracy 0.9539\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0636\t Accuracy 0.9539\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0639\t Accuracy 0.9534\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0639\t Accuracy 0.9533\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0644\t Accuracy 0.9527\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0643\t Average training accuracy 0.9527\n",
      "Epoch [16]\t Average validation loss 0.0570\t Average validation accuracy 0.9608\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0535\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0591\t Accuracy 0.9608\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0606\t Accuracy 0.9570\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0628\t Accuracy 0.9548\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0624\t Accuracy 0.9555\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0622\t Accuracy 0.9556\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0626\t Accuracy 0.9551\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0627\t Accuracy 0.9552\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0629\t Accuracy 0.9545\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0630\t Accuracy 0.9544\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0634\t Accuracy 0.9537\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0634\t Average training accuracy 0.9538\n",
      "Epoch [17]\t Average validation loss 0.0564\t Average validation accuracy 0.9620\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0531\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0584\t Accuracy 0.9612\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0599\t Accuracy 0.9579\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0620\t Accuracy 0.9560\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0616\t Accuracy 0.9567\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0614\t Accuracy 0.9569\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0618\t Accuracy 0.9563\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0619\t Accuracy 0.9563\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0622\t Accuracy 0.9556\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0622\t Accuracy 0.9555\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0626\t Accuracy 0.9549\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0626\t Average training accuracy 0.9549\n",
      "Epoch [18]\t Average validation loss 0.0559\t Average validation accuracy 0.9626\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0526\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0577\t Accuracy 0.9616\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0592\t Accuracy 0.9585\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0612\t Accuracy 0.9568\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0609\t Accuracy 0.9576\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0606\t Accuracy 0.9579\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0610\t Accuracy 0.9570\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0612\t Accuracy 0.9570\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0614\t Accuracy 0.9564\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0615\t Accuracy 0.9562\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0619\t Accuracy 0.9556\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0618\t Average training accuracy 0.9556\n",
      "Epoch [19]\t Average validation loss 0.0554\t Average validation accuracy 0.9636\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4019\t Accuracy 0.1800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4165\t Accuracy 0.1227\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3753\t Accuracy 0.1403\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3386\t Accuracy 0.1587\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3076\t Accuracy 0.1803\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2830\t Accuracy 0.1974\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2606\t Accuracy 0.2161\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2432\t Accuracy 0.2304\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2268\t Accuracy 0.2461\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2123\t Accuracy 0.2607\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1980\t Accuracy 0.2759\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1845\t Average training accuracy 0.2908\n",
      "Epoch [0]\t Average validation loss 2.0198\t Average validation accuracy 0.4844\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.0116\t Accuracy 0.5800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0228\t Accuracy 0.4718\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0122\t Accuracy 0.4862\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0042\t Accuracy 0.4896\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.9945\t Accuracy 0.4970\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.9842\t Accuracy 0.5019\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.9730\t Accuracy 0.5106\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.9661\t Accuracy 0.5152\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.9570\t Accuracy 0.5226\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.9491\t Accuracy 0.5284\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.9402\t Accuracy 0.5345\n",
      "\n",
      "Epoch [1]\t Average training loss 1.9311\t Average training accuracy 0.5411\n",
      "Epoch [1]\t Average validation loss 1.8033\t Average validation accuracy 0.6494\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.7952\t Accuracy 0.7200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.8149\t Accuracy 0.6204\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.8061\t Accuracy 0.6282\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.8013\t Accuracy 0.6246\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.7942\t Accuracy 0.6292\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.7850\t Accuracy 0.6320\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.7752\t Accuracy 0.6375\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.7708\t Accuracy 0.6395\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.7629\t Accuracy 0.6435\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.7567\t Accuracy 0.6469\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.7492\t Accuracy 0.6500\n",
      "\n",
      "Epoch [2]\t Average training loss 1.7412\t Average training accuracy 0.6534\n",
      "Epoch [2]\t Average validation loss 1.6183\t Average validation accuracy 0.7328\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.6110\t Accuracy 0.7900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.6374\t Accuracy 0.6998\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.6303\t Accuracy 0.6993\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.6283\t Accuracy 0.6935\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.6231\t Accuracy 0.6966\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.6151\t Accuracy 0.6974\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.6064\t Accuracy 0.7006\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.6041\t Accuracy 0.7013\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.5972\t Accuracy 0.7037\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.5925\t Accuracy 0.7058\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.5861\t Accuracy 0.7075\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5791\t Average training accuracy 0.7097\n",
      "Epoch [3]\t Average validation loss 1.4604\t Average validation accuracy 0.7750\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.4552\t Accuracy 0.8100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.4862\t Accuracy 0.7349\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.4808\t Accuracy 0.7387\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.4811\t Accuracy 0.7337\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.4776\t Accuracy 0.7346\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.4705\t Accuracy 0.7343\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.4629\t Accuracy 0.7366\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.4622\t Accuracy 0.7363\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.4563\t Accuracy 0.7381\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.4527\t Accuracy 0.7396\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.4474\t Accuracy 0.7406\n",
      "\n",
      "Epoch [4]\t Average training loss 1.4413\t Average training accuracy 0.7424\n",
      "Epoch [4]\t Average validation loss 1.3262\t Average validation accuracy 0.8016\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.3240\t Accuracy 0.8200\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.3578\t Accuracy 0.7596\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.3539\t Accuracy 0.7625\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.3563\t Accuracy 0.7577\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.3541\t Accuracy 0.7587\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.3479\t Accuracy 0.7586\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.3412\t Accuracy 0.7606\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.3418\t Accuracy 0.7598\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.3367\t Accuracy 0.7614\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.3341\t Accuracy 0.7625\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.3296\t Accuracy 0.7629\n",
      "\n",
      "Epoch [5]\t Average training loss 1.3242\t Average training accuracy 0.7641\n",
      "Epoch [5]\t Average validation loss 1.2124\t Average validation accuracy 0.8216\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.2139\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.2490\t Accuracy 0.7806\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.2465\t Accuracy 0.7816\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.2505\t Accuracy 0.7761\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.2494\t Accuracy 0.7766\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.2439\t Accuracy 0.7767\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.2381\t Accuracy 0.7785\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.2397\t Accuracy 0.7780\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.2353\t Accuracy 0.7793\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.2334\t Accuracy 0.7800\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.2297\t Accuracy 0.7803\n",
      "\n",
      "Epoch [6]\t Average training loss 1.2250\t Average training accuracy 0.7812\n",
      "Epoch [6]\t Average validation loss 1.1159\t Average validation accuracy 0.8358\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.1214\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.1567\t Accuracy 0.7975\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.1554\t Accuracy 0.7973\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.1609\t Accuracy 0.7907\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.1605\t Accuracy 0.7914\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.1556\t Accuracy 0.7914\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.1506\t Accuracy 0.7927\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.1530\t Accuracy 0.7917\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.1492\t Accuracy 0.7929\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.1478\t Accuracy 0.7928\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.1448\t Accuracy 0.7926\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1406\t Average training accuracy 0.7935\n",
      "Epoch [7]\t Average validation loss 1.0337\t Average validation accuracy 0.8476\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.0435\t Accuracy 0.8200\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.0782\t Accuracy 0.8059\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.0780\t Accuracy 0.8058\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.0846\t Accuracy 0.8002\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.0849\t Accuracy 0.8012\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.0804\t Accuracy 0.8014\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.0760\t Accuracy 0.8024\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.0791\t Accuracy 0.8014\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.0757\t Accuracy 0.8027\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.0749\t Accuracy 0.8027\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.0724\t Accuracy 0.8023\n",
      "\n",
      "Epoch [8]\t Average training loss 1.0686\t Average training accuracy 0.8032\n",
      "Epoch [8]\t Average validation loss 0.9636\t Average validation accuracy 0.8554\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.9776\t Accuracy 0.8200\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.0110\t Accuracy 0.8149\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.0118\t Accuracy 0.8139\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.0193\t Accuracy 0.8087\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.0201\t Accuracy 0.8096\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.0160\t Accuracy 0.8096\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.0122\t Accuracy 0.8108\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.0158\t Accuracy 0.8097\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.0128\t Accuracy 0.8107\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.0123\t Accuracy 0.8105\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.0103\t Accuracy 0.8098\n",
      "\n",
      "Epoch [9]\t Average training loss 1.0069\t Average training accuracy 0.8107\n",
      "Epoch [9]\t Average validation loss 0.9034\t Average validation accuracy 0.8634\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.9215\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.9533\t Accuracy 0.8224\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.9549\t Accuracy 0.8219\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.9632\t Accuracy 0.8165\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.9643\t Accuracy 0.8168\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.9606\t Accuracy 0.8167\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.9572\t Accuracy 0.8178\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.9613\t Accuracy 0.8166\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.9586\t Accuracy 0.8177\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.9584\t Accuracy 0.8173\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.9567\t Accuracy 0.8162\n",
      "\n",
      "Epoch [10]\t Average training loss 0.9536\t Average training accuracy 0.8170\n",
      "Epoch [10]\t Average validation loss 0.8514\t Average validation accuracy 0.8684\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8734\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.9033\t Accuracy 0.8280\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.9056\t Accuracy 0.8275\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.9147\t Accuracy 0.8224\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.9160\t Accuracy 0.8228\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.9126\t Accuracy 0.8227\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.9096\t Accuracy 0.8235\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.9139\t Accuracy 0.8223\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.9115\t Accuracy 0.8231\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.9115\t Accuracy 0.8228\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.9102\t Accuracy 0.8219\n",
      "\n",
      "Epoch [11]\t Average training loss 0.9074\t Average training accuracy 0.8226\n",
      "Epoch [11]\t Average validation loss 0.8062\t Average validation accuracy 0.8724\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8318\t Accuracy 0.8400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8598\t Accuracy 0.8335\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8627\t Accuracy 0.8320\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8723\t Accuracy 0.8267\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8739\t Accuracy 0.8276\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8707\t Accuracy 0.8277\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8681\t Accuracy 0.8284\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8726\t Accuracy 0.8272\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8704\t Accuracy 0.8279\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8706\t Accuracy 0.8274\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8696\t Accuracy 0.8267\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8670\t Average training accuracy 0.8274\n",
      "Epoch [12]\t Average validation loss 0.7666\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7956\t Accuracy 0.8400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8217\t Accuracy 0.8373\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8252\t Accuracy 0.8361\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8352\t Accuracy 0.8309\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8369\t Accuracy 0.8315\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8339\t Accuracy 0.8317\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8316\t Accuracy 0.8325\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8363\t Accuracy 0.8312\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8343\t Accuracy 0.8318\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8346\t Accuracy 0.8314\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8338\t Accuracy 0.8306\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8314\t Average training accuracy 0.8314\n",
      "Epoch [13]\t Average validation loss 0.7318\t Average validation accuracy 0.8788\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7639\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7880\t Accuracy 0.8406\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7920\t Accuracy 0.8395\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8025\t Accuracy 0.8345\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8042\t Accuracy 0.8349\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8014\t Accuracy 0.8355\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7993\t Accuracy 0.8363\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8042\t Accuracy 0.8350\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8023\t Accuracy 0.8356\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8028\t Accuracy 0.8350\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8022\t Accuracy 0.8343\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8000\t Average training accuracy 0.8352\n",
      "Epoch [14]\t Average validation loss 0.7010\t Average validation accuracy 0.8814\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7358\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7581\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7625\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7733\t Accuracy 0.8379\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7751\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7725\t Accuracy 0.8390\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7706\t Accuracy 0.8397\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7756\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7739\t Accuracy 0.8389\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.7745\t Accuracy 0.8382\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.7740\t Accuracy 0.8375\n",
      "\n",
      "Epoch [15]\t Average training loss 0.7720\t Average training accuracy 0.8381\n",
      "Epoch [15]\t Average validation loss 0.6735\t Average validation accuracy 0.8842\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7109\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7313\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7362\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7473\t Accuracy 0.8423\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.7492\t Accuracy 0.8419\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7466\t Accuracy 0.8426\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7450\t Accuracy 0.8432\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.7500\t Accuracy 0.8418\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.7484\t Accuracy 0.8422\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.7491\t Accuracy 0.8417\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.7488\t Accuracy 0.8408\n",
      "\n",
      "Epoch [16]\t Average training loss 0.7469\t Average training accuracy 0.8414\n",
      "Epoch [16]\t Average validation loss 0.6490\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.6886\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7073\t Accuracy 0.8533\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7126\t Accuracy 0.8497\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.7240\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.7258\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.7234\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.7219\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.7270\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.7255\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.7263\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.7261\t Accuracy 0.8433\n",
      "\n",
      "Epoch [17]\t Average training loss 0.7243\t Average training accuracy 0.8438\n",
      "Epoch [17]\t Average validation loss 0.6269\t Average validation accuracy 0.8888\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.6685\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.6857\t Accuracy 0.8576\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.6913\t Accuracy 0.8537\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.7029\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.7047\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.7024\t Accuracy 0.8487\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.7011\t Accuracy 0.8488\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.7062\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.7048\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.7056\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.7056\t Accuracy 0.8464\n",
      "\n",
      "Epoch [18]\t Average training loss 0.7039\t Average training accuracy 0.8468\n",
      "Epoch [18]\t Average validation loss 0.6069\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.6503\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.6660\t Accuracy 0.8602\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.6719\t Accuracy 0.8564\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.6837\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.6856\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.6833\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.6821\t Accuracy 0.8517\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.6873\t Accuracy 0.8504\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.6860\t Accuracy 0.8506\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.6868\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.6870\t Accuracy 0.8491\n",
      "\n",
      "Epoch [19]\t Average training loss 0.6854\t Average training accuracy 0.8493\n",
      "Epoch [19]\t Average validation loss 0.5888\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.3559\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.9201\t Accuracy 0.3639\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4148\t Accuracy 0.5514\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1446\t Accuracy 0.6392\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9743\t Accuracy 0.6939\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8622\t Accuracy 0.7306\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.7850\t Accuracy 0.7565\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7292\t Accuracy 0.7749\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6805\t Accuracy 0.7904\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6422\t Accuracy 0.8028\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6110\t Accuracy 0.8129\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5844\t Average training accuracy 0.8215\n",
      "Epoch [0]\t Average validation loss 0.2258\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2900\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2613\t Accuracy 0.9288\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2690\t Accuracy 0.9258\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2754\t Accuracy 0.9226\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2687\t Accuracy 0.9245\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2652\t Accuracy 0.9252\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2638\t Accuracy 0.9259\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2622\t Accuracy 0.9262\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2589\t Accuracy 0.9270\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2569\t Accuracy 0.9275\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2559\t Accuracy 0.9275\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2541\t Average training accuracy 0.9281\n",
      "Epoch [1]\t Average validation loss 0.1738\t Average validation accuracy 0.9558\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1878\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1967\t Accuracy 0.9476\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2064\t Accuracy 0.9434\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2126\t Accuracy 0.9406\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2087\t Accuracy 0.9416\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2073\t Accuracy 0.9423\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2072\t Accuracy 0.9427\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2070\t Accuracy 0.9424\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2054\t Accuracy 0.9428\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2049\t Accuracy 0.9427\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2054\t Accuracy 0.9424\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2047\t Average training accuracy 0.9427\n",
      "Epoch [2]\t Average validation loss 0.1527\t Average validation accuracy 0.9614\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1562\t Accuracy 0.9700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1667\t Accuracy 0.9567\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1757\t Accuracy 0.9513\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1804\t Accuracy 0.9483\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1778\t Accuracy 0.9496\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1770\t Accuracy 0.9499\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1772\t Accuracy 0.9502\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1774\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1762\t Accuracy 0.9506\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1761\t Accuracy 0.9508\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1771\t Accuracy 0.9503\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1768\t Average training accuracy 0.9506\n",
      "Epoch [3]\t Average validation loss 0.1386\t Average validation accuracy 0.9628\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1356\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1476\t Accuracy 0.9624\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1554\t Accuracy 0.9566\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1583\t Accuracy 0.9547\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1566\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1560\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1564\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1568\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1559\t Accuracy 0.9563\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1562\t Accuracy 0.9566\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1573\t Accuracy 0.9561\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1572\t Average training accuracy 0.9563\n",
      "Epoch [4]\t Average validation loss 0.1274\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1216\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1340\t Accuracy 0.9653\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1404\t Accuracy 0.9617\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1423\t Accuracy 0.9597\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1411\t Accuracy 0.9604\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1405\t Accuracy 0.9607\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1411\t Accuracy 0.9604\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1416\t Accuracy 0.9604\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1408\t Accuracy 0.9610\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1410\t Accuracy 0.9612\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1423\t Accuracy 0.9606\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1422\t Average training accuracy 0.9608\n",
      "Epoch [5]\t Average validation loss 0.1185\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1126\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1227\t Accuracy 0.9696\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1286\t Accuracy 0.9655\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1299\t Accuracy 0.9640\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1291\t Accuracy 0.9647\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1285\t Accuracy 0.9649\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1292\t Accuracy 0.9646\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1297\t Accuracy 0.9644\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1290\t Accuracy 0.9648\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1293\t Accuracy 0.9648\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1307\t Accuracy 0.9642\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1306\t Average training accuracy 0.9644\n",
      "Epoch [6]\t Average validation loss 0.1121\t Average validation accuracy 0.9706\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1073\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1137\t Accuracy 0.9720\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1194\t Accuracy 0.9683\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1202\t Accuracy 0.9675\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1197\t Accuracy 0.9678\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1191\t Accuracy 0.9677\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1199\t Accuracy 0.9674\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1204\t Accuracy 0.9674\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1198\t Accuracy 0.9679\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1202\t Accuracy 0.9680\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1215\t Accuracy 0.9673\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1214\t Average training accuracy 0.9676\n",
      "Epoch [7]\t Average validation loss 0.1069\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1035\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1064\t Accuracy 0.9741\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1121\t Accuracy 0.9712\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1125\t Accuracy 0.9706\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1122\t Accuracy 0.9704\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1116\t Accuracy 0.9703\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1125\t Accuracy 0.9699\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1129\t Accuracy 0.9697\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1125\t Accuracy 0.9701\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1128\t Accuracy 0.9702\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1142\t Accuracy 0.9696\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1141\t Average training accuracy 0.9699\n",
      "Epoch [8]\t Average validation loss 0.1029\t Average validation accuracy 0.9732\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1002\t Accuracy 0.9900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1006\t Accuracy 0.9759\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1062\t Accuracy 0.9734\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1063\t Accuracy 0.9732\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1061\t Accuracy 0.9729\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1055\t Accuracy 0.9727\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1064\t Accuracy 0.9721\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1068\t Accuracy 0.9720\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1064\t Accuracy 0.9722\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1068\t Accuracy 0.9722\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1081\t Accuracy 0.9716\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1080\t Average training accuracy 0.9718\n",
      "Epoch [9]\t Average validation loss 0.0995\t Average validation accuracy 0.9758\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0976\t Accuracy 0.9900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0956\t Accuracy 0.9771\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1012\t Accuracy 0.9752\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1010\t Accuracy 0.9751\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1009\t Accuracy 0.9748\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1002\t Accuracy 0.9745\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1012\t Accuracy 0.9740\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1016\t Accuracy 0.9738\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1013\t Accuracy 0.9741\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1017\t Accuracy 0.9741\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1030\t Accuracy 0.9734\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1029\t Average training accuracy 0.9736\n",
      "Epoch [10]\t Average validation loss 0.0967\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0943\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0914\t Accuracy 0.9775\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0970\t Accuracy 0.9761\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0966\t Accuracy 0.9760\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0966\t Accuracy 0.9760\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0959\t Accuracy 0.9760\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0969\t Accuracy 0.9754\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0973\t Accuracy 0.9753\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0971\t Accuracy 0.9756\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0974\t Accuracy 0.9755\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0988\t Accuracy 0.9750\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0986\t Average training accuracy 0.9751\n",
      "Epoch [11]\t Average validation loss 0.0943\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0929\t Accuracy 0.9900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0878\t Accuracy 0.9782\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0933\t Accuracy 0.9767\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0928\t Accuracy 0.9767\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0928\t Accuracy 0.9769\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0922\t Accuracy 0.9768\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0932\t Accuracy 0.9762\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0935\t Accuracy 0.9762\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0934\t Accuracy 0.9765\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0937\t Accuracy 0.9764\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0950\t Accuracy 0.9758\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0948\t Average training accuracy 0.9760\n",
      "Epoch [12]\t Average validation loss 0.0922\t Average validation accuracy 0.9778\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0908\t Accuracy 0.9900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0847\t Accuracy 0.9784\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0902\t Accuracy 0.9776\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0895\t Accuracy 0.9777\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0895\t Accuracy 0.9780\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0890\t Accuracy 0.9778\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0899\t Accuracy 0.9773\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0902\t Accuracy 0.9774\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0902\t Accuracy 0.9775\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0906\t Accuracy 0.9774\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0919\t Accuracy 0.9768\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0917\t Average training accuracy 0.9770\n",
      "Epoch [13]\t Average validation loss 0.0904\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0889\t Accuracy 0.9900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0820\t Accuracy 0.9792\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0874\t Accuracy 0.9781\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0867\t Accuracy 0.9787\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0867\t Accuracy 0.9787\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0862\t Accuracy 0.9785\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0872\t Accuracy 0.9781\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0874\t Accuracy 0.9783\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0875\t Accuracy 0.9784\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0878\t Accuracy 0.9783\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0891\t Accuracy 0.9778\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0889\t Average training accuracy 0.9779\n",
      "Epoch [14]\t Average validation loss 0.0889\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0871\t Accuracy 0.9900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0798\t Accuracy 0.9802\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0851\t Accuracy 0.9792\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0843\t Accuracy 0.9795\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0843\t Accuracy 0.9795\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0838\t Accuracy 0.9792\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0848\t Accuracy 0.9788\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0850\t Accuracy 0.9791\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0851\t Accuracy 0.9791\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0855\t Accuracy 0.9790\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0867\t Accuracy 0.9785\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0865\t Average training accuracy 0.9786\n",
      "Epoch [15]\t Average validation loss 0.0876\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0857\t Accuracy 0.9900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0777\t Accuracy 0.9820\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0830\t Accuracy 0.9806\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0822\t Accuracy 0.9805\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0822\t Accuracy 0.9804\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0817\t Accuracy 0.9801\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0826\t Accuracy 0.9797\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0829\t Accuracy 0.9799\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0831\t Accuracy 0.9799\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0834\t Accuracy 0.9798\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0846\t Accuracy 0.9794\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0844\t Average training accuracy 0.9795\n",
      "Epoch [16]\t Average validation loss 0.0864\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0847\t Accuracy 0.9900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0760\t Accuracy 0.9827\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0812\t Accuracy 0.9812\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0804\t Accuracy 0.9811\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0804\t Accuracy 0.9809\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0799\t Accuracy 0.9807\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0808\t Accuracy 0.9803\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0811\t Accuracy 0.9804\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0813\t Accuracy 0.9805\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0816\t Accuracy 0.9804\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0828\t Accuracy 0.9800\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0826\t Average training accuracy 0.9801\n",
      "Epoch [17]\t Average validation loss 0.0853\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0839\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0745\t Accuracy 0.9829\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0796\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0788\t Accuracy 0.9821\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0788\t Accuracy 0.9818\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0783\t Accuracy 0.9815\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0792\t Accuracy 0.9812\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0794\t Accuracy 0.9813\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0797\t Accuracy 0.9814\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0800\t Accuracy 0.9812\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0812\t Accuracy 0.9809\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0810\t Average training accuracy 0.9809\n",
      "Epoch [18]\t Average validation loss 0.0844\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0829\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0732\t Accuracy 0.9833\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0782\t Accuracy 0.9821\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0774\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0773\t Accuracy 0.9820\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0769\t Accuracy 0.9818\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0778\t Accuracy 0.9815\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0780\t Accuracy 0.9817\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0782\t Accuracy 0.9817\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0785\t Accuracy 0.9815\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0798\t Accuracy 0.9812\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0796\t Average training accuracy 0.9813\n",
      "Epoch [19]\t Average validation loss 0.0835\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 5.2989\t Accuracy 0.1200\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2374\t Accuracy 0.1171\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.9131\t Accuracy 0.1550\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7965\t Accuracy 0.1770\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7296\t Accuracy 0.1982\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.6846\t Accuracy 0.2171\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6519\t Accuracy 0.2342\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6266\t Accuracy 0.2499\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6047\t Accuracy 0.2668\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.5866\t Accuracy 0.2819\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5708\t Accuracy 0.2965\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5569\t Average training accuracy 0.3122\n",
      "Epoch [0]\t Average validation loss 0.4020\t Average validation accuracy 0.4880\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.3711\t Accuracy 0.5900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4062\t Accuracy 0.4865\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4005\t Accuracy 0.5002\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3992\t Accuracy 0.5013\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3960\t Accuracy 0.5086\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3922\t Accuracy 0.5169\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3892\t Accuracy 0.5234\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3872\t Accuracy 0.5281\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3838\t Accuracy 0.5351\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3810\t Accuracy 0.5407\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3782\t Accuracy 0.5479\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3752\t Average training accuracy 0.5552\n",
      "Epoch [1]\t Average validation loss 0.3328\t Average validation accuracy 0.6554\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3043\t Accuracy 0.7000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3403\t Accuracy 0.6386\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3375\t Accuracy 0.6437\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3387\t Accuracy 0.6395\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3379\t Accuracy 0.6400\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3362\t Accuracy 0.6440\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3352\t Accuracy 0.6455\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3351\t Accuracy 0.6452\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3334\t Accuracy 0.6494\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3321\t Accuracy 0.6520\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3308\t Accuracy 0.6552\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3292\t Average training accuracy 0.6593\n",
      "Epoch [2]\t Average validation loss 0.2993\t Average validation accuracy 0.7344\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2738\t Accuracy 0.7700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3085\t Accuracy 0.7078\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3069\t Accuracy 0.7120\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3090\t Accuracy 0.7058\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3089\t Accuracy 0.7036\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3080\t Accuracy 0.7049\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3077\t Accuracy 0.7047\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3082\t Accuracy 0.7030\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3072\t Accuracy 0.7047\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3065\t Accuracy 0.7055\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3059\t Accuracy 0.7073\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3047\t Average training accuracy 0.7101\n",
      "Epoch [3]\t Average validation loss 0.2792\t Average validation accuracy 0.7716\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2563\t Accuracy 0.8000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2895\t Accuracy 0.7429\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2885\t Accuracy 0.7449\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2910\t Accuracy 0.7387\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2912\t Accuracy 0.7370\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2907\t Accuracy 0.7384\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2908\t Accuracy 0.7378\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2916\t Accuracy 0.7352\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2909\t Accuracy 0.7359\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2905\t Accuracy 0.7363\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2901\t Accuracy 0.7373\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2892\t Average training accuracy 0.7391\n",
      "Epoch [4]\t Average validation loss 0.2657\t Average validation accuracy 0.7978\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2450\t Accuracy 0.8100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2766\t Accuracy 0.7633\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2760\t Accuracy 0.7645\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2789\t Accuracy 0.7587\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2792\t Accuracy 0.7568\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2789\t Accuracy 0.7576\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2791\t Accuracy 0.7569\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2800\t Accuracy 0.7549\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2795\t Accuracy 0.7559\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2793\t Accuracy 0.7564\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2791\t Accuracy 0.7570\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2783\t Average training accuracy 0.7581\n",
      "Epoch [5]\t Average validation loss 0.2559\t Average validation accuracy 0.8128\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2371\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2672\t Accuracy 0.7782\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2669\t Accuracy 0.7800\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2700\t Accuracy 0.7737\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2704\t Accuracy 0.7717\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2702\t Accuracy 0.7718\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2705\t Accuracy 0.7709\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2715\t Accuracy 0.7691\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2711\t Accuracy 0.7697\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2710\t Accuracy 0.7698\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2709\t Accuracy 0.7703\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2702\t Average training accuracy 0.7712\n",
      "Epoch [6]\t Average validation loss 0.2484\t Average validation accuracy 0.8228\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2313\t Accuracy 0.8700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2600\t Accuracy 0.7918\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2599\t Accuracy 0.7918\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2631\t Accuracy 0.7856\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2635\t Accuracy 0.7827\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2634\t Accuracy 0.7828\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2638\t Accuracy 0.7819\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2649\t Accuracy 0.7795\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2646\t Accuracy 0.7802\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2645\t Accuracy 0.7804\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2645\t Accuracy 0.7808\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2638\t Average training accuracy 0.7814\n",
      "Epoch [7]\t Average validation loss 0.2424\t Average validation accuracy 0.8330\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2268\t Accuracy 0.8800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2543\t Accuracy 0.7984\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2543\t Accuracy 0.7990\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2577\t Accuracy 0.7932\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2581\t Accuracy 0.7906\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2580\t Accuracy 0.7909\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2584\t Accuracy 0.7897\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2595\t Accuracy 0.7869\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2593\t Accuracy 0.7876\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2593\t Accuracy 0.7878\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2593\t Accuracy 0.7880\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2587\t Average training accuracy 0.7887\n",
      "Epoch [8]\t Average validation loss 0.2376\t Average validation accuracy 0.8386\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2233\t Accuracy 0.8800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2495\t Accuracy 0.8065\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2497\t Accuracy 0.8059\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2532\t Accuracy 0.8007\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2536\t Accuracy 0.7984\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2536\t Accuracy 0.7986\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2540\t Accuracy 0.7978\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2551\t Accuracy 0.7948\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2549\t Accuracy 0.7952\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2549\t Accuracy 0.7955\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2550\t Accuracy 0.7957\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2544\t Average training accuracy 0.7961\n",
      "Epoch [9]\t Average validation loss 0.2335\t Average validation accuracy 0.8448\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2203\t Accuracy 0.8800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2456\t Accuracy 0.8100\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2458\t Accuracy 0.8097\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2494\t Accuracy 0.8044\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2498\t Accuracy 0.8026\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2498\t Accuracy 0.8038\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2502\t Accuracy 0.8030\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2513\t Accuracy 0.8001\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2512\t Accuracy 0.8005\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2512\t Accuracy 0.8009\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2514\t Accuracy 0.8010\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2508\t Average training accuracy 0.8014\n",
      "Epoch [10]\t Average validation loss 0.2301\t Average validation accuracy 0.8508\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2179\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2421\t Accuracy 0.8116\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2425\t Accuracy 0.8123\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2461\t Accuracy 0.8073\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2465\t Accuracy 0.8059\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2466\t Accuracy 0.8069\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2470\t Accuracy 0.8060\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2481\t Accuracy 0.8035\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2480\t Accuracy 0.8039\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2480\t Accuracy 0.8042\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2482\t Accuracy 0.8045\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2476\t Average training accuracy 0.8048\n",
      "Epoch [11]\t Average validation loss 0.2271\t Average validation accuracy 0.8534\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2157\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2392\t Accuracy 0.8159\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2396\t Accuracy 0.8163\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2433\t Accuracy 0.8114\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2437\t Accuracy 0.8099\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2438\t Accuracy 0.8106\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2442\t Accuracy 0.8099\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2452\t Accuracy 0.8074\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2452\t Accuracy 0.8076\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2452\t Accuracy 0.8080\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2455\t Accuracy 0.8082\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2449\t Average training accuracy 0.8086\n",
      "Epoch [12]\t Average validation loss 0.2245\t Average validation accuracy 0.8572\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2138\t Accuracy 0.8900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2365\t Accuracy 0.8180\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2370\t Accuracy 0.8190\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2407\t Accuracy 0.8135\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2412\t Accuracy 0.8122\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2413\t Accuracy 0.8131\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2417\t Accuracy 0.8124\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2427\t Accuracy 0.8102\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2427\t Accuracy 0.8104\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2428\t Accuracy 0.8109\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2430\t Accuracy 0.8110\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2424\t Average training accuracy 0.8114\n",
      "Epoch [13]\t Average validation loss 0.2222\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2122\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2342\t Accuracy 0.8204\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2348\t Accuracy 0.8216\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2385\t Accuracy 0.8160\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2389\t Accuracy 0.8150\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2390\t Accuracy 0.8157\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2394\t Accuracy 0.8153\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2405\t Accuracy 0.8132\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2405\t Accuracy 0.8135\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2406\t Accuracy 0.8139\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2408\t Accuracy 0.8140\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2403\t Average training accuracy 0.8143\n",
      "Epoch [14]\t Average validation loss 0.2201\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2106\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2321\t Accuracy 0.8229\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2327\t Accuracy 0.8244\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2365\t Accuracy 0.8185\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2369\t Accuracy 0.8176\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2370\t Accuracy 0.8182\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2374\t Accuracy 0.8179\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2384\t Accuracy 0.8157\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2385\t Accuracy 0.8161\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2386\t Accuracy 0.8163\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2388\t Accuracy 0.8163\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2383\t Average training accuracy 0.8167\n",
      "Epoch [15]\t Average validation loss 0.2182\t Average validation accuracy 0.8616\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2093\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2301\t Accuracy 0.8255\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2308\t Accuracy 0.8266\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2346\t Accuracy 0.8204\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2350\t Accuracy 0.8197\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2352\t Accuracy 0.8203\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2356\t Accuracy 0.8200\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2366\t Accuracy 0.8176\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2366\t Accuracy 0.8181\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2367\t Accuracy 0.8182\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2370\t Accuracy 0.8181\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2365\t Average training accuracy 0.8186\n",
      "Epoch [16]\t Average validation loss 0.2165\t Average validation accuracy 0.8644\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2080\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2284\t Accuracy 0.8286\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2291\t Accuracy 0.8291\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2330\t Accuracy 0.8232\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2333\t Accuracy 0.8224\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2335\t Accuracy 0.8229\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2339\t Accuracy 0.8226\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2349\t Accuracy 0.8203\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2349\t Accuracy 0.8206\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2351\t Accuracy 0.8207\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2353\t Accuracy 0.8206\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2348\t Average training accuracy 0.8211\n",
      "Epoch [17]\t Average validation loss 0.2149\t Average validation accuracy 0.8668\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2068\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2268\t Accuracy 0.8308\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2276\t Accuracy 0.8312\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2314\t Accuracy 0.8250\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2318\t Accuracy 0.8240\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2320\t Accuracy 0.8246\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2323\t Accuracy 0.8244\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2333\t Accuracy 0.8220\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2334\t Accuracy 0.8226\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2335\t Accuracy 0.8227\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2338\t Accuracy 0.8226\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2333\t Average training accuracy 0.8230\n",
      "Epoch [18]\t Average validation loss 0.2135\t Average validation accuracy 0.8684\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2057\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2253\t Accuracy 0.8325\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2261\t Accuracy 0.8331\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2300\t Accuracy 0.8266\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2303\t Accuracy 0.8257\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2305\t Accuracy 0.8262\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2309\t Accuracy 0.8261\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2319\t Accuracy 0.8238\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2319\t Accuracy 0.8243\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2321\t Accuracy 0.8245\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2324\t Accuracy 0.8242\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2318\t Average training accuracy 0.8247\n",
      "Epoch [19]\t Average validation loss 0.2121\t Average validation accuracy 0.8680\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.1635\t Accuracy 0.1700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.3584\t Accuracy 0.2478\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8945\t Accuracy 0.3582\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7150\t Accuracy 0.4109\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6081\t Accuracy 0.4726\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5369\t Accuracy 0.5200\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4848\t Accuracy 0.5640\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4455\t Accuracy 0.5983\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4133\t Accuracy 0.6261\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3869\t Accuracy 0.6496\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3644\t Accuracy 0.6693\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3449\t Average training accuracy 0.6869\n",
      "Epoch [0]\t Average validation loss 0.1230\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1157\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1296\t Accuracy 0.8906\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1274\t Accuracy 0.8900\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1280\t Accuracy 0.8883\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1254\t Accuracy 0.8900\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1238\t Accuracy 0.8913\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1221\t Accuracy 0.8928\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1211\t Accuracy 0.8931\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1199\t Accuracy 0.8940\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1189\t Accuracy 0.8949\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1181\t Accuracy 0.8957\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1169\t Average training accuracy 0.8965\n",
      "Epoch [1]\t Average validation loss 0.0886\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0801\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0970\t Accuracy 0.9210\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0980\t Accuracy 0.9180\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1006\t Accuracy 0.9146\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0996\t Accuracy 0.9160\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0995\t Accuracy 0.9163\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0992\t Accuracy 0.9165\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0994\t Accuracy 0.9162\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0993\t Accuracy 0.9163\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0992\t Accuracy 0.9164\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0993\t Accuracy 0.9159\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0988\t Average training accuracy 0.9160\n",
      "Epoch [2]\t Average validation loss 0.0808\t Average validation accuracy 0.9448\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0731\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0884\t Accuracy 0.9290\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0895\t Accuracy 0.9276\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0923\t Accuracy 0.9232\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0915\t Accuracy 0.9240\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0913\t Accuracy 0.9245\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0914\t Accuracy 0.9243\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0916\t Accuracy 0.9239\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0917\t Accuracy 0.9238\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0917\t Accuracy 0.9240\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0919\t Accuracy 0.9235\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0917\t Average training accuracy 0.9232\n",
      "Epoch [3]\t Average validation loss 0.0763\t Average validation accuracy 0.9478\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0684\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0832\t Accuracy 0.9337\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0844\t Accuracy 0.9327\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0872\t Accuracy 0.9293\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0864\t Accuracy 0.9294\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0862\t Accuracy 0.9299\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0864\t Accuracy 0.9296\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0867\t Accuracy 0.9293\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0868\t Accuracy 0.9290\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0868\t Accuracy 0.9290\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0872\t Accuracy 0.9285\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0870\t Average training accuracy 0.9282\n",
      "Epoch [4]\t Average validation loss 0.0732\t Average validation accuracy 0.9516\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0649\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0795\t Accuracy 0.9375\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0807\t Accuracy 0.9367\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0834\t Accuracy 0.9334\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0828\t Accuracy 0.9333\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0825\t Accuracy 0.9335\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0828\t Accuracy 0.9331\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0831\t Accuracy 0.9328\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0832\t Accuracy 0.9325\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0833\t Accuracy 0.9326\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0837\t Accuracy 0.9321\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0835\t Average training accuracy 0.9319\n",
      "Epoch [5]\t Average validation loss 0.0709\t Average validation accuracy 0.9538\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0622\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0765\t Accuracy 0.9392\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0778\t Accuracy 0.9386\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0805\t Accuracy 0.9360\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0799\t Accuracy 0.9363\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0797\t Accuracy 0.9363\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0800\t Accuracy 0.9359\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0803\t Accuracy 0.9356\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0805\t Accuracy 0.9354\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0805\t Accuracy 0.9354\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0809\t Accuracy 0.9351\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0807\t Average training accuracy 0.9349\n",
      "Epoch [6]\t Average validation loss 0.0691\t Average validation accuracy 0.9528\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0601\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0742\t Accuracy 0.9420\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0755\t Accuracy 0.9405\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0781\t Accuracy 0.9379\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0776\t Accuracy 0.9387\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0774\t Accuracy 0.9386\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0778\t Accuracy 0.9380\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0781\t Accuracy 0.9378\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0783\t Accuracy 0.9377\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0783\t Accuracy 0.9376\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0787\t Accuracy 0.9372\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0785\t Average training accuracy 0.9371\n",
      "Epoch [7]\t Average validation loss 0.0676\t Average validation accuracy 0.9532\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0586\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0724\t Accuracy 0.9435\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0736\t Accuracy 0.9428\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0762\t Accuracy 0.9401\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0757\t Accuracy 0.9406\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0755\t Accuracy 0.9404\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0759\t Accuracy 0.9397\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0762\t Accuracy 0.9398\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0764\t Accuracy 0.9396\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0764\t Accuracy 0.9394\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0768\t Accuracy 0.9392\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0766\t Average training accuracy 0.9391\n",
      "Epoch [8]\t Average validation loss 0.0663\t Average validation accuracy 0.9538\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0574\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0707\t Accuracy 0.9453\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0719\t Accuracy 0.9443\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0744\t Accuracy 0.9420\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0740\t Accuracy 0.9423\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0738\t Accuracy 0.9421\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0743\t Accuracy 0.9417\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0745\t Accuracy 0.9416\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0747\t Accuracy 0.9412\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0747\t Accuracy 0.9411\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0751\t Accuracy 0.9407\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0749\t Average training accuracy 0.9406\n",
      "Epoch [9]\t Average validation loss 0.0651\t Average validation accuracy 0.9554\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0563\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0692\t Accuracy 0.9476\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0704\t Accuracy 0.9462\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0729\t Accuracy 0.9438\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0725\t Accuracy 0.9440\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0723\t Accuracy 0.9437\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0728\t Accuracy 0.9433\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0730\t Accuracy 0.9431\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0732\t Accuracy 0.9427\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0732\t Accuracy 0.9427\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0736\t Accuracy 0.9423\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0734\t Average training accuracy 0.9421\n",
      "Epoch [10]\t Average validation loss 0.0640\t Average validation accuracy 0.9562\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0678\t Accuracy 0.9494\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0691\t Accuracy 0.9471\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0715\t Accuracy 0.9449\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0711\t Accuracy 0.9450\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0709\t Accuracy 0.9449\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0714\t Accuracy 0.9443\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0716\t Accuracy 0.9443\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0718\t Accuracy 0.9439\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0718\t Accuracy 0.9440\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0722\t Accuracy 0.9436\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0720\t Average training accuracy 0.9435\n",
      "Epoch [11]\t Average validation loss 0.0631\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0542\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0666\t Accuracy 0.9502\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0678\t Accuracy 0.9481\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0702\t Accuracy 0.9458\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0698\t Accuracy 0.9458\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0696\t Accuracy 0.9458\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0701\t Accuracy 0.9452\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0703\t Accuracy 0.9454\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0705\t Accuracy 0.9451\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0705\t Accuracy 0.9452\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0709\t Accuracy 0.9448\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0707\t Average training accuracy 0.9447\n",
      "Epoch [12]\t Average validation loss 0.0622\t Average validation accuracy 0.9578\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0534\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0654\t Accuracy 0.9522\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0667\t Accuracy 0.9494\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0690\t Accuracy 0.9472\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0686\t Accuracy 0.9478\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0684\t Accuracy 0.9478\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0689\t Accuracy 0.9473\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0691\t Accuracy 0.9474\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0693\t Accuracy 0.9471\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0693\t Accuracy 0.9470\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0697\t Accuracy 0.9465\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0695\t Average training accuracy 0.9463\n",
      "Epoch [13]\t Average validation loss 0.0615\t Average validation accuracy 0.9584\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0527\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0643\t Accuracy 0.9535\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0656\t Accuracy 0.9511\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0679\t Accuracy 0.9487\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0675\t Accuracy 0.9492\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0673\t Accuracy 0.9489\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0678\t Accuracy 0.9483\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0680\t Accuracy 0.9485\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0682\t Accuracy 0.9482\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0682\t Accuracy 0.9484\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0685\t Accuracy 0.9477\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0683\t Average training accuracy 0.9476\n",
      "Epoch [14]\t Average validation loss 0.0608\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0523\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0633\t Accuracy 0.9543\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0646\t Accuracy 0.9517\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0669\t Accuracy 0.9499\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0665\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0662\t Accuracy 0.9498\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0668\t Accuracy 0.9492\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0669\t Accuracy 0.9493\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0671\t Accuracy 0.9490\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0671\t Accuracy 0.9492\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0675\t Accuracy 0.9485\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0673\t Average training accuracy 0.9484\n",
      "Epoch [15]\t Average validation loss 0.0602\t Average validation accuracy 0.9592\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0518\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0624\t Accuracy 0.9559\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0637\t Accuracy 0.9528\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0659\t Accuracy 0.9507\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0655\t Accuracy 0.9513\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0653\t Accuracy 0.9512\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0658\t Accuracy 0.9505\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0660\t Accuracy 0.9506\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0662\t Accuracy 0.9501\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0662\t Accuracy 0.9504\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0665\t Accuracy 0.9496\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0663\t Average training accuracy 0.9496\n",
      "Epoch [16]\t Average validation loss 0.0597\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0513\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0616\t Accuracy 0.9563\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0629\t Accuracy 0.9534\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0651\t Accuracy 0.9512\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0647\t Accuracy 0.9520\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.9519\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0650\t Accuracy 0.9513\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0651\t Accuracy 0.9515\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0653\t Accuracy 0.9510\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0653\t Accuracy 0.9513\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0656\t Accuracy 0.9506\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0654\t Average training accuracy 0.9507\n",
      "Epoch [17]\t Average validation loss 0.0591\t Average validation accuracy 0.9608\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0511\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0608\t Accuracy 0.9571\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0621\t Accuracy 0.9543\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0642\t Accuracy 0.9521\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0639\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0636\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0642\t Accuracy 0.9521\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0643\t Accuracy 0.9525\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0645\t Accuracy 0.9520\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0644\t Accuracy 0.9522\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0648\t Accuracy 0.9515\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0646\t Average training accuracy 0.9515\n",
      "Epoch [18]\t Average validation loss 0.0586\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0509\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0601\t Accuracy 0.9578\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0615\t Accuracy 0.9553\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0635\t Accuracy 0.9529\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0631\t Accuracy 0.9538\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0629\t Accuracy 0.9537\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0634\t Accuracy 0.9530\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0636\t Accuracy 0.9534\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0637\t Accuracy 0.9529\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0637\t Accuracy 0.9531\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0640\t Accuracy 0.9525\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0639\t Average training accuracy 0.9525\n",
      "Epoch [19]\t Average validation loss 0.0582\t Average validation accuracy 0.9614\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7652\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5969\t Accuracy 0.1012\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.5184\t Accuracy 0.1086\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4543\t Accuracy 0.1187\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.4070\t Accuracy 0.1384\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3696\t Accuracy 0.1563\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3388\t Accuracy 0.1763\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3129\t Accuracy 0.1947\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2895\t Accuracy 0.2137\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2695\t Accuracy 0.2313\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.2506\t Accuracy 0.2494\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2334\t Average training accuracy 0.2661\n",
      "Epoch [0]\t Average validation loss 2.0337\t Average validation accuracy 0.4708\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.0346\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0279\t Accuracy 0.4782\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0161\t Accuracy 0.4907\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.0081\t Accuracy 0.4946\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.9984\t Accuracy 0.5047\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.9879\t Accuracy 0.5110\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.9775\t Accuracy 0.5202\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.9692\t Accuracy 0.5267\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.9588\t Accuracy 0.5329\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.9501\t Accuracy 0.5389\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.9400\t Accuracy 0.5463\n",
      "\n",
      "Epoch [1]\t Average training loss 1.9303\t Average training accuracy 0.5536\n",
      "Epoch [1]\t Average validation loss 1.7986\t Average validation accuracy 0.6512\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.7952\t Accuracy 0.6400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.8024\t Accuracy 0.6514\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.7929\t Accuracy 0.6487\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.7882\t Accuracy 0.6430\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.7811\t Accuracy 0.6456\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.7722\t Accuracy 0.6479\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.7631\t Accuracy 0.6508\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.7577\t Accuracy 0.6526\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.7489\t Accuracy 0.6544\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.7422\t Accuracy 0.6567\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.7337\t Accuracy 0.6607\n",
      "\n",
      "Epoch [2]\t Average training loss 1.7254\t Average training accuracy 0.6639\n",
      "Epoch [2]\t Average validation loss 1.6011\t Average validation accuracy 0.7268\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.5976\t Accuracy 0.7600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.6138\t Accuracy 0.7143\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.6064\t Accuracy 0.7125\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.6043\t Accuracy 0.7060\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.5993\t Accuracy 0.7070\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.5916\t Accuracy 0.7063\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.5838\t Accuracy 0.7077\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.5806\t Accuracy 0.7091\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.5731\t Accuracy 0.7106\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.5681\t Accuracy 0.7123\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.5609\t Accuracy 0.7146\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5537\t Average training accuracy 0.7161\n",
      "Epoch [3]\t Average validation loss 1.4353\t Average validation accuracy 0.7668\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.4340\t Accuracy 0.7700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.4559\t Accuracy 0.7412\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.4503\t Accuracy 0.7418\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.4505\t Accuracy 0.7360\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.4470\t Accuracy 0.7384\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.4404\t Accuracy 0.7390\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.4337\t Accuracy 0.7399\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.4323\t Accuracy 0.7409\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.4259\t Accuracy 0.7429\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.4221\t Accuracy 0.7440\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.4160\t Accuracy 0.7455\n",
      "\n",
      "Epoch [4]\t Average training loss 1.4098\t Average training accuracy 0.7467\n",
      "Epoch [4]\t Average validation loss 1.2963\t Average validation accuracy 0.7952\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.2985\t Accuracy 0.8100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.3237\t Accuracy 0.7678\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.3199\t Accuracy 0.7650\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.3219\t Accuracy 0.7605\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.3196\t Accuracy 0.7623\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.3138\t Accuracy 0.7623\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.3081\t Accuracy 0.7633\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.3081\t Accuracy 0.7636\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.3027\t Accuracy 0.7656\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.2999\t Accuracy 0.7663\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.2948\t Accuracy 0.7676\n",
      "\n",
      "Epoch [5]\t Average training loss 1.2894\t Average training accuracy 0.7685\n",
      "Epoch [5]\t Average validation loss 1.1798\t Average validation accuracy 0.8126\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.1863\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.2131\t Accuracy 0.7873\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.2108\t Accuracy 0.7838\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.2143\t Accuracy 0.7791\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.2131\t Accuracy 0.7808\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.2079\t Accuracy 0.7798\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.2030\t Accuracy 0.7805\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.2042\t Accuracy 0.7800\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.1995\t Accuracy 0.7822\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.1976\t Accuracy 0.7828\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.1933\t Accuracy 0.7837\n",
      "\n",
      "Epoch [6]\t Average training loss 1.1885\t Average training accuracy 0.7843\n",
      "Epoch [6]\t Average validation loss 1.0823\t Average validation accuracy 0.8290\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.0931\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.1205\t Accuracy 0.7982\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.1195\t Accuracy 0.7959\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.1243\t Accuracy 0.7909\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.1237\t Accuracy 0.7929\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.1190\t Accuracy 0.7930\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.1149\t Accuracy 0.7937\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.1169\t Accuracy 0.7932\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.1129\t Accuracy 0.7947\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.1116\t Accuracy 0.7951\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.1080\t Accuracy 0.7958\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1038\t Average training accuracy 0.7965\n",
      "Epoch [7]\t Average validation loss 1.0002\t Average validation accuracy 0.8480\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.0155\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.0425\t Accuracy 0.8116\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.0426\t Accuracy 0.8080\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.0484\t Accuracy 0.8036\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.0483\t Accuracy 0.8052\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.0442\t Accuracy 0.8054\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.0406\t Accuracy 0.8055\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.0433\t Accuracy 0.8049\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.0398\t Accuracy 0.8063\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.0390\t Accuracy 0.8064\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.0359\t Accuracy 0.8065\n",
      "\n",
      "Epoch [8]\t Average training loss 1.0322\t Average training accuracy 0.8073\n",
      "Epoch [8]\t Average validation loss 0.9307\t Average validation accuracy 0.8566\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.9503\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.9765\t Accuracy 0.8184\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.9775\t Accuracy 0.8169\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.9841\t Accuracy 0.8123\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.9844\t Accuracy 0.8135\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.9806\t Accuracy 0.8136\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.9776\t Accuracy 0.8138\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.9808\t Accuracy 0.8132\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.9777\t Accuracy 0.8144\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.9773\t Accuracy 0.8143\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.9747\t Accuracy 0.8143\n",
      "\n",
      "Epoch [9]\t Average training loss 0.9714\t Average training accuracy 0.8151\n",
      "Epoch [9]\t Average validation loss 0.8714\t Average validation accuracy 0.8654\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8951\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.9202\t Accuracy 0.8259\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.9219\t Accuracy 0.8257\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.9293\t Accuracy 0.8212\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.9298\t Accuracy 0.8224\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.9263\t Accuracy 0.8220\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.9237\t Accuracy 0.8221\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.9273\t Accuracy 0.8215\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.9246\t Accuracy 0.8223\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.9244\t Accuracy 0.8220\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.9223\t Accuracy 0.8217\n",
      "\n",
      "Epoch [10]\t Average training loss 0.9193\t Average training accuracy 0.8223\n",
      "Epoch [10]\t Average validation loss 0.8206\t Average validation accuracy 0.8722\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8479\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8717\t Accuracy 0.8341\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8741\t Accuracy 0.8330\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8820\t Accuracy 0.8283\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8828\t Accuracy 0.8293\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8795\t Accuracy 0.8288\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8772\t Accuracy 0.8289\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8811\t Accuracy 0.8281\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8787\t Accuracy 0.8287\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8788\t Accuracy 0.8282\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8770\t Accuracy 0.8277\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8743\t Average training accuracy 0.8283\n",
      "Epoch [11]\t Average validation loss 0.7766\t Average validation accuracy 0.8754\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8073\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8296\t Accuracy 0.8408\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8327\t Accuracy 0.8391\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8411\t Accuracy 0.8342\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8419\t Accuracy 0.8349\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8388\t Accuracy 0.8343\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8369\t Accuracy 0.8343\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8410\t Accuracy 0.8332\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8388\t Accuracy 0.8338\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8391\t Accuracy 0.8331\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8376\t Accuracy 0.8328\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8351\t Average training accuracy 0.8333\n",
      "Epoch [12]\t Average validation loss 0.7382\t Average validation accuracy 0.8790\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7721\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7929\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.7964\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8052\t Accuracy 0.8387\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8062\t Accuracy 0.8396\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8033\t Accuracy 0.8387\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8016\t Accuracy 0.8388\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8059\t Accuracy 0.8376\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8039\t Accuracy 0.8380\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8044\t Accuracy 0.8372\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8031\t Accuracy 0.8369\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8008\t Average training accuracy 0.8375\n",
      "Epoch [13]\t Average validation loss 0.7045\t Average validation accuracy 0.8820\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7411\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7605\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7645\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.7737\t Accuracy 0.8415\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.7747\t Accuracy 0.8424\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.7719\t Accuracy 0.8417\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7704\t Accuracy 0.8417\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.7749\t Accuracy 0.8407\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.7730\t Accuracy 0.8411\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.7736\t Accuracy 0.8406\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.7726\t Accuracy 0.8403\n",
      "\n",
      "Epoch [14]\t Average training loss 0.7705\t Average training accuracy 0.8409\n",
      "Epoch [14]\t Average validation loss 0.6747\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7138\t Accuracy 0.8600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7318\t Accuracy 0.8506\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7362\t Accuracy 0.8503\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7457\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7467\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7440\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7428\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7473\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7457\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.7463\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.7454\t Accuracy 0.8433\n",
      "\n",
      "Epoch [15]\t Average training loss 0.7435\t Average training accuracy 0.8439\n",
      "Epoch [15]\t Average validation loss 0.6482\t Average validation accuracy 0.8864\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.6895\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7062\t Accuracy 0.8545\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7110\t Accuracy 0.8536\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7207\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.7218\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7192\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7181\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.7227\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.7212\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.7219\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.7212\t Accuracy 0.8459\n",
      "\n",
      "Epoch [16]\t Average training loss 0.7194\t Average training accuracy 0.8465\n",
      "Epoch [16]\t Average validation loss 0.6245\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.6678\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.6832\t Accuracy 0.8573\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.6883\t Accuracy 0.8565\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.6983\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.6993\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.6968\t Accuracy 0.8504\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.6959\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.7006\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.6991\t Accuracy 0.8495\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.7000\t Accuracy 0.8489\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.6994\t Accuracy 0.8485\n",
      "\n",
      "Epoch [17]\t Average training loss 0.6977\t Average training accuracy 0.8491\n",
      "Epoch [17]\t Average validation loss 0.6032\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.6482\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.6624\t Accuracy 0.8604\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.6679\t Accuracy 0.8591\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.6781\t Accuracy 0.8536\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.6791\t Accuracy 0.8543\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.6767\t Accuracy 0.8533\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.6759\t Accuracy 0.8530\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.6806\t Accuracy 0.8517\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.6792\t Accuracy 0.8520\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.6801\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.6797\t Accuracy 0.8510\n",
      "\n",
      "Epoch [18]\t Average training loss 0.6781\t Average training accuracy 0.8515\n",
      "Epoch [18]\t Average validation loss 0.5839\t Average validation accuracy 0.8934\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.6304\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.6436\t Accuracy 0.8631\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.6493\t Accuracy 0.8617\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.6597\t Accuracy 0.8561\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.6607\t Accuracy 0.8565\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.6584\t Accuracy 0.8555\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.6577\t Accuracy 0.8553\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.6624\t Accuracy 0.8541\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.6612\t Accuracy 0.8542\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.6621\t Accuracy 0.8533\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.6618\t Accuracy 0.8531\n",
      "\n",
      "Epoch [19]\t Average training loss 0.6603\t Average training accuracy 0.8536\n",
      "Epoch [19]\t Average validation loss 0.5664\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4529\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.8237\t Accuracy 0.4180\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.3112\t Accuracy 0.5939\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.0699\t Accuracy 0.6667\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.9192\t Accuracy 0.7138\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8217\t Accuracy 0.7450\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.7537\t Accuracy 0.7674\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7041\t Accuracy 0.7840\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6608\t Accuracy 0.7981\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6264\t Accuracy 0.8089\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5978\t Accuracy 0.8184\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5727\t Average training accuracy 0.8264\n",
      "Epoch [0]\t Average validation loss 0.2304\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2616\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2618\t Accuracy 0.9284\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2741\t Accuracy 0.9244\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2794\t Accuracy 0.9210\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2713\t Accuracy 0.9227\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2676\t Accuracy 0.9237\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2652\t Accuracy 0.9244\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2636\t Accuracy 0.9249\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2609\t Accuracy 0.9254\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2589\t Accuracy 0.9257\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2575\t Accuracy 0.9260\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2547\t Average training accuracy 0.9269\n",
      "Epoch [1]\t Average validation loss 0.1704\t Average validation accuracy 0.9550\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1686\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1934\t Accuracy 0.9475\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2048\t Accuracy 0.9427\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2099\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2054\t Accuracy 0.9414\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2042\t Accuracy 0.9420\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2036\t Accuracy 0.9422\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2032\t Accuracy 0.9424\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2019\t Accuracy 0.9427\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2014\t Accuracy 0.9425\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2016\t Accuracy 0.9422\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2000\t Average training accuracy 0.9427\n",
      "Epoch [2]\t Average validation loss 0.1441\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1320\t Accuracy 0.9800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1582\t Accuracy 0.9592\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1692\t Accuracy 0.9532\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1720\t Accuracy 0.9520\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1693\t Accuracy 0.9526\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1688\t Accuracy 0.9524\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1689\t Accuracy 0.9523\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1690\t Accuracy 0.9521\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1681\t Accuracy 0.9523\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1682\t Accuracy 0.9522\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1688\t Accuracy 0.9517\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1678\t Average training accuracy 0.9521\n",
      "Epoch [3]\t Average validation loss 0.1270\t Average validation accuracy 0.9672\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1124\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1359\t Accuracy 0.9663\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1458\t Accuracy 0.9599\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1468\t Accuracy 0.9581\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1453\t Accuracy 0.9589\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1449\t Accuracy 0.9583\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1455\t Accuracy 0.9579\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1457\t Accuracy 0.9580\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1450\t Accuracy 0.9581\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1451\t Accuracy 0.9582\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1459\t Accuracy 0.9580\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1451\t Average training accuracy 0.9584\n",
      "Epoch [4]\t Average validation loss 0.1137\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1006\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1198\t Accuracy 0.9694\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1280\t Accuracy 0.9638\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1282\t Accuracy 0.9628\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1273\t Accuracy 0.9634\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1270\t Accuracy 0.9631\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1278\t Accuracy 0.9628\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1280\t Accuracy 0.9628\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1273\t Accuracy 0.9630\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1275\t Accuracy 0.9632\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1283\t Accuracy 0.9630\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1277\t Average training accuracy 0.9633\n",
      "Epoch [5]\t Average validation loss 0.1046\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0922\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1068\t Accuracy 0.9729\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1142\t Accuracy 0.9687\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1137\t Accuracy 0.9683\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1131\t Accuracy 0.9685\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1131\t Accuracy 0.9680\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1140\t Accuracy 0.9675\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1142\t Accuracy 0.9675\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1137\t Accuracy 0.9676\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1138\t Accuracy 0.9678\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1147\t Accuracy 0.9675\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1142\t Average training accuracy 0.9677\n",
      "Epoch [6]\t Average validation loss 0.0981\t Average validation accuracy 0.9726\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0848\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0969\t Accuracy 0.9745\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1032\t Accuracy 0.9716\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1022\t Accuracy 0.9717\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1019\t Accuracy 0.9719\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1020\t Accuracy 0.9714\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1030\t Accuracy 0.9710\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1032\t Accuracy 0.9709\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1028\t Accuracy 0.9711\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1029\t Accuracy 0.9713\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1039\t Accuracy 0.9709\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1034\t Average training accuracy 0.9711\n",
      "Epoch [7]\t Average validation loss 0.0930\t Average validation accuracy 0.9744\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0785\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0886\t Accuracy 0.9761\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0941\t Accuracy 0.9739\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0928\t Accuracy 0.9741\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0926\t Accuracy 0.9743\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0930\t Accuracy 0.9739\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0939\t Accuracy 0.9734\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0941\t Accuracy 0.9734\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0938\t Accuracy 0.9735\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0939\t Accuracy 0.9736\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0949\t Accuracy 0.9732\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0945\t Average training accuracy 0.9732\n",
      "Epoch [8]\t Average validation loss 0.0889\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0734\t Accuracy 0.9800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0818\t Accuracy 0.9776\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0866\t Accuracy 0.9757\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0850\t Accuracy 0.9762\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0850\t Accuracy 0.9764\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0855\t Accuracy 0.9759\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0864\t Accuracy 0.9756\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0866\t Accuracy 0.9758\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0863\t Accuracy 0.9759\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0863\t Accuracy 0.9762\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0874\t Accuracy 0.9757\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0870\t Average training accuracy 0.9757\n",
      "Epoch [9]\t Average validation loss 0.0856\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0683\t Accuracy 0.9800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0756\t Accuracy 0.9790\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0800\t Accuracy 0.9779\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0783\t Accuracy 0.9787\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0785\t Accuracy 0.9788\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0790\t Accuracy 0.9782\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0799\t Accuracy 0.9779\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0801\t Accuracy 0.9780\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0798\t Accuracy 0.9781\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0799\t Accuracy 0.9782\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0809\t Accuracy 0.9777\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0805\t Average training accuracy 0.9777\n",
      "Epoch [10]\t Average validation loss 0.0831\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0645\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0703\t Accuracy 0.9808\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0743\t Accuracy 0.9802\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0726\t Accuracy 0.9808\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0728\t Accuracy 0.9806\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0733\t Accuracy 0.9801\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0742\t Accuracy 0.9798\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0744\t Accuracy 0.9799\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0742\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0743\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0753\t Accuracy 0.9794\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0750\t Average training accuracy 0.9793\n",
      "Epoch [11]\t Average validation loss 0.0810\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0614\t Accuracy 0.9800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0658\t Accuracy 0.9820\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0694\t Accuracy 0.9813\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0675\t Accuracy 0.9821\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0678\t Accuracy 0.9819\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0683\t Accuracy 0.9816\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0693\t Accuracy 0.9814\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0695\t Accuracy 0.9815\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0693\t Accuracy 0.9814\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0694\t Accuracy 0.9815\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0703\t Accuracy 0.9809\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0700\t Average training accuracy 0.9808\n",
      "Epoch [12]\t Average validation loss 0.0794\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0586\t Accuracy 0.9800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0619\t Accuracy 0.9829\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0651\t Accuracy 0.9827\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0631\t Accuracy 0.9832\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0634\t Accuracy 0.9829\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0639\t Accuracy 0.9826\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0649\t Accuracy 0.9826\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0651\t Accuracy 0.9826\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0649\t Accuracy 0.9826\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0650\t Accuracy 0.9826\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0659\t Accuracy 0.9822\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0656\t Average training accuracy 0.9821\n",
      "Epoch [13]\t Average validation loss 0.0780\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0555\t Accuracy 0.9800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0583\t Accuracy 0.9841\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0613\t Accuracy 0.9838\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0592\t Accuracy 0.9841\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0595\t Accuracy 0.9839\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0599\t Accuracy 0.9839\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0610\t Accuracy 0.9838\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0611\t Accuracy 0.9838\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0610\t Accuracy 0.9839\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0611\t Accuracy 0.9838\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0620\t Accuracy 0.9835\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0617\t Average training accuracy 0.9834\n",
      "Epoch [14]\t Average validation loss 0.0768\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0527\t Accuracy 0.9800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0551\t Accuracy 0.9859\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0578\t Accuracy 0.9851\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0557\t Accuracy 0.9854\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0559\t Accuracy 0.9850\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0564\t Accuracy 0.9850\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0574\t Accuracy 0.9850\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0576\t Accuracy 0.9850\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0576\t Accuracy 0.9850\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0576\t Accuracy 0.9849\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0584\t Accuracy 0.9845\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0582\t Average training accuracy 0.9844\n",
      "Epoch [15]\t Average validation loss 0.0757\t Average validation accuracy 0.9778\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0507\t Accuracy 0.9800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0523\t Accuracy 0.9871\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0548\t Accuracy 0.9861\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0525\t Accuracy 0.9864\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0528\t Accuracy 0.9859\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0532\t Accuracy 0.9859\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0543\t Accuracy 0.9858\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0544\t Accuracy 0.9858\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0544\t Accuracy 0.9858\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0544\t Accuracy 0.9857\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0552\t Accuracy 0.9852\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0550\t Average training accuracy 0.9852\n",
      "Epoch [16]\t Average validation loss 0.0750\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0489\t Accuracy 0.9800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0497\t Accuracy 0.9888\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0520\t Accuracy 0.9876\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0497\t Accuracy 0.9879\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0499\t Accuracy 0.9873\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0503\t Accuracy 0.9873\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0514\t Accuracy 0.9871\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0515\t Accuracy 0.9872\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0515\t Accuracy 0.9870\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0515\t Accuracy 0.9869\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0523\t Accuracy 0.9864\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0521\t Average training accuracy 0.9864\n",
      "Epoch [17]\t Average validation loss 0.0742\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0472\t Accuracy 0.9800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0474\t Accuracy 0.9896\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0494\t Accuracy 0.9883\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0471\t Accuracy 0.9888\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0473\t Accuracy 0.9882\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0476\t Accuracy 0.9880\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0487\t Accuracy 0.9877\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0488\t Accuracy 0.9880\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0489\t Accuracy 0.9878\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0489\t Accuracy 0.9877\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0496\t Accuracy 0.9873\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0494\t Average training accuracy 0.9873\n",
      "Epoch [18]\t Average validation loss 0.0736\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0457\t Accuracy 0.9800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0453\t Accuracy 0.9898\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0471\t Accuracy 0.9889\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0447\t Accuracy 0.9896\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0449\t Accuracy 0.9891\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0452\t Accuracy 0.9888\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0463\t Accuracy 0.9885\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0464\t Accuracy 0.9887\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0465\t Accuracy 0.9887\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0465\t Accuracy 0.9885\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0472\t Accuracy 0.9881\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0469\t Average training accuracy 0.9882\n",
      "Epoch [19]\t Average validation loss 0.0730\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 5.2821\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.3059\t Accuracy 0.1445\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.9502\t Accuracy 0.1630\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8232\t Accuracy 0.1783\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7497\t Accuracy 0.1954\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7022\t Accuracy 0.2145\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6658\t Accuracy 0.2337\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6381\t Accuracy 0.2482\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6151\t Accuracy 0.2649\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.5964\t Accuracy 0.2788\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5799\t Accuracy 0.2932\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5654\t Average training accuracy 0.3073\n",
      "Epoch [0]\t Average validation loss 0.4055\t Average validation accuracy 0.4888\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.3728\t Accuracy 0.5800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4082\t Accuracy 0.4780\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4019\t Accuracy 0.4950\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4006\t Accuracy 0.4969\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3970\t Accuracy 0.5035\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3941\t Accuracy 0.5088\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3903\t Accuracy 0.5182\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3879\t Accuracy 0.5225\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3849\t Accuracy 0.5294\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3826\t Accuracy 0.5339\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3802\t Accuracy 0.5398\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3774\t Average training accuracy 0.5460\n",
      "Epoch [1]\t Average validation loss 0.3372\t Average validation accuracy 0.6388\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3145\t Accuracy 0.6800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3454\t Accuracy 0.6188\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3417\t Accuracy 0.6299\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3429\t Accuracy 0.6267\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3415\t Accuracy 0.6296\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3405\t Accuracy 0.6311\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3387\t Accuracy 0.6350\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3382\t Accuracy 0.6362\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3368\t Accuracy 0.6397\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3359\t Accuracy 0.6414\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3349\t Accuracy 0.6447\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3333\t Average training accuracy 0.6479\n",
      "Epoch [2]\t Average validation loss 0.3049\t Average validation accuracy 0.7158\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2876\t Accuracy 0.7300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3153\t Accuracy 0.6810\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3125\t Accuracy 0.6914\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3146\t Accuracy 0.6874\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3139\t Accuracy 0.6892\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3135\t Accuracy 0.6892\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3124\t Accuracy 0.6913\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3125\t Accuracy 0.6916\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3116\t Accuracy 0.6937\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3112\t Accuracy 0.6946\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3107\t Accuracy 0.6961\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3096\t Average training accuracy 0.6980\n",
      "Epoch [3]\t Average validation loss 0.2852\t Average validation accuracy 0.7550\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2713\t Accuracy 0.7700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2967\t Accuracy 0.7149\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2944\t Accuracy 0.7254\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2969\t Accuracy 0.7209\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2965\t Accuracy 0.7225\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2964\t Accuracy 0.7223\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2957\t Accuracy 0.7236\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2960\t Accuracy 0.7229\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2955\t Accuracy 0.7245\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2952\t Accuracy 0.7253\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2950\t Accuracy 0.7259\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2941\t Average training accuracy 0.7275\n",
      "Epoch [4]\t Average validation loss 0.2715\t Average validation accuracy 0.7804\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2601\t Accuracy 0.7900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2836\t Accuracy 0.7367\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2817\t Accuracy 0.7474\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2846\t Accuracy 0.7436\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2843\t Accuracy 0.7438\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2843\t Accuracy 0.7434\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2838\t Accuracy 0.7446\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2843\t Accuracy 0.7438\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2839\t Accuracy 0.7449\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2838\t Accuracy 0.7456\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2837\t Accuracy 0.7458\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2829\t Average training accuracy 0.7471\n",
      "Epoch [5]\t Average validation loss 0.2613\t Average validation accuracy 0.7978\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2519\t Accuracy 0.8000\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2738\t Accuracy 0.7567\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2722\t Accuracy 0.7648\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2753\t Accuracy 0.7604\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2751\t Accuracy 0.7594\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2751\t Accuracy 0.7584\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2747\t Accuracy 0.7591\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2753\t Accuracy 0.7580\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2751\t Accuracy 0.7593\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2751\t Accuracy 0.7599\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2751\t Accuracy 0.7599\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2743\t Average training accuracy 0.7613\n",
      "Epoch [6]\t Average validation loss 0.2534\t Average validation accuracy 0.8086\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2454\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2662\t Accuracy 0.7716\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2648\t Accuracy 0.7772\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2680\t Accuracy 0.7729\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2679\t Accuracy 0.7715\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2679\t Accuracy 0.7706\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2676\t Accuracy 0.7714\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2683\t Accuracy 0.7700\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2681\t Accuracy 0.7711\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2682\t Accuracy 0.7714\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2682\t Accuracy 0.7713\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2675\t Average training accuracy 0.7724\n",
      "Epoch [7]\t Average validation loss 0.2470\t Average validation accuracy 0.8182\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2403\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2599\t Accuracy 0.7820\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2588\t Accuracy 0.7871\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2621\t Accuracy 0.7822\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2620\t Accuracy 0.7805\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2621\t Accuracy 0.7795\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2618\t Accuracy 0.7804\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2625\t Accuracy 0.7789\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2624\t Accuracy 0.7800\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2625\t Accuracy 0.7803\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2626\t Accuracy 0.7800\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2619\t Average training accuracy 0.7808\n",
      "Epoch [8]\t Average validation loss 0.2417\t Average validation accuracy 0.8270\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2360\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2548\t Accuracy 0.7916\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2538\t Accuracy 0.7958\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2572\t Accuracy 0.7903\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2572\t Accuracy 0.7886\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2572\t Accuracy 0.7876\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2570\t Accuracy 0.7879\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2577\t Accuracy 0.7861\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2576\t Accuracy 0.7872\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2578\t Accuracy 0.7874\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2579\t Accuracy 0.7874\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2572\t Average training accuracy 0.7883\n",
      "Epoch [9]\t Average validation loss 0.2373\t Average validation accuracy 0.8324\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2323\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2504\t Accuracy 0.7975\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2496\t Accuracy 0.8017\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2531\t Accuracy 0.7958\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2530\t Accuracy 0.7941\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2531\t Accuracy 0.7936\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2529\t Accuracy 0.7937\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2536\t Accuracy 0.7919\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2536\t Accuracy 0.7931\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2537\t Accuracy 0.7936\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2539\t Accuracy 0.7935\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2533\t Average training accuracy 0.7943\n",
      "Epoch [10]\t Average validation loss 0.2335\t Average validation accuracy 0.8378\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2292\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2466\t Accuracy 0.8029\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2460\t Accuracy 0.8074\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2495\t Accuracy 0.8011\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2495\t Accuracy 0.7995\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2495\t Accuracy 0.7988\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2494\t Accuracy 0.7990\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2501\t Accuracy 0.7970\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2501\t Accuracy 0.7980\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2503\t Accuracy 0.7986\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2505\t Accuracy 0.7986\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2498\t Average training accuracy 0.7994\n",
      "Epoch [11]\t Average validation loss 0.2301\t Average validation accuracy 0.8426\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2265\t Accuracy 0.8400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2433\t Accuracy 0.8092\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2428\t Accuracy 0.8130\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2464\t Accuracy 0.8060\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2464\t Accuracy 0.8043\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2464\t Accuracy 0.8036\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2463\t Accuracy 0.8038\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2470\t Accuracy 0.8016\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2470\t Accuracy 0.8027\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2472\t Accuracy 0.8030\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2475\t Accuracy 0.8032\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2468\t Average training accuracy 0.8040\n",
      "Epoch [12]\t Average validation loss 0.2272\t Average validation accuracy 0.8480\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2240\t Accuracy 0.8400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2404\t Accuracy 0.8139\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2400\t Accuracy 0.8157\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2436\t Accuracy 0.8090\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2436\t Accuracy 0.8082\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2437\t Accuracy 0.8074\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2436\t Accuracy 0.8075\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2443\t Accuracy 0.8054\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2443\t Accuracy 0.8063\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2445\t Accuracy 0.8065\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2448\t Accuracy 0.8067\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2441\t Average training accuracy 0.8076\n",
      "Epoch [13]\t Average validation loss 0.2247\t Average validation accuracy 0.8506\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2218\t Accuracy 0.8400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2377\t Accuracy 0.8194\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2375\t Accuracy 0.8196\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2412\t Accuracy 0.8123\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2412\t Accuracy 0.8115\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2412\t Accuracy 0.8107\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2411\t Accuracy 0.8108\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2419\t Accuracy 0.8086\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2419\t Accuracy 0.8094\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2421\t Accuracy 0.8095\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2424\t Accuracy 0.8096\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2417\t Average training accuracy 0.8103\n",
      "Epoch [14]\t Average validation loss 0.2223\t Average validation accuracy 0.8534\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2198\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2354\t Accuracy 0.8227\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2353\t Accuracy 0.8223\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2390\t Accuracy 0.8152\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2390\t Accuracy 0.8144\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2390\t Accuracy 0.8138\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2389\t Accuracy 0.8136\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2397\t Accuracy 0.8114\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2398\t Accuracy 0.8122\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2400\t Accuracy 0.8123\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2402\t Accuracy 0.8122\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2396\t Average training accuracy 0.8128\n",
      "Epoch [15]\t Average validation loss 0.2203\t Average validation accuracy 0.8550\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2180\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2333\t Accuracy 0.8253\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2333\t Accuracy 0.8240\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2370\t Accuracy 0.8169\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2370\t Accuracy 0.8158\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2370\t Accuracy 0.8155\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2369\t Accuracy 0.8154\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2377\t Accuracy 0.8132\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2378\t Accuracy 0.8140\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2380\t Accuracy 0.8140\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2383\t Accuracy 0.8140\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2376\t Average training accuracy 0.8146\n",
      "Epoch [16]\t Average validation loss 0.2184\t Average validation accuracy 0.8562\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2164\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2313\t Accuracy 0.8286\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2314\t Accuracy 0.8265\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2352\t Accuracy 0.8193\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2351\t Accuracy 0.8183\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2352\t Accuracy 0.8179\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2351\t Accuracy 0.8177\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2359\t Accuracy 0.8156\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2360\t Accuracy 0.8164\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2362\t Accuracy 0.8164\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2365\t Accuracy 0.8163\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2359\t Average training accuracy 0.8169\n",
      "Epoch [17]\t Average validation loss 0.2166\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2148\t Accuracy 0.8700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2296\t Accuracy 0.8314\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2297\t Accuracy 0.8287\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2335\t Accuracy 0.8213\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2335\t Accuracy 0.8206\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2335\t Accuracy 0.8205\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2335\t Accuracy 0.8202\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2342\t Accuracy 0.8180\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2343\t Accuracy 0.8190\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2345\t Accuracy 0.8189\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2348\t Accuracy 0.8188\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2342\t Average training accuracy 0.8193\n",
      "Epoch [18]\t Average validation loss 0.2150\t Average validation accuracy 0.8610\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2134\t Accuracy 0.8700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2279\t Accuracy 0.8331\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2281\t Accuracy 0.8308\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2319\t Accuracy 0.8234\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2319\t Accuracy 0.8226\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2320\t Accuracy 0.8225\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2319\t Accuracy 0.8224\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2327\t Accuracy 0.8202\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2328\t Accuracy 0.8210\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2330\t Accuracy 0.8210\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2333\t Accuracy 0.8209\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2327\t Average training accuracy 0.8214\n",
      "Epoch [19]\t Average validation loss 0.2136\t Average validation accuracy 0.8616\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.0838\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2903\t Accuracy 0.2702\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8710\t Accuracy 0.3299\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.7043\t Accuracy 0.3954\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.6052\t Accuracy 0.4594\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5361\t Accuracy 0.5151\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4853\t Accuracy 0.5579\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4458\t Accuracy 0.5910\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4128\t Accuracy 0.6199\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3858\t Accuracy 0.6440\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3630\t Accuracy 0.6645\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3433\t Average training accuracy 0.6823\n",
      "Epoch [0]\t Average validation loss 0.1209\t Average validation accuracy 0.9006\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1112\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1293\t Accuracy 0.8843\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1288\t Accuracy 0.8828\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1301\t Accuracy 0.8803\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1277\t Accuracy 0.8830\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1263\t Accuracy 0.8843\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1250\t Accuracy 0.8856\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1241\t Accuracy 0.8861\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1230\t Accuracy 0.8869\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1220\t Accuracy 0.8878\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1213\t Accuracy 0.8885\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1198\t Average training accuracy 0.8897\n",
      "Epoch [1]\t Average validation loss 0.0914\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0806\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0994\t Accuracy 0.9182\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1011\t Accuracy 0.9122\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1034\t Accuracy 0.9086\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1022\t Accuracy 0.9106\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1019\t Accuracy 0.9108\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1018\t Accuracy 0.9107\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1018\t Accuracy 0.9101\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1017\t Accuracy 0.9103\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1015\t Accuracy 0.9104\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1016\t Accuracy 0.9102\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1009\t Average training accuracy 0.9104\n",
      "Epoch [2]\t Average validation loss 0.0821\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0727\t Accuracy 0.9700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0893\t Accuracy 0.9263\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0912\t Accuracy 0.9227\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0937\t Accuracy 0.9189\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0927\t Accuracy 0.9207\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0926\t Accuracy 0.9207\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0928\t Accuracy 0.9200\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0929\t Accuracy 0.9194\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0930\t Accuracy 0.9194\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0930\t Accuracy 0.9194\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0933\t Accuracy 0.9187\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0928\t Average training accuracy 0.9189\n",
      "Epoch [3]\t Average validation loss 0.0773\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0677\t Accuracy 0.9700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0838\t Accuracy 0.9308\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0857\t Accuracy 0.9276\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0883\t Accuracy 0.9244\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0874\t Accuracy 0.9259\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0874\t Accuracy 0.9259\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0876\t Accuracy 0.9250\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0877\t Accuracy 0.9244\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0878\t Accuracy 0.9242\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0879\t Accuracy 0.9243\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0883\t Accuracy 0.9236\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0879\t Average training accuracy 0.9238\n",
      "Epoch [4]\t Average validation loss 0.0741\t Average validation accuracy 0.9448\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0642\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0801\t Accuracy 0.9345\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0819\t Accuracy 0.9314\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0845\t Accuracy 0.9281\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0837\t Accuracy 0.9300\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0836\t Accuracy 0.9299\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0838\t Accuracy 0.9294\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0840\t Accuracy 0.9291\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0841\t Accuracy 0.9289\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0842\t Accuracy 0.9290\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0846\t Accuracy 0.9284\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0843\t Average training accuracy 0.9285\n",
      "Epoch [5]\t Average validation loss 0.0717\t Average validation accuracy 0.9476\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0620\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0773\t Accuracy 0.9388\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0790\t Accuracy 0.9351\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0815\t Accuracy 0.9321\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0808\t Accuracy 0.9332\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0807\t Accuracy 0.9333\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0809\t Accuracy 0.9330\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0811\t Accuracy 0.9326\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0813\t Accuracy 0.9322\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0814\t Accuracy 0.9323\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0818\t Accuracy 0.9315\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0815\t Average training accuracy 0.9315\n",
      "Epoch [6]\t Average validation loss 0.0699\t Average validation accuracy 0.9486\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0607\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0750\t Accuracy 0.9410\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0767\t Accuracy 0.9376\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0791\t Accuracy 0.9348\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0785\t Accuracy 0.9361\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0783\t Accuracy 0.9361\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0786\t Accuracy 0.9354\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0787\t Accuracy 0.9350\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0789\t Accuracy 0.9346\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0790\t Accuracy 0.9345\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0795\t Accuracy 0.9339\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0792\t Average training accuracy 0.9338\n",
      "Epoch [7]\t Average validation loss 0.0683\t Average validation accuracy 0.9486\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0591\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0731\t Accuracy 0.9435\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0747\t Accuracy 0.9401\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0771\t Accuracy 0.9372\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0765\t Accuracy 0.9380\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0763\t Accuracy 0.9378\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0766\t Accuracy 0.9374\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0767\t Accuracy 0.9368\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0769\t Accuracy 0.9365\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0770\t Accuracy 0.9363\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0775\t Accuracy 0.9357\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0772\t Average training accuracy 0.9357\n",
      "Epoch [8]\t Average validation loss 0.0671\t Average validation accuracy 0.9506\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0577\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0715\t Accuracy 0.9461\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0730\t Accuracy 0.9425\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0753\t Accuracy 0.9397\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0748\t Accuracy 0.9402\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0746\t Accuracy 0.9399\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0749\t Accuracy 0.9394\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0750\t Accuracy 0.9392\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0752\t Accuracy 0.9389\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0753\t Accuracy 0.9387\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0758\t Accuracy 0.9381\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0755\t Average training accuracy 0.9381\n",
      "Epoch [9]\t Average validation loss 0.0660\t Average validation accuracy 0.9514\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0564\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0702\t Accuracy 0.9480\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0715\t Accuracy 0.9449\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0738\t Accuracy 0.9425\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0732\t Accuracy 0.9428\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0731\t Accuracy 0.9421\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0734\t Accuracy 0.9416\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0735\t Accuracy 0.9413\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0737\t Accuracy 0.9408\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0738\t Accuracy 0.9406\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0743\t Accuracy 0.9400\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0740\t Average training accuracy 0.9400\n",
      "Epoch [10]\t Average validation loss 0.0650\t Average validation accuracy 0.9516\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0689\t Accuracy 0.9482\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0702\t Accuracy 0.9458\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0724\t Accuracy 0.9434\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0718\t Accuracy 0.9436\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0718\t Accuracy 0.9429\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0720\t Accuracy 0.9422\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0721\t Accuracy 0.9422\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0723\t Accuracy 0.9416\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0724\t Accuracy 0.9414\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0729\t Accuracy 0.9408\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0727\t Average training accuracy 0.9408\n",
      "Epoch [11]\t Average validation loss 0.0640\t Average validation accuracy 0.9528\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0539\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0678\t Accuracy 0.9502\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0689\t Accuracy 0.9474\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0711\t Accuracy 0.9448\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0706\t Accuracy 0.9452\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0705\t Accuracy 0.9445\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0708\t Accuracy 0.9436\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0709\t Accuracy 0.9435\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0711\t Accuracy 0.9430\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0711\t Accuracy 0.9429\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0716\t Accuracy 0.9423\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0714\t Average training accuracy 0.9424\n",
      "Epoch [12]\t Average validation loss 0.0632\t Average validation accuracy 0.9544\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0529\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0666\t Accuracy 0.9502\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0678\t Accuracy 0.9477\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0699\t Accuracy 0.9455\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0694\t Accuracy 0.9459\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0693\t Accuracy 0.9455\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0696\t Accuracy 0.9447\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0697\t Accuracy 0.9446\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0699\t Accuracy 0.9440\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0700\t Accuracy 0.9439\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0705\t Accuracy 0.9434\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0703\t Average training accuracy 0.9435\n",
      "Epoch [13]\t Average validation loss 0.0625\t Average validation accuracy 0.9554\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0521\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0656\t Accuracy 0.9516\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0667\t Accuracy 0.9488\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0688\t Accuracy 0.9465\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0683\t Accuracy 0.9471\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0682\t Accuracy 0.9466\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0686\t Accuracy 0.9457\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0686\t Accuracy 0.9458\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0688\t Accuracy 0.9454\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0689\t Accuracy 0.9453\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0694\t Accuracy 0.9448\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0692\t Average training accuracy 0.9449\n",
      "Epoch [14]\t Average validation loss 0.0618\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0514\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0647\t Accuracy 0.9522\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0657\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0677\t Accuracy 0.9477\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0673\t Accuracy 0.9484\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0672\t Accuracy 0.9480\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0676\t Accuracy 0.9471\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0676\t Accuracy 0.9472\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0678\t Accuracy 0.9468\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0679\t Accuracy 0.9468\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0684\t Accuracy 0.9461\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0682\t Average training accuracy 0.9462\n",
      "Epoch [15]\t Average validation loss 0.0612\t Average validation accuracy 0.9570\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0510\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.9529\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0648\t Accuracy 0.9508\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0668\t Accuracy 0.9487\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0663\t Accuracy 0.9495\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0663\t Accuracy 0.9490\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0666\t Accuracy 0.9481\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0666\t Accuracy 0.9483\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0669\t Accuracy 0.9479\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0669\t Accuracy 0.9479\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0674\t Accuracy 0.9471\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0672\t Average training accuracy 0.9472\n",
      "Epoch [16]\t Average validation loss 0.0606\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0503\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0629\t Accuracy 0.9527\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.9512\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0659\t Accuracy 0.9495\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0654\t Accuracy 0.9503\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0653\t Accuracy 0.9501\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0657\t Accuracy 0.9492\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0657\t Accuracy 0.9493\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0659\t Accuracy 0.9489\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0660\t Accuracy 0.9489\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0665\t Accuracy 0.9482\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0663\t Average training accuracy 0.9483\n",
      "Epoch [17]\t Average validation loss 0.0599\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0500\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0619\t Accuracy 0.9541\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0630\t Accuracy 0.9524\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0650\t Accuracy 0.9507\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0645\t Accuracy 0.9514\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.9512\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0647\t Accuracy 0.9503\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0648\t Accuracy 0.9505\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0650\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0650\t Accuracy 0.9501\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0655\t Accuracy 0.9494\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0653\t Average training accuracy 0.9495\n",
      "Epoch [18]\t Average validation loss 0.0592\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0498\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0610\t Accuracy 0.9549\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0622\t Accuracy 0.9529\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0641\t Accuracy 0.9515\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0636\t Accuracy 0.9523\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0634\t Accuracy 0.9524\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0638\t Accuracy 0.9516\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0638\t Accuracy 0.9518\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0641\t Accuracy 0.9513\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0641\t Accuracy 0.9514\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0646\t Accuracy 0.9508\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0644\t Average training accuracy 0.9508\n",
      "Epoch [19]\t Average validation loss 0.0586\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.8555\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6144\t Accuracy 0.0778\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.5024\t Accuracy 0.0915\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4292\t Accuracy 0.1119\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3782\t Accuracy 0.1293\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3394\t Accuracy 0.1476\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3081\t Accuracy 0.1692\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2838\t Accuracy 0.1896\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2620\t Accuracy 0.2102\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2429\t Accuracy 0.2300\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.2256\t Accuracy 0.2493\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2093\t Average training accuracy 0.2684\n",
      "Epoch [0]\t Average validation loss 2.0146\t Average validation accuracy 0.5056\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.9795\t Accuracy 0.5900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.0092\t Accuracy 0.5118\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.0013\t Accuracy 0.5142\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.9962\t Accuracy 0.5156\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.9884\t Accuracy 0.5212\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.9781\t Accuracy 0.5269\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.9672\t Accuracy 0.5338\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.9604\t Accuracy 0.5390\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.9509\t Accuracy 0.5443\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.9424\t Accuracy 0.5504\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.9335\t Accuracy 0.5559\n",
      "\n",
      "Epoch [1]\t Average training loss 1.9240\t Average training accuracy 0.5626\n",
      "Epoch [1]\t Average validation loss 1.7918\t Average validation accuracy 0.6852\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.7601\t Accuracy 0.6600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.7954\t Accuracy 0.6592\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.7898\t Accuracy 0.6548\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.7876\t Accuracy 0.6495\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.7821\t Accuracy 0.6496\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.7730\t Accuracy 0.6523\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.7632\t Accuracy 0.6539\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.7592\t Accuracy 0.6543\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.7510\t Accuracy 0.6569\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.7443\t Accuracy 0.6593\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.7368\t Accuracy 0.6611\n",
      "\n",
      "Epoch [2]\t Average training loss 1.7285\t Average training accuracy 0.6644\n",
      "Epoch [2]\t Average validation loss 1.6015\t Average validation accuracy 0.7462\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.5770\t Accuracy 0.7300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.6135\t Accuracy 0.7192\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.6101\t Accuracy 0.7126\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.6103\t Accuracy 0.7079\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.6066\t Accuracy 0.7075\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.5986\t Accuracy 0.7094\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.5899\t Accuracy 0.7102\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.5880\t Accuracy 0.7092\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.5810\t Accuracy 0.7116\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.5757\t Accuracy 0.7135\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.5695\t Accuracy 0.7142\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5623\t Average training accuracy 0.7161\n",
      "Epoch [3]\t Average validation loss 1.4397\t Average validation accuracy 0.7824\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.4240\t Accuracy 0.7700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.4595\t Accuracy 0.7525\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.4579\t Accuracy 0.7475\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.4602\t Accuracy 0.7418\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.4579\t Accuracy 0.7426\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.4509\t Accuracy 0.7425\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.4431\t Accuracy 0.7428\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.4430\t Accuracy 0.7414\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.4370\t Accuracy 0.7431\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.4329\t Accuracy 0.7445\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.4278\t Accuracy 0.7451\n",
      "\n",
      "Epoch [4]\t Average training loss 1.4215\t Average training accuracy 0.7465\n",
      "Epoch [4]\t Average validation loss 1.3030\t Average validation accuracy 0.8074\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.2964\t Accuracy 0.7800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.3296\t Accuracy 0.7724\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.3296\t Accuracy 0.7672\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.3336\t Accuracy 0.7615\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.3325\t Accuracy 0.7624\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.3263\t Accuracy 0.7629\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.3193\t Accuracy 0.7630\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.3206\t Accuracy 0.7613\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.3154\t Accuracy 0.7629\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.3123\t Accuracy 0.7645\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.3081\t Accuracy 0.7648\n",
      "\n",
      "Epoch [5]\t Average training loss 1.3026\t Average training accuracy 0.7659\n",
      "Epoch [5]\t Average validation loss 1.1877\t Average validation accuracy 0.8246\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.1901\t Accuracy 0.8000\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.2202\t Accuracy 0.7908\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.2216\t Accuracy 0.7859\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.2270\t Accuracy 0.7786\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.2267\t Accuracy 0.7792\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.2212\t Accuracy 0.7796\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.2150\t Accuracy 0.7802\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.2174\t Accuracy 0.7784\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.2129\t Accuracy 0.7801\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.2106\t Accuracy 0.7811\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.2072\t Accuracy 0.7810\n",
      "\n",
      "Epoch [6]\t Average training loss 1.2024\t Average training accuracy 0.7819\n",
      "Epoch [6]\t Average validation loss 1.0905\t Average validation accuracy 0.8360\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.1012\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.1281\t Accuracy 0.8022\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.1305\t Accuracy 0.7993\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.1371\t Accuracy 0.7911\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.1375\t Accuracy 0.7920\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.1325\t Accuracy 0.7927\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.1270\t Accuracy 0.7930\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.1303\t Accuracy 0.7914\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.1264\t Accuracy 0.7930\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.1247\t Accuracy 0.7936\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.1219\t Accuracy 0.7935\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1176\t Average training accuracy 0.7944\n",
      "Epoch [7]\t Average validation loss 1.0084\t Average validation accuracy 0.8466\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.0264\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.0502\t Accuracy 0.8116\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.0535\t Accuracy 0.8087\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.0610\t Accuracy 0.8003\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.0619\t Accuracy 0.8005\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.0574\t Accuracy 0.8015\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.0524\t Accuracy 0.8020\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.0564\t Accuracy 0.8007\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.0530\t Accuracy 0.8020\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.0518\t Accuracy 0.8023\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.0495\t Accuracy 0.8021\n",
      "\n",
      "Epoch [8]\t Average training loss 1.0457\t Average training accuracy 0.8031\n",
      "Epoch [8]\t Average validation loss 0.9386\t Average validation accuracy 0.8550\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.9632\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.9840\t Accuracy 0.8182\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.9880\t Accuracy 0.8166\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.9963\t Accuracy 0.8091\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.9975\t Accuracy 0.8092\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.9934\t Accuracy 0.8096\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.9889\t Accuracy 0.8101\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.9933\t Accuracy 0.8085\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.9904\t Accuracy 0.8100\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.9895\t Accuracy 0.8103\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.9877\t Accuracy 0.8103\n",
      "\n",
      "Epoch [9]\t Average training loss 0.9843\t Average training accuracy 0.8112\n",
      "Epoch [9]\t Average validation loss 0.8790\t Average validation accuracy 0.8608\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.9092\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.9273\t Accuracy 0.8253\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.9318\t Accuracy 0.8232\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.9408\t Accuracy 0.8159\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.9422\t Accuracy 0.8162\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.9385\t Accuracy 0.8165\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.9344\t Accuracy 0.8171\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.9392\t Accuracy 0.8156\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.9366\t Accuracy 0.8168\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.9361\t Accuracy 0.8170\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.9346\t Accuracy 0.8171\n",
      "\n",
      "Epoch [10]\t Average training loss 0.9316\t Average training accuracy 0.8177\n",
      "Epoch [10]\t Average validation loss 0.8277\t Average validation accuracy 0.8658\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8628\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8784\t Accuracy 0.8312\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8834\t Accuracy 0.8287\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8929\t Accuracy 0.8220\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8945\t Accuracy 0.8217\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8911\t Accuracy 0.8222\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8873\t Accuracy 0.8227\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8924\t Accuracy 0.8213\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8901\t Accuracy 0.8223\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8898\t Accuracy 0.8225\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8887\t Accuracy 0.8225\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8859\t Average training accuracy 0.8230\n",
      "Epoch [11]\t Average validation loss 0.7832\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8225\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8359\t Accuracy 0.8369\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8413\t Accuracy 0.8335\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8513\t Accuracy 0.8273\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8530\t Accuracy 0.8272\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8498\t Accuracy 0.8275\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8463\t Accuracy 0.8281\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8516\t Accuracy 0.8266\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8495\t Accuracy 0.8275\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8494\t Accuracy 0.8275\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8486\t Accuracy 0.8275\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8461\t Average training accuracy 0.8282\n",
      "Epoch [12]\t Average validation loss 0.7444\t Average validation accuracy 0.8736\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7873\t Accuracy 0.8600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7986\t Accuracy 0.8418\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8045\t Accuracy 0.8385\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8148\t Accuracy 0.8317\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8165\t Accuracy 0.8316\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8136\t Accuracy 0.8317\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8104\t Accuracy 0.8325\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8159\t Accuracy 0.8312\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8139\t Accuracy 0.8320\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8140\t Accuracy 0.8318\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8134\t Accuracy 0.8318\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8111\t Average training accuracy 0.8324\n",
      "Epoch [13]\t Average validation loss 0.7103\t Average validation accuracy 0.8756\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7562\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7658\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7720\t Accuracy 0.8413\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.7826\t Accuracy 0.8354\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.7844\t Accuracy 0.8355\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.7816\t Accuracy 0.8357\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7786\t Accuracy 0.8366\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.7842\t Accuracy 0.8354\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.7825\t Accuracy 0.8362\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.7827\t Accuracy 0.8358\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.7823\t Accuracy 0.8356\n",
      "\n",
      "Epoch [14]\t Average training loss 0.7802\t Average training accuracy 0.8360\n",
      "Epoch [14]\t Average validation loss 0.6802\t Average validation accuracy 0.8780\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7285\t Accuracy 0.8600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7367\t Accuracy 0.8484\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7431\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7540\t Accuracy 0.8396\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7558\t Accuracy 0.8397\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7532\t Accuracy 0.8395\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7504\t Accuracy 0.8406\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7561\t Accuracy 0.8394\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7545\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.7548\t Accuracy 0.8398\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.7546\t Accuracy 0.8394\n",
      "\n",
      "Epoch [15]\t Average training loss 0.7526\t Average training accuracy 0.8397\n",
      "Epoch [15]\t Average validation loss 0.6533\t Average validation accuracy 0.8800\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7038\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7106\t Accuracy 0.8506\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7173\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7285\t Accuracy 0.8425\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.7303\t Accuracy 0.8427\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7278\t Accuracy 0.8427\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7252\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.7309\t Accuracy 0.8425\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.7294\t Accuracy 0.8430\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.7299\t Accuracy 0.8427\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.7298\t Accuracy 0.8425\n",
      "\n",
      "Epoch [16]\t Average training loss 0.7280\t Average training accuracy 0.8427\n",
      "Epoch [16]\t Average validation loss 0.6293\t Average validation accuracy 0.8814\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.6816\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.6872\t Accuracy 0.8543\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.6942\t Accuracy 0.8517\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.7056\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.7073\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.7049\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.7025\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.7083\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.7069\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.7074\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.7075\t Accuracy 0.8455\n",
      "\n",
      "Epoch [17]\t Average training loss 0.7058\t Average training accuracy 0.8456\n",
      "Epoch [17]\t Average validation loss 0.6077\t Average validation accuracy 0.8832\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.6615\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.6661\t Accuracy 0.8590\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.6733\t Accuracy 0.8554\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.6848\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.6865\t Accuracy 0.8497\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.6843\t Accuracy 0.8496\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.6819\t Accuracy 0.8503\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.6878\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.6865\t Accuracy 0.8496\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.6871\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.6873\t Accuracy 0.8486\n",
      "\n",
      "Epoch [18]\t Average training loss 0.6857\t Average training accuracy 0.8486\n",
      "Epoch [18]\t Average validation loss 0.5881\t Average validation accuracy 0.8854\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.6431\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.6469\t Accuracy 0.8604\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.6543\t Accuracy 0.8568\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.6660\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.6677\t Accuracy 0.8517\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.6655\t Accuracy 0.8516\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.6633\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.6691\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.6680\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.6686\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.6689\t Accuracy 0.8503\n",
      "\n",
      "Epoch [19]\t Average training loss 0.6674\t Average training accuracy 0.8503\n",
      "Epoch [19]\t Average validation loss 0.5704\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5393\t Accuracy 0.0600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.0435\t Accuracy 0.3153\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.4888\t Accuracy 0.5290\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.1967\t Accuracy 0.6248\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.0167\t Accuracy 0.6825\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8980\t Accuracy 0.7213\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8156\t Accuracy 0.7485\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7566\t Accuracy 0.7679\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7058\t Accuracy 0.7841\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6663\t Accuracy 0.7965\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6329\t Accuracy 0.8069\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6039\t Average training accuracy 0.8163\n",
      "Epoch [0]\t Average validation loss 0.2232\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2806\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2574\t Accuracy 0.9302\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2655\t Accuracy 0.9262\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2716\t Accuracy 0.9233\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2629\t Accuracy 0.9260\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2586\t Accuracy 0.9268\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2561\t Accuracy 0.9275\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2540\t Accuracy 0.9279\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2511\t Accuracy 0.9286\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2490\t Accuracy 0.9287\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2475\t Accuracy 0.9291\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2450\t Average training accuracy 0.9299\n",
      "Epoch [1]\t Average validation loss 0.1657\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1734\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1831\t Accuracy 0.9506\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1940\t Accuracy 0.9456\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1999\t Accuracy 0.9432\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1949\t Accuracy 0.9447\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1933\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1926\t Accuracy 0.9451\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1919\t Accuracy 0.9454\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1908\t Accuracy 0.9456\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1901\t Accuracy 0.9453\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1903\t Accuracy 0.9451\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1891\t Average training accuracy 0.9457\n",
      "Epoch [2]\t Average validation loss 0.1406\t Average validation accuracy 0.9628\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1343\t Accuracy 0.9700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1509\t Accuracy 0.9584\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1607\t Accuracy 0.9554\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1642\t Accuracy 0.9532\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1610\t Accuracy 0.9545\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1595\t Accuracy 0.9549\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1594\t Accuracy 0.9547\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1590\t Accuracy 0.9545\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1582\t Accuracy 0.9546\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1580\t Accuracy 0.9544\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1587\t Accuracy 0.9541\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1579\t Average training accuracy 0.9546\n",
      "Epoch [3]\t Average validation loss 0.1229\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1112\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1281\t Accuracy 0.9643\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1377\t Accuracy 0.9615\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1399\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1378\t Accuracy 0.9607\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1365\t Accuracy 0.9612\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1367\t Accuracy 0.9606\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1365\t Accuracy 0.9607\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1361\t Accuracy 0.9607\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1360\t Accuracy 0.9607\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1370\t Accuracy 0.9605\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1365\t Average training accuracy 0.9610\n",
      "Epoch [4]\t Average validation loss 0.1114\t Average validation accuracy 0.9720\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0972\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1121\t Accuracy 0.9686\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1214\t Accuracy 0.9659\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1228\t Accuracy 0.9651\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1214\t Accuracy 0.9659\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1202\t Accuracy 0.9659\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1205\t Accuracy 0.9651\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1203\t Accuracy 0.9653\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1201\t Accuracy 0.9652\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1201\t Accuracy 0.9653\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1212\t Accuracy 0.9650\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1208\t Average training accuracy 0.9654\n",
      "Epoch [5]\t Average validation loss 0.1028\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0875\t Accuracy 0.9800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1005\t Accuracy 0.9729\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1090\t Accuracy 0.9710\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1095\t Accuracy 0.9701\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1085\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1074\t Accuracy 0.9696\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1078\t Accuracy 0.9689\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1077\t Accuracy 0.9691\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1076\t Accuracy 0.9691\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1077\t Accuracy 0.9689\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1088\t Accuracy 0.9685\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1085\t Average training accuracy 0.9689\n",
      "Epoch [6]\t Average validation loss 0.0962\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0814\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0911\t Accuracy 0.9759\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0991\t Accuracy 0.9736\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0988\t Accuracy 0.9729\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0980\t Accuracy 0.9729\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0971\t Accuracy 0.9730\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0975\t Accuracy 0.9726\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0975\t Accuracy 0.9727\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0976\t Accuracy 0.9725\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0976\t Accuracy 0.9723\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0988\t Accuracy 0.9718\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0985\t Average training accuracy 0.9721\n",
      "Epoch [7]\t Average validation loss 0.0912\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0752\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0830\t Accuracy 0.9775\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0906\t Accuracy 0.9753\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0898\t Accuracy 0.9757\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0891\t Accuracy 0.9755\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0884\t Accuracy 0.9755\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0888\t Accuracy 0.9754\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0889\t Accuracy 0.9753\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0891\t Accuracy 0.9752\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0892\t Accuracy 0.9750\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0904\t Accuracy 0.9745\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0901\t Average training accuracy 0.9746\n",
      "Epoch [8]\t Average validation loss 0.0873\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0691\t Accuracy 0.9900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0759\t Accuracy 0.9792\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0832\t Accuracy 0.9770\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0821\t Accuracy 0.9776\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0816\t Accuracy 0.9775\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0810\t Accuracy 0.9774\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0814\t Accuracy 0.9773\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0816\t Accuracy 0.9774\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0819\t Accuracy 0.9772\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0820\t Accuracy 0.9770\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0831\t Accuracy 0.9765\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0829\t Average training accuracy 0.9765\n",
      "Epoch [9]\t Average validation loss 0.0843\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0636\t Accuracy 0.9900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0695\t Accuracy 0.9814\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0767\t Accuracy 0.9787\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0753\t Accuracy 0.9795\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0749\t Accuracy 0.9795\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0745\t Accuracy 0.9796\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0750\t Accuracy 0.9795\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0752\t Accuracy 0.9794\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0756\t Accuracy 0.9793\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0756\t Accuracy 0.9791\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0767\t Accuracy 0.9786\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0765\t Average training accuracy 0.9786\n",
      "Epoch [10]\t Average validation loss 0.0822\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0590\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0638\t Accuracy 0.9829\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0708\t Accuracy 0.9806\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0692\t Accuracy 0.9815\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0690\t Accuracy 0.9813\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0686\t Accuracy 0.9815\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0692\t Accuracy 0.9813\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0694\t Accuracy 0.9814\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0699\t Accuracy 0.9813\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0699\t Accuracy 0.9810\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0710\t Accuracy 0.9806\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0708\t Average training accuracy 0.9806\n",
      "Epoch [11]\t Average validation loss 0.0806\t Average validation accuracy 0.9778\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.9900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0591\t Accuracy 0.9833\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0658\t Accuracy 0.9813\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0640\t Accuracy 0.9824\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0639\t Accuracy 0.9826\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0635\t Accuracy 0.9827\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0642\t Accuracy 0.9825\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0645\t Accuracy 0.9827\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0650\t Accuracy 0.9825\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0650\t Accuracy 0.9823\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0661\t Accuracy 0.9819\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0658\t Average training accuracy 0.9819\n",
      "Epoch [12]\t Average validation loss 0.0792\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0515\t Accuracy 0.9900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0549\t Accuracy 0.9847\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0611\t Accuracy 0.9828\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0593\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0593\t Accuracy 0.9838\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0590\t Accuracy 0.9840\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0597\t Accuracy 0.9837\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0600\t Accuracy 0.9839\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0606\t Accuracy 0.9837\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0605\t Accuracy 0.9836\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0616\t Accuracy 0.9833\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0613\t Average training accuracy 0.9833\n",
      "Epoch [13]\t Average validation loss 0.0779\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0486\t Accuracy 0.9900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0513\t Accuracy 0.9863\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0569\t Accuracy 0.9841\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0551\t Accuracy 0.9852\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0552\t Accuracy 0.9852\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0549\t Accuracy 0.9854\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0556\t Accuracy 0.9850\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0560\t Accuracy 0.9852\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0565\t Accuracy 0.9850\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0565\t Accuracy 0.9849\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0575\t Accuracy 0.9846\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0572\t Average training accuracy 0.9845\n",
      "Epoch [14]\t Average validation loss 0.0765\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0461\t Accuracy 0.9900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0481\t Accuracy 0.9878\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0533\t Accuracy 0.9853\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0515\t Accuracy 0.9866\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0515\t Accuracy 0.9864\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0513\t Accuracy 0.9865\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0520\t Accuracy 0.9861\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0524\t Accuracy 0.9863\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0530\t Accuracy 0.9861\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0530\t Accuracy 0.9861\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0539\t Accuracy 0.9857\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0536\t Average training accuracy 0.9857\n",
      "Epoch [15]\t Average validation loss 0.0754\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0441\t Accuracy 0.9900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0451\t Accuracy 0.9890\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0499\t Accuracy 0.9865\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0481\t Accuracy 0.9875\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0482\t Accuracy 0.9875\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0480\t Accuracy 0.9875\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0487\t Accuracy 0.9872\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0492\t Accuracy 0.9874\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0497\t Accuracy 0.9872\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0497\t Accuracy 0.9871\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0506\t Accuracy 0.9867\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0503\t Average training accuracy 0.9867\n",
      "Epoch [16]\t Average validation loss 0.0743\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0422\t Accuracy 0.9900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0424\t Accuracy 0.9896\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0468\t Accuracy 0.9875\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0451\t Accuracy 0.9885\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0452\t Accuracy 0.9885\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0450\t Accuracy 0.9887\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0458\t Accuracy 0.9883\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0462\t Accuracy 0.9885\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0467\t Accuracy 0.9883\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0467\t Accuracy 0.9882\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0475\t Accuracy 0.9879\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0473\t Average training accuracy 0.9879\n",
      "Epoch [17]\t Average validation loss 0.0735\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0403\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0400\t Accuracy 0.9908\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0440\t Accuracy 0.9891\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0424\t Accuracy 0.9899\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0424\t Accuracy 0.9897\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0422\t Accuracy 0.9898\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0430\t Accuracy 0.9895\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0435\t Accuracy 0.9895\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0440\t Accuracy 0.9894\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0441\t Accuracy 0.9894\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0448\t Accuracy 0.9890\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0446\t Average training accuracy 0.9889\n",
      "Epoch [18]\t Average validation loss 0.0729\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0384\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0378\t Accuracy 0.9914\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0416\t Accuracy 0.9898\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0399\t Accuracy 0.9904\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0400\t Accuracy 0.9902\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0398\t Accuracy 0.9903\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0406\t Accuracy 0.9902\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0411\t Accuracy 0.9902\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0416\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0416\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0423\t Accuracy 0.9897\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0421\t Average training accuracy 0.9897\n",
      "Epoch [19]\t Average validation loss 0.0724\t Average validation accuracy 0.9784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "learning_rate_SGD = 0.001\n",
    "\n",
    "for weight_decay in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    #Euclidean+Sigmoid\n",
    "    momentum = 0.55\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]   \n",
    "\n",
    "    #Euclidean+ReLU\n",
    "    momentum = 0.99\n",
    "    criterion = EuclideanLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    reluMLP = Network()\n",
    "    # 使用FCLayer和ReLULayer构建多层感知机\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['Euclidean_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]     \n",
    "\n",
    "    #CrossEntropy+Sigmoid\n",
    "    momentum = 0.55\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    t1=time.time()\n",
    "    sigmoidMLP = Network()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    sigmoidMLP.add(FCLayer(784, 128))\n",
    "    sigmoidMLP.add(SigmoidLayer())\n",
    "    sigmoidMLP.add(FCLayer(128, 10))\n",
    "    sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_Sigmoid',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,sigmoid_loss, sigmoid_acc]         \n",
    "\n",
    "    #CrossEntropy+ReLU\n",
    "    momentum = 0.99\n",
    "    criterion = SoftmaxCrossEntropyLossLayer()\n",
    "    sgd = SGD(learning_rate_SGD, weight_decay,momentum)\n",
    "    reluMLP = Network()\n",
    "    t1=time.time()\n",
    "    # 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "    # 128为隐含层的神经元数目\n",
    "    reluMLP.add(FCLayer(784, 128))\n",
    "    reluMLP.add(ReLULayer())\n",
    "    reluMLP.add(FCLayer(128, 10))\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    t2=time.time()\n",
    "    exec_result.loc[exec_result.shape[0]] = ['CrossEntropy_ReLU',batch_size, learning_rate_SGD, momentum, weight_decay, t2-t1,relu_loss, relu_acc]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>41.829626</td>\n",
       "      <td>[0.3995154779990792, 0.3261753784520195, 0.299...</td>\n",
       "      <td>[0.4716, 0.6672, 0.7506, 0.7864, 0.80740000000...</td>\n",
       "      <td>0.76955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>41.920897</td>\n",
       "      <td>[0.2201554069124244, 0.21051490925963956, 0.20...</td>\n",
       "      <td>[0.8182, 0.8439999999999999, 0.849199999999999...</td>\n",
       "      <td>0.84993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>42.398985</td>\n",
       "      <td>[2.0799282789475524, 1.9528335114685098, 1.885...</td>\n",
       "      <td>[0.45799999999999996, 0.6342, 0.6882, 0.714200...</td>\n",
       "      <td>0.67107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>52.893564</td>\n",
       "      <td>[0.7803894738426628, 0.7822880263182782, 0.777...</td>\n",
       "      <td>[0.8870000000000001, 0.8872000000000001, 0.886...</td>\n",
       "      <td>0.88379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>84.251540</td>\n",
       "      <td>[0.4160145915536109, 0.3324502885901587, 0.294...</td>\n",
       "      <td>[0.47100000000000003, 0.6606000000000001, 0.72...</td>\n",
       "      <td>0.80825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>43.240416</td>\n",
       "      <td>[0.11757911384385265, 0.09000380383351414, 0.0...</td>\n",
       "      <td>[0.9091999999999999, 0.9320000000000002, 0.937...</td>\n",
       "      <td>0.94301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>64.363540</td>\n",
       "      <td>[2.015180303023981, 1.7914974403402264, 1.6110...</td>\n",
       "      <td>[0.45480000000000004, 0.6365999999999999, 0.72...</td>\n",
       "      <td>0.81692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>60.015743</td>\n",
       "      <td>[0.27511439749298217, 0.2515398981857088, 0.24...</td>\n",
       "      <td>[0.9308000000000001, 0.9416, 0.947200000000000...</td>\n",
       "      <td>0.95340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>65.425580</td>\n",
       "      <td>[0.39836481136813817, 0.3307370945878176, 0.29...</td>\n",
       "      <td>[0.5067999999999999, 0.6606000000000001, 0.728...</td>\n",
       "      <td>0.80736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>86.525570</td>\n",
       "      <td>[0.13042195668680456, 0.09375013764007312, 0.0...</td>\n",
       "      <td>[0.8944, 0.9232, 0.9342, 0.9403999999999999, 0...</td>\n",
       "      <td>0.94951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>87.068302</td>\n",
       "      <td>[2.019765646592555, 1.8032552455943496, 1.6182...</td>\n",
       "      <td>[0.4844, 0.6494, 0.7328, 0.775, 0.801600000000...</td>\n",
       "      <td>0.82431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>79.010023</td>\n",
       "      <td>[0.22577524112625524, 0.17379358836221712, 0.1...</td>\n",
       "      <td>[0.9378, 0.9558, 0.9613999999999998, 0.9627999...</td>\n",
       "      <td>0.97159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>81.395794</td>\n",
       "      <td>[0.40203178768559084, 0.33277713238912043, 0.2...</td>\n",
       "      <td>[0.48800000000000004, 0.6553999999999999, 0.73...</td>\n",
       "      <td>0.81051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.493162</td>\n",
       "      <td>[0.12303438996578331, 0.08860506861796655, 0.0...</td>\n",
       "      <td>[0.9087999999999999, 0.9373999999999999, 0.944...</td>\n",
       "      <td>0.95250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.822916</td>\n",
       "      <td>[2.0336907797261388, 1.798589231055107, 1.6011...</td>\n",
       "      <td>[0.4708, 0.6512, 0.7268000000000001, 0.7667999...</td>\n",
       "      <td>0.82375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>89.203245</td>\n",
       "      <td>[0.23040020357007115, 0.17043784625938993, 0.1...</td>\n",
       "      <td>[0.9358, 0.955, 0.9623999999999999, 0.9672, 0....</td>\n",
       "      <td>0.97219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>89.748165</td>\n",
       "      <td>[0.4055092736497922, 0.3372117355735208, 0.304...</td>\n",
       "      <td>[0.48879999999999996, 0.6388, 0.71580000000000...</td>\n",
       "      <td>0.79935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>86.845003</td>\n",
       "      <td>[0.12086158593266942, 0.09138613343652555, 0.0...</td>\n",
       "      <td>[0.9006000000000001, 0.9288000000000001, 0.937...</td>\n",
       "      <td>0.94818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>98.448528</td>\n",
       "      <td>[2.014637289436396, 1.7918110122740378, 1.6014...</td>\n",
       "      <td>[0.5055999999999998, 0.6851999999999999, 0.746...</td>\n",
       "      <td>0.82643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>102.327386</td>\n",
       "      <td>[0.22322786553901147, 0.16572498699567628, 0.1...</td>\n",
       "      <td>[0.938, 0.9566000000000001, 0.9628, 0.9678, 0....</td>\n",
       "      <td>0.97311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "0      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "1         Euclidean_ReLU         100              0.001      0.99   \n",
       "2   CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "3      CrossEntropy_ReLU         100              0.001      0.99   \n",
       "4      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "5         Euclidean_ReLU         100              0.001      0.99   \n",
       "6   CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "7      CrossEntropy_ReLU         100              0.001      0.99   \n",
       "8      Euclidean_Sigmoid         100              0.001      0.55   \n",
       "9         Euclidean_ReLU         100              0.001      0.99   \n",
       "10  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "11     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "12     Euclidean_Sigmoid         100              0.001      0.55   \n",
       "13        Euclidean_ReLU         100              0.001      0.99   \n",
       "14  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "15     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "16     Euclidean_Sigmoid         100              0.001      0.55   \n",
       "17        Euclidean_ReLU         100              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "19     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "0        0.10000   41.829626   \n",
       "1        0.10000   41.920897   \n",
       "2        0.10000   42.398985   \n",
       "3        0.10000   52.893564   \n",
       "4        0.01000   84.251540   \n",
       "5        0.01000   43.240416   \n",
       "6        0.01000   64.363540   \n",
       "7        0.01000   60.015743   \n",
       "8        0.00100   65.425580   \n",
       "9        0.00100   86.525570   \n",
       "10       0.00100   87.068302   \n",
       "11       0.00100   79.010023   \n",
       "12       0.00010   81.395794   \n",
       "13       0.00010   88.493162   \n",
       "14       0.00010   88.822916   \n",
       "15       0.00010   89.203245   \n",
       "16       0.00001   89.748165   \n",
       "17       0.00001   86.845003   \n",
       "18       0.00001   98.448528   \n",
       "19       0.00001  102.327386   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "0   [0.3995154779990792, 0.3261753784520195, 0.299...   \n",
       "1   [0.2201554069124244, 0.21051490925963956, 0.20...   \n",
       "2   [2.0799282789475524, 1.9528335114685098, 1.885...   \n",
       "3   [0.7803894738426628, 0.7822880263182782, 0.777...   \n",
       "4   [0.4160145915536109, 0.3324502885901587, 0.294...   \n",
       "5   [0.11757911384385265, 0.09000380383351414, 0.0...   \n",
       "6   [2.015180303023981, 1.7914974403402264, 1.6110...   \n",
       "7   [0.27511439749298217, 0.2515398981857088, 0.24...   \n",
       "8   [0.39836481136813817, 0.3307370945878176, 0.29...   \n",
       "9   [0.13042195668680456, 0.09375013764007312, 0.0...   \n",
       "10  [2.019765646592555, 1.8032552455943496, 1.6182...   \n",
       "11  [0.22577524112625524, 0.17379358836221712, 0.1...   \n",
       "12  [0.40203178768559084, 0.33277713238912043, 0.2...   \n",
       "13  [0.12303438996578331, 0.08860506861796655, 0.0...   \n",
       "14  [2.0336907797261388, 1.798589231055107, 1.6011...   \n",
       "15  [0.23040020357007115, 0.17043784625938993, 0.1...   \n",
       "16  [0.4055092736497922, 0.3372117355735208, 0.304...   \n",
       "17  [0.12086158593266942, 0.09138613343652555, 0.0...   \n",
       "18  [2.014637289436396, 1.7918110122740378, 1.6014...   \n",
       "19  [0.22322786553901147, 0.16572498699567628, 0.1...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "0   [0.4716, 0.6672, 0.7506, 0.7864, 0.80740000000...             0.76955  \n",
       "1   [0.8182, 0.8439999999999999, 0.849199999999999...             0.84993  \n",
       "2   [0.45799999999999996, 0.6342, 0.6882, 0.714200...             0.67107  \n",
       "3   [0.8870000000000001, 0.8872000000000001, 0.886...             0.88379  \n",
       "4   [0.47100000000000003, 0.6606000000000001, 0.72...             0.80825  \n",
       "5   [0.9091999999999999, 0.9320000000000002, 0.937...             0.94301  \n",
       "6   [0.45480000000000004, 0.6365999999999999, 0.72...             0.81692  \n",
       "7   [0.9308000000000001, 0.9416, 0.947200000000000...             0.95340  \n",
       "8   [0.5067999999999999, 0.6606000000000001, 0.728...             0.80736  \n",
       "9   [0.8944, 0.9232, 0.9342, 0.9403999999999999, 0...             0.94951  \n",
       "10  [0.4844, 0.6494, 0.7328, 0.775, 0.801600000000...             0.82431  \n",
       "11  [0.9378, 0.9558, 0.9613999999999998, 0.9627999...             0.97159  \n",
       "12  [0.48800000000000004, 0.6553999999999999, 0.73...             0.81051  \n",
       "13  [0.9087999999999999, 0.9373999999999999, 0.944...             0.95250  \n",
       "14  [0.4708, 0.6512, 0.7268000000000001, 0.7667999...             0.82375  \n",
       "15  [0.9358, 0.955, 0.9623999999999999, 0.9672, 0....             0.97219  \n",
       "16  [0.48879999999999996, 0.6388, 0.71580000000000...             0.79935  \n",
       "17  [0.9006000000000001, 0.9288000000000001, 0.937...             0.94818  \n",
       "18  [0.5055999999999998, 0.6851999999999999, 0.746...             0.82643  \n",
       "19  [0.938, 0.9566000000000001, 0.9628, 0.9678, 0....             0.97311  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result['acc_validate_float'] = exec_result['acc_validate'].map(lambda x: np.average(x))\n",
    "exec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f234f65d2b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6), dpi=100)\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['weight_decay'], exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'], marker='s', label = 'Euclidean+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['weight_decay'], exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'], marker='s', label = 'Euclidean+ReLU')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['weight_decay'], exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+Sigmoid')\n",
    "plt.plot(exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['weight_decay'], exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'], marker='s', label = 'CrossEntropy+ReLU')\n",
    "plt.xlabel('weight_decay')\n",
    "plt.ylabel('acc_validate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result.to_csv('./result/result_weight_decay.csv',index=False)\n",
    "# # euclidean_sigmoid.learning_rate_SGD, euclidean_relu.learning_rate_SGD, crossEntropy_sigmoid.learning_rate_SGD, crossEntropy_relu.learning_rate_SGD\n",
    "\n",
    "# exec_result=pd.read_csv('./result_learning_rate_SGD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001, 0.0001, 1e-05, 1e-05)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index = exec_result.query(\"mode == 'Euclidean_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "euclidean_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'Euclidean_ReLU'\")['acc_validate_float'].idxmax()\n",
    "euclidean_relu= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_Sigmoid'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_sigmoid= exec_result.loc[best_index]\n",
    "best_index = exec_result.query(\"mode == 'CrossEntropy_ReLU'\")['acc_validate_float'].idxmax()\n",
    "crossEntropy_relu= exec_result.loc[best_index]\n",
    "euclidean_sigmoid.weight_decay, euclidean_relu.weight_decay, crossEntropy_sigmoid.weight_decay, crossEntropy_relu.weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_SGD</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>time</th>\n",
       "      <th>loss_validate</th>\n",
       "      <th>acc_validate</th>\n",
       "      <th>acc_validate_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>81.395794</td>\n",
       "      <td>[0.40203178768559084, 0.33277713238912043, 0.2...</td>\n",
       "      <td>[0.48800000000000004, 0.6553999999999999, 0.73...</td>\n",
       "      <td>0.81051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.493162</td>\n",
       "      <td>[0.12303438996578331, 0.08860506861796655, 0.0...</td>\n",
       "      <td>[0.9087999999999999, 0.9373999999999999, 0.944...</td>\n",
       "      <td>0.95250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>88.822916</td>\n",
       "      <td>[2.0336907797261388, 1.798589231055107, 1.6011...</td>\n",
       "      <td>[0.4708, 0.6512, 0.7268000000000001, 0.7667999...</td>\n",
       "      <td>0.82375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>89.203245</td>\n",
       "      <td>[0.23040020357007115, 0.17043784625938993, 0.1...</td>\n",
       "      <td>[0.9358, 0.955, 0.9623999999999999, 0.9672, 0....</td>\n",
       "      <td>0.97219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Euclidean_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>89.748165</td>\n",
       "      <td>[0.4055092736497922, 0.3372117355735208, 0.304...</td>\n",
       "      <td>[0.48879999999999996, 0.6388, 0.71580000000000...</td>\n",
       "      <td>0.79935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Euclidean_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>86.845003</td>\n",
       "      <td>[0.12086158593266942, 0.09138613343652555, 0.0...</td>\n",
       "      <td>[0.9006000000000001, 0.9288000000000001, 0.937...</td>\n",
       "      <td>0.94818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrossEntropy_Sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>98.448528</td>\n",
       "      <td>[2.014637289436396, 1.7918110122740378, 1.6014...</td>\n",
       "      <td>[0.5055999999999998, 0.6851999999999999, 0.746...</td>\n",
       "      <td>0.82643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CrossEntropy_ReLU</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>102.327386</td>\n",
       "      <td>[0.22322786553901147, 0.16572498699567628, 0.1...</td>\n",
       "      <td>[0.938, 0.9566000000000001, 0.9628, 0.9678, 0....</td>\n",
       "      <td>0.97311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mode  batch_size  learning_rate_SGD  momentum  \\\n",
       "12     Euclidean_Sigmoid         100              0.001      0.55   \n",
       "13        Euclidean_ReLU         100              0.001      0.99   \n",
       "14  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "15     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "16     Euclidean_Sigmoid         100              0.001      0.55   \n",
       "17        Euclidean_ReLU         100              0.001      0.99   \n",
       "18  CrossEntropy_Sigmoid         100              0.001      0.55   \n",
       "19     CrossEntropy_ReLU         100              0.001      0.99   \n",
       "\n",
       "    weight_decay        time  \\\n",
       "12       0.00010   81.395794   \n",
       "13       0.00010   88.493162   \n",
       "14       0.00010   88.822916   \n",
       "15       0.00010   89.203245   \n",
       "16       0.00001   89.748165   \n",
       "17       0.00001   86.845003   \n",
       "18       0.00001   98.448528   \n",
       "19       0.00001  102.327386   \n",
       "\n",
       "                                        loss_validate  \\\n",
       "12  [0.40203178768559084, 0.33277713238912043, 0.2...   \n",
       "13  [0.12303438996578331, 0.08860506861796655, 0.0...   \n",
       "14  [2.0336907797261388, 1.798589231055107, 1.6011...   \n",
       "15  [0.23040020357007115, 0.17043784625938993, 0.1...   \n",
       "16  [0.4055092736497922, 0.3372117355735208, 0.304...   \n",
       "17  [0.12086158593266942, 0.09138613343652555, 0.0...   \n",
       "18  [2.014637289436396, 1.7918110122740378, 1.6014...   \n",
       "19  [0.22322786553901147, 0.16572498699567628, 0.1...   \n",
       "\n",
       "                                         acc_validate  acc_validate_float  \n",
       "12  [0.48800000000000004, 0.6553999999999999, 0.73...             0.81051  \n",
       "13  [0.9087999999999999, 0.9373999999999999, 0.944...             0.95250  \n",
       "14  [0.4708, 0.6512, 0.7268000000000001, 0.7667999...             0.82375  \n",
       "15  [0.9358, 0.955, 0.9623999999999999, 0.9672, 0....             0.97219  \n",
       "16  [0.48879999999999996, 0.6388, 0.71580000000000...             0.79935  \n",
       "17  [0.9006000000000001, 0.9288000000000001, 0.937...             0.94818  \n",
       "18  [0.5055999999999998, 0.6851999999999999, 0.746...             0.82643  \n",
       "19  [0.938, 0.9566000000000001, 0.9628, 0.9678, 0....             0.97311  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result.query(\"weight_decay <= 0.00010\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cf1e4799745e9ccc5e4a5d8e027719ef2e43f9255145d7e4068adaea12ce15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
